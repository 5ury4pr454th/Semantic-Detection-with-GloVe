{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GloVe Project.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/5ury4pr454th/Semantic-Detection-with-GloVe/blob/main/Corpus_Extraction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G18bpacAEw1z"
      },
      "source": [
        "# Detecting semantically similar words using Stanford's GloVe\n",
        "\n",
        "**NOTE!**: This Notebook is only for Corpus Extraction. Run 'run.sh' to get corresponding word vectors for the corpus."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import warnings\n",
        "\n",
        "import wikipedia\n",
        "from bs4 import BeautifulSoup\n",
        "from wikipedia import DisambiguationError, PageError\n",
        "\n",
        "import regex as re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "id": "YN0U7hsezMrX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "tkpv0cQPg1_2",
        "outputId": "9c94fdd9-7eb1-4608-c2cc-c0344e55d09c"
      },
      "source": [
        "# after execution, restart runtime \n",
        "!pip install --upgrade wikipedia"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wikipedia\n",
            "  Downloading wikipedia-1.4.0.tar.gz (27 kB)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from wikipedia) (4.6.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wikipedia) (2.23.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (1.24.3)\n",
            "Building wheels for collected packages: wikipedia\n",
            "  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11696 sha256=c7cda48a82567e0a034a99ff6dfa0e5f9cc7d32cdd63c95f1e884a1a3f226e56\n",
            "  Stored in directory: /root/.cache/pip/wheels/15/93/6d/5b2c68b8a64c7a7a04947b4ed6d89fb557dcc6bc27d1d7f3ba\n",
            "Successfully built wikipedia\n",
            "Installing collected packages: wikipedia\n",
            "Successfully installed wikipedia-1.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gtS9pqs6hLOM",
        "outputId": "cd477558-a62e-44bf-8b2b-39287a3223ef"
      },
      "source": [
        "# to get main Category\n",
        "wikipedia.search(\"Computer Science\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Computer science',\n",
              " 'Computer graphics (computer science)',\n",
              " 'Semantics (computer science)',\n",
              " 'Glossary of computer science',\n",
              " 'Computer science and engineering',\n",
              " 'Heuristic (computer science)',\n",
              " 'State (computer science)',\n",
              " 'Scope (computer science)',\n",
              " 'Record (computer science)',\n",
              " 'Integer (computer science)']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "85IpbARnhWX3"
      },
      "source": [
        "# using BeautifulSoup, extract and clean content to get list of subcategories\n",
        "topic_list = []\n",
        "\n",
        "r = requests.get(\"https://en.wikipedia.org/wiki/Outline_of_computer_science\")\n",
        "soup = BeautifulSoup(r.content)\n",
        "\n",
        "for link in soup.find_all('a'):\n",
        "  if link.parent.name == 'li':\n",
        "    if link.get('title')!=None:\n",
        "      topic_list.append(link.get('title'))\n",
        "\n",
        "for i in range(len(topic_list)):\n",
        "  if ':' in topic_list[i]:\n",
        "    topic_list = topic_list[:i]\n",
        "    break\n",
        "topic_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WEFls02JkKHN",
        "outputId": "d06f2768-c599-4671-9df9-3235c932c6fc"
      },
      "source": [
        "# from each subcategory, get main text\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "document_collection = dict()\n",
        "\n",
        "for _,i in enumerate(topic_list):\n",
        "  try:  \n",
        "    document = wikipedia.page(wikipedia.search(i)[0]).content\n",
        "    document_collection[i] = document\n",
        "  except (DisambiguationError, PageError):\n",
        "    pass\n",
        "  if _%10==0:\n",
        "    print(f\"{_} pages explored...\")\n",
        "\n",
        "print(\"Process Completed!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 pages explored...\n",
            "10 pages explored...\n",
            "20 pages explored...\n",
            "30 pages explored...\n",
            "40 pages explored...\n",
            "50 pages explored...\n",
            "60 pages explored...\n",
            "70 pages explored...\n",
            "80 pages explored...\n",
            "90 pages explored...\n",
            "100 pages explored...\n",
            "110 pages explored...\n",
            "120 pages explored...\n",
            "130 pages explored...\n",
            "Process Completed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S4eHcLctp96J",
        "outputId": "3a7cf013-63ab-4cce-ce53-c8d3cf6e4478"
      },
      "source": [
        "# checking for corpus length\n",
        "total_number = 0\n",
        "for i in document_collection.values():\n",
        "  total_number += len(i)\n",
        "total_number"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3104054"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wS7eWp9HLRQ-"
      },
      "source": [
        "# Stuff to do:\n",
        "# Remove newline tags\n",
        "# keep only alphabets\n",
        "# convert all to lowercase\n",
        "# remove all stopwords\n",
        "\n",
        "# check for sample text\n",
        "document_collection.values()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eUA_vIBAMp4I"
      },
      "source": [
        "# remove numbers, remove \\n, remove \\' , remove \",.-: (exclusive signs)\n",
        "\n",
        "for i in document_collection.keys():\n",
        "  re.sub('[+-:;\\']', '', document_collection.get(i))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "tVxwmqTYM_wi",
        "outputId": "0985f7df-ae56-49ab-f801-1a09c6d7cef6"
      },
      "source": [
        "# check if it works (don't remove '.' or '-', important for showing discontinuity and including words connected by - as a single word)\n",
        "my_string = \"the q.123ui\\'ck br-own fox\\n\\n jumped o+ver th-e l:az;y dog\"\n",
        "new_string = re.sub('\\n','', my_string)\n",
        "re.sub('[^A-Za-z \\.\\-]','', new_string)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'the q.uick br-own fox jumped over th-e lazy dog'"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YcT5SLH-QqP3"
      },
      "source": [
        "# creating a copy and removing symbols\n",
        "# document_collections is the copy of original, while document_collection is the edited one\n",
        "\n",
        "document_collections = document_collection.copy()\n",
        "\n",
        "document_collection = dict()\n",
        "\n",
        "for i in document_collections.keys():\n",
        "  raw_doc_string = re.sub('\\n', '', document_collections.get(i))\n",
        "  document_collection[i] = re.sub('[^A-Za-z \\.\\-]',' ', raw_doc_string)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FJWXA8HFRFYk",
        "outputId": "525aebfd-d26c-4b89-edea-78b92e2b0579"
      },
      "source": [
        "# removes stopwords\n",
        "\n",
        "nltk.download('stopwords')\n",
        "stopword_list = stopwords.words('english')\n",
        "\n",
        "for i in document_collection.keys():\n",
        "  for j in stopword_list:\n",
        "    document_collection[i] = re.sub(' '+j+' ', ' ', document_collection[i])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NGvvHkPmW5pt"
      },
      "source": [
        "# combining documents\n",
        "for i in document_collection.keys():\n",
        "  list_of_words = re.sub('\\.', ' ',document_collection[i].lower())\n",
        "  document_collection[i] = list_of_words\n",
        "\n",
        "document_collection"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "22XntymFW_g3"
      },
      "source": [
        "# if you want to see each of the entries.\n",
        "\n",
        "# from collections import Counter\n",
        "\n",
        "# counter_dict = dict()\n",
        "# for i in document_collection.keys():\n",
        "#   counter_dict[i] = Counter(document_collection[i])\n",
        "# counter_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p55pb37Na3_z"
      },
      "source": [
        "# # now, write a code to create windows (atleast three for each word) (not required as already implemented by glove)\n",
        "\n",
        "# def string_co(cont_words, window_size):\n",
        "#   co_list = []\n",
        "#   for i in range(len(cont_words)):\n",
        "#     child_list = []\n",
        "#     for j in range(-window_size,window_size+1):\n",
        "#       if (i+j) >= 0 and (i+j) < len(cont_words):\n",
        "#         child_list.append(cont_words[i+j])\n",
        "#       else:\n",
        "#         child_list.append('#PAD#')\n",
        "\n",
        "#     co_list.append(child_list)\n",
        "#   return co_list\n",
        "\n",
        "# my_string = \"the quick brown fox jumped over the lazy dog\"\n",
        "# string_co(list(my_string.split(' ')), 2)\n",
        "# # Not required\n",
        "\n",
        "# ################################### TUNABLE PARAMETER WINDOW SIZE!!! ##########################################\n",
        "\n",
        "# window_word_collection = dict()\n",
        "\n",
        "# for i in document_collection.keys():\n",
        "#   window_word_collection[i] = string_co(document_collection[i], 2)\n",
        "  \n",
        "# window_word_collection\n",
        "# window_word_collection"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kw5AjyWvgGAz"
      },
      "source": [
        "# create corpus and save as txt\n",
        "corpus_file = open(\"corpus.txt\", \"w+\")\n",
        "for i in document_collection.keys():\n",
        "  corpus_file.write(document_collection[i])\n",
        "  corpus_file.write(\"\\n\")\n",
        "corpus_file.close()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}