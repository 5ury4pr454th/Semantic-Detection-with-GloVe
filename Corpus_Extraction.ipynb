{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GloVe Project.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPY9AgfyjUCeDTCOxwN/H2X",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/5ury4pr454th/Semantic-Detection-with-GloVe/blob/main/Corpus_Extraction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G18bpacAEw1z"
      },
      "source": [
        "# Detecting semantically similar words using Stanford's GloVe\n",
        "\n",
        "**NOTE!**: This Notebook is only for Corpus Extraction. Refer to the other notebook for training and execution."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import warnings\n",
        "\n",
        "import wikipedia\n",
        "from bs4 import BeautifulSoup\n",
        "from wikipedia import DisambiguationError, PageError\n",
        "\n",
        "import regex as re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "id": "YN0U7hsezMrX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "tkpv0cQPg1_2",
        "outputId": "9c94fdd9-7eb1-4608-c2cc-c0344e55d09c"
      },
      "source": [
        "# after execution, restart runtime \n",
        "!pip install --upgrade wikipedia"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wikipedia\n",
            "  Downloading wikipedia-1.4.0.tar.gz (27 kB)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from wikipedia) (4.6.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wikipedia) (2.23.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (1.24.3)\n",
            "Building wheels for collected packages: wikipedia\n",
            "  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11696 sha256=c7cda48a82567e0a034a99ff6dfa0e5f9cc7d32cdd63c95f1e884a1a3f226e56\n",
            "  Stored in directory: /root/.cache/pip/wheels/15/93/6d/5b2c68b8a64c7a7a04947b4ed6d89fb557dcc6bc27d1d7f3ba\n",
            "Successfully built wikipedia\n",
            "Installing collected packages: wikipedia\n",
            "Successfully installed wikipedia-1.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gtS9pqs6hLOM",
        "outputId": "cd477558-a62e-44bf-8b2b-39287a3223ef"
      },
      "source": [
        "# to get main Category\n",
        "wikipedia.search(\"Computer Science\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Computer science',\n",
              " 'Computer graphics (computer science)',\n",
              " 'Semantics (computer science)',\n",
              " 'Glossary of computer science',\n",
              " 'Computer science and engineering',\n",
              " 'Heuristic (computer science)',\n",
              " 'State (computer science)',\n",
              " 'Scope (computer science)',\n",
              " 'Record (computer science)',\n",
              " 'Integer (computer science)']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "85IpbARnhWX3"
      },
      "source": [
        "# using BeautifulSoup, extract and clean content to get list of subcategories\n",
        "topic_list = []\n",
        "\n",
        "r = requests.get(\"https://en.wikipedia.org/wiki/Outline_of_computer_science\")\n",
        "soup = BeautifulSoup(r.content)\n",
        "\n",
        "for link in soup.find_all('a'):\n",
        "  if link.parent.name == 'li':\n",
        "    if link.get('title')!=None:\n",
        "      topic_list.append(link.get('title'))\n",
        "\n",
        "for i in range(len(topic_list)):\n",
        "  if ':' in topic_list[i]:\n",
        "    topic_list = topic_list[:i]\n",
        "    break\n",
        "topic_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WEFls02JkKHN",
        "outputId": "d06f2768-c599-4671-9df9-3235c932c6fc"
      },
      "source": [
        "# from each subcategory, get main text\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "document_collection = dict()\n",
        "\n",
        "for _,i in enumerate(topic_list):\n",
        "  try:  \n",
        "    document = wikipedia.page(wikipedia.search(i)[0]).content\n",
        "    document_collection[i] = document\n",
        "  except (DisambiguationError, PageError):\n",
        "    pass\n",
        "  if _%10==0:\n",
        "    print(f\"{_} pages explored...\")\n",
        "\n",
        "print(\"Process Completed!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 pages explored...\n",
            "10 pages explored...\n",
            "20 pages explored...\n",
            "30 pages explored...\n",
            "40 pages explored...\n",
            "50 pages explored...\n",
            "60 pages explored...\n",
            "70 pages explored...\n",
            "80 pages explored...\n",
            "90 pages explored...\n",
            "100 pages explored...\n",
            "110 pages explored...\n",
            "120 pages explored...\n",
            "130 pages explored...\n",
            "Process Completed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S4eHcLctp96J",
        "outputId": "3a7cf013-63ab-4cce-ce53-c8d3cf6e4478"
      },
      "source": [
        "# checking for corpus length\n",
        "total_number = 0\n",
        "for i in document_collection.values():\n",
        "  total_number += len(i)\n",
        "total_number"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3104054"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wS7eWp9HLRQ-",
        "outputId": "a7e7fd7a-1bb5-4d91-d595-aaedd10d1013"
      },
      "source": [
        "# Stuff to do:\n",
        "# Remove newline tags\n",
        "# keep only alphabets\n",
        "# convert all to lowercase\n",
        "# remove all stopwords\n",
        "\n",
        "# check for sample text\n",
        "document_collection.values()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_values(['An academic discipline or academic field is a subdivision of knowledge that is taught and researched at the college or university level. Disciplines are defined (in part) and recognized by the academic journals in which research is published, and the learned societies and academic departments or faculties within colleges and universities to which their practitioners belong. Academic disciplines are conventionally divided into the humanities, including language, art and cultural studies, and the scientific disciplines, such as physics, chemistry, and biology; the social sciences are sometimes considered a third category.\\nIndividuals associated with academic disciplines are commonly referred to as experts or specialists. Others, who may have studied liberal arts or systems theory rather than concentrating in a specific academic discipline, are classified as generalists.\\nWhile academic disciplines in and of themselves are more or less focused practices, scholarly approaches such as multidisciplinarity/interdisciplinarity, transdisciplinarity, and cross-disciplinarity integrate aspects from multiple academic disciplines, therefore addressing any problems that may arise from narrow concentration within specialized fields of study. For example, professionals may encounter trouble communicating across academic disciplines because of differences in language, specified concepts or methodology.\\nSome researchers believe that academic disciplines may, in the future, be replaced by what is known as Mode 2 or \"post-academic science\", which involves the acquisition of cross-disciplinary knowledge through collaboration of specialists from various academic disciplines.\\n\\n\\n== Terminology ==\\nAlso known as a field of study, field of inquiry, research field and branch of knowledge. The different terms are used in different countries and fields.\\n\\n\\n== History of the concept ==\\nThe University of Paris in 1231 consisted of four faculties: Theology, Medicine, Canon Law and Arts. Educational institutions originally used the term \"discipline\" to catalog and archive the new and expanding body of information produced by the scholarly community. Disciplinary designations originated in German universities during the beginning of the nineteenth century.\\nMost academic disciplines have their roots in the mid-to-late-nineteenth century secularization of universities, when the traditional curricula were supplemented with non-classical languages and literatures, social sciences such as political science, economics, sociology and public administration, and natural science and technology disciplines such as physics, chemistry, biology, and engineering.\\nIn the early twentieth century, new academic disciplines such as education and psychology were added. In the 1970s and 1980s, there was an explosion of new academic disciplines focusing on specific themes, such as media studies, women\\'s studies, and Africana studies. Many academic disciplines designed as preparation for careers and professions, such as nursing, hospitality management, and corrections, also emerged in the universities. Finally, interdisciplinary scientific fields of study such as biochemistry and geophysics gained prominence as their contribution to knowledge became widely recognized. Some new disciplines, such as public administration, can be found in more than one disciplinary setting; some public administration programs are associated with business schools (thus emphasizing the public management aspect), while others are linked to the political science field (emphasizing the policy analysis aspect).\\nAs the twentieth century approached, these designations were gradually adopted by other countries and became the accepted conventional subjects. However, these designations differed between various countries. In the twentieth century, the natural science disciplines included: physics, chemistry, biology, geology, and astronomy. The social science disciplines included: economics, politics, sociology, and psychology.\\nPrior to the twentieth century, categories were broad and general, which was expected due to the lack of interest in science at the time. With rare exceptions, practitioners of science tended to be amateurs and were referred to as \"natural historians\" and \"natural philosophers\"—labels that date back to Aristotle—instead of \"scientists\". Natural history referred to what we now call life sciences and natural philosophy referred to the current physical sciences.\\nPrior to the twentieth century, few opportunities existed for science as an occupation outside the educational system. Higher education provided the institutional structure for scientific investigation, as well as economic support for research and teaching. Soon, the volume of scientific information rapidly increased and researchers realized the importance of concentrating on smaller, narrower fields of scientific activity. Because of this narrowing, scientific specializations emerged. As these specializations developed, modern scientific disciplines in universities also improved their sophistication. Eventually, academia\\'s identified disciplines became the foundations for scholars of specific specialized interests and expertise.\\n\\n\\n== Functions and criticism ==\\nAn influential critique of the concept of academic disciplines came from Michel Foucault in his 1975 book, Discipline and Punish. Foucault asserts that academic disciplines originate from the same social movements and mechanisms of control that established the modern prison and penal system in eighteenth-century France, and that this fact reveals essential aspects they continue to have in common: \"The disciplines characterize, classify, specialize; they distribute along a scale, around a norm, hierarchize individuals in relation to one another and, if necessary, disqualify and invalidate.\" (Foucault, 1975/1979, p. 223)\\n\\n\\n== Communities of academic disciplines ==\\nCommunities of academic disciplines can be found outside academia within corporations, government agencies, and independent organizations, where they take the form of associations of professionals with common interests and specific knowledge. Such communities include corporate think tanks, NASA, and IUPAC. Communities such as these exist to benefit the organizations affiliated with them by providing specialized new ideas, research, and findings.\\nNations at various developmental stages will find need for different academic disciplines during different times of growth. A newly developing nation will likely prioritize government, political matters and engineering over those of the humanities, arts and social sciences. On the other hand, a well-developed nation may be capable of investing more in the arts and social sciences. Communities of academic disciplines would contribute at varying levels of importance during different stages of development.\\n\\n\\n== Interactions ==\\nThese categories explain how the different academic disciplines interact with one another.\\n\\n\\n=== Multidisciplinary ===\\n\\nMultidisciplinary knowledge is associated with more than one existing academic discipline or profession.\\nA multidisciplinary community or project is made up of people from different academic disciplines and professions. These people are engaged in working together as equal stakeholders in addressing a common challenge. A multidisciplinary person is one with degrees from two or more academic disciplines. This one person can take the place of two or more people in a multidisciplinary community. Over time, multidisciplinary work does not typically lead to an increase or a decrease in the number of academic disciplines. One key question is how well the challenge can be decomposed into subparts, and then addressed via the distributed knowledge in the community. The lack of shared vocabulary between people and communication overhead can sometimes be an issue in these communities and projects. If challenges of a particular type need to be repeatedly addressed so that each one can be properly decomposed, a multidisciplinary community can be exceptionally efficient and effective.There are many examples of a particular idea appearing in different academic disciplines, all of which came about around the same time. One example of this scenario is the shift from the approach of focusing on sensory awareness of the whole, \"an attention to the \\'total field\\'\", a \"sense of the whole pattern, of form and function as a unity\", an \"integral idea of structure and configuration\". This has happened in art (in the form of cubism), physics, poetry, communication and educational theory. According to Marshall McLuhan, this paradigm shift was due to the passage from the era of mechanization, which brought sequentiality, to the era of the instant speed of electricity, which brought simultaneity.Multidisciplinary approaches also encourage people to help shape the innovation of the future. The political dimensions of forming new multidisciplinary partnerships to solve the so-called societal Grand Challenges were presented in the Innovation Union and in the European Framework Programme, the Horizon 2020 operational overlay. Innovation across academic disciplines is considered the pivotal foresight of the creation of new products, systems, and processes for the benefit of all societies\\' growth and wellbeing. Regional examples such as Biopeople and industry-academia initiatives in translational medicine such as SHARE.ku.dk in Denmark provides the evidence of the successful endeavour of multidisciplinary innovation and facilitation of the paradigm shift.\\n\\n\\n=== Transdisciplinary ===\\n\\nIn practice, transdisciplinary can be thought of as the union of all interdisciplinary efforts. While interdisciplinary teams may be creating new knowledge that lies between several existing disciplines, a transdisciplinary team is more holistic and seeks to relate all disciplines into a coherent whole.\\n\\n\\n=== Cross-disciplinary ===\\nCross-disciplinary knowledge is that which explains aspects of one discipline in terms of another. Common examples of cross-disciplinary approaches are studies of the physics of music or the politics of literature.\\n\\n\\n== Bibliometric studies of disciplines ==\\nBibliometrics can be used to map several issues in relation to disciplines, for example the flow of ideas within and among disciplines (Lindholm-Romantschuk, 1998) or the existence of specific national traditions within disciplines. Scholarly impact and influence of one discipline on another may be understood by analyzing the flow of citations.The Bibliometrics approach is described as straightforward because it is based on simple counting. The method is also objective but the quantitative method may not be compatible with a qualitative assessment and therefore manipulated. The number of citations is dependent on the number of persons working in the same domain instead of inherent quality or published result\\'s originality.\\n\\n\\n== See also ==\\nOutline of academic disciplines\\nList of academic fields\\n\\n\\n== References ==\\n\\n\\n== Further reading ==\\nAbbott, A. (1988). The System of Professions: An Essay on the Division of Expert Labor, University of Chicago Press. ISBN 978-0-226-00069-5\\nAugsburg, T. (2005), Becoming Interdisciplinary: An Introduction to Interdisciplinary Studies.\\nDogan, M. & Pahre, R. (1990). \"The fate of formal disciplines: from coherence to dispersion.\" In Creative Marginality: Innovation at the Intersections of Social Sciences. Boulder, CO: Westview. pp. 85–113.\\nDullemeijer, P. (1980). \"Dividing biology into disciplines: Chaos or multiformity?\" Journal Acta Biotheoretica, 29(2), 87–93.\\nFagin, R.; Halpern, J.Y.; Moses, Y.  & Vardi, M.Y. (1995). Reasoning about Knowledge, MIT Press. ISBN 0-262-56200-6\\nGibbons, M.; Limoges, C.; Nowotny, H.; Schwartzman, S.; Scott, P. & Trow, M. (1994). The New Production of Knowledge: The Dynamics of Science and Research in Contemporary Societies. London: Sage.\\nGolinski, J. (1998/2005). Making Natural Knowledge: Constructivis, and the History of Science. New York: Cambridge University Press. Chapter 2: \"Identity and discipline.\" Part II: The Disciplinary Mold. pp. 66–78.\\nHicks, D. (2004). \"The Four Literatures of Social Science\". IN: Handbook of Quantitative Science and Technology Research: The Use of Publication and Patent Statistics in Studies of S&T Systems. Ed. Henk Moed. Dordrecht: Kluwer Academic.\\nHyland, K. (2004). Disciplinary Discourses: Social Interactions in Academic Writing. New edition. University of Michigan Press/ESL.\\nKlein, J.T. (1990). Interdisciplinarity: History, Theory, and Practice. Detroit: Wayne State University Press.\\nKrishnan, Armin (January 2009), What are Academic Disciplines? Some observations on the Disciplinarity vs. Interdisciplinarity debate (PDF), NCRM Working Paper Series, Southampton: ESRC National Centre for Research Methods, retrieved September 10, 2017\\nLeydesdorff, L. & Rafols, I. (2008). A global map of science based on the ISI subject categories. Journal of the American Society for Information Science and Technology.\\nLindholm-Romantschuk, Y. (1998). Scholarly Book Reviewing in the Social Sciences and Humanities: The Flow of Ideas within and among Disciplines. Westport, Connecticut: Greenwood Press.\\nMartin, B. (1998). Information Liberation: Challenging the Corruptions of Information Power. London: Freedom Press\\nMorillo, F.; Bordons, M. & Gomez, I. (2001). \"An approach to interdisciplinarity bibliometric indicators.\" Scientometrics, 51(1), 203–22.\\nMorillo, F.; Bordons, M. & Gomez, I. (2003). \"Interdisciplinarity in science: A tentative typology of disciplines and research areas\". Journal of the American Society for Information Science and Technology, 54(13), 1237–49.\\nNewell, A. (1983). \"Reflections on the structure of an interdiscipline.\" In Machlup, F. & U. Mansfield (Eds.), The Study of Information: Interdisciplinary Messages. pp. 99–110. NY: John Wiley & Sons.\\nPierce, S.J. (1991). \"Subject areas, disciplines and the concept of authority\". Library and Information Science Research, 13, 21–35.\\nPorter, A.L.; Roessner, J.D.; Cohen, A.S. & Perreault, M. (2006). \"Interdisciplinary research: meaning, metrics and nurture.\" Research Evaluation, 15(3), 187–95.\\nPrior, P. (1998). Writing/Disciplinarity: A Sociohistoric Account of Literate Activity in the Academy. Lawrence Erlbaum. (Rhetoric, Knowledge and Society Series)\\nQin, J.; Lancaster, F.W. & Allen, B. (1997). \"Types and levels of collaboration in interdisciplinary research in the sciences.\" Journal of the American Society for Information Science, 48(10), 893–916.\\nRinia, E.J.; van Leeuwen, T.N.; Bruins, E.E.W.; van Vuren, H.G. & van Raan, A.F.J. (2002). \"Measuring knowledge transfer between fields of science.\" Scientometrics, 54(3), 347–62.\\nSanz-Menendez, L.; Bordons, M. & Zulueta, M. A. (2001). \"Interdisciplinarity as a multidimensional concept: its measure in three different research areas.\" Research Evaluation, 10(1), 47–58.\\nStichweh, R. (2001). \"Scientific Disciplines, History of\".  Smelser, N.J. & Baltes, P.B. (eds.). International Encyclopedia of the Social and Behavioral Sciences. Oxford: Elsevier Science. pp. 13727–31.\\nSzostak, R. (October 2000). Superdisciplinarity: A Simple Definition of Interdisciplinarity With Profound Implications. Association for Integrative Studies, Portland, Oregon. (Meeting presentation)\\nTengström, E. (1993). Biblioteks- och informationsvetenskapen – ett fler- eller tvärvetenskapligt område? Svensk Biblioteksforskning (1), 9–20.\\nTomov, D.T. & Mutafov, H.G. (1996). \"Comparative indicators of interdisciplinarity in modern science.\" Scientometrics, 37(2), 267–78.\\nvan Leeuwen, T.N. & Tijssen, R.J.W. (1993). \"Assessing multidisciplinary areas of science and technology – A synthetic bibliometric study of Dutch nuclear-energy research.\" Scientometrics, 26(1), 115–33.\\nvan Leeuwen, T.N. & Tijssen, R.J.W. (2000). \"Interdisciplinary dynamics of modern science: analysis of cross-disciplinary citation flows.\" Research Evaluation, 9(3), 183–87.\\nWeisgerber, D.W. (1993). \"Interdisciplinary searching – problems and suggested remedies – A Report from the ICSTI Group on Interdisciplinary Searching.\" Journal of Documentation, 49(3), 231–54.\\nWittrock, B. (2001). \"Disciplines, History of, in the Social Sciences.\"  International Encyclopedia of the Social & Behavioral Sciences, pp. 3721–28. Smeltser, N.J. & Baltes, P.B. (eds.). Amsterdam: Elsevier.\\n\\n\\n== External links ==\\nAssociation for Interdisciplinary Studies\\nRyan Shaw. 2020. \"Periodization in ISKO Encyclopedia of Knowledge Organization, eds. Birger Hjørland and Claudio Gnoli.\\nSandoz, R. (ed.), Interactive Historical Atlas of the Disciplines, University of Geneva', 'Applied science is the use of the scientific method and knowledge obtained via conclusions from the method to attain practical goals. It includes a broad range of disciplines such as engineering and medicine. Applied science is often contrasted with basic science, which is focused on advancing scientific theories and laws that explain and predict events in the natural world. \\nApplied science can also apply formal science, such as statistics and probability theory, as in epidemiology. Genetic epidemiology is an applied science applying both biological and statistical methods.\\n\\n\\n== Applied research ==\\nApplied research is the practical application of science. It accesses and uses accumulated theories, knowledge, methods, and techniques, for a specific, state-, business-, or client-driven purpose.  Applied research is contrasted with pure research (basic research) in discussion about research ideals, methodologies, programs, and projects. Applied research usually has specific commercial objectives related to products, procedures, or services. The comparison of pure research and applied research provides a basic framework and direction for businesses to follow.\\nApplied research deals with solving practical problems and generally employs empirical methodologies. Because applied research resides in the messy real world, strict research protocols may need to be relaxed. For example, it may be impossible to use a random sample. Thus, transparency in the methodology is crucial. Implications for interpretation of results brought about by relaxing an otherwise strict canon of methodology should also be considered.Since applied research has a provisional close-to-the-problem and close-to-the-data orientation, it may also use a more provisional conceptual framework such as working hypotheses or pillar questions. \\nThe OECD\\'s Frascati Manual describes applied research as one of the three forms of research, along with basic research & experimental development.Due to its practical focus, applied research information will be found in the literature associated with individual disciplines.\\n\\n\\n== Branches ==\\n\\nEngineering fields include thermodynamics, heat transfer, fluid mechanics, statics, dynamics, mechanics of materials, kinematics, electromagnetism, materials science, earth sciences, engineering physics.\\nMedical sciences, for instance medical microbiology and clinical virology, are applied sciences that apply biology toward medical knowledge and inventions, but not necessarily medical technology, whose development is more specifically biomedicine or biomedical engineering.\\n\\n\\n== In education ==\\nIn Canada, the Netherlands and other places the Bachelor of Applied Science (BASc) is sometimes equivalent to the Bachelor of Engineering, and is classified as a professional degree. This is based on the age of the school where applied science used to include boiler making, surveying and engineering. There are also Bachelor of Applied Science degrees in Child Studies. The BASc tends to focus more on the application of the engineering sciences.  In Australia and New Zealand, this degree is awarded in various fields of study and is considered a highly specialized professional degree.\\nIn the United Kingdom\\'s educational system, Applied Science refers to a suite of \"vocational\" science qualifications that run alongside \"traditional\" General Certificate of Secondary Education or A-Level Sciences. Applied Science courses generally contain more coursework (also known as portfolio or internally assessed work) compared to their traditional counterparts. These are an evolution of the GNVQ qualifications that were offered up to 2005.\\nThese courses regularly come under scrutiny and are due for review following the Wolf Report 2011;\\nhowever, their merits are argued elsewhere.In the United States, The College of William & Mary offers an undergraduate minor as well as Master of Science and Doctor of Philosophy degrees in \"applied science.\" Courses and research cover varied fields including neuroscience, optics, materials science and engineering, nondestructive testing, and nuclear magnetic resonance. University of Nebraska–Lincoln offers a Bachelor of Science in applied science, an online completion Bachelor of Science in applied science and a Master of Applied Science. Course work is centered on science, agriculture and natural resources with a wide range of options including ecology, food genetics, entrepreneurship, economics, policy, animal science and plant science. In New York City, the Bloomberg administration awarded the consortium of Cornell-Technion $100 million in City capital to construct the universities\\' proposed Applied Sciences campus on Roosevelt Island.\\n\\n\\n== See also ==\\n\\nBasic research\\nExact sciences\\nHard and soft science\\nInvention\\nSecondary research\\n\\n\\n== References ==\\n\\n\\n== External links ==\\n Media related to Applied sciences at Wikimedia Commons', 'Coding theory is the study of the properties of codes and their respective fitness for specific applications. Codes are used for data compression, cryptography, error detection and correction, data transmission and data storage. Codes are studied by various scientific disciplines—such as information theory, electrical engineering,  mathematics, linguistics, and computer science—for the purpose of designing efficient and reliable data transmission methods. This typically involves the removal of redundancy and the correction or detection of errors in the transmitted data.\\nThere are four types of coding:\\nData compression (or source coding)\\nError control (or channel coding)\\nCryptographic coding\\nLine codingData compression attempts to remove redundancy from the data from a source in order to transmit it more efficiently. For example, ZIP data compression makes data files smaller, for purposes such as to reduce Internet traffic. Data compression and error correction may be studied in combination.\\nError correction adds extra data bits to make the transmission of data more robust to disturbances present on the transmission channel. The ordinary user may not be aware of many applications using error correction. A typical music compact disc (CD) uses the Reed–Solomon code to correct for scratches and dust. In this application the transmission channel is the CD itself. Cell phones also use coding techniques to correct for the fading and noise of high frequency radio transmission. Data modems, telephone transmissions, and the NASA Deep Space Network all employ channel coding techniques to get the bits through, for example the turbo code and LDPC codes.\\n\\n\\n== History of coding theory ==\\nIn 1948, Claude Shannon published \"A Mathematical Theory of Communication\", an article in two parts in the July and October issues of the Bell System Technical Journal. This work focuses on the problem of how best to encode the information a sender wants to transmit. In this fundamental work he used tools in probability theory, developed by Norbert Wiener, which were in their nascent stages of being applied to communication theory at that time. Shannon developed information entropy as a measure for the uncertainty in a message while essentially inventing the field of information theory.\\nThe binary Golay code was developed in 1949. It is an error-correcting code capable of correcting up to three errors in each 24-bit word, and detecting a fourth.\\nRichard Hamming won the Turing Award in 1968 for his work at Bell Labs in numerical methods, automatic coding systems, and error-detecting and error-correcting codes. He invented the concepts known as Hamming codes, Hamming windows, Hamming numbers, and Hamming distance.\\nIn 1972, Nasir Ahmed proposed the discrete cosine transform (DCT), which he developed with T. Natarajan and K. R. Rao in 1973. The DCT is the most widely used lossy compression algorithm, the basis for multimedia formats such as JPEG, MPEG and MP3.\\n\\n\\n== Source coding ==\\n\\nThe aim of source coding is to take the source data and make it smaller.\\n\\n\\n=== Definition ===\\nData can be seen as a random variable \\n  \\n    \\n      \\n        X\\n        :\\n        Ω\\n        →\\n        \\n          \\n            X\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle X:\\\\Omega \\\\to {\\\\mathcal {X}}}\\n  , where \\n  \\n    \\n      \\n        x\\n        ∈\\n        \\n          \\n            X\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle x\\\\in {\\\\mathcal {X}}}\\n   appears with probability \\n  \\n    \\n      \\n        \\n          P\\n        \\n        [\\n        X\\n        =\\n        x\\n        ]\\n      \\n    \\n    {\\\\displaystyle \\\\mathbb {P} [X=x]}\\n  .\\nData are encoded by strings (words) over an alphabet \\n  \\n    \\n      \\n        Σ\\n      \\n    \\n    {\\\\displaystyle \\\\Sigma }\\n  .\\nA code is a function\\n\\n  \\n    \\n      \\n        C\\n        :\\n        \\n          \\n            X\\n          \\n        \\n        →\\n        \\n          Σ\\n          \\n            ∗\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle C:{\\\\mathcal {X}}\\\\to \\\\Sigma ^{*}}\\n   (or \\n  \\n    \\n      \\n        \\n          Σ\\n          \\n            +\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle \\\\Sigma ^{+}}\\n   if the empty string is not part of the alphabet).\\n  \\n    \\n      \\n        C\\n        (\\n        x\\n        )\\n      \\n    \\n    {\\\\displaystyle C(x)}\\n   is the code word associated with \\n  \\n    \\n      \\n        x\\n      \\n    \\n    {\\\\displaystyle x}\\n  .\\nLength of the code word is written as\\n\\n  \\n    \\n      \\n        l\\n        (\\n        C\\n        (\\n        x\\n        )\\n        )\\n        .\\n      \\n    \\n    {\\\\displaystyle l(C(x)).}\\n  Expected length of a code is\\n\\n  \\n    \\n      \\n        l\\n        (\\n        C\\n        )\\n        =\\n        \\n          ∑\\n          \\n            x\\n            ∈\\n            \\n              \\n                X\\n              \\n            \\n          \\n        \\n        l\\n        (\\n        C\\n        (\\n        x\\n        )\\n        )\\n        \\n          P\\n        \\n        [\\n        X\\n        =\\n        x\\n        ]\\n        .\\n      \\n    \\n    {\\\\displaystyle l(C)=\\\\sum _{x\\\\in {\\\\mathcal {X}}}l(C(x))\\\\mathbb {P} [X=x].}\\n  The concatenation of code words \\n  \\n    \\n      \\n        C\\n        (\\n        \\n          x\\n          \\n            1\\n          \\n        \\n        ,\\n        …\\n        ,\\n        \\n          x\\n          \\n            k\\n          \\n        \\n        )\\n        =\\n        C\\n        (\\n        \\n          x\\n          \\n            1\\n          \\n        \\n        )\\n        C\\n        (\\n        \\n          x\\n          \\n            2\\n          \\n        \\n        )\\n        ⋯\\n        C\\n        (\\n        \\n          x\\n          \\n            k\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle C(x_{1},\\\\ldots ,x_{k})=C(x_{1})C(x_{2})\\\\cdots C(x_{k})}\\n  .\\nThe code word of the empty string is the empty string itself:\\n\\n  \\n    \\n      \\n        C\\n        (\\n        ϵ\\n        )\\n        =\\n        ϵ\\n      \\n    \\n    {\\\\displaystyle C(\\\\epsilon )=\\\\epsilon }\\n  \\n\\n\\n=== Properties ===\\n\\n  \\n    \\n      \\n        C\\n        :\\n        \\n          \\n            X\\n          \\n        \\n        →\\n        \\n          Σ\\n          \\n            ∗\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle C:{\\\\mathcal {X}}\\\\to \\\\Sigma ^{*}}\\n   is non-singular if injective.\\n\\n  \\n    \\n      \\n        C\\n        :\\n        \\n          \\n            \\n              X\\n            \\n          \\n          \\n            ∗\\n          \\n        \\n        →\\n        \\n          Σ\\n          \\n            ∗\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle C:{\\\\mathcal {X}}^{*}\\\\to \\\\Sigma ^{*}}\\n   is uniquely decodable if injective.\\n\\n  \\n    \\n      \\n        C\\n        :\\n        \\n          \\n            X\\n          \\n        \\n        →\\n        \\n          Σ\\n          \\n            ∗\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle C:{\\\\mathcal {X}}\\\\to \\\\Sigma ^{*}}\\n   is instantaneous if \\n  \\n    \\n      \\n        C\\n        (\\n        \\n          x\\n          \\n            1\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle C(x_{1})}\\n   is not a prefix of \\n  \\n    \\n      \\n        C\\n        (\\n        \\n          x\\n          \\n            2\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle C(x_{2})}\\n   (and vice versa).\\n\\n\\n=== Principle ===\\nEntropy of a source is the measure of information. Basically, source codes try to reduce the redundancy present in the source, and represent the source with fewer bits that carry more information.\\nData compression which explicitly tries to minimize the average length of messages according to a particular assumed probability model is called entropy encoding.\\nVarious techniques used by source coding schemes try to achieve the limit of entropy of the source. C(x) ≥ H(x), where H(x) is entropy of source (bitrate), and  C(x) is the bitrate after compression. In particular, no source coding scheme can be better than the entropy of the source.\\n\\n\\n=== Example ===\\nFacsimile transmission uses a simple run length code. Source coding removes all data superfluous to the need of the transmitter, decreasing the bandwidth required for transmission.\\n\\n\\n== Channel coding ==\\n\\nThe purpose of channel coding theory is to find codes which transmit quickly, contain many valid code words and can correct or at least detect many errors. While not mutually exclusive, performance in these areas is a trade off. So, different codes are optimal for different applications. The needed properties of this code mainly depend on the probability of errors happening during transmission. In a typical CD, the impairment is mainly dust or scratches.\\nCDs use cross-interleaved Reed–Solomon coding to spread the data out over the disk.Although not a very good code, a simple repeat code can serve as an understandable example. Suppose we take a block of data bits (representing sound) and send it three times. At the receiver we will examine the three repetitions bit by bit and take a majority vote. The twist on this is that we don\\'t merely send the bits in order. We interleave them. The block of data bits is first divided into 4 smaller blocks. Then we cycle through the block and send one bit from the first, then the second, etc. This is done three times to spread the data out over the surface of the disk. In the context of the simple repeat code, this may not appear effective. However, there are more powerful codes known which are very effective at correcting the \"burst\" error of a scratch or a dust spot when this interleaving technique is used.\\nOther codes are more appropriate for different applications. Deep space communications are limited by the thermal noise of the receiver which is more of a continuous nature than a bursty nature. Likewise, narrowband modems are limited by the noise, present in the telephone network and also modeled better as a continuous disturbance. Cell phones are subject to rapid fading. The high frequencies used can cause rapid fading of the signal even if the receiver is moved a few inches. Again there are a class of channel codes that are designed to combat fading.\\n\\n\\n=== Linear codes ===\\n\\nThe term algebraic coding theory denotes the sub-field of coding theory where the properties of codes are expressed in algebraic terms and then further researched.Algebraic coding theory is basically divided into two major types of codes:\\nLinear block codes\\nConvolutional codesIt analyzes the following three properties of a code – mainly:\\nCode word length\\nTotal number of valid code words\\nThe minimum distance between two valid code words, using mainly the Hamming distance, sometimes also other distances like the Lee distance\\n\\n\\n==== Linear block codes ====\\n\\nLinear block codes have the property of linearity, i.e. the sum of any two codewords is also a code word, and they are applied to the source bits in blocks, hence the name linear block codes. There are block codes that are not linear, but it is difficult to prove that a code is a good one without this property.Linear block codes are summarized by their symbol alphabets (e.g., binary or ternary) and parameters (n,m,dmin) where\\n\\nn is the length of the codeword, in symbols,\\nm is the number of source symbols that will be used for encoding at once,\\ndmin is the minimum hamming distance for the code.There are many types of linear block codes, such as\\n\\nCyclic codes (e.g., Hamming codes)\\nRepetition codes\\nParity codes\\nPolynomial codes (e.g., BCH codes)\\nReed–Solomon codes\\nAlgebraic geometric codes\\nReed–Muller codes\\nPerfect codesBlock codes are tied to the sphere packing problem, which has received some attention over the years. In two dimensions, it is easy to visualize. Take a bunch of pennies flat on the table and push them together. The result is a hexagon pattern like a bee\\'s nest. But block codes rely on more dimensions which cannot easily be visualized. The powerful (24,12) Golay code used in deep space communications uses 24 dimensions. If used as a binary code (which it usually is) the dimensions refer to the length of the codeword as defined above.\\nThe theory of coding uses the N-dimensional sphere model. For example, how many pennies can be packed into a circle on a tabletop, or in 3 dimensions, how many marbles can be packed into a globe. Other considerations enter the choice of a code. For example, hexagon packing into the constraint of a rectangular box will leave empty space at the corners. As the dimensions get larger, the percentage of empty space grows smaller. But at certain dimensions, the packing uses all the space and these codes are the so-called \"perfect\" codes. The only nontrivial and useful perfect codes are the distance-3 Hamming codes with parameters satisfying (2r – 1, 2r – 1 – r, 3), and the [23,12,7] binary and [11,6,5] ternary Golay codes.Another code property is the number of neighbors that a single codeword may have.\\nAgain, consider pennies as an example. First we pack the pennies in a rectangular grid. Each penny will have 4 near neighbors (and 4 at the corners which are farther away). In a hexagon, each penny will have 6 near neighbors. When we increase the dimensions, the number of near neighbors increases very rapidly.  The result is the number of ways for noise to make the receiver choose a neighbor (hence an error) grows as well. This is a fundamental limitation of block codes, and indeed all codes. It may be harder to cause an error to a single neighbor, but the number of neighbors can be large enough so the total error probability actually suffers.Properties of linear block codes are used in many applications.  For example, the syndrome-coset uniqueness property of linear block codes is used in trellis shaping, one of the best-known shaping codes.\\n\\n\\n==== Convolutional codes ====\\n\\nThe idea behind a convolutional code is to make every codeword symbol be the weighted sum of the various input message symbols. This is like convolution used in LTI systems to find the output of a system, when you know the input and impulse response.\\nSo we generally find the output of the system convolutional encoder, which is the convolution of the input bit, against the states of the convolution encoder, registers.\\nFundamentally, convolutional codes do not offer more protection against noise than an equivalent block code. In many cases, they generally offer greater simplicity of implementation over a block code of equal power. The encoder is usually a simple circuit which has state memory and some feedback logic, normally XOR gates. The decoder can be implemented in software or firmware.\\nThe Viterbi algorithm is the optimum algorithm used to decode convolutional codes. There are simplifications to reduce the computational load. They rely on searching only the most likely paths. Although not optimum, they have generally been found to give good results in low noise environments.\\nConvolutional codes are used in voiceband modems (V.32, V.17, V.34) and in GSM mobile phones, as well as satellite and military communication devices.\\n\\n\\n== Cryptographic coding ==\\n\\nCryptography or cryptographic coding is the practice and study of techniques for secure communication in the presence of third parties (called adversaries). More generally, it is about constructing and analyzing protocols that block adversaries; various aspects in information security such as data confidentiality, data integrity, authentication, and non-repudiation are central to modern cryptography. Modern cryptography exists at the intersection of the disciplines of mathematics, computer science, and electrical engineering. Applications of cryptography include ATM cards, computer passwords, and electronic commerce.\\nCryptography prior to the modern age was effectively synonymous with encryption, the conversion of information from a readable state to apparent nonsense. The originator of an encrypted message shared the decoding technique needed to recover the original information only with intended recipients, thereby precluding unwanted persons from doing the same. Since World War I and the advent of the computer, the methods used to carry out cryptology have become increasingly complex and its application more widespread.\\nModern cryptography is heavily based on mathematical theory and computer science practice; cryptographic algorithms are designed around computational hardness assumptions, making such algorithms hard to break in practice by any adversary. It is theoretically possible to break such a system, but it is infeasible to do so by any known practical means. These schemes are therefore termed computationally secure; theoretical advances, e.g., improvements in integer factorization algorithms, and faster computing technology require these solutions to be continually adapted. There exist information-theoretically secure schemes that provably cannot be broken even with unlimited computing power—an example is the one-time pad—but these schemes are more difficult to implement than the best theoretically breakable but computationally secure mechanisms.\\n\\n\\n== Line coding ==\\n\\nA line code (also called digital baseband modulation or digital baseband transmission method) is a code chosen for use within a communications system for baseband transmission purposes. Line coding is often used for digital data transport.\\nLine coding consists of representing the digital signal to be transported by an amplitude- and time-discrete signal that is optimally tuned for the specific properties of the physical channel (and of the receiving equipment). The waveform pattern of voltage or current used to represent the 1s and 0s of a digital data on a transmission link is called line encoding. The common types of line encoding are unipolar, polar, bipolar, and Manchester encoding.\\n\\n\\n== Other applications of coding theory ==\\nAnother concern of coding theory is designing codes that help synchronization. A code may be designed so that a phase shift can be easily detected and corrected and that multiple signals can be sent on the same channel.Another application of codes, used in some mobile phone systems, is code-division multiple access (CDMA). Each phone is assigned a code sequence that is approximately uncorrelated with the codes of other phones. When transmitting, the code word is used to modulate the data bits representing the voice message. At the receiver, a demodulation process is performed to recover the data. The properties of this class of codes allow many users (with different codes) to use the same radio channel at the same time. To the receiver, the signals of other users will appear to the demodulator only as a low-level noise.Another general class of codes are the automatic repeat-request (ARQ) codes. In these codes the sender adds redundancy to each message for error checking, usually by adding check bits. If the check bits are not consistent with the rest of the message when it arrives, the receiver will ask the sender to retransmit the message. All but the simplest wide area network protocols use ARQ. Common protocols include SDLC (IBM), TCP (Internet), X.25 (International) and many others. There is an extensive field of research on this topic because of the problem of matching a rejected packet against a new packet. Is it a new one or is it a retransmission? Typically numbering schemes are used, as in TCP.\"RFC793\". RFCs. Internet Engineering Task Force (IETF). September 1981.\\n\\n\\n=== Group testing ===\\nGroup testing uses codes in a different way. Consider a large group of items in which a very few are different in a particular way (e.g., defective products or infected test subjects). The idea of group testing is to determine which items are \"different\" by using as few tests as possible. The origin of the problem has its roots in the Second World War when the United States Army Air Forces needed to test its soldiers for syphilis.\\n\\n\\n=== Analog coding ===\\nInformation is encoded analogously in the neural networks of brains, in analog signal processing, and analog electronics. Aspects of analog coding include analog error correction,\\nanalog data compression and analog encryption.\\n\\n\\n== Neural coding ==\\nNeural coding is a neuroscience-related field concerned with how sensory and other information is represented in the brain by networks of neurons. The main goal of studying neural coding is to characterize the relationship between the stimulus and the individual or ensemble neuronal responses and the relationship among electrical activity of the neurons in the ensemble. It is thought that neurons can encode both digital and analog information, and that neurons follow the principles of information theory and compress information, and detect and correct\\nerrors in the signals that are sent throughout the brain and wider nervous system.\\n\\n\\n== See also ==\\n\\nCoding gain\\nCovering code\\nError correction code\\nFolded Reed–Solomon code\\nGroup testing\\nHamming distance, Hamming weight\\nLee distance\\nList of algebraic coding theory topics\\nSpatial coding and MIMO in multiple antenna research\\nSpatial diversity coding is spatial coding that transmits replicas of the information signal along different spatial paths, so as to increase the reliability of the data transmission.\\nSpatial interference cancellation coding\\nSpatial multiplex coding\\nTimeline of information theory, data compression, and error correcting codes\\n\\n\\n== Notes ==\\n\\n\\n== References ==\\nElwyn R. Berlekamp (2014), Algebraic Coding Theory, World Scientific Publishing (revised edition), ISBN 978-9-81463-589-9.\\nMacKay, David J. C.. Information Theory, Inference, and Learning Algorithms Cambridge: Cambridge University Press, 2003. ISBN 0-521-64298-1\\nVera Pless (1982), Introduction to the Theory of Error-Correcting Codes, John Wiley & Sons, Inc., ISBN 0-471-08684-3.\\nRandy Yates, A Coding Theory Tutorial.', 'Cybernetics is a transdisciplinary and \"antidisciplinary\" approach concerned with regulatory and purposive systems—their structures, constraints, and possibilities. The core concept of the discipline is circular causality or feedback—that is, where the outcomes of actions are taken as inputs for further action. Cybernetics is concerned with such processes however they are embodied, including in ecological, technological, biological, cognitive, and social systems, and in the context of practical activities such as designing, learning, managing, conversation, and the practice of cybernetics itself.\\n\\n\\n== Overview ==\\nCybernetics has its origins in the intersection of numerous fields during the 1940s, including anthropology, mathematics, neuroscience, psychology, and engineering. Initial developments were consolidated through meetings such as the Macy Conferences and the Ratio Club. At its most prominent during the 1950s and 1960s, cybernetics is a precursor to fields such as computing, artificial intelligence, cognitive science, complexity science, and robotics amongst others. It is closely related to systems science, which was developed in parallel. Early focuses included purposeful behaviour, neural networks, heterarchy, information theory, and self-organising systems. As cybernetics developed, it became broader in scope to include work in domains such as design, family therapy, management and organisation, pedagogy,  sociology, and the creative arts. At the same time, questions arising from circular causality have been explored in relation to the philosophy of science, ethics, and constructivist approaches, while cybernetics has also been associated with counter-cultural movements. Contemporary cybernetics thus varies widely in scope and focus, with cyberneticians variously adopting and combining technical, scientific, philosophical, creative, and critical approaches.\\n\\n\\n=== Definitions ===\\nCybernetics has been defined in a variety of ways, reflecting \"the richness of its conceptual base\". One of the most well known definitions is that of Norbert Wiener who characterised cybernetics as concerned with \"control and communication in the animal and the machine\". Another early definition is that of the Macy cybernetics conferences, where cybernetics was understood as the study of \"circular causal and feedback mechanisms in biological and social systems\". Margaret Mead emphasised the role of cybernetics as \"a form of cross-disciplinary thought which made it possible for members of many disciplines to communicate with each other easily in a language which all could understand\".Other definitions include: “the art of governing or the science of government” (André-Marie Ampère); \"the art of steersmanship\" (Ross Ashby); \"the study of systems of any nature which are capable of receiving, storing, and processing information so as to use it for control\" (Andrey Kolmogorov); \"a branch of mathematics dealing with problems of control, recursiveness, and information, focuses on forms and the patterns that connect\" (Gregory Bateson); \"the art of securing efficient operation\" (Louis Couffignal); \"the art of effective organization.\" (Stafford Beer); \"the science or the art of manipulating defensible metaphors; showing how they may be constructed and what can be inferred as a result of their existence\" (Gordon Pask); \"the art of creating equilibrium in a world of constraints and possibilities\" (Ernst von Glasersfeld); \"the science and art of understanding\" (Humberto Maturana); \"the ability to cure all temporary truth of eternal triteness\" (Herbert Brun); \"a way of thinking about ways of thinking (of which it is one)\" (Larry Richards);\\n\\n\\n=== Etymology ===\\n\\nThe word cybernetics comes from Greek κυβερνητική (kybernētikḗ), meaning \"governance\", i.e., all that are pertinent to κυβερνάω (kybernáō), the latter meaning \"to steer, navigate or govern\", hence κυβέρνησις (kybérnēsis), meaning \"government\", is the government while κυβερνήτης (kybernḗtēs) is the governor, pilot, or \"helmsperson\" of the \"ship\".\\nFrench physicist and mathematician André-Marie Ampère first coined the word \"cybernetique\" in his 1834 essay Essai sur la philosophie des sciences to describe the science of civil government. The term was used by Norbert Wiener, in his book Cybernetics, to define the study of control and communication in the animal and the machine. In the book, he states: \"Although the term cybernetics does not date further back than the summer of 1947, we shall find it convenient to use in referring to earlier epochs of the development of the field.\"\\n\\n\\n=== Key concepts ===\\nKey concepts in cybernetics include:\\n\\nThe Black Box\\nDistinction\\nFeedback and circular causality\\nHomeostasis\\nSelf-organising systems\\nVariety and Requisite Variety\\n\\n\\n== History ==\\n\\n\\n=== Pre 20th century ===\\nThe word cybernetics was first used in the context of \"the study of self-governance\" by Plato in Republic and in Alcibiades to signify the governance of people. The word \\'cybernétique\\' was also used in 1834 by the physicist André-Marie Ampère (1775–1836) to denote the sciences of government in his classification system of human knowledge.\\n\\nThe first artificial automatic regulatory system was a water clock, invented by the mechanician Ktesibios; based on a tank which poured water into a reservoir before using it to run the mechanism, it used a cone-shaped float to monitor the level of the water in its reservoir and adjust the rate of flow of the water accordingly to maintain a constant level of water in the reservoir. This was the first artificial truly automatic self-regulatory device that required no outside intervention between the feedback and the controls of the mechanism. Although they considered this part of engineering (the use of the term cybernetics is much posterior), Ktesibios and others such as Heron and Su Song are considered to be some of the first to study cybernetic principles.The study of teleological mechanisms (from the Greek τέλος or télos for end, goal, or purpose) in machines with corrective feedback dates from as far back as the late 18th century when James Watt\\'s steam engine was equipped with a governor (1775–1800), a centrifugal feedback valve for controlling the speed of the engine. Alfred Russel Wallace identified this as the principle of evolution in his famous 1858 paper. In 1868 James Clerk Maxwell published a theoretical article on governors, one of the first to discuss and refine the principles of self-regulating devices. Jakob von Uexküll applied the feedback mechanism via his model of functional cycle (Funktionskreis) in order to explain animal behaviour and the origins of meaning in general.\\n\\n\\n=== Early 20th century ===\\nContemporary cybernetics began as an interdisciplinary study connecting the fields of control systems, electrical network theory, mechanical engineering, logic modeling, evolutionary biology and neuroscience in the 1940s; the ideas are also related to the biological work of Ludwig von Bertalanffy in General Systems Theory. Electronic control systems originated with the 1927 work of Bell Telephone Laboratories engineer Harold S. Black on using negative feedback to control amplifiers.\\nEarly applications of negative feedback in electronic circuits included the feedback amplifier and the control of gun mounts and radar antenna during World War II.  The founder of System Dynamics, Jay Forrester, worked with Gordon S. Brown during WWII as a graduate student at the Servomechanisms Laboratory at MIT to develop electronic control systems for the U.S. Navy. Forrester later applied these ideas to social organizations, such as corporations and cities, and he became an original organizer of the MIT School of Industrial Management at the MIT Sloan School of Management.\\nW. Edwards Deming, the Total Quality Management guru for whom Japan named its top post-WWII industrial prize, was an intern at Bell Telephone Labs in 1927 and may have been influenced by network theory; Deming made \"Understanding Systems\" one of the four pillars of what he described as \"Profound Knowledge\" in his book The New Economics.\\nNumerous papers spearheaded the coalescing of the field. In 1935 Russian physiologist P. K. Anokhin published a book in which the concept of feedback (\"back afferentation\") was studied. The study and mathematical modelling of regulatory processes became a continuing research effort and two key articles were published in 1943: \"Behavior, Purpose and Teleology\" by Arturo Rosenblueth, Norbert Wiener, and Julian Bigelow –based on the research on living organisms that Arturo Rosenblueth did in Mexico–; and the paper \"A Logical Calculus of the Ideas Immanent in Nervous Activity\" by Warren McCulloch and Walter Pitts.\\nIn 1936, Ștefan Odobleja published \"Phonoscopy and the clinical semiotics\". In 1937, he participated in the IX International Congress of Military Medicine with \"Demonstration de phonoscopie\"; in the paper he disseminated a prospectus announcing his future work, \"Psychologie consonantiste\", the most important of his writings, where he lays the theoretical foundations of generalized cybernetics. The book, published in Paris by Librairie Maloine (vol. I in 1938 and vol. II in 1939), contains almost 900 pages and includes 300 figures in the text. The author wrote at the time that \"this book is ... a table of contents, an index or a dictionary of psychology, [for] a ... great Treatise of Psychology that should contain 20–30 volumes\". Due to the beginning of World War II, the publication went unnoticed (the first Romanian edition of this work did not appear until 1982).\\n\\nCybernetics as a discipline was firmly established by Norbert Wiener, McCulloch, Arturo Rosenblueth and others, such as W. Ross Ashby, mathematician Alan Turing, and W. Grey Walter.  In the spring of 1947, Wiener was invited to a congress on harmonic analysis, held in Nancy (France was an important geographical locus of early cybernetics together with the US and UK); the event was organized by the Bourbaki and mathematician Szolem Mandelbrojt.\\nDuring this stay in France, Wiener received the offer to write a manuscript on the unifying character of this part of applied mathematics, which is found in the study of Brownian motion and in telecommunication engineering. The following summer, back in the United States, Wiener decided to introduce the neologism cybernetics, coined to denote the study of \"teleological mechanisms\", into his scientific theory: it was popularized through his book Cybernetics: Or Control and Communication in the Animal and the Machine. In the UK this became the focus for the Ratio Club.\\n\\nIn the early 1940s John von Neumann contributed a unique and unusual addition to the world of cybernetics: von Neumann cellular automata, and their logical follow up, the von Neumann Universal Constructor.  The result of these deceptively simple thought-experiments was the concept of self replication, which cybernetics adopted as a core concept.  The concept that the same properties of genetic reproduction applied to social memes, living cells, and even computer viruses is further proof of the somewhat surprising universality of cybernetic study.\\nIn 1950, Wiener popularized the social implications of cybernetics, drawing analogies between automatic systems (such as a regulated steam engine) and human institutions in his best-selling The Human Use of Human Beings: Cybernetics and Society (Houghton-Mifflin).\\n\\n\\n=== Split from artificial intelligence ===\\nArtificial intelligence (AI) was founded as a distinct discipline at the Dartmouth workshop in 1956. After some uneasy coexistence, AI gained funding and prominence. Consequently, cybernetic sciences such as the study of artificial neural networks were downplayed; the discipline shifted into the world of social sciences and therapy.Prominent cyberneticians during this period include Gregory Bateson and Aksel Berg.\\n\\n\\n=== Late 20th century ===\\nCybernetics in the Soviet Union was initially considered a \"pseudoscience\" and \"ideological weapon\" of \"imperialist reactionaries\" (Soviet Philosophical Dictionary, 1954) and later criticised as a narrow form of cybernetics. In the mid to late 1950s Viktor Glushkov and others salvaged the reputation of the field. Soviet cybernetics incorporated much of what became known as computer science in the West.Published in 1954, Qian Xuesen published work \"Engineering Cybernetics\" was the basis of science in segregating the engineering concepts of Cybernetics from the theoretical understanding of Cybernetics as described so far historically.\\nWhile not the only instance of a research organization focused on cybernetics, the Biological Computer Lab at the University of Illinois at Urbana–Champaign, under the direction of Heinz von Foerster, was a major center of cybernetic research for almost 20 years, beginning in 1958.\\n\\n\\n=== New cybernetics ===\\n\\nIn the 1970s, new cyberneticians emerged in multiple fields, but especially in biology. The ideas of Maturana, Varela and Atlan, according to Jean-Pierre Dupuy (1986) \"realized that the cybernetic metaphors of the program upon which molecular biology had been based rendered a conception of the autonomy of the living being impossible. Consequently, these thinkers were led to invent a new cybernetics, one more suited to the organizations which mankind discovers in nature - organizations he has not himself invented\". However, during the 1980s the question of whether the features of this new cybernetics could be applied to social forms of organization remained open to debate.In political science, Project Cybersyn attempted to introduce a cybernetically controlled economy during the early 1970s. In the 1980s, according to Harries-Jones (1988) \"unlike its predecessor, the new cybernetics concerns itself with the interaction of autonomous political actors and subgroups, and the practical and reflexive consciousness of the subjects who produce and reproduce the structure of a political community. A dominant consideration is that of recursiveness, or self-reference of political action both with regards to the expression of political consciousness and with the ways in which systems build upon themselves\".One characteristic of the emerging new cybernetics considered in that time by Felix Geyer and Hans van der Zouwen, according to Bailey (1994), was \"that it views information as constructed and reconstructed by an individual interacting with the environment. This provides an epistemological foundation of science, by viewing it as observer-dependent. Another characteristic of the new cybernetics is its contribution towards bridging the micro-macro gap. That is, it links the individual with the society\". Another characteristic noted was the \"transition from classical cybernetics to the new cybernetics [that] involves a transition from classical problems to new problems. These shifts in thinking involve, among others, (a) a change from emphasis on the system being steered to the system doing the steering, and the factor which guides the steering decisions; and (b) new emphasis on communication between several systems which are trying to steer each other\".Recent endeavors into the true focus of cybernetics, systems of control and emergent behavior, by such related fields as game theory (the analysis of group interaction), systems of feedback in evolution, and metamaterials (the study of materials with properties beyond the Newtonian properties of their constituent atoms), have led to a revived interest in the field.\\n\\n\\n== Notable subfields and theories ==\\nNotable subfields and theories of cybernetics include:\\n\\n\\n=== Autopoiesis ===\\n\\n\\n=== Biological cybernetics ===\\n\\nCybernetics in biology is the study of cybernetic systems present in biological organisms, primarily focusing on how animals adapt to their environment, and how information in the form of genes is passed from generation to generation. There is also a secondary focus on combining artificial systems with biological systems. A notable application to the biology world would be that, in 1955, the physicist George Gamow published a prescient article in Scientific American called \"Information transfer in the living cell\", and cybernetics gave biologists Jacques Monod and François Jacob a language for formulating their early theory of gene regulatory networks in the 1960s.\\n\\n\\n=== Conversation Theory ===\\n\\n\\n=== Engineering cybernetics ===\\n\\n\\n=== Management cybernetics ===\\n\\nManagement as a field of study covers the task of managing a multitude of systems (often business systems), which presents a wide natural overlap with many of the classical concepts of cybernetics.\\n\\n\\n=== Medical cybernetics ===\\n\\nCybernetics has been used as a general reference for the science between the interjection of disciplines Medicine and technology. This involves sciences such as Bionics, Prosthetics, Neural network, Microchip implants, Neuroprosthetics and Brain-computer interface.\\n\\n\\n=== Perceptual control theory ===\\n\\n\\n=== Second-order cybernetics ===\\n\\nSecond-order cybernetics, also known as the cybernetics of cybernetics, is the recursive application of cybernetics to itself and the practice of cybernetics according to such a critique. It has seen development of cybernetics in relation to family therapy, the social sciences, the creative arts, design research, and philosophy. It is associated with Margaret Mead, Heinz von Foerster, the Biological Computer Laboratory and the American Society for Cybernetics.\\n\\n\\n=== Sociocybernetics ===\\n\\nBy examining group behavior through the lens of cybernetics, sociologists can seek the reasons for such spontaneous events as smart mobs and riots, as well as how communities develop rules such as etiquette by consensus without formal discussion. Affect Control Theory explains role behavior, emotions, and labeling theory in terms of homeostatic maintenance of sentiments associated with cultural categories.\\nThe most comprehensive attempt ever made in the social sciences to increase cybernetics in a generalized theory of society was made by Talcott Parsons. In this way, cybernetics establishes the basic hierarchy in Parsons\\' AGIL paradigm, which is the ordering system-dimension of his action theory. These and other cybernetic models in sociology are reviewed in a book edited by McClelland and Fararo.\\n\\n\\n== Relations with other fields ==\\nCybernetics\\' broad scope and tendency to transgress disciplinary norms means its own boundaries have shifted over time and can be difficult to define. Cybernetics has a close relationship with systems science, and many contemporary disciplines can trace their origins in whole or part to work carried out in cybernetics.\\n\\n\\n=== Relation to the systems sciences ===\\nCybernetics is sometimes understood within the context of the broad field of the systems sciences.Systems approaches influenced by cybernetics include:\\n\\nCritical systems thinking, which incorporates the viable system model from management cybernetics.\\nSystemic design, which has drawn on the work of cyberneticians Ranulph Glanville, Klaus Krippendorff, and Paul Pangaro.\\n\\n\\n=== Intersecting fields ===\\nMany fields trace their origins in whole or part to work carried out in cybernetics, or were partially absorbed into cybernetics when it was developed. These include:\\n\\nArtificial intelligence\\nBionics\\nCognitive science\\nControl theory\\nComplexity science\\nComputer science\\nInformation theory\\nRobotics\\n\\n\\n== Applications and influence ==\\nCybernetics\\' transdisciplinary origins have led to a wide variety of applications, approaches and associations.\\n\\n\\n=== In the arts ===\\n\\nNicolas Schöffer\\'s CYSP I (1956) was perhaps the first artwork to explicitly employ cybernetic principles (CYSP is an acronym that joins the first two letters of the words \"CYbernetic\" and \"SPatiodynamic\"). \\nThe prominent and influential Cybernetic Serendipity exhibition was held at the Institute of Contemporary Arts in 1968 curated by Jasia Reichardt, including Schöffer\\'s CYSP I and Gordon Pask\\'s Colloquy of Mobiles installation. Pask\\'s reflections on Colloquy connected it to his earlier Musicolour installation and to what he termed \"aesthetically potent environments\", a concept that connected this artistic work to his concerns with teaching and learning.The artist Roy Ascott elaborated an extensive theory of cybernetic art in  \"Behaviourist Art and the Cybernetic Vision\" (Cybernetica, Journal of the International Association for Cybernetics (Namur), Volume IX, No.4, 1966; Volume X No.1, 1967) and in \"The Cybernetic Stance: My Process and Purpose\" (Leonardo Vol 1, No 2, 1968). \\nArt historian Edward A. Shanken has written about the history of art and cybernetics in essays including \"Cybernetics and Art: Cultural Convergence in the 1960s\" and From Cybernetics to Telematics: The Art, Pedagogy, and Theory of Roy Ascott (2003), which traces the trajectory of Ascott\\'s work from cybernetic art to telematic art (art using computer networking as its medium, a precursor to net.art).\\n\\n\\n=== In architecture and design ===\\n\\nCybernetics was an influence on thinking in architecture and design in the decades after the Second World War. Ashby and Pask were drawn on by design theorists such as Horst Rittel, Christopher Alexander and Bruce Archer. Pask was a consultant to Nicholas Negroponte\\'s Architecture Machine Group, forerunner of the MIT Media Lab, and collaborated with architect Cedric Price and theatre director Joan Littlewood on the influential Fun Palace project during the 1960s. Pask\\'s 1950s Musicolour installation was the inspiration for John and Julia Frazer\\'s work on Price\\'s Generator project.The cybernetic study of design has contributed to design methods research and to the development of systemic design practices.\\n\\n\\n=== In counter culture ===\\nCybernetics was influential on the development of countercultural movements through figures such as Stewart Brand and publications such as the Whole Earth Catalogue and Co-Evolution Quarterly.\\n\\n\\n=== In economics ===\\nThe design of self-regulating control systems for a real-time planned economy was explored by economist Oskar Lange, cyberneticist Viktor Glushkov, and other Soviet cyberneticists during the 1960s. By the time information technology was developed enough to enable feasible economic planning based on computers, the Soviet Union and eastern bloc countries began moving away from planning and eventually collapsed.\\nAfter the fall of the Soviet Union a proposal for a \"New Socialism\" was outlined by the computer scientists Paul Cockshott and Allin Cottrell in 1995 (Towards a New Socialism), where computers determine and manage the flows and allocation of resources among socially owned enterprises.On the other hand, Friedrich Hayek also mentions cybernetics as a discipline that could help economists understand the \"self-organizing or self-generating systems\" called markets. Being \"complex phenomena\", the best way to examine market functions is by using the feedback mechanism, explained by cybernetic theorists. That way, economists could make \"pattern predictions\".Therefore, the market for Hayek is a \"communication system\", an \"efficient mechanism for digesting dispersed information\". The economist and a cyberneticist are like garderners who are \"providing the appropriate environment\". Hayek\\'s definition of information is idiosyncratic and precedes the information theory used in cybernetics and the natural sciences.\\nFinally, Hayek also considers Adam Smith\\'s idea of the invisible hand as an anticipation of the operation of the feedback mechanism in cybernetics. In the same book, Law, Legislation and Liberty, Hayek mentions, along with cybernetics, that economists should rely on the scientific findings of Ludwig von Bertalanffy general systems theory, along with information and communication theory and semiotics.\\n\\n\\n=== In family therapy ===\\n\\nThe development of family therapy was significantly influenced by cybernetics through the work of Gregory Bateson.\\n\\n\\n=== In feminism ===\\n\\nIdeas from cybernetics have influenced feminism through the work of Donna Haraway.\\n\\n\\n=== In philosophy ===\\n\\nIn his 1990 essay \"Postscript on the Societies of Control\" Gilles Deleuze argues that society is undergoing a shift in structure and control. The author claims institutions and technologies introduced since World War II have dissolved the boundaries between these enclosures. As a result, social coercion and discipline have moved into the lives of individuals considered as \"masses, samples, data, markets, or \\'banks\\'\" to be controlled cybernetically.  These mechanisms of modern societies of control are described as continuous, following and tracking individuals throughout their existence via transaction records, mobile location tracking, and other personally identifiable information.Gregory Bateson saw the world as a series of systems containing those of individuals, societies and ecosystems. Each of these systems has adaptive changes which depend upon feedback loops to control balance by changing multiple variables. He saw the natural ecological system as innately good as long as it was allowed to maintain homeostasis, and that the key unit of survival in evolution was an organism and its environment.Bateson, in this subject, presents western epistemology as a method of thinking that leads to a mindset in which man exerts an autocratic rule over all cybernetic systems and in doing so he unbalances the natural cybernetic system of controlled competition and mutual dependency. Bateson claims that humanity will never be able to control the whole system because it does not operate in a linear fashion, and if humanity creates his own rules for the system, he opens himself up to becoming a slave to the self-made system due to the non-linear nature of cybernetics.  Lastly, man\\'s technological prowess combined with his scientific hubris gives him the potential to irrevocably damage and destroy the \"supreme cybernetic system\" (i.e. the biosphere), instead of just disrupting the system temporally until the system can self-correct.\\n\\n\\n=== Other applications ===\\nOther applications of cybernetics include: \\n\\nIn Earth system science. Geocybernetics aims to study and control the complex co-evolution of ecosphere and anthroposphere, for example, for dealing with planetary problems such as anthropogenic global warming. Geocybernetics applies a dynamical systems perspective to Earth system analysis. It provides a theoretical framework for studying the implications of following different sustainability paradigms on co-evolutionary trajectories of the planetary socio-ecological system to reveal attractors in this system, their stability, resilience and reachability. Concepts such as tipping points in the climate system, planetary boundaries, the safe operating space for humanity and proposals for manipulating Earth system dynamics on a global scale such as geoengineering have been framed in the language of geocybernetic Earth system analysis.In sport. A model of cybernetics in Sport was introduced by Yuri Verkhoshansky and Mel C. Siff in 1999 in their book Supertraining.\\n\\n\\n== Journals ==\\nConstructivist Foundations\\nCybernetics and Human Knowing\\nCybernetics and Systems\\nIEEE Transactions on Systems, Man, and Cybernetics: Systems\\nIEEE Transactions on Human-Machine Systems\\nIEEE Transactions on Cybernetics\\nIEEE Transactions on Computational Social Systems\\nKybernetes\\n\\n\\n== Organisations ==\\n\\nOrganisations primarily concerned with cybernetics or aspects of it include:\\n\\n\\n=== American Society for Cybernetics ===\\n\\n\\n=== Cybernetics Society ===\\n\\n\\n=== Metaphorum ===\\nThe Metaphorum group was set up in 2003 to develop Stafford Beer\\'s legacy in Organizational Cybernetics. The Metaphorum Group was born in a Syntegration in 2003 and have every year after developed a Conference on issues related to Organizational Cybernetics\\' theory and practice.\\n\\n\\n=== IEEE Systems, Man, and Cybernetics Society ===\\n\\n\\n== See also ==\\n\\n\\n== External links ==\\n\\nGeneralNorbert Wiener and Stefan Odobleja - A Comparative Analysis\\nReading List for Cybernetics \\nPrincipia Cybernetica Web\\nWeb Dictionary of Cybernetics and Systems\\nGlossary Slideshow (136 slides)\\n\"Basics of Cybernetics\". Archived from the original on 2010-08-11. Retrieved 2016-01-23.\\nWhat is Cybernetics? Livas short introductory videos on YouTubeSocietiesAmerican Society for Cybernetics\\nIEEE Systems, Man, & Cybernetics Society\\nInternational Society for Cybernetics and Systems Research\\nThe Cybernetics Society\\n\\n\\n== Further reading ==\\nArbib, Michael A. (1987). Brains, machines, and mathematics (2nd ed.). New York: Springer-Verlag. ISBN 978-0387965390.\\nArbib, Michael A. (1972). The Metaphorical Brain. Wiley. ISBN 978-0-471-03249-6.\\nAscott, Roy (1967). Behaviourist Art and the Cybernetic Vision. Cybernetica, Journal of the International Association for Cybernetics (Namur), 10, pp. 25–56\\nAshby, William Ross (1956). An introduction to cybernetics (PDF). Chapman & Hall. Retrieved 3 June 2012.\\nBeer, Stafford (1974). Designing freedom. Chichester, West Sussex, England: Wiley. ISBN 978-0471951650.\\nFrançois, Charles (1999). \"Systemics and cybernetics in a historical perspective\". In: Systems Research and Behavioral Science. Vol 16, pp. 203–219 (1999)\\nGeorge, F. H. (1971). Cybernetics. Teach Yourself Books. ISBN 978-0-340-05941-8.\\nGerovitch, Slava (2002). From newspeak to cyberspeak : a history of Soviet cybernetics. Cambridge, Massachusetts [u.a.]: MIT Press. ISBN 978-0262-07232-8.\\nHeims, Steve Joshua (1993). Constructing a social science for postwar America : the cybernetics group, 1946-1953 (1st ed.). Cambridge, Massachusetts u.a.: MIT Press. ISBN 9780262581233.\\nHelvey, T.C. (1971). The age of information; an interdisciplinary survey of cybernetics. Englewood Cliffs, N.J.: Educational Technology Publications. ISBN 9780877780083.\\nHeylighen, Francis, and Cliff Joslyn (2002). \"Cybernetics and Second Order Cybernetics\", in: R.A. Meyers (ed.), Encyclopedia of Physical Science & Technology (3rd ed.), Vol. 4, (Academic Press, San Diego), p. 155-169.\\nHyötyniemi, Heikki (2006). Neocybernetics in Biological Systems. Espoo: Helsinki University of Technology, Control Engineering Laboratory.\\nIlgauds, Hans Joachim (1980), Norbert Wiener, Leipzig.\\nJohnston, John (2008). The allure of machinic life : cybernetics, artificial life, and the new AI. Cambridge, Massachusetts: MIT Press. ISBN 978-0-262-10126-4.\\nMedina, Eden (2011). Cybernetic revolutionaries : technology and politics in Allende\\'s Chile. Cambridge, Massachusetts: MIT Press. ISBN 978-0-262-01649-0.\\nPangaro, Paul. \"Cybernetics — A Definition\".\\nPask, Gordon (1972). \"Cybernetics\". Encyclopædia Britannica. Archived from the original on 2011-09-28. Retrieved 2007-09-26.\\nPatten, Bernard C.; Odum, Eugene P. (December 1981). \"The Cybernetic Nature of Ecosystems\". The American Naturalist. 118 (6): 886–895. doi:10.1086/283881. JSTOR 2460822?. S2CID 84672792.\\nPekelis, V. (1974). Cybernetics A to Z. Moscow: Mir Publishers.\\nPickering, Andrew (2010). The cybernetic brain : sketches of another future ([Online-Ausg.] ed.). Chicago: University of Chicago Press. ISBN 978-0226667898.\\nUmpleby, Stuart (1989). \"The science of cybernetics and the cybernetics of science\", in: Cybernetics and Systems\", Vol. 21, No. 1, (1990), pp. 109–121.\\nvon Foerster, Heinz, (1995), Ethics and Second-Order Cybernetics.\\nWiener, Norbert (1948).  Hermann & Cie (ed.). Cybernetics; or, Control and communication in the animal and the machine. Paris: Technology Press. Retrieved 3 June 2012.\\nWiener, Norbert (1950). Cybernetics and Society: The Human Use of Human Beings. Houghton Mifflin.\\n\\n\\n== References ==', 'Discrete mathematics is the study of mathematical structures that are fundamentally discrete rather than continuous. In contrast to real numbers that have the property of varying \"smoothly\", the objects studied in discrete mathematics – such as integers, graphs, and statements in logic – do not vary smoothly in this way, but have distinct, separated values. Discrete mathematics therefore excludes topics in \"continuous mathematics\" such as calculus or Euclidean geometry. Discrete objects can often be enumerated by integers. More formally, discrete mathematics has been characterized as the branch of mathematics dealing with countable sets (finite sets or sets with the same cardinality as the natural numbers). However, there is no exact definition of the term \"discrete mathematics.\" Indeed, discrete mathematics is described less by what is included than by what is excluded: continuously varying quantities and related notions.\\nThe set of objects studied in discrete mathematics can be finite or infinite. The term finite mathematics is sometimes applied to parts of the field of discrete mathematics that deals with finite sets, particularly those areas relevant to business.\\nResearch in discrete mathematics increased in the latter half of the twentieth century partly due to the development of digital computers which operate in discrete steps and store data in discrete bits. Concepts and notations from discrete mathematics are useful in studying and describing objects and problems in branches of computer science, such as computer algorithms, programming languages, cryptography, automated theorem proving, and software development. Conversely, computer implementations are significant in applying ideas from discrete mathematics to real-world problems, such as in operations research.\\nAlthough the main objects of study in discrete mathematics are discrete objects, analytic methods from continuous mathematics are often employed as well.\\nIn university curricula, \"Discrete Mathematics\" appeared in the 1980s, initially as a computer science support course; its contents were somewhat haphazard at the time. The curriculum has thereafter developed in conjunction with efforts by ACM and MAA into a course that is basically intended to develop mathematical maturity in first-year students; therefore, it is nowadays a prerequisite for mathematics majors in some universities as well. Some high-school-level discrete mathematics textbooks have appeared as well. At this level, discrete mathematics is sometimes seen as a preparatory course, not unlike precalculus in this respect.The Fulkerson Prize is awarded for outstanding papers in discrete mathematics.\\n\\n\\n== Grand challenges, past and present ==\\n\\nThe history of discrete mathematics has involved a number of challenging problems which have focused attention within areas of the field. In graph theory, much research was motivated by attempts to prove the four color theorem, first stated in 1852, but not proved until 1976 (by Kenneth Appel and Wolfgang Haken, using substantial computer assistance).In logic, the second problem on David Hilbert\\'s list of open problems presented in 1900 was to prove that the axioms of arithmetic are consistent. Gödel\\'s second incompleteness theorem, proved in 1931, showed that this was not possible – at least not within arithmetic itself. Hilbert\\'s tenth problem was to determine whether a given polynomial Diophantine equation with integer coefficients has an integer solution. In 1970, Yuri Matiyasevich proved that this could not be done.\\nThe need to break German codes in World War II led to advances in cryptography and theoretical computer science, with the first programmable digital electronic computer being developed at England\\'s Bletchley Park with the guidance of Alan Turing and his seminal work, On Computable Numbers. At the same time, military requirements motivated advances in operations research. The Cold War meant that cryptography remained important, with fundamental advances such as public-key cryptography being developed in the following decades. Operations research remained important as a tool in business and project management, with the critical path method being developed in the 1950s. The telecommunication industry has also motivated advances in discrete mathematics, particularly in graph theory and information theory. Formal verification of statements in logic has been necessary for software development of safety-critical systems, and advances in automated theorem proving have been driven by this need.\\nComputational geometry has been an important part of the computer graphics incorporated into modern video games and computer-aided design tools.\\nSeveral fields of discrete mathematics, particularly theoretical computer science, graph theory, and combinatorics, are important in addressing the challenging bioinformatics problems associated with understanding the tree of life.Currently, one of the most famous open problems in theoretical computer science is the P = NP problem, which involves the relationship between the complexity classes P and NP. The Clay Mathematics Institute has offered a $1 million USD prize for the first correct proof, along with prizes for six other mathematical problems.\\n\\n\\n== Topics in discrete mathematics ==\\n\\n\\n=== Theoretical computer science ===\\n\\nTheoretical computer science includes areas of discrete mathematics relevant to computing. It draws heavily on graph theory and mathematical logic. Included within theoretical computer science is the study of algorithms and data structures. Computability studies what can be computed in principle, and has close ties to logic, while complexity studies the time, space, and other resources taken by computations. Automata theory and formal language theory are closely related to computability. Petri nets and process algebras are used to model computer systems, and methods from discrete mathematics are used in analyzing VLSI electronic circuits. Computational geometry applies algorithms to geometrical problems, while computer image analysis applies them to representations of images. Theoretical computer science also includes the study of various continuous computational topics.\\n\\n\\n=== Information theory ===\\n\\nInformation theory involves the quantification of information. Closely related is coding theory which is used to design efficient and reliable data transmission and storage methods. Information theory also includes continuous topics such as: analog signals, analog coding, analog encryption.\\n\\n\\n=== Logic ===\\n\\nLogic is the study of the principles of valid reasoning and inference, as well as of consistency, soundness, and completeness. For example, in most systems of logic (but not in intuitionistic logic) Peirce\\'s law (((P→Q)→P)→P) is a theorem. For classical logic, it can be easily verified with a truth table. The study of mathematical proof is particularly important in logic, and has applications to automated theorem proving and formal verification of software.\\nLogical formulas are discrete structures, as are proofs, which form finite trees or, more generally, directed acyclic graph structures (with each inference step combining one or more premise branches to give a single conclusion). The truth values of logical formulas usually form a finite set, generally restricted to two values: true and false, but logic can also be continuous-valued, e.g., fuzzy logic. Concepts such as infinite proof trees or infinite derivation trees have also been studied, e.g. infinitary logic.\\n\\n\\n=== Set theory ===\\n\\nSet theory is the branch of mathematics that studies sets, which are collections of objects, such as {blue, white, red} or the (infinite) set of all prime numbers. Partially ordered sets and sets with other relations have applications in several areas.\\nIn discrete mathematics, countable sets (including finite sets) are the main focus. The beginning of set theory as a branch of mathematics is usually marked by Georg Cantor\\'s work distinguishing between different kinds of infinite set, motivated by the study of trigonometric series, and further development of the theory of infinite sets is outside the scope of discrete mathematics. Indeed, contemporary work in descriptive set theory makes extensive use of traditional continuous mathematics.\\n\\n\\n=== Combinatorics ===\\n\\nCombinatorics studies the way in which discrete structures can be combined or arranged.\\nEnumerative combinatorics concentrates on counting the number of certain combinatorial objects - e.g. the twelvefold way provides a unified framework for counting permutations, combinations and partitions.\\nAnalytic combinatorics concerns the enumeration (i.e., determining the number) of combinatorial structures using tools from complex analysis and probability theory. In contrast with enumerative combinatorics which uses explicit combinatorial formulae and generating functions to describe the results, analytic combinatorics aims at obtaining asymptotic formulae.\\nDesign theory is a study of combinatorial designs, which are collections of subsets with certain intersection properties.\\nPartition theory studies various enumeration and asymptotic problems related to integer partitions, and is closely related to q-series, special functions and orthogonal polynomials. Originally a part of number theory and analysis, partition theory is now considered a part of combinatorics or an independent field.\\nOrder theory is the study of partially ordered sets, both finite and infinite.\\n\\n\\n=== Graph theory ===\\n\\nGraph theory, the study of graphs and networks, is often considered part of combinatorics, but has grown large enough and distinct enough, with its own kind of problems, to be regarded as a subject in its own right. Graphs are one of the prime objects of study in discrete mathematics. They are among the most ubiquitous models of both natural and human-made structures. They can model many types of relations and process dynamics in physical, biological and social systems. In computer science, they can represent networks of communication, data organization, computational devices, the flow of computation, etc. In mathematics, they are useful in geometry and certain parts of topology, e.g. knot theory. Algebraic graph theory has close links with group theory. There are also continuous graphs; however, for the most part, research in graph theory falls within the domain of discrete mathematics.\\n\\n\\n=== Probability ===\\n\\nDiscrete probability theory deals with events that occur in countable sample spaces. For example, count observations such as the numbers of birds in flocks comprise only natural number values {0, 1, 2, ...}. On the other hand, continuous observations such as the weights of birds comprise real number values and would typically be modeled by a continuous probability distribution such as the normal. Discrete probability distributions can be used to approximate continuous ones and vice versa. For highly constrained situations such as throwing dice or experiments with decks of cards, calculating the probability of events is basically enumerative combinatorics.\\n\\n\\n=== Number theory ===\\n\\nNumber theory is concerned with the properties of numbers in general, particularly integers. It has applications to cryptography and cryptanalysis, particularly with regard to modular arithmetic, diophantine equations, linear and quadratic congruences, prime numbers and primality testing. Other discrete aspects of number theory include geometry of numbers. In analytic number theory, techniques from continuous mathematics are also used. Topics that go beyond discrete objects include transcendental numbers, diophantine approximation, p-adic analysis and function fields.\\n\\n\\n=== Algebraic structures ===\\n\\nAlgebraic structures occur as both discrete examples and continuous examples. Discrete algebras include: boolean algebra used in logic gates and programming; relational algebra used in databases; discrete and finite versions of groups, rings and fields are important in algebraic coding theory; discrete semigroups and monoids appear in the theory of formal languages.\\n\\n\\n=== Calculus of finite differences, discrete calculus or discrete analysis ===\\n\\nA function defined on an interval of the integers is usually called a sequence. A sequence could be a finite sequence from a data source or an infinite sequence from a discrete dynamical system. Such a discrete function could be defined explicitly by a list (if its domain is finite), or by a formula for its general term, or it could be given implicitly by a recurrence relation or difference equation. Difference equations are similar to differential equations, but replace differentiation by taking the difference between adjacent terms; they can be used to approximate differential equations or (more often) studied in their own right. Many questions and methods concerning differential equations have counterparts for difference equations. For instance, where there are integral transforms in harmonic analysis for studying continuous functions or analogue signals, there are discrete transforms for discrete functions or digital signals. As well as the discrete metric there are more general discrete or finite metric spaces and finite topological spaces.\\n\\n\\n=== Geometry ===\\n\\nDiscrete geometry and combinatorial geometry are about combinatorial properties of discrete collections of geometrical objects. A long-standing topic in discrete geometry is tiling of the plane. Computational geometry applies algorithms to geometrical problems.\\n\\n\\n=== Topology ===\\nAlthough topology is the field of mathematics that formalizes and generalizes the intuitive notion of \"continuous deformation\" of objects, it gives rise to many discrete topics; this can be attributed in part to the focus on topological invariants, which themselves usually take discrete values.\\nSee combinatorial topology, topological graph theory, topological combinatorics, computational topology, discrete topological space, finite topological space, topology (chemistry).\\n\\n\\n=== Operations research ===\\n\\nOperations research provides techniques for solving practical problems in engineering, business, and other fields — problems such as allocating resources to maximize profit, and scheduling project activities to minimize risk. Operations research techniques include linear programming and other areas of optimization, queuing theory, scheduling theory, and network theory. Operations research also includes continuous topics such as continuous-time Markov process, continuous-time martingales, process optimization, and continuous and hybrid control theory.\\n\\n\\n=== Game theory, decision theory, utility theory, social choice theory ===\\nDecision theory is concerned with identifying the values, uncertainties and other issues relevant in a given decision, its rationality, and the resulting optimal decision.\\nUtility theory is about measures of the relative economic satisfaction from, or desirability of, consumption of various goods and services.\\nSocial choice theory is about voting. A more puzzle-based approach to voting is ballot theory.\\nGame theory deals with situations where success depends on the choices of others, which makes choosing the best course of action more complex. There are even continuous games, see differential game. Topics include auction theory and fair division.\\n\\n\\n=== Discretization ===\\n\\nDiscretization concerns the process of transferring continuous models and equations into discrete counterparts, often for the purposes of making calculations easier by using approximations. Numerical analysis provides an important example.\\n\\n\\n=== Discrete analogues of continuous mathematics ===\\nThere are many concepts in continuous mathematics which have discrete versions, such as discrete calculus, discrete probability distributions, discrete Fourier transforms, discrete geometry, discrete logarithms, discrete differential geometry, discrete exterior calculus, discrete Morse theory, difference equations, discrete dynamical systems, and discrete vector measures.\\nIn applied mathematics, discrete modelling is the discrete analogue of continuous modelling. In discrete modelling, discrete formulae are fit to data. A common method in this form of modelling is to use recurrence relation.\\nIn algebraic geometry, the concept of a curve can be extended to discrete geometries by taking the spectra of polynomial rings over finite fields to be models of the affine spaces over that field, and letting subvarieties or spectra of other rings provide the curves that lie in that space. Although the space in which the curves appear has a finite number of points, the curves are not so much sets of points as analogues of curves in continuous settings. For example, every point of the form \\n  \\n    \\n      \\n        V\\n        (\\n        x\\n        −\\n        c\\n        )\\n        ⊂\\n        Spec\\n        \\u2061\\n        K\\n        [\\n        x\\n        ]\\n        =\\n        \\n          \\n            A\\n          \\n          \\n            1\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle V(x-c)\\\\subset \\\\operatorname {Spec} K[x]=\\\\mathbb {A} ^{1}}\\n   for \\n  \\n    \\n      \\n        K\\n      \\n    \\n    {\\\\displaystyle K}\\n   a field can be studied either as \\n  \\n    \\n      \\n        Spec\\n        \\u2061\\n        K\\n        [\\n        x\\n        ]\\n        \\n          /\\n        \\n        (\\n        x\\n        −\\n        c\\n        )\\n        ≅\\n        Spec\\n        \\u2061\\n        K\\n      \\n    \\n    {\\\\displaystyle \\\\operatorname {Spec} K[x]/(x-c)\\\\cong \\\\operatorname {Spec} K}\\n  , a point, or as the spectrum \\n  \\n    \\n      \\n        Spec\\n        \\u2061\\n        K\\n        [\\n        x\\n        \\n          ]\\n          \\n            (\\n            x\\n            −\\n            c\\n            )\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle \\\\operatorname {Spec} K[x]_{(x-c)}}\\n   of the local ring at (x-c), a point together with a neighborhood around it. Algebraic varieties also have a well-defined notion of tangent space called the Zariski tangent space, making many features of calculus applicable even in finite settings.\\n\\n\\n=== Hybrid discrete and continuous mathematics ===\\nThe time scale calculus is a unification of the theory of difference equations with that of differential equations, which has applications to fields requiring simultaneous modelling of discrete and continuous data. Another way of modeling such a situation is the notion of hybrid dynamical systems.\\n\\n\\n== See also ==\\n\\nOutline of discrete mathematics\\nCyberchase, a show that teaches Discrete Mathematics to children\\n\\n\\n== References ==\\n\\n\\n== Further reading ==\\nNorman L. Biggs (2002-12-19). Discrete Mathematics. Oxford University Press. ISBN 978-0-19-850717-8.\\nJohn Dwyer (2010). An Introduction to Discrete Mathematics for Business & Computing. ISBN 978-1-907934-00-1.\\nSusanna S. Epp (2010-08-04). Discrete Mathematics With Applications. Thomson Brooks/Cole. ISBN 978-0-495-39132-6.\\nRonald Graham, Donald E. Knuth, Oren Patashnik, Concrete Mathematics.\\nRalph P. Grimaldi (2004). Discrete and Combinatorial Mathematics: An Applied Introduction. Addison Wesley. ISBN 978-0-201-72634-3.\\nDonald E. Knuth (2011-03-03). The Art of Computer Programming, Volumes 1-4a Boxed Set. Addison-Wesley Professional. ISBN 978-0-321-75104-1.\\nJiří Matoušek; Jaroslav Nešetřil (1998). Discrete Mathematics. Oxford University Press. ISBN 978-0-19-850208-1.\\nObrenic, Bojana (2003-01-29). Practice Problems in Discrete Mathematics. Prentice Hall. ISBN 978-0-13-045803-2.\\nKenneth H. Rosen; John G. Michaels (2000). Hand Book of Discrete and Combinatorial Mathematics. CRC PressI Llc. ISBN 978-0-8493-0149-0.\\nKenneth H. Rosen (2007). Discrete Mathematics: And Its Applications. McGraw-Hill College. ISBN 978-0-07-288008-3.\\nAndrew Simpson (2002). Discrete Mathematics by Example. McGraw-Hill Incorporated. ISBN 978-0-07-709840-7.\\n\\n\\n== External links ==\\n Media related to Discrete mathematics at Wikimedia Commons\\nDiscrete mathematics at the utk.edu Mathematics Archives, providing links to syllabi, tutorials, programs, etc.\\nIowa Central: Electrical Technologies Program Discrete mathematics for Electrical engineering.', 'In mathematics, graph theory is the study of graphs, which are mathematical structures used to model pairwise relations between objects. A graph in this context is made up of vertices (also called nodes or points) which are connected by edges (also called links or lines). A distinction is made between undirected graphs, where edges link two vertices symmetrically, and directed graphs, where edges link two vertices asymmetrically. Graphs are one of the principal objects of study in discrete mathematics.\\n\\n\\n== Definitions ==\\n\\nDefinitions in graph theory vary. The following are some of the more basic ways of defining graphs and related mathematical structures.\\n\\n\\n=== Graph ===\\n\\nIn one restricted but very common sense of the term, a graph is an ordered pair \\n  \\n    \\n      \\n        G\\n        =\\n        (\\n        V\\n        ,\\n        E\\n        )\\n      \\n    \\n    {\\\\displaystyle G=(V,E)}\\n   comprising:\\n\\n  \\n    \\n      \\n        V\\n      \\n    \\n    {\\\\displaystyle V}\\n  , a set of vertices (also called nodes or points);\\n\\n  \\n    \\n      \\n        E\\n        ⊆\\n        {\\n        {\\n        x\\n        ,\\n        y\\n        }\\n        ∣\\n        x\\n        ,\\n        y\\n        ∈\\n        V\\n        \\n        \\n          \\n            and\\n          \\n        \\n        \\n        x\\n        ≠\\n        y\\n        }\\n      \\n    \\n    {\\\\displaystyle E\\\\subseteq \\\\{\\\\{x,y\\\\}\\\\mid x,y\\\\in V\\\\;{\\\\textrm {and}}\\\\;x\\\\neq y\\\\}}\\n  , a set of edges (also called links or lines), which are unordered pairs of vertices (that is, an edge is associated with two distinct vertices).To avoid ambiguity, this type of object may be called precisely an undirected simple graph.\\nIn the edge \\n  \\n    \\n      \\n        {\\n        x\\n        ,\\n        y\\n        }\\n      \\n    \\n    {\\\\displaystyle \\\\{x,y\\\\}}\\n  , the vertices \\n  \\n    \\n      \\n        x\\n      \\n    \\n    {\\\\displaystyle x}\\n   and \\n  \\n    \\n      \\n        y\\n      \\n    \\n    {\\\\displaystyle y}\\n   are called the endpoints of the edge. The edge is said to join \\n  \\n    \\n      \\n        x\\n      \\n    \\n    {\\\\displaystyle x}\\n   and \\n  \\n    \\n      \\n        y\\n      \\n    \\n    {\\\\displaystyle y}\\n   and to be incident on \\n  \\n    \\n      \\n        x\\n      \\n    \\n    {\\\\displaystyle x}\\n   and on \\n  \\n    \\n      \\n        y\\n      \\n    \\n    {\\\\displaystyle y}\\n  . A vertex may exist in a graph and not belong to an edge. Multiple edges, not allowed under the definition above, are two or more edges that join the same two vertices.\\nIn one more general sense of the term allowing multiple edges, a graph is an ordered triple \\n  \\n    \\n      \\n        G\\n        =\\n        (\\n        V\\n        ,\\n        E\\n        ,\\n        ϕ\\n        )\\n      \\n    \\n    {\\\\displaystyle G=(V,E,\\\\phi )}\\n   comprising:\\n\\n  \\n    \\n      \\n        V\\n      \\n    \\n    {\\\\displaystyle V}\\n  , a set of vertices (also called nodes or points);\\n\\n  \\n    \\n      \\n        E\\n      \\n    \\n    {\\\\displaystyle E}\\n  , a set of edges (also called links or lines);\\n\\n  \\n    \\n      \\n        ϕ\\n        :\\n        E\\n        →\\n        {\\n        {\\n        x\\n        ,\\n        y\\n        }\\n        ∣\\n        x\\n        ,\\n        y\\n        ∈\\n        V\\n        \\n        \\n          \\n            and\\n          \\n        \\n        \\n        x\\n        ≠\\n        y\\n        }\\n      \\n    \\n    {\\\\displaystyle \\\\phi :E\\\\to \\\\{\\\\{x,y\\\\}\\\\mid x,y\\\\in V\\\\;{\\\\textrm {and}}\\\\;x\\\\neq y\\\\}}\\n  , an incidence function mapping every edge to an unordered pair of vertices (that is, an edge is associated with two distinct vertices).To avoid ambiguity, this type of object may be called precisely an undirected multigraph.\\nA loop is an edge that joins a vertex to itself. Graphs as defined in the two definitions above cannot have loops, because a loop joining a vertex \\n  \\n    \\n      \\n        x\\n      \\n    \\n    {\\\\displaystyle x}\\n   to itself is the edge (for an undirected simple graph) or is incident on (for an undirected multigraph) \\n  \\n    \\n      \\n        {\\n        x\\n        ,\\n        x\\n        }\\n        =\\n        {\\n        x\\n        }\\n      \\n    \\n    {\\\\displaystyle \\\\{x,x\\\\}=\\\\{x\\\\}}\\n   which is not in \\n  \\n    \\n      \\n        {\\n        {\\n        x\\n        ,\\n        y\\n        }\\n        ∣\\n        x\\n        ,\\n        y\\n        ∈\\n        V\\n        \\n        \\n          \\n            and\\n          \\n        \\n        \\n        x\\n        ≠\\n        y\\n        }\\n      \\n    \\n    {\\\\displaystyle \\\\{\\\\{x,y\\\\}\\\\mid x,y\\\\in V\\\\;{\\\\textrm {and}}\\\\;x\\\\neq y\\\\}}\\n  . So to allow loops the definitions must be expanded. For undirected simple graphs, the definition of \\n  \\n    \\n      \\n        E\\n      \\n    \\n    {\\\\displaystyle E}\\n   should be modified to \\n  \\n    \\n      \\n        E\\n        ⊆\\n        {\\n        {\\n        x\\n        ,\\n        y\\n        }\\n        ∣\\n        x\\n        ,\\n        y\\n        ∈\\n        V\\n        }\\n      \\n    \\n    {\\\\displaystyle E\\\\subseteq \\\\{\\\\{x,y\\\\}\\\\mid x,y\\\\in V\\\\}}\\n  . For undirected multigraphs, the definition of \\n  \\n    \\n      \\n        ϕ\\n      \\n    \\n    {\\\\displaystyle \\\\phi }\\n   should be modified to \\n  \\n    \\n      \\n        ϕ\\n        :\\n        E\\n        →\\n        {\\n        {\\n        x\\n        ,\\n        y\\n        }\\n        ∣\\n        x\\n        ,\\n        y\\n        ∈\\n        V\\n        }\\n      \\n    \\n    {\\\\displaystyle \\\\phi :E\\\\to \\\\{\\\\{x,y\\\\}\\\\mid x,y\\\\in V\\\\}}\\n  . To avoid ambiguity, these types of objects may be called undirected simple graph permitting loops and undirected multigraph permitting loops, respectively.\\n\\n  \\n    \\n      \\n        V\\n      \\n    \\n    {\\\\displaystyle V}\\n   and \\n  \\n    \\n      \\n        E\\n      \\n    \\n    {\\\\displaystyle E}\\n   are usually taken to be finite, and many of the well-known results are not true (or are rather different) for infinite graphs because many of the arguments fail in the infinite case. Moreover, \\n  \\n    \\n      \\n        V\\n      \\n    \\n    {\\\\displaystyle V}\\n   is often assumed to be non-empty, but \\n  \\n    \\n      \\n        E\\n      \\n    \\n    {\\\\displaystyle E}\\n   is allowed to be the empty set. The order of a graph is \\n  \\n    \\n      \\n        \\n          |\\n        \\n        V\\n        \\n          |\\n        \\n      \\n    \\n    {\\\\displaystyle |V|}\\n  , its number of vertices. The size of a graph is \\n  \\n    \\n      \\n        \\n          |\\n        \\n        E\\n        \\n          |\\n        \\n      \\n    \\n    {\\\\displaystyle |E|}\\n  , its number of edges. The degree or valency of a vertex is the number of edges that are incident to it, where a loop is counted twice. The degree of a graph is the maximum of the degrees of its vertices.\\nIn an undirected simple graph of order n, the maximum degree of each vertex is n − 1 and the maximum size of the graph is n(n − 1)/2.\\nThe edges of an undirected simple graph permitting loops \\n  \\n    \\n      \\n        G\\n      \\n    \\n    {\\\\displaystyle G}\\n   induce a symmetric homogeneous relation ~ on the vertices of \\n  \\n    \\n      \\n        G\\n      \\n    \\n    {\\\\displaystyle G}\\n   that is called the adjacency relation of \\n  \\n    \\n      \\n        G\\n      \\n    \\n    {\\\\displaystyle G}\\n  . Specifically, for each edge \\n  \\n    \\n      \\n        (\\n        x\\n        ,\\n        y\\n        )\\n      \\n    \\n    {\\\\displaystyle (x,y)}\\n  , its endpoints \\n  \\n    \\n      \\n        x\\n      \\n    \\n    {\\\\displaystyle x}\\n   and \\n  \\n    \\n      \\n        y\\n      \\n    \\n    {\\\\displaystyle y}\\n   are said to be adjacent to one another, which is denoted \\n  \\n    \\n      \\n        x\\n      \\n    \\n    {\\\\displaystyle x}\\n   ~ \\n  \\n    \\n      \\n        y\\n      \\n    \\n    {\\\\displaystyle y}\\n  .\\n\\n\\n=== Directed graph ===\\n\\nA directed graph or digraph is a graph in which edges have orientations.\\nIn one restricted but very common sense of the term, a directed graph is an ordered pair \\n  \\n    \\n      \\n        G\\n        =\\n        (\\n        V\\n        ,\\n        E\\n        )\\n      \\n    \\n    {\\\\displaystyle G=(V,E)}\\n   comprising:\\n\\n  \\n    \\n      \\n        V\\n      \\n    \\n    {\\\\displaystyle V}\\n  , a set of vertices (also called nodes or points);\\n\\n  \\n    \\n      \\n        E\\n        ⊆\\n        \\n          {\\n          \\n            (\\n            x\\n            ,\\n            y\\n            )\\n            ∣\\n            (\\n            x\\n            ,\\n            y\\n            )\\n            ∈\\n            \\n              V\\n              \\n                2\\n              \\n            \\n            \\n            \\n              \\n                and\\n              \\n            \\n            \\n            x\\n            ≠\\n            y\\n          \\n          }\\n        \\n      \\n    \\n    {\\\\displaystyle E\\\\subseteq \\\\left\\\\{(x,y)\\\\mid (x,y)\\\\in V^{2}\\\\;{\\\\textrm {and}}\\\\;x\\\\neq y\\\\right\\\\}}\\n  , a set of edges (also called directed edges, directed links, directed lines, arrows or arcs) which are ordered pairs of vertices (that is, an edge is associated with two distinct vertices).To avoid ambiguity, this type of object may be called precisely a directed simple graph.\\nIn the edge \\n  \\n    \\n      \\n        (\\n        x\\n        ,\\n        y\\n        )\\n      \\n    \\n    {\\\\displaystyle (x,y)}\\n   directed from \\n  \\n    \\n      \\n        x\\n      \\n    \\n    {\\\\displaystyle x}\\n   to \\n  \\n    \\n      \\n        y\\n      \\n    \\n    {\\\\displaystyle y}\\n  , the vertices \\n  \\n    \\n      \\n        x\\n      \\n    \\n    {\\\\displaystyle x}\\n   and \\n  \\n    \\n      \\n        y\\n      \\n    \\n    {\\\\displaystyle y}\\n   are called the endpoints of the edge, \\n  \\n    \\n      \\n        x\\n      \\n    \\n    {\\\\displaystyle x}\\n   the tail of the edge and \\n  \\n    \\n      \\n        y\\n      \\n    \\n    {\\\\displaystyle y}\\n   the head of the edge. The edge is said to join \\n  \\n    \\n      \\n        x\\n      \\n    \\n    {\\\\displaystyle x}\\n   and \\n  \\n    \\n      \\n        y\\n      \\n    \\n    {\\\\displaystyle y}\\n   and to be incident on \\n  \\n    \\n      \\n        x\\n      \\n    \\n    {\\\\displaystyle x}\\n   and on \\n  \\n    \\n      \\n        y\\n      \\n    \\n    {\\\\displaystyle y}\\n  . A vertex may exist in a graph and not belong to an edge. The edge \\n  \\n    \\n      \\n        (\\n        y\\n        ,\\n        x\\n        )\\n      \\n    \\n    {\\\\displaystyle (y,x)}\\n   is called the inverted edge of \\n  \\n    \\n      \\n        (\\n        x\\n        ,\\n        y\\n        )\\n      \\n    \\n    {\\\\displaystyle (x,y)}\\n  . Multiple edges, not allowed under the definition above, are two or more edges with both the same tail and the same head.\\nIn one more general sense of the term allowing multiple edges, a directed graph is an ordered triple \\n  \\n    \\n      \\n        G\\n        =\\n        (\\n        V\\n        ,\\n        E\\n        ,\\n        ϕ\\n        )\\n      \\n    \\n    {\\\\displaystyle G=(V,E,\\\\phi )}\\n   comprising:\\n\\n  \\n    \\n      \\n        V\\n      \\n    \\n    {\\\\displaystyle V}\\n  , a set of vertices (also called nodes or points);\\n\\n  \\n    \\n      \\n        E\\n      \\n    \\n    {\\\\displaystyle E}\\n  , a set of edges (also called directed edges, directed links, directed lines, arrows or arcs);\\n\\n  \\n    \\n      \\n        ϕ\\n        :\\n        E\\n        →\\n        \\n          {\\n          \\n            (\\n            x\\n            ,\\n            y\\n            )\\n            ∣\\n            (\\n            x\\n            ,\\n            y\\n            )\\n            ∈\\n            \\n              V\\n              \\n                2\\n              \\n            \\n            \\n            \\n              \\n                and\\n              \\n            \\n            \\n            x\\n            ≠\\n            y\\n          \\n          }\\n        \\n      \\n    \\n    {\\\\displaystyle \\\\phi :E\\\\to \\\\left\\\\{(x,y)\\\\mid (x,y)\\\\in V^{2}\\\\;{\\\\textrm {and}}\\\\;x\\\\neq y\\\\right\\\\}}\\n  , an incidence function mapping every edge to an ordered pair of vertices (that is, an edge is associated with two distinct vertices).To avoid ambiguity, this type of object may be called precisely a directed multigraph.\\nA loop is an edge that joins a vertex to itself. Directed graphs as defined in the two definitions above cannot have loops, because a loop joining a vertex \\n  \\n    \\n      \\n        x\\n      \\n    \\n    {\\\\displaystyle x}\\n   to itself is the edge (for a directed simple graph) or is incident on (for a directed multigraph) \\n  \\n    \\n      \\n        (\\n        x\\n        ,\\n        x\\n        )\\n      \\n    \\n    {\\\\displaystyle (x,x)}\\n   which is not in \\n  \\n    \\n      \\n        \\n          {\\n          \\n            (\\n            x\\n            ,\\n            y\\n            )\\n            ∣\\n            (\\n            x\\n            ,\\n            y\\n            )\\n            ∈\\n            \\n              V\\n              \\n                2\\n              \\n            \\n            \\n            \\n              \\n                and\\n              \\n            \\n            \\n            x\\n            ≠\\n            y\\n          \\n          }\\n        \\n      \\n    \\n    {\\\\displaystyle \\\\left\\\\{(x,y)\\\\mid (x,y)\\\\in V^{2}\\\\;{\\\\textrm {and}}\\\\;x\\\\neq y\\\\right\\\\}}\\n  . So to allow loops the definitions must be expanded. For directed simple graphs, the definition of \\n  \\n    \\n      \\n        E\\n      \\n    \\n    {\\\\displaystyle E}\\n   should be modified to \\n  \\n    \\n      \\n        E\\n        ⊆\\n        \\n          {\\n          \\n            (\\n            x\\n            ,\\n            y\\n            )\\n            ∣\\n            (\\n            x\\n            ,\\n            y\\n            )\\n            ∈\\n            \\n              V\\n              \\n                2\\n              \\n            \\n          \\n          }\\n        \\n      \\n    \\n    {\\\\displaystyle E\\\\subseteq \\\\left\\\\{(x,y)\\\\mid (x,y)\\\\in V^{2}\\\\right\\\\}}\\n  . For directed multigraphs, the definition of \\n  \\n    \\n      \\n        ϕ\\n      \\n    \\n    {\\\\displaystyle \\\\phi }\\n   should be modified to \\n  \\n    \\n      \\n        ϕ\\n        :\\n        E\\n        →\\n        \\n          {\\n          \\n            (\\n            x\\n            ,\\n            y\\n            )\\n            ∣\\n            (\\n            x\\n            ,\\n            y\\n            )\\n            ∈\\n            \\n              V\\n              \\n                2\\n              \\n            \\n          \\n          }\\n        \\n      \\n    \\n    {\\\\displaystyle \\\\phi :E\\\\to \\\\left\\\\{(x,y)\\\\mid (x,y)\\\\in V^{2}\\\\right\\\\}}\\n  . To avoid ambiguity, these types of objects may be called precisely a directed simple graph permitting loops and a directed multigraph permitting loops (or a quiver) respectively.\\nThe edges of a directed simple graph permitting loops \\n  \\n    \\n      \\n        G\\n      \\n    \\n    {\\\\displaystyle G}\\n   is a homogeneous relation ~ on the vertices of \\n  \\n    \\n      \\n        G\\n      \\n    \\n    {\\\\displaystyle G}\\n   that is called the adjacency relation of \\n  \\n    \\n      \\n        G\\n      \\n    \\n    {\\\\displaystyle G}\\n  . Specifically, for each edge \\n  \\n    \\n      \\n        (\\n        x\\n        ,\\n        y\\n        )\\n      \\n    \\n    {\\\\displaystyle (x,y)}\\n  , its endpoints \\n  \\n    \\n      \\n        x\\n      \\n    \\n    {\\\\displaystyle x}\\n   and \\n  \\n    \\n      \\n        y\\n      \\n    \\n    {\\\\displaystyle y}\\n   are said to be adjacent to one another, which is denoted \\n  \\n    \\n      \\n        x\\n      \\n    \\n    {\\\\displaystyle x}\\n   ~ \\n  \\n    \\n      \\n        y\\n      \\n    \\n    {\\\\displaystyle y}\\n  .\\n\\n\\n== Applications ==\\n\\nGraphs can be used to model many types of relations and processes in physical, biological, social and information systems. Many practical problems can be represented by graphs. Emphasizing their application to real-world systems, the term network is sometimes defined to mean a graph in which attributes (e.g. names) are associated with the vertices and edges, and the subject that expresses and understands the real-world systems as a network is called network science.\\n\\n\\n=== Computer science ===\\nIn computer science, graphs are used to represent networks of communication, data organization, computational devices, the flow of computation, etc. For instance, the link structure of a website can be represented by a directed graph, in which the vertices represent web pages and directed edges represent links from one page to another. A similar approach can be taken to problems in social media, travel, biology, computer chip design, mapping the progression of neuro-degenerative diseases, and many other fields. The development of algorithms to handle graphs is therefore of major interest in computer science. The transformation of graphs is often formalized and represented by graph rewrite systems. Complementary to graph transformation systems focusing on rule-based in-memory manipulation of graphs are graph databases geared towards transaction-safe, persistent storing and querying of graph-structured data.\\n\\n\\n=== Linguistics ===\\nGraph-theoretic methods, in various forms, have proven particularly useful in linguistics, since natural language often lends itself well to discrete structure. Traditionally, syntax and compositional semantics follow tree-based structures, whose expressive power lies in the principle of compositionality, modeled in a hierarchical graph. More contemporary approaches such as head-driven phrase structure grammar model the syntax of natural language using typed feature structures, which are directed acyclic graphs. \\nWithin lexical semantics, especially as applied to computers, modeling word meaning is easier when a given word is understood in terms of related words; semantic networks are therefore important in computational linguistics. Still, other methods in phonology (e.g. optimality theory, which uses lattice graphs) and morphology (e.g. finite-state morphology, using finite-state transducers) are common in the analysis of language as a graph. Indeed, the usefulness of this area of mathematics to linguistics has borne organizations such as TextGraphs, as well as various \\'Net\\' projects, such as WordNet, VerbNet, and others.\\n\\n\\n=== Physics and chemistry ===\\nGraph theory is also used to study molecules in chemistry and physics. In condensed matter physics, the three-dimensional structure of complicated simulated atomic structures can be studied quantitatively by gathering statistics on graph-theoretic properties related to the topology of the atoms. Also, \"the Feynman graphs and rules of calculation summarize quantum field theory in a form in close contact with the experimental numbers one wants to understand.\" In chemistry a graph makes a natural model for a molecule, where vertices represent atoms and edges bonds. This approach is especially used in computer processing of molecular structures, ranging from chemical editors to database searching. In statistical physics, graphs can represent local connections between interacting parts of a system, as well as the dynamics of a physical process on such\\nsystems. Similarly, in computational neuroscience graphs can be used to represent functional connections between brain areas that interact to give rise to various cognitive processes, where the vertices represent different areas of the brain and the edges represent the connections between those areas. Graph theory plays an important role in electrical modeling of electrical networks, here, weights are associated with resistance of the wire segments to obtain electrical properties of network structures. Graphs are also used to represent the micro-scale channels of porous media, in which the vertices represent the pores and the edges represent the smaller channels connecting the pores. Chemical graph theory uses the molecular graph as a means to model molecules.\\nGraphs and networks are excellent models to study and understand phase transitions and critical phenomena.\\nRemoval of nodes or edges lead to a critical transition where the network breaks into small clusters which is studied as a phase transition. This breakdown is studied via percolation theory.\\n\\n\\n=== Social sciences ===\\n\\nGraph theory is also widely used in sociology as a way, for example, to measure actors\\' prestige or to explore rumor spreading, notably through the use of social network analysis software. Under the umbrella of social networks are many different types of graphs. Acquaintanceship and friendship graphs describe whether people know each other. Influence graphs model whether certain people can influence the behavior of others. Finally, collaboration graphs model whether two people work together in a particular way, such as acting in a movie together.\\n\\n\\n=== Biology ===\\nLikewise, graph theory is useful in biology and conservation efforts where a vertex can represent regions where certain species exist (or inhabit) and the edges represent migration paths or movement between the regions. This information is important when looking at breeding patterns or tracking the spread of disease, parasites or how changes to the movement can affect other species.\\nGraphs are also commonly used in molecular biology and genomics to model and analyse datasets with complex relationships. For example, graph-based methods are often used to \\'cluster\\' cells together into cell-types in single-cell transcriptome analysis. Another use is to model genes or proteins in a pathway and study the relationships between them, such as metabolic pathways and gene regulatory networks. Evolutionary trees, ecological networks, and hierarchical clustering of gene expression patterns are also represented as graph structures. \\nGraph theory is also used in connectomics; nervous systems can be seen as a graph, where the nodes are neurons and the edges are the connections between them.\\n\\n\\n=== Mathematics ===\\nIn mathematics, graphs are useful in geometry and certain parts of topology such as knot theory. Algebraic graph theory has close links with group theory. Algebraic graph theory has been applied to many areas including dynamic systems and complexity.\\n\\n\\n=== Other topics ===\\nA graph structure can be extended by assigning a weight to each edge of the graph. Graphs with weights, or weighted graphs, are used to represent structures in which pairwise connections have some numerical values. For example, if a graph represents a road network, the weights could represent the length of each road. There may be several weights associated with each edge, including distance (as in the previous example), travel time, or monetary cost. Such weighted graphs are commonly used to program GPS\\'s, and travel-planning search engines that compare flight times and costs.\\n\\n\\n== History ==\\n\\nThe paper written by Leonhard Euler on the Seven Bridges of Königsberg and published in 1736 is regarded as the first paper in the history of graph theory. This paper, as well as the one written by Vandermonde on the knight problem, carried on with the analysis situs initiated by Leibniz. Euler\\'s formula relating the number of edges, vertices, and faces of a convex polyhedron was studied and generalized by Cauchy and L\\'Huilier, and represents the beginning of the branch of mathematics known as topology.\\nMore than one century after Euler\\'s paper on the bridges of Königsberg and while Listing was introducing the concept of topology, Cayley was led by an interest in particular analytical forms arising from differential calculus to study a particular class of graphs, the trees. This study had many implications for theoretical chemistry. The techniques he used mainly concern the enumeration of graphs with particular properties. Enumerative graph theory then arose from the results of Cayley and the fundamental results published by Pólya between 1935 and 1937. These were generalized by De Bruijn in 1959. Cayley linked his results on trees with contemporary studies of chemical composition. The fusion of ideas from mathematics with those from chemistry began what has become part of the standard terminology of graph theory.\\nIn particular, the term \"graph\" was introduced by Sylvester in a paper published in 1878 in Nature, where he draws an analogy between \"quantic invariants\" and \"co-variants\" of algebra and molecular diagrams:\\n\"[…] Every invariant and co-variant thus becomes expressible by a graph precisely identical with a Kekuléan diagram or chemicograph. […] I give a rule for the geometrical multiplication of graphs, i.e. for constructing a graph to the product of in- or co-variants whose separate graphs are given. […]\" (italics as in the original).The first textbook on graph theory was written by Dénes Kőnig, and published in 1936. Another book by Frank Harary, published in 1969, was \"considered the world over to be the definitive textbook on the subject\", and enabled mathematicians, chemists, electrical engineers and social scientists to talk to each other. Harary donated all of the royalties to fund the Pólya Prize.One of the most famous and stimulating problems in graph theory is the four color problem: \"Is it true that any map drawn in the plane may have its regions colored with four colors, in such a way that any two regions having a common border have different colors?\" This problem was first posed by Francis Guthrie in 1852 and its first written record is in a letter of De Morgan addressed to Hamilton the same year. Many incorrect proofs have been proposed, including those by Cayley, Kempe, and others. The study and the generalization of this problem by Tait, Heawood, Ramsey and Hadwiger led to the study of the colorings of the graphs embedded on surfaces with arbitrary genus. Tait\\'s reformulation generated a new class of problems, the factorization problems, particularly studied by Petersen and Kőnig. The works of Ramsey on colorations and more specially the results obtained by Turán in 1941 was at the origin of another branch of graph theory, extremal graph theory.\\nThe four color problem remained unsolved for more than a century. In 1969 Heinrich Heesch published a method for solving the problem using computers. A computer-aided proof produced in 1976 by Kenneth Appel and Wolfgang Haken makes fundamental use of the notion of \"discharging\" developed by Heesch. The proof involved checking the properties of 1,936 configurations by computer, and was not fully accepted at the time due to its complexity. A simpler proof considering only 633 configurations was given twenty years later by Robertson, Seymour, Sanders and Thomas.The autonomous development of topology from 1860 and 1930 fertilized graph theory back through the works of Jordan, Kuratowski and Whitney. Another important factor of common development of graph theory and topology came from the use of the techniques of modern algebra. The first example of such a use comes from the work of the physicist Gustav Kirchhoff, who published in 1845 his Kirchhoff\\'s circuit laws for calculating the voltage and current in electric circuits.\\nThe introduction of probabilistic methods in graph theory, especially in the study of Erdős and Rényi of the asymptotic probability of graph connectivity, gave rise to yet another branch, known as random graph theory, which has been a fruitful source of graph-theoretic results.\\n\\n\\n== Graph drawing ==\\n\\nGraphs are represented visually by drawing a point or circle for every vertex, and drawing a line between two vertices if they are connected by an edge. If the graph is directed, the direction is indicated by drawing an arrow.\\nA graph drawing should not be confused with the graph itself (the abstract, non-visual structure) as there are several ways to structure the graph drawing. All that matters is which vertices are connected to which others by how many edges and not the exact layout. In practice, it is often difficult to decide if two drawings represent the same graph. Depending on the problem domain some layouts may be better suited and easier to understand than others.\\nThe pioneering work of W. T. Tutte was very influential on the subject of graph drawing. Among other achievements, he introduced the use of linear algebraic methods to obtain graph drawings.\\nGraph drawing also can be said to encompass problems that deal with the crossing number and its various generalizations. The crossing number of a graph is the minimum number of intersections between edges that a drawing of the graph in the plane must contain. For a planar graph, the crossing number is zero by definition.\\nDrawings on surfaces other than the plane are also studied.\\n\\n\\n== Graph-theoretic data structures ==\\n\\nThere are different ways to store graphs in a computer system. The data structure used depends on both the graph structure and the algorithm used for manipulating the graph. Theoretically one can distinguish between list and matrix structures but in concrete applications the best structure is often a combination of both. List structures are often preferred for sparse graphs as they have smaller memory requirements. Matrix structures on the other hand provide faster access for some applications but can consume huge amounts of memory. Implementations of sparse matrix structures that are efficient on modern parallel computer architectures are an object of current investigation.List structures include the edge list, an array of pairs of vertices, and the adjacency list, which separately lists the neighbors of each vertex: Much like the edge list, each vertex has a list of which vertices it is adjacent to.\\nMatrix structures include the incidence matrix, a matrix of 0\\'s and 1\\'s whose rows represent vertices and whose columns represent edges, and the adjacency matrix, in which both the rows and columns are indexed by vertices. In both cases a 1 indicates two adjacent objects and a 0 indicates two non-adjacent objects. The degree matrix indicates the degree of vertices. The Laplacian matrix is a modified form of the adjacency matrix that incorporates information about the degrees of the vertices, and is useful in some calculations such as Kirchhoff\\'s theorem on the number of spanning trees of a graph.\\nThe distance matrix, like the adjacency matrix, has both its rows and columns indexed by vertices, but rather than containing a 0 or a 1 in each cell it contains the length of a shortest path between two vertices.\\n\\n\\n== Problems ==\\n\\n\\n=== Enumeration ===\\nThere is a large literature on graphical enumeration: the problem of counting graphs meeting specified conditions. Some of this work is found in Harary and Palmer (1973).\\n\\n\\n=== Subgraphs, induced subgraphs, and minors ===\\nA common problem, called the subgraph isomorphism problem, is finding a fixed graph as a subgraph in a given graph. One reason to be interested in such a question is that many graph properties are hereditary for subgraphs, which means that a graph has the property if and only if all subgraphs have it too.\\nUnfortunately, finding maximal subgraphs of a certain kind is often an NP-complete problem. For example:\\n\\nFinding the largest complete subgraph is called the clique problem (NP-complete).One special case of subgraph isomorphism is the graph isomorphism problem.  It asks whether two graphs are isomorphic. It is not known whether this problem is NP-complete, nor whether it can be solved in polynomial time.\\nA similar problem is finding induced subgraphs in a given graph. Again, some important graph properties are hereditary with respect to induced subgraphs, which means that a graph has a property if and only if all induced subgraphs also have it. Finding maximal induced subgraphs of a certain kind is also often NP-complete. For example:\\n\\nFinding the largest edgeless induced subgraph or independent set is called the independent set problem (NP-complete).Still another such problem, the minor containment problem, is to find a fixed graph as a minor of a given graph. A minor or subcontraction of a graph is any graph obtained by taking a subgraph and contracting some (or no) edges. Many graph properties are hereditary for minors, which means that a graph has a property if and only if all minors have it too. For example, Wagner\\'s Theorem states: \\n\\nA graph is planar if it contains as a minor neither the complete bipartite graph K3,3 (see the Three-cottage problem) nor the complete graph K5.A similar problem, the subdivision containment problem, is to find a fixed graph as a subdivision of a given graph. A subdivision or homeomorphism of a graph is any graph obtained by subdividing some (or no) edges. Subdivision containment is related to graph properties such as planarity. For example, Kuratowski\\'s Theorem states:   \\n\\nA graph is planar if it contains as a subdivision neither the complete bipartite graph K3,3 nor the complete graph K5.Another problem in subdivision containment is the Kelmans–Seymour conjecture:\\n\\nEvery 5-vertex-connected graph that is not planar contains a subdivision of the 5-vertex complete graph K5.Another class of problems has to do with the extent to which various species and generalizations of graphs are determined by their point-deleted subgraphs. For example:\\n\\nThe reconstruction conjecture\\n\\n\\n=== Graph coloring ===\\n\\nMany problems and theorems in graph theory have to do with various ways of coloring graphs.  Typically, one is interested in coloring a graph so that no two adjacent vertices have the same color, or with other similar restrictions.  One may also consider coloring edges (possibly so that no two coincident edges are the same color), or other variations.  Among the famous results and conjectures concerning graph coloring are the following:\\n\\nFour-color theorem\\nStrong perfect graph theorem\\nErdős–Faber–Lovász conjecture (unsolved)\\nTotal coloring conjecture, also called Behzad\\'s conjecture (unsolved)\\nList coloring conjecture (unsolved)\\nHadwiger conjecture (graph theory) (unsolved)\\n\\n\\n=== Subsumption and unification ===\\nConstraint modeling theories concern families of directed graphs related by a partial order. In these applications, graphs are ordered by specificity, meaning that more constrained graphs—which are more specific and thus contain a greater amount of information—are subsumed by those that are more general. Operations between graphs include evaluating the direction of a subsumption relationship between two graphs, if any, and computing graph unification. The unification of two argument graphs is defined as the most general graph (or the computation thereof) that is consistent with (i.e. contains all of the information in) the inputs, if such a graph exists; efficient unification algorithms are known.\\nFor constraint frameworks which are strictly compositional, graph unification is the sufficient satisfiability and combination function. Well-known applications include automatic theorem proving and modeling the elaboration of linguistic structure.\\n\\n\\n=== Route problems ===\\nHamiltonian path problem\\nMinimum spanning tree\\nRoute inspection problem (also called the \"Chinese postman problem\")\\nSeven bridges of Königsberg\\nShortest path problem\\nSteiner tree\\nThree-cottage problem\\nTraveling salesman problem (NP-hard)\\n\\n\\n=== Network flow ===\\nThere are numerous problems arising especially from applications that have to do with various notions of flows in networks, for example:\\n\\nMax flow min cut theorem\\n\\n\\n=== Visibility problems ===\\nMuseum guard problem\\n\\n\\n=== Covering problems ===\\nCovering problems in graphs may refer to various  set cover problems on subsets of vertices/subgraphs.\\n\\nDominating set problem is the special case of set cover problem where sets are the closed neighborhoods.\\nVertex cover problem is the special case of set cover problem where sets to cover are every edges.\\nThe original set cover problem, also called hitting set, can be described as a vertex cover in a hypergraph.\\n\\n\\n=== Decomposition problems ===\\nDecomposition, defined as partitioning the edge set of a graph (with as many vertices as necessary accompanying the edges of each part of the partition), has a wide variety of questions. Often, the problem is to decompose a graph into subgraphs isomorphic to a fixed graph; for instance, decomposing a complete graph into Hamiltonian cycles. Other problems specify a family of graphs into which a given graph should be decomposed, for instance, a family of cycles, or decomposing a complete graph Kn into n − 1 specified trees having, respectively, 1, 2, 3, ..., n − 1 edges.\\nSome specific decomposition problems that have been studied include:\\n\\nArboricity, a decomposition into as few forests as possible\\nCycle double cover, a decomposition into a collection of cycles covering each edge exactly twice\\nEdge coloring, a decomposition into as few matchings as possible\\nGraph factorization, a decomposition of a regular graph into regular subgraphs of given degrees\\n\\n\\n=== Graph classes ===\\nMany problems involve characterizing the members of various classes of graphs. Some examples of such questions are below:\\n\\nEnumerating the members of a class\\nCharacterizing a class in terms of forbidden substructures\\nAscertaining relationships among classes (e.g. does one property of graphs imply another)\\nFinding efficient algorithms to decide membership in a class\\nFinding representations for members of a class\\n\\n\\n== See also ==\\n\\n\\n== Notes ==\\n\\n\\n== References ==\\nBender, Edward A.; Williamson, S. Gill (2010). Lists, Decisions and Graphs. With an Introduction to Probability.\\nClaude, Claude (1958). Théorie des graphes et ses applications. Paris: Dunod. English edition, Wiley 1961; Methuen & Co, New York 1962; Russian, Moscow 1961; Spanish, Mexico 1962; Roumanian, Bucharest 1969; Chinese, Shanghai 1963; Second printing of the 1962 first English edition, Dover, New York 2001.\\nBiggs, N.; Lloyd, E.; Wilson, R. (1986). Graph Theory, 1736–1936. Oxford University Press.\\nBondy, J. A.; Murty, U. S. R. (2008). Graph Theory. Springer. ISBN 978-1-84628-969-9.\\nBollobás, Béla; Riordan, O. M. (2003). Mathematical results on scale-free random graphs in \"Handbook of Graphs and Networks\" (S. Bornholdt and H.G. Schuster (eds)) (1st ed.). Weinheim: Wiley VCH.\\nChartrand, Gary (1985). Introductory Graph Theory. Dover. ISBN 0-486-24775-9.\\nDeo, Narsingh (1974). Graph Theory with Applications to Engineering and Computer Science (PDF). Englewood, New Jersey: Prentice-Hall. ISBN 0-13-363473-6.\\nGibbons, Alan (1985). Algorithmic Graph Theory. Cambridge University Press.\\nReuven Cohen, Shlomo Havlin (2010). Complex Networks: Structure, Robustness and Function. Cambridge University Press. ISBN 9781139489270.\\nGolumbic, Martin (1980). Algorithmic Graph Theory and Perfect Graphs. Academic Press.\\nHarary, Frank (1969). Graph Theory. Reading, Massachusetts: Addison-Wesley.\\nHarary, Frank; Palmer, Edgar M. (1973). Graphical Enumeration. New York, New York: Academic Press.\\nMahadev, N. V. R.; Peled, Uri N. (1995). Threshold Graphs and Related Topics. North-Holland.\\nNewman, Mark (2010). Networks: An Introduction. Oxford University Press.\\nKepner, Jeremy; Gilbert, John (2011). Graph Algorithms in The Language of Linear Algebra. Philadelphia, Pennsylvania: SIAM. ISBN 978-0-898719-90-1.\\n\\n\\n== External links ==\\n\"Graph theory\", Encyclopedia of Mathematics, EMS Press, 2001 [1994]\\nGraph theory tutorial\\nA searchable database of small connected graphs\\nImage gallery: graphs at the Wayback Machine (archived February 6, 2006)\\nConcise, annotated list of graph theory resources for researchers\\nrocs — a graph theory IDE\\nThe Social Life of Routers — non-technical paper discussing graphs of people and computers\\nGraph Theory Software — tools to teach and learn graph theory\\nOnline books, and library resources in your library and in other libraries about graph theory\\nA list of graph algorithms with references and links to graph library implementations\\n\\n\\n=== Online textbooks ===\\nPhase Transitions in Combinatorial Optimization Problems, Section 3: Introduction to Graphs (2006) by Hartmann and Weigt\\nDigraphs: Theory Algorithms and Applications 2007 by Jorgen Bang-Jensen and Gregory Gutin\\nGraph Theory, by Reinhard Diestel', 'Mathematical logic is the study of logic within mathematics. Major subareas include model theory, proof theory, set theory, and recursion theory. Research in mathematical logic commonly addresses the mathematical properties of formal systems of logic such as their expressive or deductive power. However, it can also include uses of logic to characterize correct mathematical reasoning or to establish foundations of mathematics.\\nSince its inception, mathematical logic has both contributed to, and has been motivated by, the study of foundations of mathematics. This study began in the late 19th century with the development of axiomatic frameworks for geometry, arithmetic, and analysis. In the early 20th century it was shaped by David Hilbert\\'s program to prove the consistency of foundational theories. Results of Kurt Gödel, Gerhard Gentzen, and others provided partial resolution to the program, and clarified the issues involved in proving consistency. Work in set theory showed that almost all ordinary mathematics can be formalized in terms of sets, although there are some theorems that cannot be proven in common axiom systems for set theory. Contemporary work in the foundations of mathematics often focuses on establishing which parts of mathematics can be formalized in particular formal systems (as in reverse mathematics) rather than trying to find theories in which all of mathematics can be developed.\\n\\n\\n== Subfields and scope ==\\nThe Handbook of Mathematical Logic in 1977 makes a rough division of contemporary mathematical logic into four areas:\\n\\nset theory\\nmodel theory\\nrecursion theory, and\\nproof theory and constructive mathematics (considered as parts of a single area).Each area has a distinct focus, although many techniques and results are shared among multiple areas. The borderlines amongst these fields, and the lines separating mathematical logic and other fields of mathematics, are not always sharp.  Gödel\\'s incompleteness theorem marks not only a milestone in recursion theory and proof theory, but has also led to Löb\\'s theorem in modal logic. The method of forcing is employed in set theory, model theory, and recursion theory, as well as in the study of intuitionistic mathematics.\\nThe mathematical field of category theory uses many formal axiomatic methods, and includes the study of categorical logic, but category theory is not ordinarily considered a subfield of mathematical logic. Because of its applicability in diverse fields of mathematics, mathematicians including Saunders Mac Lane have proposed category theory as a foundational system for mathematics, independent of set theory. These foundations use toposes, which resemble generalized models of set theory that may employ classical or nonclassical logic.\\n\\n\\n== History ==\\nMathematical logic emerged in the mid-19th century as a subfield of mathematics, reflecting the confluence of two traditions: formal philosophical logic and mathematics.  \"Mathematical logic, also called  \\'logistic\\', \\'symbolic logic\\', the \\'algebra of logic\\', and, more recently, simply \\'formal logic\\', is the set of logical theories elaborated in the course of the last [nineteenth] century with the aid of an artificial notation and a rigorously deductive method.\"  Before this emergence, logic was studied with rhetoric, with calculationes, through the syllogism, and with philosophy.  The first half of the 20th century saw an explosion of fundamental results, accompanied by vigorous debate over the foundations of mathematics.\\n\\n\\n=== Early history ===\\n\\nTheories of logic were developed in many cultures in history, including China, India, Greece and the Islamic world.  Greek methods, particularly Aristotelian logic (or term logic) as found in the Organon, found wide application and acceptance in Western science and mathematics for millennia. The Stoics, especially Chrysippus, began the development of predicate logic. In 18th-century Europe, attempts to treat the operations of formal logic in a symbolic or algebraic way had been made by philosophical mathematicians including Leibniz and Lambert, but their labors remained isolated and little known.\\n\\n\\n=== 19th century ===\\nIn the middle of the nineteenth century, George Boole and then Augustus De Morgan presented systematic mathematical treatments of logic.  Their work, building on work by algebraists such as George Peacock, extended the traditional Aristotelian doctrine of logic into a sufficient framework for the study of foundations of mathematics. Charles Sanders Peirce later built upon the work of Boole to develop a logical system for relations and quantifiers, which he published in several papers from 1870 to 1885.\\nGottlob Frege presented an independent development of logic with quantifiers in his Begriffsschrift, published in 1879, a work generally considered as marking a turning point in the history of logic. Frege\\'s work remained obscure, however, until Bertrand Russell began to promote it near the turn of the century.  The two-dimensional notation Frege developed was never widely adopted and is unused in contemporary texts.\\nFrom 1890 to 1905, Ernst Schröder published Vorlesungen über die Algebra der Logik in three volumes. This work summarized and extended the work of Boole, De Morgan, and Peirce, and was a comprehensive reference to symbolic logic as it was understood at the end of the 19th century.\\n\\n\\n==== Foundational theories ====\\nConcerns that mathematics had not been built on a proper foundation led to the development of axiomatic systems for fundamental areas of mathematics such as arithmetic, analysis, and geometry.\\nIn logic, the term arithmetic refers to the theory of the natural numbers. Giuseppe Peano published a set of axioms for arithmetic that came to bear his name (Peano axioms), using a variation of the logical system of Boole and Schröder but adding quantifiers. Peano was unaware of Frege\\'s work at the time. Around the same time Richard Dedekind showed that the natural numbers are uniquely characterized by their induction properties. Dedekind proposed a different characterization, which lacked the formal logical character of Peano\\'s axioms. Dedekind\\'s work, however, proved theorems inaccessible in Peano\\'s system, including the uniqueness of the set of natural numbers (up to isomorphism) and the  recursive definitions of addition and multiplication from the successor function and mathematical induction.\\nIn the mid-19th century, flaws in Euclid\\'s axioms for geometry became known.  In addition to the independence of the parallel postulate, established by Nikolai Lobachevsky in 1826, mathematicians discovered that certain theorems taken for granted by Euclid were not in fact provable from his axioms. Among these is the theorem that a line contains at least two points, or that circles of the same radius whose centers are separated by that radius must intersect. Hilbert developed a complete set of axioms for geometry, building on previous work by Pasch.  The success in axiomatizing geometry motivated Hilbert to seek complete axiomatizations of other areas of mathematics, such as the natural numbers and the real line.  This would prove to be a major area of research in the first half of the 20th century.\\nThe 19th century saw great advances in the theory of real analysis, including theories of convergence of functions and Fourier series. Mathematicians such as Karl Weierstrass began to construct functions that stretched intuition, such as nowhere-differentiable continuous functions. Previous conceptions of a function as a rule for computation, or a smooth graph, were no longer adequate.  Weierstrass began to advocate the arithmetization of analysis, which sought to axiomatize analysis using properties of the natural numbers. The modern (ε, δ)-definition of limit and continuous functions was already developed by Bolzano in 1817, but remained relatively unknown.\\nCauchy in 1821 defined continuity in terms of infinitesimals (see Cours d\\'Analyse, page 34).  In 1858, Dedekind proposed a definition of the real numbers in terms of Dedekind cuts of rational numbers, a definition still employed in contemporary texts.Georg Cantor developed the fundamental concepts of infinite set theory. His early results developed the theory of cardinality and proved that the reals and the natural numbers have different cardinalities. Over the next twenty years, Cantor developed a theory of transfinite numbers in a series of publications. In 1891, he published a new proof of the uncountability of the real numbers that introduced the diagonal argument, and used this method to prove Cantor\\'s theorem that no set can have the same cardinality as its powerset. Cantor believed that every set could be well-ordered, but was unable to produce a proof for this result, leaving it as an open problem in 1895.\\n\\n\\n=== 20th century ===\\nIn the early decades of the 20th century, the main areas of study were set theory and formal logic. The discovery of paradoxes in informal set theory caused some to wonder whether mathematics itself is inconsistent, and to look for proofs of consistency.\\nIn 1900, Hilbert posed a famous list of 23 problems for the next century. The first two of these were to resolve the continuum hypothesis and prove the consistency of elementary arithmetic, respectively; the tenth was to produce a method that could decide whether a multivariate polynomial equation over the integers has a solution. Subsequent work to resolve these problems shaped the direction of mathematical logic, as did the effort to resolve Hilbert\\'s Entscheidungsproblem, posed in 1928. This problem asked for a procedure that would decide, given a formalized mathematical statement, whether the statement is true or false.\\n\\n\\n==== Set theory and paradoxes ====\\nErnst Zermelo gave a proof that every set could be well-ordered, a result Georg Cantor had been unable to obtain. To achieve the proof, Zermelo introduced the axiom of choice, which drew heated debate and research among mathematicians and the pioneers of set theory. The immediate criticism of the method led Zermelo to publish a second exposition of his result, directly addressing criticisms of his proof. This paper led to the general acceptance of the axiom of choice in the mathematics community.\\nSkepticism about the axiom of choice was reinforced by recently discovered paradoxes in naive set theory. Cesare Burali-Forti was the first to state a paradox: the Burali-Forti paradox shows that the collection of all ordinal numbers cannot form a set. Very soon thereafter, Bertrand Russell discovered Russell\\'s paradox in 1901, and Jules Richard  discovered Richard\\'s paradox.Zermelo provided the first set of axioms for set theory. These axioms, together with the additional axiom of replacement proposed by Abraham Fraenkel, are now called Zermelo–Fraenkel set theory (ZF). Zermelo\\'s axioms incorporated the principle of limitation of size to avoid Russell\\'s paradox.\\nIn 1910, the first volume of Principia Mathematica by Russell and Alfred North Whitehead was published. This seminal work developed the theory of functions and cardinality in a completely formal framework of type theory, which Russell and Whitehead developed in an effort to avoid the paradoxes. Principia Mathematica is considered one of the most influential works of the 20th century, although the framework of type theory did not prove popular as a foundational theory for mathematics.Fraenkel proved that the axiom of choice cannot be proved from the axioms of Zermelo\\'s set theory with urelements. Later work by Paul Cohen showed that the addition of urelements is not needed, and the axiom of choice is unprovable in ZF. Cohen\\'s proof developed the method of forcing, which is now an important tool for establishing independence results in set theory.\\n\\n\\n==== Symbolic logic ====\\nLeopold Löwenheim and Thoralf Skolem obtained the Löwenheim–Skolem theorem, which says that first-order logic cannot control the cardinalities of infinite structures. Skolem realized that this theorem would apply to first-order formalizations of set theory, and that it implies any such formalization has a countable model. This counterintuitive fact became known as Skolem\\'s paradox.\\nIn his doctoral thesis, Kurt Gödel proved the completeness theorem, which establishes a correspondence between syntax and semantics in first-order logic. Gödel used the completeness theorem to prove the compactness theorem, demonstrating the finitary nature of first-order logical consequence. These results helped establish first-order logic as the dominant logic used by mathematicians.\\nIn 1931, Gödel published On Formally Undecidable Propositions of Principia Mathematica and Related Systems, which proved the incompleteness (in a different meaning of the word) of all sufficiently strong, effective first-order theories. This result, known as Gödel\\'s incompleteness theorem, establishes severe limitations on axiomatic foundations for mathematics, striking a strong blow to Hilbert\\'s program. It showed the impossibility of providing a consistency proof of arithmetic within any formal theory of arithmetic.  Hilbert, however, did not acknowledge the importance of the incompleteness theorem for some time.Gödel\\'s theorem shows that a consistency proof of any sufficiently strong, effective axiom system cannot be obtained in the system itself, if the system is consistent, nor in any weaker system. This leaves open the possibility of consistency proofs that cannot be formalized within the system they consider. Gentzen proved the consistency of arithmetic using a finitistic system together with a principle of transfinite induction. Gentzen\\'s result introduced the ideas of cut elimination and proof-theoretic ordinals, which became key tools in proof theory.  Gödel gave a different consistency proof, which reduces the consistency of classical arithmetic to that of intuitionistic arithmetic in higher types.The first textbook on symbolic logic for the layman was written by Lewis Carroll, author of Alice in Wonderland, in 1896.\\n\\n\\n==== Beginnings of the other branches ====\\nAlfred Tarski developed the basics of model theory.\\nBeginning in 1935, a group of prominent mathematicians collaborated under the pseudonym Nicolas Bourbaki to publish Éléments de mathématique, a series of encyclopedic mathematics texts. These texts, written in an austere and axiomatic style, emphasized rigorous presentation and set-theoretic foundations. Terminology coined by these texts, such as the words bijection, injection, and surjection, and the set-theoretic foundations the texts employed, were widely adopted throughout mathematics.\\nThe study of computability came to be known as recursion theory or computability theory, because early formalizations by Gödel and Kleene relied on recursive definitions of functions. When these definitions were shown equivalent to Turing\\'s formalization involving Turing machines, it became clear that a new concept – the computable function – had been discovered, and that this definition was robust enough to admit numerous independent characterizations. In his work on the incompleteness theorems in 1931, Gödel lacked a rigorous concept of an effective formal system; he immediately realized that the new definitions of computability could be used for this purpose, allowing him to state the incompleteness theorems in generality that could only be implied in the original paper.\\nNumerous results in recursion theory were obtained in the 1940s by Stephen Cole Kleene and Emil Leon Post. Kleene introduced the concepts of relative computability, foreshadowed by Turing, and the arithmetical hierarchy. Kleene later generalized recursion theory to higher-order functionals. Kleene and Georg Kreisel studied formal versions of intuitionistic mathematics, particularly in the context of proof theory.\\n\\n\\n== Formal logical systems ==\\nAt its core, mathematical logic deals with mathematical concepts expressed using formal logical systems. These systems, though they differ in many details, share the common property of considering only expressions in a fixed formal language.  The systems of propositional logic and first-order logic are the most widely studied today, because of their applicability to foundations of mathematics and because of their desirable proof-theoretic properties.  Stronger classical logics such as second-order logic or infinitary logic are also studied, along with Non-classical logics such as intuitionistic logic.\\n\\n\\n=== First-order logic ===\\n\\nFirst-order logic is a particular formal system of logic. Its syntax involves only finite expressions as well-formed formulas, while its semantics are characterized by the limitation of all quantifiers to a fixed domain of discourse.\\nEarly results from formal logic established limitations of first-order logic. The Löwenheim–Skolem theorem (1919) showed that if a set of sentences in a countable first-order language has an infinite model then it has at least one model of each infinite cardinality. This shows that it is impossible for a set of first-order axioms to characterize the natural numbers, the real numbers, or any other infinite structure up to isomorphism. As the goal of early foundational studies was to produce axiomatic theories for all parts of mathematics, this limitation was particularly stark.\\nGödel\\'s completeness theorem established the equivalence between semantic and syntactic definitions of logical consequence in first-order logic. It shows that if a particular sentence is true in every model that satisfies a particular set of axioms, then there must be a finite deduction of the sentence from the axioms. The compactness theorem first appeared as a lemma in Gödel\\'s proof of the completeness theorem, and it took many years before logicians grasped its significance and began to apply it routinely. It says that a set of sentences has a model if and only if every finite subset has a model, or in other words that an inconsistent set of formulas must have a finite inconsistent subset. The completeness and compactness theorems allow for sophisticated analysis of logical consequence in first-order logic and the development of model theory, and they are a key reason for the prominence of first-order logic in mathematics.\\nGödel\\'s incompleteness theorems establish additional limits on first-order axiomatizations. The first incompleteness theorem states that for any consistent, effectively given (defined below) logical system that is capable of interpreting arithmetic, there exists a statement that is true (in the sense that it holds for the natural numbers) but not provable within that logical system (and which indeed may fail in some non-standard models of arithmetic which may be consistent with the logical system). For example, in every logical system capable of expressing the Peano axioms, the Gödel sentence holds for the natural numbers but cannot be proved.\\nHere a logical system is said to be effectively given if it is possible to decide, given any formula in the language of the system, whether the formula is an axiom, and one which can express the Peano axioms is called \"sufficiently strong.\" When applied to first-order logic, the first incompleteness theorem implies that any sufficiently strong, consistent, effective first-order theory has models that are not elementarily equivalent, a stronger limitation than the one established by the Löwenheim–Skolem theorem. The second incompleteness theorem states that no sufficiently strong, consistent, effective axiom system for arithmetic can prove its own consistency, which has been interpreted to show that Hilbert\\'s program cannot be reached.\\n\\n\\n=== Other classical logics ===\\nMany logics besides first-order logic are studied.  These include infinitary logics, which allow for formulas to provide an infinite amount of information, and higher-order logics, which include a portion of set theory directly in their semantics.\\nThe most well studied infinitary logic is \\n  \\n    \\n      \\n        \\n          L\\n          \\n            \\n              ω\\n              \\n                1\\n              \\n            \\n            ,\\n            ω\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle L_{\\\\omega _{1},\\\\omega }}\\n  . In this logic, quantifiers may only be nested to finite depths, as in first-order logic, but formulas may have finite or countably infinite conjunctions and disjunctions within them. Thus, for example, it is possible to say that an object is a whole number using a formula of \\n  \\n    \\n      \\n        \\n          L\\n          \\n            \\n              ω\\n              \\n                1\\n              \\n            \\n            ,\\n            ω\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle L_{\\\\omega _{1},\\\\omega }}\\n   such as\\n\\n  \\n    \\n      \\n        (\\n        x\\n        =\\n        0\\n        )\\n        ∨\\n        (\\n        x\\n        =\\n        1\\n        )\\n        ∨\\n        (\\n        x\\n        =\\n        2\\n        )\\n        ∨\\n        ⋯\\n        .\\n      \\n    \\n    {\\\\displaystyle (x=0)\\\\lor (x=1)\\\\lor (x=2)\\\\lor \\\\cdots .}\\n  Higher-order logics allow for quantification not only of elements of the domain of discourse, but subsets of the domain of discourse, sets of such subsets, and other objects of higher type. The semantics are defined so that, rather than having a separate domain for each higher-type quantifier to range over, the quantifiers instead range over all objects of the appropriate type.  The logics studied before the development of first-order logic, for example Frege\\'s logic, had similar set-theoretic aspects. Although higher-order logics are more expressive, allowing complete axiomatizations of structures such as the natural numbers, they do not satisfy analogues of the completeness and compactness theorems from first-order logic, and are thus less amenable to proof-theoretic analysis.\\nAnother type of logics are fixed-point logics that allow inductive definitions, like one writes for primitive recursive functions.\\nOne can formally define an extension of first-order logic — a notion which encompasses all logics in this section because they behave like first-order logic in certain fundamental ways, but does not encompass all logics in general, e.g. it does not encompass intuitionistic, modal or fuzzy logic.\\nLindström\\'s theorem implies that the only extension of first-order logic satisfying both the compactness theorem and the downward Löwenheim–Skolem theorem is first-order logic.\\n\\n\\n=== Nonclassical and modal logic ===\\n\\nModal logics include additional modal operators, such as an operator which states that a particular formula is not only true, but necessarily true. Although modal logic is not often used to axiomatize mathematics, it has been used to study the properties of first-order provability and set-theoretic forcing.Intuitionistic logic was developed by Heyting to study Brouwer\\'s program of intuitionism, in which Brouwer himself avoided formalization. Intuitionistic logic specifically does not include the law of the excluded middle, which states that each sentence is either true or its negation is true.  Kleene\\'s work with the proof theory of intuitionistic logic showed that constructive information can be recovered from intuitionistic proofs. For example, any provably total function in intuitionistic arithmetic is computable; this is not true in classical theories of arithmetic such as Peano arithmetic.\\n\\n\\n=== Algebraic logic ===\\nAlgebraic logic uses the methods of abstract algebra to study the semantics of formal logics. A fundamental example is the use of Boolean algebras to represent truth values in classical propositional logic, and the use of Heyting algebras to represent truth values in intuitionistic propositional logic. Stronger logics, such as first-order logic and higher-order logic, are studied using more complicated algebraic structures such as cylindric algebras.\\n\\n\\n== Set theory ==\\n\\nSet theory is the study of sets, which are abstract collections of objects. Many of the basic notions, such as ordinal and cardinal numbers, were developed informally by Cantor before formal axiomatizations of set theory were developed. The first such axiomatization, due to Zermelo, was extended slightly to become Zermelo–Fraenkel set theory (ZF), which is now the most widely used foundational theory for mathematics.\\nOther formalizations of set theory have been proposed, including von Neumann–Bernays–Gödel set theory (NBG), Morse–Kelley set theory (MK), and New Foundations (NF).  Of these, ZF, NBG, and MK are similar in describing a cumulative hierarchy of sets. New Foundations takes a different approach; it allows objects such as the set of all sets at the cost of restrictions on its set-existence axioms. The system of Kripke–Platek set theory is closely related to generalized recursion theory.\\nTwo famous statements in set theory are the axiom of choice and the continuum hypothesis. The axiom of choice, first stated by Zermelo, was proved independent of ZF by Fraenkel, but has come to be widely accepted by mathematicians.  It states that given a collection of nonempty sets there is a single set C that contains exactly one element from each set in the collection. The set C is said to \"choose\" one element from each set in the collection. While the ability to make such a choice is considered obvious by some, since each set in the collection is nonempty, the lack of a general, concrete rule by which the choice can be made renders the axiom nonconstructive. Stefan Banach and Alfred Tarski showed that the axiom of choice can be used to decompose a solid ball into a finite number of pieces which can then be rearranged, with no scaling, to make two solid balls of the original size. This theorem, known as the Banach–Tarski paradox, is one of many counterintuitive results of the axiom of choice.\\nThe continuum hypothesis, first proposed as a conjecture by Cantor, was listed by David Hilbert as one of his 23 problems in 1900. Gödel showed that the continuum hypothesis cannot be disproven from the axioms of Zermelo–Fraenkel set theory (with or without the axiom of choice), by developing the constructible universe of set theory in which the continuum hypothesis must hold. In 1963, Paul Cohen showed that the continuum hypothesis cannot be proven from the axioms of Zermelo–Fraenkel set theory. This independence result did not completely settle Hilbert\\'s question, however, as it is possible that new axioms for set theory could resolve the hypothesis. Recent work along these lines has been conducted by W. Hugh Woodin, although its importance is not yet clear.Contemporary research in set theory includes the study of large cardinals and determinacy.  Large cardinals are cardinal numbers with particular properties so strong that the existence of such cardinals cannot be proved in ZFC. The existence of the smallest large cardinal typically studied, an inaccessible cardinal, already implies the consistency of ZFC.  Despite the fact that large cardinals have extremely high cardinality, their existence has many ramifications for the structure of the real line.  Determinacy refers to the possible existence of winning strategies for certain two-player games (the games are said to be determined). The existence of these strategies implies structural properties of the real line and other Polish spaces.\\n\\n\\n== Model theory ==\\n\\nModel theory studies the models of various formal theories.  Here a theory is a set of formulas in a particular formal logic and signature, while a model is a structure that gives a concrete interpretation of the theory. Model theory is closely related to universal algebra and algebraic geometry, although the methods of model theory focus more on logical considerations than those fields.\\nThe set of all models of a particular theory is called an elementary class; classical model theory seeks to determine the properties of models in a particular elementary class, or determine whether certain classes of structures form elementary classes.\\nThe method of quantifier elimination can be used to show that definable sets in particular theories cannot be too complicated. Tarski established quantifier elimination for real-closed fields, a result which also shows the theory of the field of real numbers is decidable. He also noted that his methods were equally applicable to algebraically closed fields of arbitrary characteristic. A modern subfield developing from this is concerned with o-minimal structures.\\nMorley\\'s categoricity theorem, proved by Michael D. Morley, states that if a first-order theory in a countable language is categorical in some uncountable cardinality, i.e. all models of this cardinality are isomorphic, then it is categorical in all uncountable cardinalities.\\nA trivial consequence of the continuum hypothesis is that a complete theory with less than continuum many nonisomorphic countable models can have only countably many. Vaught\\'s conjecture, named after Robert Lawson Vaught, says that this is true even independently of the continuum hypothesis.  Many special cases of this conjecture have been established.\\n\\n\\n== Recursion theory ==\\n\\nRecursion theory, also called computability theory, studies the properties of computable functions and the Turing degrees, which divide the uncomputable functions into sets that have the same level of uncomputability.  Recursion theory also includes the study of generalized computability and definability.  Recursion theory grew from the work of Rózsa Péter, Alonzo Church and Alan Turing in the 1930s, which was greatly extended by Kleene and Post in the 1940s.Classical recursion theory focuses on the computability of functions from the natural numbers to the natural numbers. The fundamental results establish a robust, canonical class of computable functions with numerous independent, equivalent characterizations using Turing machines, λ calculus, and other systems.  More advanced results concern the structure of the Turing degrees and the lattice of recursively enumerable sets.\\nGeneralized recursion theory extends the ideas of recursion theory to computations that are no longer necessarily finite. It includes the study of computability in higher types as well as areas such as hyperarithmetical theory and α-recursion theory.\\nContemporary research in recursion theory includes the study of applications such as algorithmic randomness, computable model theory, and reverse mathematics, as well as new results in pure recursion theory.\\n\\n\\n=== Algorithmically unsolvable problems ===\\nAn important subfield of recursion theory studies algorithmic unsolvability; a decision problem or function problem is algorithmically unsolvable if there is no possible computable algorithm that returns the correct answer for all legal inputs to the problem. The first results about unsolvability, obtained independently by Church and Turing in 1936, showed that the Entscheidungsproblem is algorithmically unsolvable. Turing proved this by establishing the unsolvability of the halting problem, a result with far-ranging implications in both recursion theory and computer science.\\nThere are many known examples of undecidable problems from ordinary mathematics. The word problem for groups was proved algorithmically unsolvable by Pyotr Novikov in 1955 and independently by W. Boone in 1959.  The busy beaver problem, developed by Tibor Radó in 1962, is another well-known example.\\nHilbert\\'s tenth problem asked for an algorithm to determine whether a multivariate polynomial equation with integer coefficients has a solution in the integers. Partial progress was made by Julia Robinson, Martin Davis and Hilary Putnam. The algorithmic unsolvability of the problem was proved by Yuri Matiyasevich in 1970.\\n\\n\\n== Proof theory and constructive mathematics ==\\n\\nProof theory is the study of formal proofs in various logical deduction systems. These proofs are represented as formal mathematical objects, facilitating their analysis by mathematical techniques.  Several deduction systems are commonly considered, including Hilbert-style deduction systems, systems of natural deduction, and the sequent calculus developed by Gentzen.\\nThe study of constructive mathematics, in the context of mathematical logic, includes the study of systems in non-classical logic such as intuitionistic logic, as well as the study of predicative systems.  An early proponent of predicativism was Hermann Weyl, who showed it is possible to develop a large part of real analysis using only predicative methods.Because proofs are entirely finitary, whereas truth in a structure is not, it is common for work in constructive mathematics to emphasize provability.   The relationship between provability in classical (or nonconstructive) systems and provability in intuitionistic (or constructive, respectively) systems is of particular interest.  Results such as the Gödel–Gentzen negative translation show that it is possible to embed (or translate) classical logic into intuitionistic logic, allowing some properties about intuitionistic proofs to be transferred back to classical proofs.\\nRecent developments in proof theory include the study of proof mining by Ulrich Kohlenbach and the study of proof-theoretic ordinals by Michael Rathjen.\\n\\n\\n== Applications ==\\n\"Mathematical logic has been successfully applied not only to mathematics and its foundations (G. Frege, B. Russell, D. Hilbert, P. Bernays, H. Scholz, R. Carnap, S. Lesniewski, T. Skolem), but also to physics (R. Carnap, A. Dittrich, B. Russell, C. E. Shannon, A. N. Whitehead, H. Reichenbach, P. Fevrier), to biology (J. H. Woodger, A. Tarski), to psychology (F. B. Fitch, C. G. Hempel), to law and morals (K. Menger, U. Klug, P. Oppenheim), to economics (J. Neumann, O. Morgenstern), to practical questions (E. C. Berkeley, E. Stamm), and even to metaphysics (J. [Jan] Salamucha, H. Scholz, J. M. Bochenski).  Its applications to the history of logic have proven extremely fruitful (J. Lukasiewicz, H. Scholz, B. Mates, A. Becker, E. Moody, J. Salamucha, K. Duerr, Z. Jordan, P. Boehner, J. M. Bochenski, S. [Stanislaw] T. Schayer, D. Ingalls).\" \"Applications have also been made to theology (F. Drewnowski, J. Salamucha, I. Thomas).\"\\n\\n\\n== Connections with computer science ==\\n\\nThe study of computability theory in computer science is closely related to the study of computability in mathematical logic.  There is a difference of emphasis, however.  Computer scientists often focus on concrete programming languages and feasible computability, while researchers in mathematical logic often focus on computability as a theoretical concept and on noncomputability.\\nThe theory of semantics of programming languages is related to model theory, as is program verification (in particular, model checking). The Curry–Howard correspondence between proofs and programs relates to proof theory, especially intuitionistic logic. Formal calculi such as the lambda calculus and combinatory logic are now studied as idealized programming languages.\\nComputer science also contributes to mathematics by developing techniques for the automatic checking or even finding of proofs, such as automated theorem proving and logic programming.\\nDescriptive complexity theory relates logics to computational complexity. The first significant result in this area, Fagin\\'s theorem (1974) established that NP is precisely the set of languages expressible by sentences of existential second-order logic.\\n\\n\\n== Foundations of mathematics ==\\n\\nIn the 19th century, mathematicians became aware of logical gaps and inconsistencies in their field. It was shown that Euclid\\'s axioms for geometry, which had been taught for centuries as an example of the axiomatic method, were incomplete. The use of infinitesimals, and the very definition of function, came into question in analysis, as pathological examples such as Weierstrass\\' nowhere-differentiable continuous function were discovered.\\nCantor\\'s study of arbitrary infinite sets also drew criticism. Leopold Kronecker famously stated \"God made the integers; all else is the work of man,\" endorsing a return to the study of finite, concrete objects in mathematics. Although Kronecker\\'s argument was carried forward by constructivists in the 20th century, the mathematical community as a whole rejected them. David Hilbert argued in favor of the study of the infinite, saying \"No one shall expel us from the Paradise that Cantor has created.\"\\nMathematicians began to search for axiom systems that could be used to formalize large parts of mathematics. In addition to removing ambiguity from previously naive terms such as function, it was hoped that this axiomatization would allow for consistency proofs.  In the 19th century, the main method of proving the consistency of a set of axioms was to provide a model for it. Thus, for example, non-Euclidean geometry can be proved consistent by defining point to mean a point on a fixed sphere and line to mean a great circle on the sphere. The resulting structure, a model of elliptic geometry, satisfies the axioms of plane geometry except the parallel postulate.\\nWith the development of formal logic, Hilbert asked whether it would be possible to prove that an axiom system is consistent by analyzing the structure of possible proofs in the system, and showing through this analysis that it is impossible to prove a contradiction. This idea led to the study of proof theory. Moreover, Hilbert proposed that the analysis should be entirely concrete, using the term finitary to refer to the methods he would allow but not precisely defining them. This project, known as Hilbert\\'s program, was seriously affected by Gödel\\'s incompleteness theorems, which show that the consistency of formal theories of arithmetic cannot be established using methods formalizable in those theories. Gentzen showed that it is possible to produce a proof of the consistency of arithmetic in a finitary system augmented with axioms of transfinite induction, and the techniques he developed to do so were seminal in proof theory.\\nA second thread in the history of foundations of mathematics involves nonclassical logics and constructive mathematics. The study of constructive mathematics includes many different programs with various definitions of constructive. At the most accommodating end, proofs in ZF set theory that do not use the axiom of choice are called constructive by many mathematicians. More limited versions of constructivism limit themselves to natural numbers, number-theoretic functions, and sets of natural numbers (which can be used to represent real numbers, facilitating the study of mathematical analysis). A common idea is that a concrete means of computing the values of the function must be known before the function itself can be said to exist. \\nIn the early 20th century, Luitzen Egbertus Jan Brouwer founded intuitionism as a part of philosophy of mathematics . This philosophy, poorly understood at first, stated that in order for a mathematical statement to be true to a mathematician, that person must be able to intuit the statement, to not only believe its truth but understand the reason for its truth. A consequence of this definition of truth was the rejection of the law of the excluded middle, for there are statements that, according to Brouwer, could not be claimed to be true while their negations also could not be claimed true.  Brouwer\\'s philosophy was influential, and the cause of bitter disputes among prominent mathematicians. Later, Kleene and Kreisel would study formalized versions of intuitionistic logic (Brouwer rejected formalization, and presented his work in unformalized natural language). With the advent of the BHK interpretation and Kripke models, intuitionism became easier to reconcile with classical mathematics.\\n\\n\\n== See also ==\\n\\nArgument\\nInformal logic\\nKnowledge representation and reasoning\\nLogic\\nList of computability and complexity topics\\nList of first-order theories\\nList of logic symbols\\nList of mathematical logic topics\\nList of set theory topics\\nMereology\\nPropositional calculus\\nWell-formed formula\\n\\n\\n== Notes ==\\n\\n\\n== References ==\\n\\n\\n=== Undergraduate texts ===\\nWalicki, Michał (2011). Introduction to Mathematical Logic. Singapore: World Scientific Publishing. ISBN 9789814343879.\\nBoolos, George; Burgess, John; Jeffrey, Richard (2002). Computability and Logic (4th ed.). Cambridge University Press. ISBN 9780521007580.\\nCrossley, J.N.; Ash, C.J.; Brickhill, C.J.; Stillwell, J.C.; Williams, N.H. (1972). What is mathematical logic?. London, Oxford, New York City: Oxford University Press. ISBN 9780198880875. Zbl 0251.02001.\\nEnderton, Herbert (2001). A mathematical introduction to logic (2nd ed.). Boston MA: Academic Press. ISBN 978-0-12-238452-3.\\nFisher, Alec (1982). Formal Number Theory and Computability: A Workbook. (suitable as a first course for independent study) (1st ed.). Oxford University Press. ISBN 978-0-19-853188-3.\\nHamilton, A.G. (1988). Logic for Mathematicians (2nd ed.). Cambridge University Press. ISBN 978-0-521-36865-0.\\nEbbinghaus, H.-D.; Flum, J.; Thomas, W. (1994). Mathematical Logic (2nd ed.). New York City: Springer. ISBN 9780387942582.\\nKatz, Robert (1964). Axiomatic Analysis. Boston MA: D. C. Heath and Company.\\nMendelson, Elliott (1997). Introduction to Mathematical Logic (4th ed.). London: Chapman & Hall. ISBN 978-0-412-80830-2.\\nRautenberg, Wolfgang (2010). A Concise Introduction to Mathematical Logic (3rd ed.). New York City: Springer. doi:10.1007/978-1-4419-1221-3. ISBN 9781441912206.\\nSchwichtenberg, Helmut (2003–2004). Mathematical Logic (PDF). Munich: Mathematisches Institut der Universität München. Retrieved 2016-02-24.\\nShawn Hedman, A first course in logic: an introduction to model theory, proof theory, computability, and complexity, Oxford University Press, 2004, ISBN 0-19-852981-3. Covers logics in close relation with computability theory and complexity theory\\nvan Dalen, Dirk (2013). Logic and Structure. Universitext. Berlin: Springer. doi:10.1007/978-1-4471-4558-5. ISBN 978-1-4471-4557-8.\\n\\n\\n=== Graduate texts ===\\nAndrews, Peter B. (2002). An Introduction to Mathematical Logic and Type Theory: To Truth Through Proof (2nd ed.). Boston: Kluwer Academic Publishers. ISBN 978-1-4020-0763-7.\\nBarwise, Jon, ed. (1989). Handbook of Mathematical Logic. Studies in Logic and the Foundations of Mathematics. Amsterdam: Elsevier. ISBN 9780444863881.\\nHodges, Wilfrid (1997). A shorter model theory. Cambridge University Press. ISBN 9780521587136.\\nJech, Thomas (2003). Set Theory: Millennium Edition. Springer Monographs in Mathematics. Berlin, New York: Springer. ISBN 9783540440857.\\nKleene, Stephen Cole.(1952), Introduction to Metamathematics. New York: Van Nostrand. (Ishi Press: 2009 reprint).\\nKleene, Stephen Cole. (1967),  Mathematical Logic. John Wiley. Dover reprint, 2002. ISBN 0-486-42533-9.\\nShoenfield, Joseph R. (2001) [1967]. Mathematical Logic (2nd ed.). A K Peters. ISBN 9781568811352.\\nTroelstra, Anne Sjerp; Schwichtenberg, Helmut (2000). Basic Proof Theory. Cambridge Tracts in Theoretical Computer Science (2nd ed.). Cambridge University Press. ISBN 978-0-521-77911-1.\\n\\n\\n=== Research papers, monographs, texts, and surveys ===\\nAugusto, Luis M. (2017). Logical consequences. Theory and applications: An introduction. London: College Publications. ISBN 978-1-84890-236-7.\\nBoehner, Philotheus (1950). Medieval Logic. Manchester.\\nCohen, Paul J. (1966). Set Theory and the Continuum Hypothesis. Menlo Park CA: W. A. Benjamin.\\nCohen, Paul J. (2008) [1966]. Set theory and the continuum hypothesis. Mineola NY: Dover Publications. ISBN 9780486469218.\\nJ.D. Sneed, The Logical Structure of Mathematical Physics. Reidel, Dordrecht, 1971 (revised edition 1979).\\nDavis, Martin (1973). \"Hilbert\\'s tenth problem is unsolvable\". The American Mathematical Monthly. 80 (3): 233–269. doi:10.2307/2318447. JSTOR 2318447. Reprinted as an appendix in Martin Davis (1985). Computability and Unsolvability. Dover. ISBN 9780486614717.\\nFelscher, Walter (2000). \"Bolzano, Cauchy, Epsilon, Delta\". The American Mathematical Monthly. 107 (9): 844–862. doi:10.2307/2695743. JSTOR 2695743.\\nFerreirós, José (2001). \"The Road to Modern Logic-An Interpretation\" (PDF). Bulletin of Symbolic Logic. 7 (4): 441–484. doi:10.2307/2687794. hdl:11441/38373. JSTOR 2687794. S2CID 43258676.\\nHamkins, Joel David; Löwe, Benedikt (2007). \"The modal logic of forcing\". Transactions of the American Mathematical Society. 360 (4): 1793–1818. arXiv:math/0509616. doi:10.1090/s0002-9947-07-04297-3. S2CID 14724471.\\nKatz, Victor J. (1998). A History of Mathematics. Addison–Wesley. ISBN 9780321016188.\\nMorley, Michael (1965). \"Categoricity in Power\". Transactions of the American Mathematical Society. 114 (2): 514–538. doi:10.2307/1994188. JSTOR 1994188.\\nSoare, Robert I. (1996). \"Computability and recursion\". Bulletin of Symbolic Logic. 2 (3): 284–321. CiteSeerX 10.1.1.35.5803. doi:10.2307/420992. JSTOR 420992.\\nSolovay, Robert M. (1976). \"Provability Interpretations of Modal Logic\". Israel Journal of Mathematics. 25 (3–4): 287–304. doi:10.1007/BF02757006. S2CID 121226261.\\nWoodin, W. Hugh (2001). \"The Continuum Hypothesis, Part I\" (PDF). Notices of the American Mathematical Society. 48 (6).\\n\\n\\n=== Classical papers, texts, and collections ===\\nBanach, Stefan; Tarski, Alfred (1924). \"Sur la décomposition des ensembles de points en parties respectivement congruentes\" (PDF). Fundamenta Mathematicae (in French). 6: 244–277. doi:10.4064/fm-6-1-244-277.Bochenski, Jozef Maria, ed. (1959). A Precis of Mathematical Logic. Synthese Library, Vol. 1. Translated by Otto Bird. Dordrecht: Springer. doi:10.1007/978-94-017-0592-9. ISBN 9789048183296.\\n\\nBurali-Forti, Cesare (1897). A question on transfinite numbers. Reprinted in van Heijenoort 1976, pp. 104–111Cantor, Georg (1874). \"Ueber eine Eigenschaft des Inbegriffes aller reellen algebraischen Zahlen\" (PDF). Journal für die Reine und Angewandte Mathematik. 1874 (77): 258–262. doi:10.1515/crll.1874.77.258. S2CID 199545885.\\nCarroll, Lewis (1896). Symbolic Logic. Kessinger Legacy Reprints. ISBN 9781163444955.\\n\\nDedekind, Richard (1872). Stetigkeit und irrationale Zahlen (in German). English translation as: \"Consistency and irrational numbers\".\\nDedekind, Richard (1888). Was sind und was sollen die Zahlen?. Two English translations:\\n1963 (1901). Essays on the Theory of Numbers. Beman, W. W., ed. and trans. Dover.\\n1996. In From Kant to Hilbert: A Source Book in the Foundations of Mathematics, 2 vols, Ewald, William B., ed., Oxford University Press: 787–832.\\nFraenkel, Abraham A. (1922). \"Der Begriff \\'definit\\' und die Unabhängigkeit des Auswahlsaxioms\". Sitzungsberichte der Preussischen Akademie der Wissenschaften, Physikalisch-mathematische Klasse (in German). pp. 253–257. Reprinted in English translation as \"The notion of \\'definite\\' and the independence of the axiom of choice\" in van Heijenoort 1976, pp. 284–289.\\nFrege, Gottlob (1879), Begriffsschrift, eine der arithmetischen nachgebildete Formelsprache des reinen Denkens. Halle a. S.: Louis Nebert. Translation: Concept Script, a formal language of pure thought modelled upon that of arithmetic, by S. Bauer-Mengelberg in van Heijenoort 1976.\\nFrege, Gottlob (1884), Die Grundlagen der Arithmetik: eine logisch-mathematische Untersuchung über den Begriff der Zahl. Breslau: W. Koebner. Translation: J. L. Austin, 1974. The Foundations of Arithmetic: A logico-mathematical enquiry into the concept of number, 2nd ed. Blackwell.\\nGentzen, Gerhard (1936). \"Die Widerspruchsfreiheit der reinen Zahlentheorie\". Mathematische Annalen. 112: 132–213. doi:10.1007/BF01565428. S2CID 122719892. Reprinted in English translation in Gentzen\\'s Collected works, M. E. Szabo, ed., North-Holland, Amsterdam, 1969.\\nGödel, Kurt (1929). Über die Vollständigkeit des Logikkalküls [Completeness of the logical calculus]. doctoral dissertation. University Of Vienna.\\nGödel, Kurt (1930). \"Die Vollständigkeit der Axiome des logischen Funktionen-kalküls\" [The completeness of the axioms of the calculus of logical functions]. Monatshefte für Mathematik und Physik (in German). 37: 349–360. doi:10.1007/BF01696781. S2CID 123343522.\\nGödel, Kurt (1931). \"Über formal unentscheidbare Sätze der Principia Mathematica und verwandter Systeme I\" [On Formally Undecidable Propositions of Principia Mathematica and Related Systems]. Monatshefte für Mathematik und Physik (in German). 38 (1): 173–198. doi:10.1007/BF01700692. S2CID 197663120.\\nGödel, Kurt (1958). \"Über eine bisher noch nicht benützte Erweiterung des finiten Standpunktes\". Dialectica (in German). 12 (3–4): 280–287. doi:10.1111/j.1746-8361.1958.tb01464.x. Reprinted in English translation in Gödel\\'s Collected Works, vol II, Solomon Feferman et al., eds. Oxford University Press, 1993.\\nvan Heijenoort, Jean, ed. (1976) [1967]. From Frege to Gödel: A Source Book in Mathematical Logic, 1879–1931 (3rd ed.). Cambridge MA: Harvard University Press. ISBN 9780674324497. (pbk.).\\nHilbert, David (1899). Grundlagen der Geometrie (in German). Leipzig: Teubner. English 1902 edition (The Foundations of Geometry) republished 1980, Open Court, Chicago.\\nHilbert, David (1929). \"Probleme der Grundlegung der Mathematik\". Mathematische Annalen. 102: 1–9. doi:10.1007/BF01782335. S2CID 122870563. Lecture given at the International Congress of Mathematicians, 3 September 1928. Published in English translation as \"The Grounding of Elementary Number Theory\", in Mancosu 1998, pp. 266–273.\\nHilbert, David; Bernays, Paul (1934). Grundlagen der Mathematik. I. Die Grundlehren der mathematischen Wissenschaften. 40. Berlin, New York City: Springer. ISBN 9783540041344. JFM 60.0017.02. MR 0237246.\\nKleene, Stephen Cole (1943). \"Recursive Predicates and Quantifiers\". Transactions of the American Mathematical Society. 53 (1): 41–73. doi:10.2307/1990131. JSTOR 1990131.\\nLobachevsky, Nikolai (1840). Geometrishe Untersuchungen zur Theorie der Parellellinien (in German). Reprinted in English translation as Robert Bonola, ed. (1955). \"Geometric Investigations on the Theory of Parallel Lines\". Non-Euclidean Geometry. Dover. ISBN 0-486-60027-0.\\nLöwenheim, Leopold (1915). \"Über Möglichkeiten im Relativkalkül\". Mathematische Annalen (in German). 76 (4): 447–470. doi:10.1007/BF01458217. ISSN 0025-5831. S2CID 116581304. Translated as \"On possibilities in the calculus of relatives\" in Jean van Heijenoort (1967). A Source Book in Mathematical Logic, 1879–1931. Harvard Univ. Press. pp. 228–251.\\nMancosu, Paolo, ed. (1998). From Brouwer to Hilbert. The Debate on the Foundations of Mathematics in the 1920s. Oxford University Press.\\nPasch, Moritz (1882). Vorlesungen über neuere Geometrie.\\nPeano, Giuseppe (1889). Arithmetices principia, nova methodo exposita (in Lithuanian). Excerpt reprinted in English translation as \"The principles of arithmetic, presented by a new method\"in van Heijenoort 1976, pp. 83–97.\\nRichard, Jules (1905). \"Les principes des mathématiques et le problème des ensembles\". Revue Générale des Sciences Pures et Appliquées (in French). 16: 541. Reprinted in English translation as \"The principles of mathematics and the problems of sets\" in van Heijenoort 1976, pp. 142–144.\\nSkolem, Thoralf (1920). \"Logisch-kombinatorische Untersuchungen über die Erfüllbarkeit oder Beweisbarkeit mathematischer Sätze nebst einem Theoreme über dichte Mengen\". Videnskapsselskapet Skrifter, I. Matematisk-naturvidenskabelig Klasse (in German). 6: 1–36.Soare, Robert Irving (22 December 2011). \"Computability Theory and Applications: The Art of Classical Computability\" (PDF). Department of Mathematics. University of Chicago. Retrieved 23 August 2017.\\nSwineshead, Richard (1498). Calculationes Suiseth Anglici (in Lithuanian). Papie: Per Franciscum Gyrardengum.\\n\\nTarski, Alfred (1948). A decision method for elementary algebra and geometry. Santa Monica CA: RAND Corporation.\\nTuring, Alan M. (1939). \"Systems of Logic Based on Ordinals\". Proceedings of the London Mathematical Society. 45 (2): 161–228. doi:10.1112/plms/s2-45.1.161. hdl:21.11116/0000-0001-91CE-3.\\nWeyl, Hermann (1918). Das Kontinuum. Kritische Untersuchungen über die Grund lagen der Analysis (in German). Leipzig.\\nZermelo, Ernst (1904). \"Beweis, daß jede Menge wohlgeordnet werden kann\". Mathematische Annalen (in German). 59 (4): 514–516. doi:10.1007/BF01445300. S2CID 124189935. Reprinted in English translation as \"Proof that every set can be well-ordered\" in van Heijenoort 1976, pp. 139–141.\\nZermelo, Ernst (1908a). \"Neuer Beweis für die Möglichkeit einer Wohlordnung\". Mathematische Annalen (in German). 65: 107–128. doi:10.1007/BF01450054. ISSN 0025-5831. S2CID 119924143. Reprinted in English translation as \"A new proof of the possibility of a well-ordering\" in van Heijenoort 1976, pp. 183–198.\\nZermelo, Ernst (1908b). \"Untersuchungen über die Grundlagen der Mengenlehre\". Mathematische Annalen. 65 (2): 261–281. doi:10.1007/BF01449999. S2CID 120085563.\\n\\n\\n== External links ==\\n\"Mathematical logic\", Encyclopedia of Mathematics, EMS Press, 2001 [1994]\\nPolyvalued logic and Quantity Relation Logic\\nforall x: an introduction to formal logic, a free textbook by P. D. Magnus.\\nA Problem Course in Mathematical Logic, a free textbook by Stefan Bilaniuk.\\nDetlovs, Vilnis, and Podnieks, Karlis (University of Latvia), Introduction to Mathematical Logic. (hyper-textbook).\\nIn the Stanford Encyclopedia of Philosophy:\\nClassical Logic by Stewart Shapiro.\\nFirst-order Model Theory by Wilfrid Hodges.\\nIn the London Philosophy Study Guide:\\nMathematical Logic\\nSet Theory & Further Logic\\nPhilosophy of Mathematics', 'In mathematics and mathematical logic, Boolean algebra is the branch of algebra in which the values of the variables are the truth values true and false, usually denoted 1 and 0, respectively. Instead of elementary algebra, where the values of the variables are numbers and the prime operations are addition and multiplication, the main operations of Boolean algebra are the conjunction (and) denoted as ∧, the disjunction (or) denoted as ∨, and the negation (not) denoted as ¬. It is thus a formalism for describing logical operations, in the same way that elementary algebra describes numerical operations.\\nBoolean algebra was introduced by George Boole in his first book The Mathematical Analysis of Logic (1847), and set forth more fully in his An Investigation of the Laws of Thought (1854).\\nAccording to Huntington, the term \"Boolean algebra\" was first suggested by Sheffer in 1913, although Charles Sanders Peirce gave the title \"A Boolean Algebra with One Constant\" to the first chapter of his \"The Simplest Mathematics\" in 1880.\\nBoolean algebra has been fundamental in the development of digital electronics, and is provided for in all modern programming languages. It is also used in set theory and statistics.\\n\\n\\n== History ==\\nA precursor of Boolean algebra was Gottfried Wilhelm Leibniz\\'s algebra of concepts. Leibniz\\'s algebra of concepts is deductively equivalent to the Boolean algebra of sets.Boole\\'s algebra predated the modern developments in abstract algebra and mathematical logic; it is however seen as connected to the origins of both fields. In an abstract setting, Boolean algebra was perfected in the late 19th century by Jevons, Schröder, Huntington and others, until it reached the modern conception of an (abstract) mathematical structure. For example, the empirical observation that one can manipulate expressions in the algebra of sets, by translating them into expressions in Boole\\'s algebra, is explained in modern terms by saying that the algebra of sets is a Boolean algebra (note the indefinite article). In fact, M. H. Stone proved in 1936 that every Boolean algebra is isomorphic to a field of sets.\\nIn the 1930s, while studying switching circuits, Claude Shannon observed that one could also apply the rules of Boole\\'s algebra in this setting, and he introduced switching algebra as a way to analyze and design circuits by algebraic means in terms of logic gates. Shannon already had at his disposal the abstract mathematical apparatus, thus he cast his switching algebra as the two-element Boolean algebra. In modern circuit engineering settings, there is little need to consider other Boolean algebras, thus \"switching algebra\" and \"Boolean algebra\" are often used interchangeably.Efficient implementation of Boolean functions is a fundamental problem in the design of combinational logic circuits. Modern electronic design automation tools for VLSI circuits often rely on an efficient representation of Boolean functions known as (reduced ordered) binary decision diagrams (BDD) for logic synthesis and formal verification.Logic sentences that can be expressed in classical propositional calculus have an equivalent expression in Boolean algebra. Thus, Boolean logic is sometimes used to denote propositional calculus performed in this way. Boolean algebra is not sufficient to capture logic formulas using quantifiers, like those from first order logic.\\nAlthough the development of mathematical logic did not follow Boole\\'s program, the connection between his algebra and logic was later put on firm ground in the setting of algebraic logic, which also studies the algebraic systems of many other logics. The problem of determining whether the variables of a given Boolean (propositional) formula can be assigned in such a way as to make the formula evaluate to true is called the Boolean satisfiability problem (SAT), and is of importance to theoretical computer science, being the first problem shown to be NP-complete. The closely related model of computation known as a Boolean circuit relates time complexity (of an algorithm) to circuit complexity.\\n\\n\\n== Values ==\\nWhereas expressions denote mainly numbers in elementary algebra, in Boolean algebra, they denote the truth values false and true. These values are represented with the bits (or binary digits), namely 0 and 1. They do not behave like the integers 0 and 1, for which 1 + 1 = 2, but may be identified with the elements of the two-element field GF(2), that is, integer arithmetic modulo 2, for which 1 + 1 = 0. Addition and multiplication then play the Boolean roles of XOR (exclusive-or) and AND (conjunction), respectively, with disjunction x ∨ y (inclusive-or) definable as x + y − xy.\\nBoolean algebra also deals with functions which have their values in the set {0, 1}.\\nA sequence of bits is a commonly used for such functions. Another common example is the subsets of a set E: to a subset F of E, one can define the indicator function that takes the value 1 on F, and 0 outside F. The most general example is the elements of a Boolean algebra, with all of the foregoing being instances thereof.\\nAs with elementary algebra, the purely equational part of the theory may be developed, without considering explicit values for the variables.\\n\\n\\n== Operations ==\\n\\n\\n=== Basic operations ===\\nThe basic operations of Boolean algebra are conjunction, disjunction, and negation. These Boolean operations are expressed with the corresponding binary operators AND, and OR and the unary operator NOT, collectively referred to as Boolean operators.The basic Boolean operations on variables x and y are defined as follows:\\n\\nAlternatively the values of x∧y, x∨y, and ¬x can be expressed by tabulating their values with truth tables as follows:\\n\\nIf the truth values 0 and 1 are interpreted as integers, these operations may be expressed with the ordinary operations of arithmetic (where x + y uses addition and xy uses multiplication), or by the minimum/maximum functions:\\n\\n  \\n    \\n      \\n        \\n          \\n            \\n              \\n                x\\n                ∧\\n                y\\n              \\n              \\n                \\n                =\\n                x\\n                y\\n                =\\n                min\\n                (\\n                x\\n                ,\\n                y\\n                )\\n              \\n            \\n            \\n              \\n                x\\n                ∨\\n                y\\n              \\n              \\n                \\n                =\\n                x\\n                +\\n                y\\n                −\\n                x\\n                y\\n                =\\n                max\\n                (\\n                x\\n                ,\\n                y\\n                )\\n              \\n            \\n            \\n              \\n                ¬\\n                x\\n              \\n              \\n                \\n                =\\n                1\\n                −\\n                x\\n              \\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\begin{aligned}x\\\\wedge y&=xy=\\\\min(x,y)\\\\\\\\x\\\\vee y&=x+y-xy=\\\\max(x,y)\\\\\\\\\\\\neg x&=1-x\\\\end{aligned}}}\\n  One might consider that only negation and one of the two other operations are basic, because of the following identities that allow one to define conjunction in terms of negation and the disjunction, and vice versa (De Morgan\\'s laws):\\n\\n  \\n    \\n      \\n        \\n          \\n            \\n              \\n                x\\n                ∧\\n                y\\n              \\n              \\n                \\n                =\\n                ¬\\n                (\\n                ¬\\n                x\\n                ∨\\n                ¬\\n                y\\n                )\\n              \\n            \\n            \\n              \\n                x\\n                ∨\\n                y\\n              \\n              \\n                \\n                =\\n                ¬\\n                (\\n                ¬\\n                x\\n                ∧\\n                ¬\\n                y\\n                )\\n              \\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\begin{aligned}x\\\\wedge y&=\\\\neg (\\\\neg x\\\\vee \\\\neg y)\\\\\\\\x\\\\vee y&=\\\\neg (\\\\neg x\\\\wedge \\\\neg y)\\\\end{aligned}}}\\n  \\n\\n\\n=== Secondary operations ===\\nThe three Boolean operations described above are referred to as basic, meaning that they can be taken as a basis for other Boolean operations that can be built up from them by composition, the manner in which operations are combined or compounded. Operations composed from the basic operations include the following examples:\\n\\nThese definitions give rise to the following truth tables giving the values of these operations for all four possible inputs.\\n\\nMaterial conditional\\nThe first operation, x → y, or Cxy, is called material implication. If x is true, then the value of x → y is taken to be that of y (e.g. if x is true and y is false, then x → y is also false). But if x is false, then the value of y can be ignored; however, the operation must return some boolean value and there are only two choices. So by definition, x → y is true when x is false.  (relevance logic suggests this definition, by viewing an implication with a false premise as something other than either true or false.)\\nExclusive OR (XOR)\\nThe second operation, x ⊕ y, or Jxy, is called exclusive or (often abbreviated as XOR) to distinguish it from disjunction as the inclusive kind. It excludes the possibility of both x and y being true (e.g. see table): if both are true then result is false. Defined in terms of arithmetic it is addition where mod 2 is 1 + 1 = 0.\\nLogical equivalence\\nThe third operation, the complement of exclusive or, is equivalence or Boolean equality: x ≡ y, or Exy, is true just when x and y have the same value. Hence x ⊕ y as its complement can be understood as x ≠ y, being true just when x and y are different. Thus, its counterpart in arithmetic mod 2 is x + y. Equivalence\\'s counterpart in arithmetic mod 2 is x + y + 1.Given two operands, each with two possible values, there are 22 = 4 possible combinations of inputs. Because each output can have two possible values, there are a total of 24 = 16 possible binary Boolean operations. Any such operation or function (as well as any Boolean function with more inputs) can be expressed with the basic operations from above. Hence the basic operations are functionally complete.\\n\\n\\n== Laws ==\\nA law of Boolean algebra is an identity such as x ∨ (y ∨ z) = (x ∨ y) ∨ z between two Boolean terms, where a Boolean term is defined as an expression built up from variables and the constants 0 and 1 using the operations ∧, ∨, and ¬. The concept can be extended to terms involving other Boolean operations such as ⊕, →, and ≡, but such extensions are unnecessary for the purposes to which the laws are put. Such purposes include the definition of a Boolean algebra as any model of the Boolean laws, and as a means for deriving new laws from old as in the derivation of x ∨ (y ∧ z) = x ∨ (z ∧ y) from y ∧ z = z ∧ y (as treated in § Axiomatizing Boolean algebra).\\n\\n\\n=== Monotone laws ===\\nBoolean algebra satisfies many of the same laws as ordinary algebra when one matches up ∨ with addition and ∧ with multiplication. In particular the following laws are common to both kinds of algebra:\\nThe following laws hold in Boolean algebra, but not in ordinary algebra:\\n\\nTaking x = 2 in the third law above shows that it is not an ordinary algebra law, since 2 × 2 = 4. The remaining five laws can be falsified in ordinary algebra by taking all variables to be 1. For example, in Absorption Law 1, the left hand side would be 1(1 + 1) = 2, while the right hand side would be 1 (and so on).\\nAll of the laws treated thus far have been for conjunction and disjunction. These operations have the property that changing either argument either leaves the output unchanged, or the output changes in the same way as the input. Equivalently, changing any variable from 0 to 1 never results in the output changing from 1 to 0. Operations with this property are said to be monotone. Thus the axioms thus far have all been for monotonic Boolean logic. Nonmonotonicity enters via complement ¬ as follows.\\n\\n\\n=== Nonmonotone laws ===\\nThe complement operation is defined by the following two laws.\\n\\n  \\n    \\n      \\n        \\n          \\n            \\n              \\n              \\n                \\n                  Complementation 1\\n                \\n              \\n              \\n                x\\n                ∧\\n                ¬\\n                x\\n              \\n              \\n                \\n                =\\n                0\\n              \\n            \\n            \\n              \\n              \\n                \\n                  Complementation 2\\n                \\n              \\n              \\n                x\\n                ∨\\n                ¬\\n                x\\n              \\n              \\n                \\n                =\\n                1\\n              \\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\begin{aligned}&{\\\\text{Complementation 1}}&x\\\\wedge \\\\neg x&=0\\\\\\\\&{\\\\text{Complementation 2}}&x\\\\vee \\\\neg x&=1\\\\end{aligned}}}\\n  All properties of negation including the laws below follow from the above two laws alone.In both ordinary and Boolean algebra, negation works by exchanging pairs of elements, whence in both algebras it satisfies the double negation law (also called involution law)\\n\\n  \\n    \\n      \\n        \\n          \\n            \\n              \\n              \\n                \\n                  Double negation\\n                \\n              \\n              \\n                ¬\\n                \\n                  (\\n                  ¬\\n                  \\n                    x\\n                  \\n                  )\\n                \\n              \\n              \\n                \\n                =\\n                x\\n              \\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\begin{aligned}&{\\\\text{Double negation}}&\\\\neg {(\\\\neg {x})}&=x\\\\end{aligned}}}\\n  But whereas ordinary algebra satisfies the two laws\\n\\n  \\n    \\n      \\n        \\n          \\n            \\n              \\n                (\\n                −\\n                x\\n                )\\n                (\\n                −\\n                y\\n                )\\n              \\n              \\n                \\n                =\\n                x\\n                y\\n              \\n            \\n            \\n              \\n                (\\n                −\\n                x\\n                )\\n                +\\n                (\\n                −\\n                y\\n                )\\n              \\n              \\n                \\n                =\\n                −\\n                (\\n                x\\n                +\\n                y\\n                )\\n              \\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\begin{aligned}(-x)(-y)&=xy\\\\\\\\(-x)+(-y)&=-(x+y)\\\\end{aligned}}}\\n  Boolean algebra satisfies De Morgan\\'s laws:\\n\\n  \\n    \\n      \\n        \\n          \\n            \\n              \\n              \\n                \\n                  De Morgan 1\\n                \\n              \\n              \\n                ¬\\n                x\\n                ∧\\n                ¬\\n                y\\n              \\n              \\n                \\n                =\\n                ¬\\n                \\n                  (\\n                  x\\n                  ∨\\n                  y\\n                  )\\n                \\n              \\n            \\n            \\n              \\n              \\n                \\n                  De Morgan 2\\n                \\n              \\n              \\n                ¬\\n                x\\n                ∨\\n                ¬\\n                y\\n              \\n              \\n                \\n                =\\n                ¬\\n                \\n                  (\\n                  x\\n                  ∧\\n                  y\\n                  )\\n                \\n              \\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\begin{aligned}&{\\\\text{De Morgan 1}}&\\\\neg x\\\\wedge \\\\neg y&=\\\\neg {(x\\\\vee y)}\\\\\\\\&{\\\\text{De Morgan 2}}&\\\\neg x\\\\vee \\\\neg y&=\\\\neg {(x\\\\wedge y)}\\\\end{aligned}}}\\n  \\n\\n\\n=== Completeness ===\\nThe laws listed above define Boolean algebra, in the sense that they entail the rest of the subject. The laws Complementation 1 and 2, together with the monotone laws, suffice for this purpose and can therefore be taken as one possible complete set of laws or axiomatization of Boolean algebra. Every law of Boolean algebra follows logically from these axioms. Furthermore, Boolean algebras can then be defined as the models of these axioms as treated in § Boolean algebras.\\nTo clarify, writing down further laws of Boolean algebra cannot give rise to any new consequences of these axioms, nor can it rule out any model of them. In contrast, in a list of some but not all of the same laws, there could have been Boolean laws that did not follow from those on the list, and moreover there would have been models of the listed laws that were not Boolean algebras.\\nThis axiomatization is by no means the only one, or even necessarily the most natural given that we did not pay attention to whether some of the axioms followed from others but simply chose to stop when we noticed we had enough laws, treated further in § Axiomatizing Boolean algebra. Or the intermediate notion of axiom can be sidestepped altogether by defining a Boolean law directly as any tautology, understood as an equation that holds for all values of its variables over 0 and 1. All these definitions of Boolean algebra can be shown to be equivalent.\\n\\n\\n=== Duality principle ===\\nPrinciple: If {X, R} is a poset, then {X, R(inverse)} is also a poset.\\nThere is nothing magical about the choice of symbols for the values of Boolean algebra. We could rename 0 and 1 to say α and β, and as long as we did so consistently throughout it would still be Boolean algebra, albeit with some obvious cosmetic differences.\\nBut suppose we rename 0 and 1 to 1 and 0 respectively. Then it would still be Boolean algebra, and moreover operating on the same values. However it would not be identical to our original Boolean algebra because now we find ∨ behaving the way ∧ used to do and vice versa. So there are still some cosmetic differences to show that we\\'ve been fiddling with the notation, despite the fact that we\\'re still using 0s and 1s.\\nBut if in addition to interchanging the names of the values we also interchange the names of the two binary operations, now there is no trace of what we have done. The end product is completely indistinguishable from what we started with. We might notice that the columns for x ∧ y and x ∨ y in the truth tables had changed places, but that switch is immaterial.\\nWhen values and operations can be paired up in a way that leaves everything important unchanged when all pairs are switched simultaneously, we call the members of each pair dual to each other. Thus 0 and 1 are dual, and ∧ and ∨ are dual. The Duality Principle, also called De Morgan duality, asserts that Boolean algebra is unchanged when all dual pairs are interchanged.\\nOne change we did not need to make as part of this interchange was to complement. We say that complement is a self-dual operation. The identity or do-nothing operation x (copy the input to the output) is also self-dual. A more complicated example of a self-dual operation is (x ∧ y) ∨ (y ∧ z) ∨ (z ∧ x). There is no self-dual binary operation that depends on both its arguments. A composition of self-dual operations is a self-dual operation. For example, if f(x, y, z) = (x ∧ y) ∨ (y ∧ z) ∨ (z ∧ x), then f(f(x, y, z), x, t) is a self-dual operation of four arguments x, y, z, t.\\nThe principle of duality can be explained from a group theory perspective by the fact that there are exactly four functions that are one-to-one mappings (automorphisms) of the set of Boolean polynomials back to itself: the identity function, the complement function, the dual function and the contradual function (complemented dual). These four functions form a group under function composition, isomorphic to the Klein four-group, acting on the set of Boolean polynomials. Walter Gottschalk remarked that consequently a more appropriate name for the phenomenon would be the principle (or square) of quaternality.\\n\\n\\n== Diagrammatic representations ==\\n\\n\\n=== Venn diagrams ===\\nA Venn diagram can be used as a representation of a Boolean operation using shaded overlapping regions. There is one region for each variable, all circular in the examples here. The interior and exterior of region x corresponds respectively to the values 1 (true) and 0 (false) for variable x. The shading indicates the value of the operation for each combination of regions, with dark denoting 1 and light 0 (some authors use the opposite convention).\\nThe three Venn diagrams in the figure below represent respectively conjunction x∧y, disjunction x∨y, and complement ¬x.\\n\\nFor conjunction, the region inside both circles is shaded to indicate that x∧y is 1 when both variables are 1. The other regions are left unshaded to indicate that x∧y is 0 for the other three combinations.\\nThe second diagram represents disjunction x∨y by shading those regions that lie inside either or both circles. The third diagram represents complement ¬x by shading the region not inside the circle.\\nWhile we have not shown the Venn diagrams for the constants 0 and 1, they are trivial, being respectively a white box and a dark box, neither one containing a circle. However we could put a circle for x in those boxes, in which case each would denote a function of one argument, x, which returns the same value independently of x, called a constant function. As far as their outputs are concerned, constants and constant functions are indistinguishable; the difference is that a constant takes no arguments, called a zeroary or nullary operation, while a constant function takes one argument, which it ignores, and is a unary operation.\\nVenn diagrams are helpful in visualizing laws. The commutativity laws for ∧ and ∨ can be seen from the symmetry of the diagrams: a binary operation that was not commutative would not have a symmetric diagram because interchanging x and y would have the effect of reflecting the diagram horizontally and any failure of commutativity would then appear as a failure of symmetry.\\nIdempotence of ∧ and ∨ can be visualized by sliding the two circles together and noting that the shaded area then becomes the whole circle, for both ∧ and ∨.\\nTo see the first absorption law, x∧(x∨y) = x, start with the diagram in the middle for x∨y and note that the portion of the shaded area in common with the x circle is the whole of the x circle. For the second absorption law, x∨(x∧y) = x, start with the left diagram for x∧y and note that shading the whole of the x circle results in just the x circle being shaded, since the previous shading was inside the x circle.\\nThe double negation law can be seen by complementing the shading in the third diagram for ¬x, which shades the x circle.\\nTo visualize the first De Morgan\\'s law, (¬x)∧(¬y) = ¬(x∨y), start with the middle diagram for x∨y and complement its shading so that only the region outside both circles is shaded, which is what the right hand side of the law describes. The result is the same as if we shaded that region which is both outside the x circle and outside the y circle, i.e. the conjunction of their exteriors, which is what the left hand side of the law describes.\\nThe second De Morgan\\'s law, (¬x)∨(¬y) = ¬(x∧y), works the same way with the two diagrams interchanged.\\nThe first complement law, x∧¬x = 0, says that the interior and exterior of the x circle have no overlap. The second complement law, x∨¬x = 1, says that everything is either inside or outside the x circle.\\n\\n\\n=== Digital logic gates ===\\nDigital logic is the application of the Boolean algebra of 0 and 1 to electronic hardware consisting of logic gates connected to form a circuit diagram. Each gate implements a Boolean operation, and is depicted schematically by a shape indicating the operation. The shapes associated with the gates for conjunction (AND-gates), disjunction (OR-gates), and complement (inverters) are as follows.\\n\\nThe lines on the left of each gate represent input wires or ports. The value of the input is represented by a voltage on the lead. For so-called \"active-high\" logic, 0 is represented by a voltage close to zero or \"ground\", while 1 is represented by a voltage close to the supply voltage; active-low reverses this.  The line on the right of each gate represents the output port, which normally follows the same voltage conventions as the input ports.\\nComplement is implemented with an inverter gate. The triangle denotes the operation that simply copies the input to the output; the small circle on the output denotes the actual inversion complementing the input. The convention of putting such a circle on any port means that the signal passing through this port is complemented on the way through, whether it is an input or output port.\\nThe Duality Principle, or De Morgan\\'s laws, can be understood as asserting that complementing all three ports of an AND gate converts it to an OR gate and vice versa, as shown in Figure 4 below. Complementing both ports of an inverter however leaves the operation unchanged.\\n\\nMore generally one may complement any of the eight subsets of the three ports of either an AND or OR gate. The resulting sixteen possibilities give rise to only eight Boolean operations, namely those with an odd number of 1\\'s in their truth table. There are eight such because the \"odd-bit-out\" can be either 0 or 1 and can go in any of four positions in the truth table. There being sixteen binary Boolean operations, this must leave eight operations with an even number of 1\\'s in their truth tables. Two of these are the constants 0 and 1 (as binary operations that ignore both their inputs); four are the operations that depend nontrivially on exactly one of their two inputs, namely x, y, ¬x, and ¬y; and the remaining two are x⊕y (XOR) and its complement x≡y.\\n\\n\\n== Boolean algebras ==\\n\\nThe term \"algebra\" denotes both a subject, namely the subject of algebra, and an object, namely an algebraic structure. Whereas the foregoing has addressed the subject of Boolean algebra, this section deals with mathematical objects called Boolean algebras, defined in full generality as any model of the Boolean laws. We begin with a special case of the notion definable without reference to the laws, namely concrete Boolean algebras, and then give the formal definition of the general notion.\\n\\n\\n=== Concrete Boolean algebras ===\\nA concrete Boolean algebra or field of sets is any nonempty set of subsets of a given set X closed under the set operations of union, intersection, and complement relative to X.(As an aside, historically X itself was required to be nonempty as well to exclude the degenerate or one-element Boolean algebra, which is the one exception to the rule that all Boolean algebras satisfy the same equations since the degenerate algebra satisfies every equation. However this exclusion conflicts with the preferred purely equational definition of \"Boolean algebra,\" there being no way to rule out the one-element algebra using only equations— 0 ≠ 1 does not count, being a negated equation. Hence modern authors allow the degenerate Boolean algebra and let X be empty.)\\nExample 1. The power set 2X of X, consisting of all subsets of X. Here X may be any set: empty, finite, infinite, or even uncountable.\\nExample 2. The empty set and X. This two-element algebra shows that a concrete Boolean algebra can be finite even when it consists of subsets of an infinite set. It can be seen that every field of subsets of X must contain the empty set and X. Hence no smaller example is possible, other than the degenerate algebra obtained by taking X to be empty so as to make the empty set and X coincide.\\nExample 3. The set of finite and cofinite sets of integers, where a cofinite set is one omitting only finitely many integers. This is clearly closed under complement, and is closed under union because the union of a cofinite set with any set is cofinite, while the union of two finite sets is finite.  Intersection behaves like union with \"finite\" and \"cofinite\" interchanged.\\nExample 4. For a less trivial example of the point made by Example 2, consider a Venn diagram formed by n closed curves partitioning the diagram into 2n regions, and let X be the (infinite) set of all points in the plane not on any curve but somewhere within the diagram. The interior of each region is thus an infinite subset of X, and every point in X is in exactly one region.  Then the set of all 22n possible unions of regions (including the empty set obtained as the union of the empty set of regions and X obtained as the union of all 2n regions) is closed under union, intersection, and complement relative to X and therefore forms a concrete Boolean algebra. Again we have finitely many subsets of an infinite set forming a concrete Boolean algebra, with Example 2 arising as the case n = 0 of no curves.\\n\\n\\n=== Subsets as bit vectors ===\\nA subset Y of X can be identified with an indexed family of bits with index set X, with the bit indexed by x ∈ X being 1 or 0 according to whether or not x ∈ Y. (This is the so-called characteristic function notion of a subset.)  For example, a 32-bit computer word consists of 32 bits indexed by the set {0,1,2,...,31}, with 0 and 31 indexing the low and high order bits respectively. For a smaller example, if X = {a,b,c} where a, b, c are viewed as bit positions in that order from left to right, the eight subsets {}, {c}, {b}, {b,c}, {a}, {a,c}, {a,b}, and {a,b,c} of X can be identified with the respective bit vectors 000, 001, 010, 011, 100, 101, 110, and 111. Bit vectors indexed by the set of natural numbers are infinite sequences of bits, while those indexed by the reals in the unit interval [0,1] are packed too densely to be able to write conventionally but nonetheless form well-defined indexed families (imagine coloring every point of the interval [0,1] either black or white independently; the black points then form an arbitrary subset of [0,1]).\\nFrom this bit vector viewpoint, a concrete Boolean algebra can be defined equivalently as a nonempty set of bit vectors all of the same length (more generally, indexed by the same set) and closed under the bit vector operations of bitwise ∧, ∨, and ¬, as in 1010∧0110 = 0010, 1010∨0110 = 1110, and ¬1010 = 0101, the bit vector realizations of intersection, union, and complement respectively.\\n\\n\\n=== The prototypical Boolean algebra ===\\n\\nThe set {0,1} and its Boolean operations as treated above can be understood as the special case of bit vectors of length one, which by the identification of bit vectors with subsets can also be understood as the two subsets of a one-element set. We call this the prototypical Boolean algebra, justified by the following observation.\\n\\nThe laws satisfied by all nondegenerate concrete Boolean algebras coincide with those satisfied by the prototypical Boolean algebra.This observation is easily proved as follows. Certainly any law satisfied by all concrete Boolean algebras is satisfied by the prototypical one since it is concrete. Conversely any law that fails for some concrete Boolean algebra must have failed at a particular bit position, in which case that position by itself furnishes a one-bit counterexample to that law. Nondegeneracy ensures the existence of at least one bit position because there is only one empty bit vector.\\nThe final goal of the next section can be understood as eliminating \"concrete\" from the above observation. We shall however reach that goal via the surprisingly stronger observation that, up to isomorphism, all Boolean algebras are concrete.\\n\\n\\n=== Boolean algebras: the definition ===\\nThe Boolean algebras we have seen so far have all been concrete, consisting of bit vectors or equivalently of subsets of some set. Such a Boolean algebra consists of a set and operations on that set which can be shown to satisfy the laws of Boolean algebra.\\nInstead of showing that the Boolean laws are satisfied, we can instead postulate a set X, two binary operations on X, and one unary operation, and require that those operations satisfy the laws of Boolean algebra. The elements of X need not be bit vectors or subsets but can be anything at all. This leads to the more general abstract definition.\\n\\nA Boolean algebra is any set with binary operations ∧ and ∨ and a unary operation ¬ thereon satisfying the Boolean laws.For the purposes of this definition it is irrelevant how the operations came to satisfy the laws, whether by fiat or proof. All concrete Boolean algebras satisfy the laws (by proof rather than fiat), whence every concrete Boolean algebra is a Boolean algebra according to our definitions. This axiomatic definition of a Boolean algebra as a set and certain operations satisfying certain laws or axioms by fiat is entirely analogous to the abstract definitions of group, ring, field etc. characteristic of modern or abstract algebra.\\nGiven any complete axiomatization of Boolean algebra, such as the axioms for a complemented distributive lattice, a sufficient condition for an algebraic structure of this kind to satisfy all the Boolean laws is that it satisfy just those axioms. The following is therefore an equivalent definition.\\n\\nA Boolean algebra is a complemented distributive lattice.The section on axiomatization lists other axiomatizations, any of which can be made the basis of an equivalent definition.\\n\\n\\n=== Representable Boolean algebras ===\\nAlthough every concrete Boolean algebra is a Boolean algebra, not every Boolean algebra need be concrete. Let n be a square-free positive integer, one not divisible by the square of an integer, for example 30 but not 12. The operations of greatest common divisor, least common multiple, and division into n (that is, ¬x = n/x), can be shown to satisfy all the Boolean laws when their arguments range over the positive divisors of n. Hence those divisors form a Boolean algebra. These divisors are not subsets of a set, making the divisors of n a Boolean algebra that is not concrete according to our definitions.\\nHowever, if we represent each divisor of n by the set of its prime factors, we find that this nonconcrete Boolean algebra is isomorphic to the concrete Boolean algebra consisting of all sets of prime factors of n, with union corresponding to least common multiple, intersection to greatest common divisor, and complement to division into n. So this example while not technically concrete is at least \"morally\" concrete via this representation, called an isomorphism. This example is an instance of the following notion.\\n\\nA Boolean algebra is called representable when it is isomorphic to a concrete Boolean algebra.The obvious next question is answered positively as follows.\\n\\nEvery Boolean algebra is representable.That is, up to isomorphism, abstract and concrete Boolean algebras are the same thing. This quite nontrivial result depends on the Boolean prime ideal theorem, a choice principle slightly weaker than the axiom of choice, and is treated in more detail in the article Stone\\'s representation theorem for Boolean algebras. This strong relationship implies a weaker result strengthening the observation in the previous subsection to the following easy consequence of representability.\\n\\nThe laws satisfied by all Boolean algebras coincide with those satisfied by the prototypical Boolean algebra.It is weaker in the sense that it does not of itself imply representability. Boolean algebras are special here, for example a relation algebra is a Boolean algebra with additional structure but it is not the case that every relation algebra is representable in the sense appropriate to relation algebras.\\n\\n\\n== Axiomatizing Boolean algebra ==\\n\\nThe above definition of an abstract Boolean algebra as a set and operations satisfying \"the\" Boolean laws raises the question, what are those laws?  A simple-minded answer is \"all Boolean laws,\" which can be defined as all equations that hold for the Boolean algebra of 0 and 1. Since there are infinitely many such laws this is not a terribly satisfactory answer in practice, leading to the next question: does it suffice to require only finitely many laws to hold?\\nIn the case of Boolean algebras the answer is yes. In particular the finitely many equations we have listed above suffice. We say that Boolean algebra is finitely axiomatizable or finitely based.\\nCan this list be made shorter yet? Again the answer is yes. To begin with, some of the above laws are implied by some of the others. A sufficient subset of the above laws consists of the pairs of associativity, commutativity, and absorption laws, distributivity of ∧ over ∨ (or the other distributivity law—one suffices), and the two complement laws. In fact this is the traditional axiomatization of Boolean algebra as a complemented distributive lattice.\\nBy introducing additional laws not listed above it becomes possible to shorten the list yet further; for instance, with the vertical bar representing the Sheffer stroke operation, the single axiom \\n  \\n    \\n      \\n        (\\n        (\\n        a\\n        ∣\\n        b\\n        )\\n        ∣\\n        c\\n        )\\n        ∣\\n        (\\n        a\\n        ∣\\n        (\\n        (\\n        a\\n        ∣\\n        c\\n        )\\n        ∣\\n        a\\n        )\\n        )\\n        =\\n        c\\n      \\n    \\n    {\\\\displaystyle ((a\\\\mid b)\\\\mid c)\\\\mid (a\\\\mid ((a\\\\mid c)\\\\mid a))=c}\\n   is sufficient to completely axiomatize Boolean algebra. It is also possible to find longer single axioms using more conventional operations; see Minimal axioms for Boolean algebra.\\n\\n\\n== Propositional logic ==\\n\\nPropositional logic is a logical system that is intimately connected to Boolean algebra.  Many syntactic concepts of Boolean algebra carry over to propositional logic with only minor changes in notation and terminology, while the semantics of propositional logic are defined via Boolean algebras in a way that the tautologies (theorems) of propositional logic correspond to equational theorems of Boolean algebra.\\nSyntactically, every Boolean term corresponds to a propositional formula of propositional logic. In this translation between Boolean algebra and propositional logic, Boolean variables x,y... become propositional variables (or atoms) P,Q,..., Boolean terms such as x∨y become propositional formulas P∨Q, 0 becomes false or ⊥, and 1 becomes true or T. It is convenient when referring to generic propositions to use Greek letters Φ, Ψ,... as metavariables (variables outside the language of propositional calculus, used when talking about propositional calculus) to denote propositions.\\nThe semantics of propositional logic rely on truth assignments. The essential idea of a truth assignment is that the propositional variables are mapped to elements of a fixed Boolean algebra, and then the truth value of a propositional formula using these letters is the element of the Boolean algebra that is obtained by computing the value of the Boolean term corresponding to the formula. In classical semantics, only the two-element Boolean algebra is used, while in Boolean-valued semantics arbitrary Boolean algebras are considered. A tautology is a propositional formula that is assigned truth value 1 by every truth assignment of its propositional variables to an arbitrary Boolean algebra (or, equivalently, every truth assignment to the two element Boolean algebra).\\nThese semantics permit a translation between tautologies of propositional logic and equational theorems of Boolean algebra. Every tautology Φ of propositional logic can be expressed as the Boolean equation Φ = 1, which will be a theorem of Boolean algebra. Conversely every theorem Φ = Ψ of Boolean algebra corresponds to the tautologies (Φ∨¬Ψ) ∧ (¬Φ∨Ψ) and (Φ∧Ψ) ∨ (¬Φ∧¬Ψ). If → is in the language these last tautologies can also be written as (Φ→Ψ) ∧ (Ψ→Φ), or as two separate theorems Φ→Ψ and Ψ→Φ; if ≡ is available then the single tautology Φ ≡ Ψ can be used.\\n\\n\\n=== Applications ===\\nOne motivating application of propositional calculus is the analysis of propositions and deductive arguments in natural language. Whereas the proposition \"if x = 3 then x+1 = 4\" depends on the meanings of such symbols as + and 1, the proposition \"if x = 3 then x = 3\" does not; it is true merely by virtue of its structure, and remains true whether \"x = 3\" is replaced by \"x = 4\" or \"the moon is made of green cheese.\"  The generic or abstract form of this tautology is \"if P then P\", or in the language of Boolean algebra, \"P → P\".Replacing P by x = 3 or any other proposition is called instantiation of P by that proposition. The result of instantiating P in an abstract proposition is called an instance of the proposition.  Thus \"x = 3 → x = 3\" is a tautology by virtue of being an instance of the abstract tautology \"P → P\". All occurrences of the instantiated variable must be instantiated with the same proposition, to avoid such nonsense as P → x = 3 or x = 3 → x = 4.\\nPropositional calculus restricts attention to abstract propositions, those built up from propositional variables using Boolean operations. Instantiation is still possible within propositional calculus, but only by instantiating propositional variables by abstract propositions, such as instantiating Q by Q→P in P→(Q→P) to yield the instance P→((Q→P)→P).\\n(The availability of instantiation as part of the machinery of propositional calculus avoids the need for metavariables within the language of propositional calculus, since ordinary propositional variables can be considered within the language to denote arbitrary propositions. The metavariables themselves are outside the reach of instantiation, not being part of the language of propositional calculus but rather part of the same language for talking about it that this sentence is written in, where we need to be able to distinguish propositional variables and their instantiations as being distinct syntactic entities.)\\n\\n\\n=== Deductive systems for propositional logic ===\\nAn axiomatization of propositional calculus is a set of tautologies called axioms and one or more inference rules for producing new tautologies from old. A proof in an axiom system A is a finite nonempty sequence of propositions each of which is either an instance of an axiom of A or follows by some rule of A from propositions appearing earlier in the proof (thereby disallowing circular reasoning). The last proposition is the theorem proved by the proof. Every nonempty initial segment of a proof is itself a proof, whence every proposition in a proof is itself a theorem. An axiomatization is sound when every theorem is a tautology, and complete when every tautology is a theorem.\\n\\n\\n==== Sequent calculus ====\\n\\nPropositional calculus is commonly organized as a Hilbert system, whose operations are just those of Boolean algebra and whose theorems are Boolean tautologies, those Boolean terms equal to the Boolean constant 1. Another form is sequent calculus, which has two sorts, propositions as in ordinary propositional calculus, and pairs of lists of propositions called sequents, such as A∨B, A∧C,... \\n  \\n    \\n      \\n        ⊢\\n      \\n    \\n    {\\\\displaystyle \\\\vdash }\\n   A, B→C,.... The two halves of a sequent are called the antecedent and the succedent respectively. The customary metavariable denoting an antecedent or part thereof is Γ, and for a succedent Δ; thus Γ,A \\n  \\n    \\n      \\n        ⊢\\n      \\n    \\n    {\\\\displaystyle \\\\vdash }\\n   Δ would denote a sequent whose succedent is a list Δ and whose antecedent is a list Γ with an additional proposition A appended after it. The antecedent is interpreted as the conjunction of its propositions, the succedent as the disjunction of its propositions, and the sequent itself as the entailment of the succedent by the antecedent.\\nEntailment differs from implication in that whereas the latter is a binary operation that returns a value in a Boolean algebra, the former is a binary relation which either holds or does not hold. In this sense entailment is an external form of implication, meaning external to the Boolean algebra, thinking of the reader of the sequent as also being external and interpreting and comparing antecedents and succedents in some Boolean algebra. The natural interpretation of \\n  \\n    \\n      \\n        ⊢\\n      \\n    \\n    {\\\\displaystyle \\\\vdash }\\n   is as ≤ in the partial order of the Boolean algebra defined by x ≤ y just when x∨y = y. This ability to mix external implication \\n  \\n    \\n      \\n        ⊢\\n      \\n    \\n    {\\\\displaystyle \\\\vdash }\\n   and internal implication → in the one logic is among the essential differences between sequent calculus and propositional calculus.\\n\\n\\n== Applications ==\\nBoolean algebra as the calculus of two values is fundamental to computer circuits, computer programming, and mathematical logic, and is also used in other areas of mathematics such as set theory and statistics.\\n\\n\\n=== Computers ===\\nIn the early 20th century, several electrical engineers intuitively recognized that Boolean algebra was analogous to the behavior of certain types of electrical circuits. Claude Shannon formally proved such behavior was logically equivalent to Boolean algebra in his 1937 master\\'s thesis, A Symbolic Analysis of Relay and Switching Circuits.\\nToday, all modern general purpose computers perform their functions using two-value Boolean logic; that is, their electrical circuits are a physical manifestation of two-value Boolean logic. They achieve this in various ways: as voltages on wires in high-speed circuits and capacitive storage devices, as orientations of a magnetic domain in ferromagnetic storage devices, as holes in punched cards or paper tape, and so on. (Some early computers used decimal circuits or mechanisms instead of two-valued logic circuits.)\\nOf course, it is possible to code more than two symbols in any given medium. For example, one might use respectively 0, 1, 2, and 3 volts to code a four-symbol alphabet on a wire, or holes of different sizes in a punched card. In practice, the tight constraints of high speed, small size, and low power combine to make noise a major factor. This makes it hard to distinguish between symbols when there are several possible symbols that could occur at a single site. Rather than attempting to distinguish between four voltages on one wire, digital designers have settled on two voltages per wire, high and low.\\nComputers use two-value Boolean circuits for the above reasons. The most common computer architectures use ordered sequences of Boolean values, called bits, of 32 or 64 values, e.g. 01101000110101100101010101001011. When programming in machine code, assembly language, and certain other programming languages, programmers work with the low-level digital structure of the data registers. These registers operate on voltages, where zero volts represents Boolean 0, and a reference voltage (often +5 V, +3.3 V, +1.8 V) represents Boolean 1. Such languages support both numeric operations and logical operations. In this context, \"numeric\" means that the computer treats sequences of bits as binary numbers (base two numbers) and executes arithmetic operations like add, subtract, multiply, or divide. \"Logical\" refers to the Boolean logical operations of disjunction, conjunction, and negation between two sequences of bits, in which each bit in one sequence is simply compared to its counterpart in the other sequence. Programmers therefore have the option of working in and applying the rules of either numeric algebra or Boolean algebra as needed. A core differentiating feature between these families of operations is the existence of the carry operation in the first but not the second.\\n\\n\\n=== Two-valued logic ===\\nOther areas where two values is a good choice are the law and mathematics. In everyday relaxed conversation, nuanced or complex answers such as \"maybe\" or \"only on the weekend\" are acceptable. In more focused situations such as a court of law or theorem-based mathematics however it is deemed advantageous to frame questions so as to admit a simple yes-or-no answer—is the defendant guilty or not guilty, is the proposition true or false—and to disallow any other answer. However much of a straitjacket this might prove in practice for the respondent, the principle of the simple yes-no question has become a central feature of both judicial and mathematical logic, making two-valued logic deserving of organization and study in its own right.\\nA central concept of set theory is membership. Now an organization may permit multiple degrees of membership, such as novice, associate, and full. With sets however an element is either in or out. The candidates for membership in a set work just like the wires in a digital computer: each candidate is either a member or a nonmember, just as each wire is either high or low.\\nAlgebra being a fundamental tool in any area amenable to mathematical treatment, these considerations combine to make the algebra of two values of fundamental importance to computer hardware, mathematical logic, and set theory.\\nTwo-valued logic can be extended to multi-valued logic, notably by replacing the Boolean domain {0, 1} with the unit interval [0,1], in which case rather than only taking values 0 or 1, any value between and including 0 and 1 can be assumed. Algebraically, negation (NOT) is replaced with 1 − x, conjunction (AND) is replaced with multiplication (\\n  \\n    \\n      \\n        x\\n        y\\n      \\n    \\n    {\\\\displaystyle xy}\\n  ), and disjunction (OR) is defined via De Morgan\\'s law. Interpreting these values as logical truth values yields a multi-valued logic, which forms the basis for fuzzy logic and probabilistic logic. In these interpretations, a value is interpreted as the \"degree\" of truth – to what extent a proposition is true, or the probability that the proposition is true.\\n\\n\\n=== Boolean operations ===\\nThe original application for Boolean operations was mathematical logic, where it combines the truth values, true or false, of individual formulas.\\n\\n\\n==== Natural language ====\\nNatural languages such as English have words for several Boolean operations, in particular conjunction (and), disjunction (or), negation (not), and implication (implies). But not is synonymous with and not. When used to combine situational assertions such as \"the block is on the table\" and \"cats drink milk,\" which naively are either true or false, the meanings of these logical connectives often have the meaning of their logical counterparts. However, with descriptions of behavior such as \"Jim walked through the door\", one starts to notice differences such as failure of commutativity, for example the conjunction of \"Jim opened the door\" with \"Jim walked through the door\" in that order is not equivalent to their conjunction in the other order, since and usually means and then in such cases. Questions can be similar: the order \"Is the sky blue, and why is the sky blue?\" makes more sense than the reverse order. Conjunctive commands about behavior are like behavioral assertions, as in get dressed and go to school. Disjunctive commands such love me or leave me or fish or cut bait tend to be asymmetric via the implication that one alternative is less preferable. Conjoined nouns such as tea and milk generally describe aggregation as with set union while tea or milk is a choice. However context can reverse these senses, as in your choices are coffee and tea which usually means the same as your choices are coffee or tea (alternatives). Double negation as in \"I don\\'t not like milk\" rarely means literally \"I do like milk\" but rather conveys some sort of hedging, as though to imply that there is a third possibility. \"Not not P\" can be loosely interpreted as \"surely P\", and although P necessarily implies \"not not P\" the converse is suspect in English, much as with intuitionistic logic. In view of the highly idiosyncratic usage of conjunctions in natural languages, Boolean algebra cannot be considered a reliable framework for interpreting them.\\n\\n\\n==== Digital logic ====\\nBoolean operations are used in digital logic to combine the bits carried on individual wires, thereby interpreting them over {0,1}. When a vector of n identical binary gates are used to combine two bit vectors each of n bits, the individual bit operations can be understood collectively as a single operation on values from a Boolean algebra with 2n elements.\\n\\n\\n==== Naive set theory ====\\nNaive set theory interprets Boolean operations as acting on subsets of a given set X. As we saw earlier this behavior exactly parallels the coordinate-wise combinations of bit vectors, with the union of two sets corresponding to the disjunction of two bit vectors and so on.\\n\\n\\n==== Video cards ====\\nThe 256-element free Boolean algebra on three generators is deployed in computer displays based on raster graphics, which use bit blit to manipulate whole regions consisting of pixels, relying on Boolean operations to specify how the source region should be combined with the destination, typically with the help of a third region called the mask. Modern video cards offer all 223 = 256 ternary operations for this purpose, with the choice of operation being a one-byte (8-bit) parameter. The constants SRC = 0xaa or 10101010, DST = 0xcc or 11001100, and MSK = 0xf0 or 11110000 allow Boolean operations such as (SRC^DST)&MSK (meaning XOR the source and destination and then AND the result with the mask) to be written directly as a constant denoting a byte calculated at compile time, 0x60 in the (SRC^DST)&MSK example, 0x66 if just SRC^DST, etc. At run time the video card interprets the byte as the raster operation indicated by the original expression in a uniform way that requires remarkably little hardware and which takes time completely independent of the complexity of the expression.\\n\\n\\n==== Modeling and CAD ====\\nSolid modeling systems for computer aided design offer a variety of methods for building objects from other objects, combination by Boolean operations being one of them. In this method the space in which objects exist is understood as a set S of voxels (the three-dimensional analogue of pixels in two-dimensional graphics) and shapes are defined as subsets of S, allowing objects to be combined as sets via union, intersection, etc. One obvious use is in building a complex shape from simple shapes simply as the union of the latter. Another use is in sculpting understood as removal of material: any grinding, milling, routing, or drilling operation that can be performed with physical machinery on physical materials can be simulated on the computer with the Boolean operation x ∧ ¬y or x − y, which in set theory is set difference, remove the elements of y from those of x. Thus given two shapes one to be machined and the other the material to be removed, the result of machining the former to remove the latter is described simply as their set difference.\\n\\n\\n==== Boolean searches ====\\nSearch engine queries also employ Boolean logic. For this application, each web page on the Internet may be considered to be an \"element\" of a \"set\". The following examples use a syntax supported by Google.\\nDoublequotes are used to combine whitespace-separated words into a single search term.\\nWhitespace is used to specify logical AND, as it is the default operator for joining search terms:\"Search term 1\" \"Search term 2\"\\n\\nThe OR keyword is used for logical OR:\"Search term 1\" OR \"Search term 2\"\\n\\nA prefixed minus sign is used for logical NOT:\"Search term 1\" −\"Search term 2\"\\n\\n\\n== See also ==\\n\\n\\n== References ==\\n\\n\\n== Sources ==\\nMano, Morris; Ciletti, Michael D. (2013). Digital Design. Pearson. ISBN 978-0-13-277420-8.\\n\\n\\n== Further reading ==\\nJ. Eldon Whitesitt (1995). Boolean algebra and its applications. Courier Dover Publications. ISBN 978-0-486-68483-3. Suitable introduction for students in applied fields.\\nDwinger, Philip (1971). Introduction to Boolean algebras. Würzburg: Physica Verlag.\\nSikorski, Roman (1969). Boolean Algebras (3/e ed.). Berlin: Springer-Verlag. ISBN 978-0-387-04469-9.\\nBocheński, Józef Maria (1959). A Précis of Mathematical Logic. Translated from the French and German editions by Otto Bird. Dordrecht, South Holland:  D. Reidel.\\n\\n\\n=== Historical perspective ===\\nGeorge Boole (1848). \"The Calculus of Logic,\" Cambridge and Dublin Mathematical Journal III: 183–98.\\nTheodore Hailperin (1986). Boole\\'s logic and probability: a critical exposition from the standpoint of contemporary algebra, logic, and probability theory (2nd ed.). Elsevier. ISBN 978-0-444-87952-3.\\nDov M. Gabbay, John Woods, ed. (2004). The rise of modern logic: from Leibniz to Frege. Handbook of the History of Logic. 3. Elsevier. ISBN 978-0-444-51611-4., several relevant chapters by Hailperin, Valencia, and Grattan-Guinness\\nCalixto Badesa (2004). The birth of model theory: Löwenheim\\'s theorem in the frame of the theory of relatives. Princeton University Press. ISBN 978-0-691-05853-5., chapter 1, \"Algebra of Classes and Propositional Calculus\"\\nBurris, Stanley, 2009. The Algebra of Logic Tradition. Stanford Encyclopedia of Philosophy.\\nRadomir S. Stankovic; Jaakko Astola (2011). From Boolean Logic to Switching Circuits and Automata: Towards Modern Information Technology. Springer. ISBN 978-3-642-11681-0.\\n\\n\\n== External links ==\\nBoolean Algebra chapter on All About Circuits\\nHow Stuff Works – Boolean Logic\\nScience and Technology - Boolean Algebra Archived 2013-02-16 at the Wayback Machine contains a list and proof of Boolean theorems and laws.', 'A mathematical model is a description of a  system using mathematical concepts and language. The process of developing a mathematical model is termed mathematical modeling. Mathematical models are used in the natural sciences (such as physics, biology, earth science, chemistry) and engineering disciplines (such as computer science, electrical engineering), as well as in non-physical systems such as the social sciences (such as economics, psychology, sociology, political science). The use of mathematical models to solve problems in business or military operations is a large part of the field of operations research.  Mathematical models are also used in music, linguistics,philosophy (for example, intensively in analytic philosophy), and religion (for example, the recurring uses of the numbers 7, 12 & 40 in the Bible).\\nA model may help to explain a system and to study the effects of different components, and to make predictions about behavior.\\n\\n\\n== Elements of a mathematical model ==\\nMathematical models can take many forms, including dynamical systems, statistical models, differential equations, or game theoretic models.  These and other types of models can overlap, with a given model involving a variety of abstract structures. In general, mathematical models may include logical models.  In many cases, the quality of a scientific field depends on how well the mathematical models developed on the theoretical side agree with results of repeatable experiments.  Lack of agreement between theoretical mathematical models and experimental measurements often leads to important advances as better theories are developed.\\nIn the physical sciences, a traditional mathematical model contains most of the following elements:\\n\\nGoverning equations\\nSupplementary sub-models\\nDefining equations\\nConstitutive equations\\nAssumptions and constraints\\nInitial and boundary conditions\\nClassical constraints and kinematic equations\\n\\n\\n== Classifications ==\\nMathematical models are usually composed of relationships and variables. Relationships can be described by operators, such as algebraic operators, functions, differential operators, etc. Variables are abstractions of system parameters of interest, that can be quantified. Several classification criteria can be used for mathematical models according to their structure:\\n\\nLinear vs. nonlinear: If all the operators in a mathematical model exhibit linearity, the resulting mathematical model is defined as linear. A model is considered to be nonlinear otherwise. The definition of linearity and nonlinearity is dependent on context, and linear models may have nonlinear expressions in them.  For example, in a statistical linear model, it is assumed that a relationship is linear in the parameters, but it may be nonlinear in the predictor variables.  Similarly, a differential equation is said to be linear if it can be written with linear differential operators, but it can still have nonlinear expressions in it.  In a mathematical programming model, if the objective functions and constraints are represented entirely by linear equations, then the model is regarded as a linear model.  If one or more of the objective functions or constraints are represented with a nonlinear equation, then the model is known as a nonlinear model.Linear structure implies that a problem can be decomposed into simpler parts that can be treated independently and/or analyzed at a different scale and the results obtained will remain valid for the initial problem when recomposed and rescaled.Nonlinearity, even in fairly simple systems, is often associated with phenomena such as chaos and irreversibility.  Although there are exceptions, nonlinear systems and models tend to be more difficult to study than linear ones.  A common approach to nonlinear problems is linearization, but this can be problematic if one is trying to study aspects such as irreversibility, which are strongly tied to nonlinearity.\\nStatic vs. dynamic: A dynamic model accounts for time-dependent changes in the state of the system, while a static (or steady-state) model calculates the system in equilibrium, and thus is time-invariant.  Dynamic models typically are represented by differential equations or difference equations.\\nExplicit vs. implicit: If all of the input parameters of the overall model are known, and the output parameters can be calculated by a finite series of computations, the model is said to be explicit. But sometimes it is the output parameters which are known, and the corresponding inputs must be solved for by an iterative procedure, such as Newton\\'s method or Broyden\\'s method. In such a case the model is said to be implicit. For example, a jet engine\\'s physical properties such as turbine and nozzle throat areas can be explicitly calculated given a design thermodynamic cycle (air and fuel flow rates, pressures, and temperatures) at a specific flight condition and power setting, but the engine\\'s operating cycles at other flight conditions and power settings cannot be explicitly calculated from the constant physical properties.\\nDiscrete vs. continuous: A discrete model treats objects as discrete, such as the particles in a molecular model or the states in a statistical model; while a continuous model represents the objects in a continuous manner, such as the velocity field of fluid in pipe flows, temperatures and stresses in a solid, and electric field that applies continuously over the entire model due to a point charge.\\nDeterministic vs. probabilistic (stochastic): A deterministic model is one in which every set of variable states is uniquely determined by parameters in the model and by sets of previous states of these variables; therefore, a deterministic model always performs the same way for a given set of initial conditions. Conversely, in a stochastic model—usually called a \"statistical model\"—randomness is present, and variable states are not described by unique values, but rather by probability distributions.\\nDeductive, inductive, or floating: A deductive model is a logical structure based on a theory. An inductive model arises from empirical findings and generalization from them. The floating model rests on neither theory nor observation, but is merely the invocation of expected structure. Application of mathematics in social sciences outside of economics has been criticized for unfounded models. Application of catastrophe theory in science has been characterized as a floating model.\\nStrategic vs non-strategic Models used in game theory are different in a sense that they model agents with incompatible incentives, such as competing species or bidders in an auction. Strategic models assume that players are autonomous decision makers who rationally choose actions that maximize their objective function. A key challenge of using strategic models is defining and computing solution concepts such as Nash equilibrium. An interesting property of strategic models is that they separate reasoning about rules of the game from reasoning about behavior of the players.\\n\\n\\n== Construction ==\\nIn business and engineering, mathematical models may be used to maximize a certain output. The system under consideration will require certain inputs. The system relating inputs to outputs depends on other variables too: decision variables, state variables, exogenous variables, and random variables.\\nDecision variables are sometimes known as independent variables.  Exogenous variables are sometimes known as parameters or constants.\\nThe variables are not independent of each other as the state variables are dependent on the decision, input, random, and exogenous variables.  Furthermore, the output variables are dependent on the state of the system (represented by the state variables).\\nObjectives and constraints of the system and its users can be represented as functions of the output variables or state variables.  The objective functions will depend on the perspective of the model\\'s user.  Depending on the context, an objective function is also known as an index of performance, as it is some measure of interest to the user.  Although there is no limit to the number of objective functions and constraints a model can have, using or optimizing the model becomes more involved (computationally) as the number increases.\\nFor example, economists often apply linear algebra when using input-output models. Complicated mathematical models that have many variables may be consolidated by use of vectors where one symbol represents several variables.\\n\\n\\n=== A priori information ===\\n\\nMathematical modeling problems are often classified into black box or white box models, according to how much a priori information on the system is available. A black-box model is a system of which there is no a priori information available. A white-box model (also called glass box or clear box) is a system where all necessary information is available. Practically all systems are somewhere between the black-box and white-box models, so this concept is useful only as an intuitive guide for deciding which approach to take.\\nUsually it is preferable to use as much a priori information as possible to make the model more accurate. Therefore, the white-box models are usually considered easier, because if you have used the information correctly, then the model will behave correctly. Often the a priori information comes in forms of knowing the type of functions relating different variables. For example, if we make a model of how a medicine works in a human system, we know that usually the amount of medicine in the blood is an exponentially decaying function. But we are still left with several unknown parameters; how rapidly does the medicine amount decay, and what is the initial amount of medicine in blood? This example is therefore not a completely white-box model. These parameters have to be estimated through some means before one can use the model.\\nIn black-box models one tries to estimate both the functional form of relations between variables and the numerical parameters in those functions. Using a priori information we could end up, for example, with a set of functions that probably could describe the system adequately. If there is no a priori information we would try to use functions as general as possible to cover all different models. An often used approach for black-box models are neural networks which usually do not make assumptions about incoming data. Alternatively the NARMAX (Nonlinear AutoRegressive Moving Average model with eXogenous inputs) algorithms which were developed as part of nonlinear system identification can be used to select the model terms, determine the model structure, and estimate the unknown parameters in the presence of correlated and nonlinear noise. The advantage of NARMAX models compared to neural networks is that NARMAX produces models that can be written down and related to the underlying process, whereas neural networks produce an approximation that is opaque.\\n\\n\\n==== Subjective information ====\\nSometimes it is useful to incorporate subjective information into a mathematical model.  This can be done based on intuition, experience, or expert opinion, or based on convenience of mathematical form. Bayesian statistics provides a theoretical framework for incorporating such subjectivity into a rigorous analysis: we specify a prior probability distribution (which can be subjective), and then update this distribution based on empirical data.\\nAn example of when such approach would be necessary is a situation in which an experimenter bends a coin slightly and tosses it once, recording whether it comes up heads, and is then given the task of predicting the probability that the next flip comes up heads.  After bending the coin, the true probability that the coin will come up heads is unknown; so the experimenter would need to make a decision (perhaps by looking at the shape of the coin) about what prior distribution to use.  Incorporation of such subjective information might be important to get an accurate estimate of the probability.\\n\\n\\n=== Complexity ===\\nIn general, model complexity involves a trade-off between simplicity and accuracy of the model. Occam\\'s razor is a principle particularly relevant to modeling, its essential idea being that among models with roughly equal predictive power, the simplest one is the most desirable. While added complexity usually improves the realism of a model, it can make the model difficult to understand and analyze, and can also pose computational problems, including numerical instability. Thomas Kuhn argues that as science progresses, explanations tend to become more complex before a paradigm shift offers radical simplification.For example, when modeling the flight of an aircraft, we could embed each mechanical part of the aircraft into our model and would thus acquire an almost white-box model of the system. However, the computational cost of adding such a huge amount of detail would effectively inhibit the usage of such a model. Additionally, the uncertainty would increase due to an overly complex system, because each separate part induces some amount of variance into the model. It is therefore usually appropriate to make some approximations to reduce the model to a sensible size. Engineers often can accept some approximations in order to get a more robust and simple model. For example, Newton\\'s classical mechanics is an approximated model of the real world. Still, Newton\\'s model is quite sufficient for most ordinary-life situations, that is, as long as particle speeds are well below the speed of light, and we study macro-particles only.\\nNote that better accuracy does not necessarily mean a better model. Statistical models are prone to overfitting which means that a model is fitted to data too much and it has lost its ability to generalize to new events that were not observed before.\\n\\n\\n=== Training and tuning ===\\nAny model which is not pure white-box contains some parameters that can be used to fit the model to the system it is intended to describe. If the modeling is done by an artificial neural network or other machine learning, the optimization of parameters is called training, while the optimization of model hyperparameters is called tuning and often uses cross-validation. In more conventional modeling through explicitly given mathematical functions, parameters are often determined by curve fitting.\\n\\n\\n=== Model evaluation ===\\nA crucial part of the modeling process is the evaluation of whether or not a given mathematical model describes a system accurately.  This question can be difficult to answer as it involves several different types of evaluation.\\n\\n\\n==== Fit to empirical data ====\\nUsually, the easiest part of model evaluation is checking whether a model fits experimental measurements or other empirical data.  In models with parameters, a common approach to test this fit is to split the data into two disjoint subsets: training data and verification data. The training data are used to estimate the model parameters.  An accurate model will closely match the verification data even though these data were not used to set the model\\'s parameters. This practice is referred to as cross-validation in statistics.\\nDefining a metric to measure distances between observed and predicted data is a useful tool for assessing model fit.  In statistics, decision theory, and some economic models, a loss function plays a similar role.\\nWhile it is rather straightforward to test the appropriateness of parameters, it can be more difficult to test the validity of the general mathematical form of a model.  In general, more mathematical tools have been developed to test the fit of statistical models than models involving differential equations.  Tools from nonparametric statistics can sometimes be used to evaluate how well the data fit a known distribution or to come up with a general model that makes only minimal assumptions about the model\\'s mathematical form.\\n\\n\\n==== Scope of the model ====\\nAssessing the scope of a model, that is, determining what situations the model is applicable to, can be less straightforward.  If the model was constructed based on a set of data, one must determine for which systems or situations the known data is a \"typical\" set of data.\\nThe question of whether the model describes well the properties of the system between data points is called interpolation, and the same question for events or data points outside the observed data is called extrapolation.\\nAs an example of the typical limitations of the scope of a model, in evaluating Newtonian classical mechanics, we can note that Newton made his measurements without advanced equipment, so he could not measure properties of particles travelling at speeds close to the speed of light.  Likewise, he did not measure the movements of molecules and other small particles, but macro particles only. It is then not surprising that his model does not extrapolate well into these domains, even though his model is quite sufficient for ordinary life physics.\\n\\n\\n==== Philosophical considerations ====\\nMany types of modeling implicitly involve claims about causality.  This is usually (but not always) true of models involving differential equations.  As the purpose of modeling is to increase our understanding of the world, the validity of a model rests not only on its fit to empirical observations, but also on its ability to extrapolate to situations or data beyond those originally described in the model. One can think of this as the differentiation between qualitative and quantitative predictions. One can also argue that a model is worthless unless it provides some insight which goes beyond what is already known from direct investigation of the phenomenon being studied.\\nAn example of such criticism is the argument that the mathematical models of optimal foraging theory do not offer insight that goes beyond the common-sense conclusions of evolution and other basic principles of ecology.\\n\\n\\n== Significance in the natural sciences ==\\nMathematical models are of great importance in the natural sciences, particularly in physics. Physical theories are almost invariably expressed using mathematical models.\\nThroughout history, more and more accurate mathematical models have been developed. Newton\\'s laws accurately describe many everyday phenomena, but at certain limits theory of relativity and quantum mechanics must be used. \\nIt is common to use idealized models in physics to simplify things. Massless ropes, point particles, ideal gases and the particle in a box are among the many simplified models used in physics. The laws of physics are represented with simple equations such as Newton\\'s laws, Maxwell\\'s equations and the Schrödinger equation. These laws are a basis for making mathematical models of real situations. Many real situations are very complex and thus modeled approximate on a computer, a model that is computationally feasible to compute is made from the basic laws or from approximate models made from the basic laws. For example, molecules can be modeled by molecular orbital models that are approximate solutions to the Schrödinger equation. In engineering, physics models are often made by mathematical methods such as finite element analysis.\\nDifferent mathematical models use different geometries that are not necessarily accurate descriptions of the geometry of the universe. Euclidean geometry is much used in classical physics, while special relativity and general relativity are examples of theories that use geometries which are not Euclidean.\\n\\n\\n== Some applications ==\\nOften when engineers analyze a system to be controlled or optimized, they use a mathematical model. In analysis, engineers can build a descriptive model of the system as a hypothesis of how the system could work, or try to estimate how an unforeseeable event could affect the system. Similarly, in control of a system, engineers can try out different control approaches in simulations.\\nA mathematical model usually describes a system by a set of variables and a set of equations that establish relationships between the variables. Variables may be of many types; real or integer numbers, boolean values or strings, for example. The variables represent some properties of the system, for example, measured system outputs often in the form of signals, timing data, counters, and event occurrence (yes/no). The actual model is the set of functions that describe the relations between the different variables.\\n\\n\\n== Examples ==\\nOne of the popular examples in computer science is the mathematical models of various machines, an example is the deterministic finite automaton (DFA) which is defined as an abstract mathematical concept, but due to the deterministic nature of a DFA, it is implementable in hardware and software for solving various specific problems. For example, the following is a DFA M with a binary alphabet, which requires that the input contains an even number of 0s.\\nM = (Q, Σ, δ, q0, F) where\\n\\nQ = {S1, S2},\\nΣ = {0, 1},\\nq0 = S1,\\nF = {S1}, and\\nδ is defined by the following state transition table:The state S1 represents that there has been an even number of 0s in the input so far, while S2 signifies an odd number. A 1 in the input does not change the state of the automaton. When the input ends, the state will show whether the input contained an even number of 0s or not. If the input did contain an even number of 0s, M will finish in state S1, an accepting state, so the input string will be accepted.\\nThe language recognized by M is the regular language given by the regular expression 1*( 0 (1*) 0 (1*) )*, where \"*\" is the Kleene star, e.g., 1* denotes any non-negative number (possibly zero) of symbols \"1\".\\n\\nMany everyday activities carried out without a thought are uses of mathematical models. A geographical map projection of a region of the earth onto a small, plane surface is a model which can be used for many purposes such as planning travel.\\nAnother simple activity is predicting the position of a vehicle from its initial position, direction and speed of travel, using the equation that distance traveled is the product of time and speed. This is known as dead reckoning when used more formally. Mathematical modeling in this way does not necessarily require formal mathematics; animals have been shown to use dead reckoning.\\nPopulation Growth. A simple (though approximate) model of population growth is the Malthusian growth model. A slightly more realistic and largely used population growth model is the logistic function, and its extensions.\\nModel of a particle in a potential-field. In this model we consider a particle as being a point of mass which describes a trajectory in space which is modeled by a function giving its coordinates in space as a function of time. The potential field is given by a function \\n  \\n    \\n      \\n        V\\n        \\n        :\\n        \\n          \\n            R\\n          \\n          \\n            3\\n          \\n        \\n        \\n        →\\n        \\n          R\\n        \\n      \\n    \\n    {\\\\displaystyle V\\\\!:\\\\mathbb {R} ^{3}\\\\!\\\\rightarrow \\\\mathbb {R} }\\n   and the trajectory, that is a function \\n  \\n    \\n      \\n        \\n          r\\n        \\n        \\n        :\\n        \\n          R\\n        \\n        →\\n        \\n          \\n            R\\n          \\n          \\n            3\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle \\\\mathbf {r} \\\\!:\\\\mathbb {R} \\\\rightarrow \\\\mathbb {R} ^{3}}\\n  , is the solution of the differential equation:\\n  \\n    \\n      \\n        −\\n        \\n          \\n            \\n              \\n                \\n                  d\\n                \\n                \\n                  2\\n                \\n              \\n              \\n                r\\n              \\n              (\\n              t\\n              )\\n            \\n            \\n              \\n                d\\n              \\n              \\n                t\\n                \\n                  2\\n                \\n              \\n            \\n          \\n        \\n        m\\n        =\\n        \\n          \\n            \\n              ∂\\n              V\\n              [\\n              \\n                r\\n              \\n              (\\n              t\\n              )\\n              ]\\n            \\n            \\n              ∂\\n              x\\n            \\n          \\n        \\n        \\n          \\n            \\n              x\\n              ^\\n            \\n          \\n        \\n        +\\n        \\n          \\n            \\n              ∂\\n              V\\n              [\\n              \\n                r\\n              \\n              (\\n              t\\n              )\\n              ]\\n            \\n            \\n              ∂\\n              y\\n            \\n          \\n        \\n        \\n          \\n            \\n              y\\n              ^\\n            \\n          \\n        \\n        +\\n        \\n          \\n            \\n              ∂\\n              V\\n              [\\n              \\n                r\\n              \\n              (\\n              t\\n              )\\n              ]\\n            \\n            \\n              ∂\\n              z\\n            \\n          \\n        \\n        \\n          \\n            \\n              z\\n              ^\\n            \\n          \\n        \\n        ,\\n      \\n    \\n    {\\\\displaystyle -{\\\\frac {\\\\mathrm {d} ^{2}\\\\mathbf {r} (t)}{\\\\mathrm {d} t^{2}}}m={\\\\frac {\\\\partial V[\\\\mathbf {r} (t)]}{\\\\partial x}}\\\\mathbf {\\\\hat {x}} +{\\\\frac {\\\\partial V[\\\\mathbf {r} (t)]}{\\\\partial y}}\\\\mathbf {\\\\hat {y}} +{\\\\frac {\\\\partial V[\\\\mathbf {r} (t)]}{\\\\partial z}}\\\\mathbf {\\\\hat {z}} ,}\\n  that can be written also as:\\n\\n  \\n    \\n      \\n        m\\n        \\n          \\n            \\n              \\n                \\n                  d\\n                \\n                \\n                  2\\n                \\n              \\n              \\n                r\\n              \\n              (\\n              t\\n              )\\n            \\n            \\n              \\n                d\\n              \\n              \\n                t\\n                \\n                  2\\n                \\n              \\n            \\n          \\n        \\n        =\\n        −\\n        ∇\\n        V\\n        [\\n        \\n          r\\n        \\n        (\\n        t\\n        )\\n        ]\\n        .\\n      \\n    \\n    {\\\\displaystyle m{\\\\frac {\\\\mathrm {d} ^{2}\\\\mathbf {r} (t)}{\\\\mathrm {d} t^{2}}}=-\\\\nabla V[\\\\mathbf {r} (t)].}\\n  Note this model assumes the particle is a point mass, which is certainly known to be false in many cases in which we use this model; for example, as a model of planetary motion.Model of rational behavior for a consumer.  In this model we assume a consumer faces a choice of n commodities labeled 1,2,...,n each with a market price p1, p2,..., pn. The consumer is assumed to have an ordinal utility function U (ordinal in the sense that only the sign of the differences between two utilities, and not the level of each utility, is meaningful), depending on the amounts of commodities x1, x2,..., xn consumed.  The model further assumes that the consumer has a budget M which is used to purchase a vector x1, x2,..., xn in such a way as to maximize U(x1, x2,..., xn).  The problem of rational behavior in this model then becomes a mathematical optimization problem, that is:\\n  \\n    \\n      \\n        max\\n        U\\n        (\\n        \\n          x\\n          \\n            1\\n          \\n        \\n        ,\\n        \\n          x\\n          \\n            2\\n          \\n        \\n        ,\\n        …\\n        ,\\n        \\n          x\\n          \\n            n\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle \\\\max U(x_{1},x_{2},\\\\ldots ,x_{n})}\\n  \\nsubject to:\\n\\n  \\n    \\n      \\n        \\n          ∑\\n          \\n            i\\n            =\\n            1\\n          \\n          \\n            n\\n          \\n        \\n        \\n          p\\n          \\n            i\\n          \\n        \\n        \\n          x\\n          \\n            i\\n          \\n        \\n        ≤\\n        M\\n        .\\n      \\n    \\n    {\\\\displaystyle \\\\sum _{i=1}^{n}p_{i}x_{i}\\\\leq M.}\\n  \\n\\n  \\n    \\n      \\n        \\n          x\\n          \\n            i\\n          \\n        \\n        ≥\\n        0\\n        \\n        \\n        \\n        ∀\\n        i\\n        ∈\\n        {\\n        1\\n        ,\\n        2\\n        ,\\n        …\\n        ,\\n        n\\n        }\\n      \\n    \\n    {\\\\displaystyle x_{i}\\\\geq 0\\\\;\\\\;\\\\;\\\\forall i\\\\in \\\\{1,2,\\\\ldots ,n\\\\}}\\n  \\nThis model has been used in a wide variety of economic contexts, such as in general equilibrium theory to show existence and Pareto efficiency of economic equilibria.Neighbour-sensing model is a model that explains the mushroom formation from the initially chaotic fungal network.\\nIn computer science, mathematical models may be used to simulate computer networks.\\nIn mechanics, mathematical models may be used to analyze the movement of a rocket model.\\n\\n\\n== See also ==\\n\\n\\n== References ==\\n\\n\\n== Further reading ==\\n\\n\\n=== Books ===\\nAris, Rutherford [ 1978 ] ( 1994 ). Mathematical Modelling Techniques, New York: Dover. ISBN 0-486-68131-9\\nBender, E.A. [ 1978 ] ( 2000 ). An Introduction to Mathematical Modeling, New York: Dover. ISBN 0-486-41180-X\\nGary Chartrand (1977) Graphs as Mathematical Models, Prindle, Webber & Schmidt ISBN 0871502364\\nDubois, G. (2018) \"Modeling and Simulation\", Taylor & Francis, CRC Press.\\nGershenfeld, N. (1998) The Nature of Mathematical Modeling, Cambridge University Press ISBN 0-521-57095-6 .\\nLin, C.C. & Segel, L.A. ( 1988 ). Mathematics Applied to Deterministic Problems in the Natural Sciences, Philadelphia: SIAM. ISBN 0-89871-229-7\\n\\n\\n=== Specific applications ===\\nPapadimitriou, Fivos. (2010). Mathematical Modelling of Spatial-Ecological Complex Systems: an Evaluation. Geography, Environment, Sustainability 1(3), 67-80. doi:10.24057/2071-9388-2010-3-1-67-80\\nPeierls, R. (1980). \"Model-making in physics\". Contemporary Physics. 21: 3–17. Bibcode:1980ConPh..21....3P. doi:10.1080/00107518008210938.\\nAn Introduction to Infectious Disease Modelling by Emilia Vynnycky and Richard G White.\\n\\n\\n== External links ==\\nGeneral referencePatrone, F. Introduction to modeling via differential equations, with critical remarks.\\nPlus teacher and student package: Mathematical Modelling. Brings together all articles on mathematical modeling from Plus Magazine, the online mathematics magazine produced by the Millennium Mathematics Project at the University of Cambridge.PhilosophicalFrigg, R. and S. Hartmann, Models in Science, in: The Stanford Encyclopedia of Philosophy, (Spring 2006 Edition)\\nGriffiths, E. C. (2010) What is a model?', 'Number theory (or arithmetic or higher arithmetic in older usage) is a branch of pure mathematics devoted primarily to the study of the integers and integer-valued functions. German mathematician Carl Friedrich Gauss (1777–1855) said, \"Mathematics is the queen of the sciences—and number theory is the queen of mathematics.\" Number theorists study prime numbers as well as the properties of mathematical objects made out of integers (for example, rational numbers) or defined as generalizations of the integers (for example, algebraic integers). \\nIntegers can be considered either in themselves or as solutions to equations (Diophantine geometry). Questions in number theory are often best understood through the study of analytical objects (for example, the Riemann zeta function) that encode properties of the integers, primes or other number-theoretic objects in some fashion (analytic number theory). One may also study real numbers in relation to rational numbers, for example, as approximated by the latter (Diophantine approximation).\\nThe older term for number theory is arithmetic. By the early twentieth century, it had been superseded by \"number theory\". (The word \"arithmetic\" is used by the general public to mean \"elementary calculations\"; it has also acquired other meanings in mathematical logic, as in Peano arithmetic, and computer science, as in floating point arithmetic.) The use of the term arithmetic for number theory regained some ground in the second half of the 20th century, arguably in part due to French influence. In particular, arithmetical is commonly preferred as an adjective to number-theoretic.\\n\\n\\n== History ==\\n\\n\\n=== Origins ===\\n\\n\\n==== Dawn of arithmetic ====\\n\\nThe earliest historical find of an arithmetical nature is a fragment of a table: the broken clay tablet Plimpton 322 (Larsa, Mesopotamia, ca. 1800 BCE) contains a list of \"Pythagorean triples\", that is, integers \\n  \\n    \\n      \\n        (\\n        a\\n        ,\\n        b\\n        ,\\n        c\\n        )\\n      \\n    \\n    {\\\\displaystyle (a,b,c)}\\n   such that \\n  \\n    \\n      \\n        \\n          a\\n          \\n            2\\n          \\n        \\n        +\\n        \\n          b\\n          \\n            2\\n          \\n        \\n        =\\n        \\n          c\\n          \\n            2\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle a^{2}+b^{2}=c^{2}}\\n  .\\nThe triples are too many and too large to have been obtained by brute force. The heading over the first column reads: \"The takiltum of the diagonal which has been subtracted such that the width...\"The table\\'s layout suggests that it was constructed by means of what amounts, in modern language, to the identity\\n\\n  \\n    \\n      \\n        \\n          \\n            (\\n            \\n              \\n                \\n                  1\\n                  2\\n                \\n              \\n              \\n                (\\n                \\n                  x\\n                  −\\n                  \\n                    \\n                      1\\n                      x\\n                    \\n                  \\n                \\n                )\\n              \\n            \\n            )\\n          \\n          \\n            2\\n          \\n        \\n        +\\n        1\\n        =\\n        \\n          \\n            (\\n            \\n              \\n                \\n                  1\\n                  2\\n                \\n              \\n              \\n                (\\n                \\n                  x\\n                  +\\n                  \\n                    \\n                      1\\n                      x\\n                    \\n                  \\n                \\n                )\\n              \\n            \\n            )\\n          \\n          \\n            2\\n          \\n        \\n        ,\\n      \\n    \\n    {\\\\displaystyle \\\\left({\\\\frac {1}{2}}\\\\left(x-{\\\\frac {1}{x}}\\\\right)\\\\right)^{2}+1=\\\\left({\\\\frac {1}{2}}\\\\left(x+{\\\\frac {1}{x}}\\\\right)\\\\right)^{2},}\\n  which is implicit in routine Old Babylonian exercises. If some other method was used, the triples were first constructed and then reordered by \\n  \\n    \\n      \\n        c\\n        \\n          /\\n        \\n        a\\n      \\n    \\n    {\\\\displaystyle c/a}\\n  , presumably for actual use as a \"table\", for example, with a view to applications.\\nIt is not known what these applications may have been, or whether there could have been any; Babylonian astronomy, for example, truly came into its own only later. It has been suggested instead that the table was a source of numerical examples for school problems.While Babylonian number theory—or what survives of Babylonian mathematics that can be called thus—consists of this single, striking fragment, Babylonian algebra (in the secondary-school sense of \"algebra\") was exceptionally well developed. Late Neoplatonic sources state that Pythagoras learned mathematics from the Babylonians. Much earlier sources state that Thales and Pythagoras traveled and studied in Egypt.\\nEuclid IX 21–34 is very probably Pythagorean; it is very simple material (\"odd times even is even\", \"if an odd number measures [= divides] an even number, then it also measures [= divides] half of it\"), but it is all that is needed to prove that \\n  \\n    \\n      \\n        \\n          \\n            2\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\sqrt {2}}}\\n  \\nis irrational. Pythagorean mystics gave great importance to the odd and the even.\\nThe discovery that \\n  \\n    \\n      \\n        \\n          \\n            2\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\sqrt {2}}}\\n   is irrational is credited to the early Pythagoreans (pre-Theodorus). By revealing (in modern terms) that numbers could be irrational, this discovery seems to have provoked the first foundational crisis in mathematical history; its proof or its divulgation are sometimes credited to Hippasus, who was expelled or split from the Pythagorean sect. This forced a distinction between numbers (integers and the rationals—the subjects of arithmetic), on the one hand, and lengths and proportions (which we would identify with real numbers, whether rational or not), on the other hand.\\nThe Pythagorean tradition spoke also of so-called polygonal or figurate numbers. While square numbers, cubic numbers, etc., are seen now as more natural than triangular numbers, pentagonal numbers, etc., the study of the sums of triangular and pentagonal numbers would prove fruitful in the early modern period (17th to early 19th century).\\nWe know of no clearly arithmetical material in ancient Egyptian or Vedic sources, though there is some algebra in each. The Chinese remainder theorem appears as an exercise  in Sunzi Suanjing (3rd, 4th or 5th century CE). (There is one important step glossed over in Sunzi\\'s solution: it is the problem that was later solved by Āryabhaṭa\\'s Kuṭṭaka – see below.)\\nThere is also some numerical mysticism in Chinese mathematics, but, unlike that of the Pythagoreans, it seems to have led nowhere. Like the Pythagoreans\\' perfect numbers, magic squares have passed from superstition into recreation.\\n\\n\\n==== Classical Greece and the early Hellenistic period ====\\n\\nAside from a few fragments, the mathematics of Classical Greece is known to us either through the reports of contemporary non-mathematicians or through mathematical works from the early Hellenistic period. In the case of number theory, this means, by and large, Plato and Euclid, respectively.\\nWhile Asian mathematics influenced Greek and Hellenistic learning, it seems to be the case that Greek mathematics is also an indigenous tradition.\\nEusebius, PE X, chapter 4 mentions of Pythagoras:\\n\\n\"In fact the said Pythagoras, while busily studying the wisdom of each nation, visited Babylon, and Egypt, and all Persia, being instructed by the Magi and the priests: and in addition to these he is related to have studied under the Brahmans (these are Indian philosophers); and from some he gathered astrology, from others geometry, and arithmetic and music from others, and different things from different nations, and only from the wise men of Greece did he get nothing, wedded as they were to a poverty and dearth of wisdom: so on the contrary he himself became the author of instruction to the Greeks in the learning which he had procured from abroad.\"\\nAristotle claimed that the philosophy of Plato closely followed the teachings of the Pythagoreans, and Cicero repeats this claim: Platonem ferunt didicisse Pythagorea omnia (\"They say Plato learned all things Pythagorean\").Plato had a keen interest in mathematics, and distinguished clearly between arithmetic and calculation. (By arithmetic he meant, in part, theorising on number, rather than what arithmetic or number theory have come to mean.) It is through one of Plato\\'s dialogues—namely, Theaetetus—that we know that Theodorus had proven that \\n  \\n    \\n      \\n        \\n          \\n            3\\n          \\n        \\n        ,\\n        \\n          \\n            5\\n          \\n        \\n        ,\\n        …\\n        ,\\n        \\n          \\n            17\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\sqrt {3}},{\\\\sqrt {5}},\\\\dots ,{\\\\sqrt {17}}}\\n   are irrational. Theaetetus was, like Plato, a disciple of Theodorus\\'s; he worked on distinguishing different kinds of incommensurables, and was thus arguably a pioneer in the study of number systems. (Book X of Euclid\\'s Elements is described by Pappus as being largely based on Theaetetus\\'s work.)\\nEuclid devoted part of his Elements to prime numbers and divisibility, topics that belong unambiguously to number theory and are basic to it (Books VII to IX of Euclid\\'s Elements). In particular, he gave an algorithm for computing the greatest common divisor of two numbers (the Euclidean algorithm; Elements, Prop. VII.2) and the first known proof of the infinitude of primes (Elements, Prop. IX.20).\\nIn 1773, Lessing published an epigram he had found in a manuscript during his work as a librarian; it claimed to be a letter sent by Archimedes to Eratosthenes. The epigram proposed what has become known as\\nArchimedes\\'s cattle problem; its solution (absent from the manuscript) requires solving an indeterminate quadratic equation (which reduces to what would later be misnamed Pell\\'s equation). As far as we know, such equations were first successfully treated by the Indian school. It is not known whether Archimedes himself had a method of solution.\\n\\n\\n==== Diophantus ====\\n\\nVery little is known about Diophantus of Alexandria; he probably lived in the third century CE, that is, about five hundred years after Euclid. Six out of the thirteen books of Diophantus\\'s Arithmetica survive in the original Greek and four more survive in an Arabic translation. The Arithmetica is a collection of worked-out problems where the task is invariably to find rational solutions to a system of polynomial equations, usually of the form \\n  \\n    \\n      \\n        f\\n        (\\n        x\\n        ,\\n        y\\n        )\\n        =\\n        \\n          z\\n          \\n            2\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle f(x,y)=z^{2}}\\n   or \\n  \\n    \\n      \\n        f\\n        (\\n        x\\n        ,\\n        y\\n        ,\\n        z\\n        )\\n        =\\n        \\n          w\\n          \\n            2\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle f(x,y,z)=w^{2}}\\n  . Thus, nowadays, we speak of Diophantine equations when we speak of polynomial equations to which rational or integer solutions must be found.\\nOne may say that Diophantus was studying rational points, that is, points whose coordinates are rational—on curves and algebraic varieties; however, unlike the Greeks of the Classical period, who did what we would now call basic algebra in geometrical terms, Diophantus did what we would now call basic algebraic geometry in purely algebraic terms. In modern language, what Diophantus did was to find rational parametrizations of varieties; that is, given an equation of the form (say)\\n\\n  \\n    \\n      \\n        f\\n        (\\n        \\n          x\\n          \\n            1\\n          \\n        \\n        ,\\n        \\n          x\\n          \\n            2\\n          \\n        \\n        ,\\n        \\n          x\\n          \\n            3\\n          \\n        \\n        )\\n        =\\n        0\\n      \\n    \\n    {\\\\displaystyle f(x_{1},x_{2},x_{3})=0}\\n  , his aim was to find (in essence) three rational functions \\n  \\n    \\n      \\n        \\n          g\\n          \\n            1\\n          \\n        \\n        ,\\n        \\n          g\\n          \\n            2\\n          \\n        \\n        ,\\n        \\n          g\\n          \\n            3\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle g_{1},g_{2},g_{3}}\\n   such that, for all values of \\n  \\n    \\n      \\n        r\\n      \\n    \\n    {\\\\displaystyle r}\\n   and \\n  \\n    \\n      \\n        s\\n      \\n    \\n    {\\\\displaystyle s}\\n  , setting\\n\\n  \\n    \\n      \\n        \\n          x\\n          \\n            i\\n          \\n        \\n        =\\n        \\n          g\\n          \\n            i\\n          \\n        \\n        (\\n        r\\n        ,\\n        s\\n        )\\n      \\n    \\n    {\\\\displaystyle x_{i}=g_{i}(r,s)}\\n   for \\n  \\n    \\n      \\n        i\\n        =\\n        1\\n        ,\\n        2\\n        ,\\n        3\\n      \\n    \\n    {\\\\displaystyle i=1,2,3}\\n   gives a solution to \\n  \\n    \\n      \\n        f\\n        (\\n        \\n          x\\n          \\n            1\\n          \\n        \\n        ,\\n        \\n          x\\n          \\n            2\\n          \\n        \\n        ,\\n        \\n          x\\n          \\n            3\\n          \\n        \\n        )\\n        =\\n        0.\\n      \\n    \\n    {\\\\displaystyle f(x_{1},x_{2},x_{3})=0.}\\n  \\nDiophantus also studied the equations of some non-rational curves, for which no rational parametrisation is possible. He managed to find some rational points on these curves (elliptic curves, as it happens, in what seems to be their first known occurrence) by means of what amounts to a tangent construction: translated into coordinate geometry\\n(which did not exist in Diophantus\\'s time), his method would be visualised as drawing a tangent to a curve at a known rational point, and then finding the other point of intersection of the tangent with the curve; that other point is a new rational point. (Diophantus also resorted to what could be called a special case of a secant construction.)\\nWhile Diophantus was concerned largely with rational solutions, he assumed some results on integer numbers, in particular that every integer is the sum of four squares (though he never stated as much explicitly).\\n\\n\\n==== Āryabhaṭa, Brahmagupta, Bhāskara ====\\nWhile Greek astronomy probably influenced Indian learning, to the point of introducing trigonometry, it seems to be the case that Indian mathematics is otherwise an indigenous tradition; in particular, there is no evidence that Euclid\\'s Elements reached India before the 18th century.Āryabhaṭa (476–550 CE) showed that pairs of simultaneous congruences \\n  \\n    \\n      \\n        n\\n        ≡\\n        \\n          a\\n          \\n            1\\n          \\n        \\n        \\n          \\n            mod\\n            \\n              m\\n            \\n          \\n          \\n            1\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle n\\\\equiv a_{1}{\\\\bmod {m}}_{1}}\\n  , \\n  \\n    \\n      \\n        n\\n        ≡\\n        \\n          a\\n          \\n            2\\n          \\n        \\n        \\n          \\n            mod\\n            \\n              m\\n            \\n          \\n          \\n            2\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle n\\\\equiv a_{2}{\\\\bmod {m}}_{2}}\\n   could be solved by a method he called kuṭṭaka, or pulveriser; this is a procedure close to (a generalisation of) the Euclidean algorithm, which was probably discovered independently in India. Āryabhaṭa seems to have had in mind applications to astronomical calculations.Brahmagupta (628 CE) started the systematic study of indefinite quadratic equations—in particular, the misnamed Pell equation, in which Archimedes may have first been interested, and which did not start to be solved in the West until the time of Fermat and Euler. Later Sanskrit authors would follow, using Brahmagupta\\'s technical terminology. A general procedure (the chakravala, or \"cyclic method\") for solving Pell\\'s equation was finally found by Jayadeva (cited in the eleventh century; his work is otherwise lost); the earliest surviving exposition appears in Bhāskara II\\'s Bīja-gaṇita (twelfth century).Indian mathematics remained largely unknown in Europe until the late eighteenth century; Brahmagupta and Bhāskara\\'s work was translated into English in 1817 by Henry Colebrooke.\\n\\n\\n==== Arithmetic in the Islamic golden age ====\\n\\nIn the early ninth century, the caliph Al-Ma\\'mun ordered translations of many Greek mathematical works and at least one Sanskrit work (the Sindhind,\\nwhich may  or may not be Brahmagupta\\'s Brāhmasphuṭasiddhānta).\\nDiophantus\\'s main work, the Arithmetica, was translated into Arabic by Qusta ibn Luqa (820–912).\\nPart of the treatise al-Fakhri (by al-Karajī, 953 – ca. 1029) builds on it to some extent. According to Rashed Roshdi, Al-Karajī\\'s contemporary Ibn al-Haytham knew what would later be called Wilson\\'s theorem.\\n\\n\\n==== Western Europe in the Middle Ages ====\\nOther than a treatise on squares in arithmetic progression by Fibonacci—who traveled and studied in north Africa and Constantinople—no number theory to speak of was done in western Europe during the Middle Ages. Matters started to change in Europe in the late Renaissance, thanks to a renewed study of the works of Greek antiquity. A catalyst was the textual emendation and translation into Latin of Diophantus\\' Arithmetica.\\n\\n\\n=== Early modern number theory ===\\n\\n\\n==== Fermat ====\\n\\nPierre de Fermat (1607–1665) never published his writings; in particular, his work on number theory is contained almost entirely in letters to mathematicians and in private marginal notes. In his notes and letters, he scarcely wrote any proofs - he had no models in the area.Over his lifetime, Fermat made the following contributions to the field:\\n\\nOne of Fermat\\'s first interests was perfect numbers (which appear in Euclid, Elements IX) and amicable numbers; these topics led him to work on integer divisors, which were from the beginning among the subjects of the correspondence (1636 onwards) that put him in touch with the mathematical community of the day.In 1638, Fermat claimed, without proof, that all whole numbers can be expressed as the sum of four squares or fewer.\\nFermat\\'s little theorem (1640): if a is not divisible by a prime p, then \\n  \\n    \\n      \\n        \\n          a\\n          \\n            p\\n            −\\n            1\\n          \\n        \\n        ≡\\n        1\\n        \\n          mod\\n          \\n            p\\n          \\n        \\n        .\\n      \\n    \\n    {\\\\displaystyle a^{p-1}\\\\equiv 1{\\\\bmod {p}}.}\\n  \\nIf a and b are coprime, then \\n  \\n    \\n      \\n        \\n          a\\n          \\n            2\\n          \\n        \\n        +\\n        \\n          b\\n          \\n            2\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle a^{2}+b^{2}}\\n   is not divisible by any prime congruent to −1 modulo 4; and every prime congruent to 1 modulo 4 can be written in the form \\n  \\n    \\n      \\n        \\n          a\\n          \\n            2\\n          \\n        \\n        +\\n        \\n          b\\n          \\n            2\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle a^{2}+b^{2}}\\n  . These two statements also date from 1640; in 1659, Fermat stated to Huygens that he had proven the latter statement by the method of infinite descent.\\nIn 1657, Fermat posed the problem of solving \\n  \\n    \\n      \\n        \\n          x\\n          \\n            2\\n          \\n        \\n        −\\n        N\\n        \\n          y\\n          \\n            2\\n          \\n        \\n        =\\n        1\\n      \\n    \\n    {\\\\displaystyle x^{2}-Ny^{2}=1}\\n   as a challenge to English mathematicians. The problem was solved in a few months by Wallis and Brouncker. Fermat considered their solution valid, but pointed out they had provided an algorithm without a proof (as had Jayadeva and Bhaskara, though Fermat wasn\\'t aware of this). He stated that a proof could be found by infinite descent.\\nFermat stated and proved (by infinite descent) in the appendix to Observations on Diophantus (Obs. XLV) that \\n  \\n    \\n      \\n        \\n          x\\n          \\n            4\\n          \\n        \\n        +\\n        \\n          y\\n          \\n            4\\n          \\n        \\n        =\\n        \\n          z\\n          \\n            4\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle x^{4}+y^{4}=z^{4}}\\n   has no non-trivial solutions in the integers. Fermat also mentioned to his correspondents that \\n  \\n    \\n      \\n        \\n          x\\n          \\n            3\\n          \\n        \\n        +\\n        \\n          y\\n          \\n            3\\n          \\n        \\n        =\\n        \\n          z\\n          \\n            3\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle x^{3}+y^{3}=z^{3}}\\n   has no non-trivial solutions, and that this could also be proven by infinite descent. The first known proof is due to Euler (1753; indeed by infinite descent).\\nFermat claimed (Fermat\\'s Last Theorem) to have shown there are no solutions to \\n  \\n    \\n      \\n        \\n          x\\n          \\n            n\\n          \\n        \\n        +\\n        \\n          y\\n          \\n            n\\n          \\n        \\n        =\\n        \\n          z\\n          \\n            n\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle x^{n}+y^{n}=z^{n}}\\n   for all \\n  \\n    \\n      \\n        n\\n        ≥\\n        3\\n      \\n    \\n    {\\\\displaystyle n\\\\geq 3}\\n  ; this claim appears in his annotations in the margins of his copy of Diophantus.\\n\\n\\n==== Euler ====\\n\\nThe interest of Leonhard Euler (1707–1783) in number theory was first spurred in 1729, when a friend of his, the amateur Goldbach, pointed him towards some of Fermat\\'s work on the subject. This has been called the \"rebirth\" of modern number theory, after Fermat\\'s relative lack of success in getting his contemporaries\\' attention for the subject. Euler\\'s work on number theory includes the following:\\nProofs for Fermat\\'s statements. This includes Fermat\\'s little theorem (generalised by Euler to non-prime moduli); the fact that \\n  \\n    \\n      \\n        p\\n        =\\n        \\n          x\\n          \\n            2\\n          \\n        \\n        +\\n        \\n          y\\n          \\n            2\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle p=x^{2}+y^{2}}\\n   if and only if \\n  \\n    \\n      \\n        p\\n        ≡\\n        1\\n        \\n          mod\\n          \\n            4\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle p\\\\equiv 1{\\\\bmod {4}}}\\n  ; initial work towards a proof that every integer is the sum of four squares (the first complete proof is by Joseph-Louis Lagrange (1770), soon improved by Euler himself); the lack of non-zero integer solutions to \\n  \\n    \\n      \\n        \\n          x\\n          \\n            4\\n          \\n        \\n        +\\n        \\n          y\\n          \\n            4\\n          \\n        \\n        =\\n        \\n          z\\n          \\n            2\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle x^{4}+y^{4}=z^{2}}\\n   (implying the case n=4 of Fermat\\'s last theorem, the case n=3 of which Euler also proved by a related method).\\nPell\\'s equation, first misnamed by Euler. He wrote on the link between continued fractions and Pell\\'s equation.\\nFirst steps towards analytic number theory. In his work of sums of four squares, partitions, pentagonal numbers, and the distribution of prime numbers, Euler pioneered the use of what can be seen as analysis (in particular, infinite series) in number theory. Since he lived before the development of complex analysis, most of his work is restricted to the formal manipulation of power series. He did, however, do some very notable (though not fully rigorous) early work on what would later be called the Riemann zeta function.\\nQuadratic forms. Following Fermat\\'s lead, Euler did further research on the question of which primes can be expressed in the form \\n  \\n    \\n      \\n        \\n          x\\n          \\n            2\\n          \\n        \\n        +\\n        N\\n        \\n          y\\n          \\n            2\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle x^{2}+Ny^{2}}\\n  , some of it prefiguring quadratic reciprocity. \\nDiophantine equations. Euler worked on some Diophantine equations of genus 0 and 1. In particular, he studied Diophantus\\'s work; he tried to systematise it, but the time was not yet ripe for such an endeavour—algebraic geometry was still in its infancy. He did notice there was a connection between Diophantine problems and elliptic integrals, whose study he had himself initiated.\\n\\n\\n==== Lagrange, Legendre, and Gauss ====\\n\\nJoseph-Louis Lagrange (1736–1813) was the first to give full proofs of some of Fermat\\'s and Euler\\'s work and observations—for instance, the four-square theorem and the basic theory of the misnamed \"Pell\\'s equation\" (for which an algorithmic solution was found by Fermat and his contemporaries, and also by Jayadeva and Bhaskara II before them.) He also studied quadratic forms in full generality (as opposed to \\n  \\n    \\n      \\n        m\\n        \\n          X\\n          \\n            2\\n          \\n        \\n        +\\n        n\\n        \\n          Y\\n          \\n            2\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle mX^{2}+nY^{2}}\\n  )—defining their equivalence relation, showing how to put them in reduced form, etc.\\nAdrien-Marie Legendre (1752–1833) was the first to state the law of quadratic reciprocity. He also\\nconjectured what amounts to the prime number theorem and Dirichlet\\'s theorem on arithmetic progressions. He gave a full treatment of the equation \\n  \\n    \\n      \\n        a\\n        \\n          x\\n          \\n            2\\n          \\n        \\n        +\\n        b\\n        \\n          y\\n          \\n            2\\n          \\n        \\n        +\\n        c\\n        \\n          z\\n          \\n            2\\n          \\n        \\n        =\\n        0\\n      \\n    \\n    {\\\\displaystyle ax^{2}+by^{2}+cz^{2}=0}\\n   and worked on quadratic forms along the lines later developed fully by Gauss. In his old age, he was the first to prove Fermat\\'s Last Theorem for \\n  \\n    \\n      \\n        n\\n        =\\n        5\\n      \\n    \\n    {\\\\displaystyle n=5}\\n   (completing work by Peter Gustav Lejeune Dirichlet, and crediting both him and Sophie Germain).\\n\\nIn his Disquisitiones Arithmeticae (1798), Carl Friedrich Gauss (1777–1855) proved the law of quadratic reciprocity and developed the theory of quadratic forms (in particular, defining their composition). He also introduced some basic notation (congruences) and devoted a section to computational matters, including primality tests. The last section of the Disquisitiones established a link between roots of unity and number theory:\\n\\nThe theory of the division of the circle...which is treated in sec. 7 does not belong\\nby itself to arithmetic, but its principles can only be drawn from higher arithmetic.\\nIn this way, Gauss arguably made a first foray towards both Évariste Galois\\'s work and algebraic number theory.\\n\\n\\n=== Maturity and division into subfields ===\\n\\nStarting early in the nineteenth century, the following developments gradually took place:\\n\\nThe rise to self-consciousness of number theory (or higher arithmetic) as a field of study.\\nThe development of much of modern mathematics necessary for basic modern number theory: complex analysis, group theory, Galois theory—accompanied by greater rigor in analysis and abstraction in algebra.\\nThe rough subdivision of number theory into its modern subfields—in particular, analytic and algebraic number theory.Algebraic number theory may be said to start with the study of reciprocity and cyclotomy, but truly came into its own with the development of abstract algebra and early ideal theory and valuation theory; see below. A conventional starting point for analytic number theory is Dirichlet\\'s theorem on arithmetic progressions (1837),  whose proof introduced L-functions and involved some asymptotic analysis and a limiting process on a real variable. The first use of analytic ideas in number theory actually\\ngoes back to Euler (1730s),  who used formal power series and non-rigorous (or implicit) limiting arguments. The use of complex analysis in number theory comes later: the work of Bernhard Riemann (1859) on the zeta function is the canonical starting point; Jacobi\\'s four-square theorem (1839), which predates it, belongs to an initially different strand that has by now taken a leading role in analytic number theory (modular forms).The history of each subfield is briefly addressed in its own section below; see the main article of each subfield for fuller treatments. Many of the most interesting questions in each area remain open and are being actively worked on.\\n\\n\\n== Main subdivisions ==\\n\\n\\n=== Elementary number theory ===\\nThe term elementary generally denotes a method that does not use complex analysis. For example, the prime number theorem was first proven using complex analysis in 1896, but an elementary proof was found only in 1949 by Erdős and Selberg. The term is somewhat ambiguous: for example, proofs based on complex Tauberian theorems (for example, Wiener–Ikehara) are often seen as quite enlightening but not elementary, in spite of using Fourier analysis, rather than complex analysis as such. Here as elsewhere, an elementary proof may be longer and more difficult for most readers than a non-elementary one.\\n\\nNumber theory has the reputation of being a field many of whose results can be stated to the layperson. At the same time, the proofs of these results are not particularly accessible, in part because the range of tools they use is, if anything, unusually broad within mathematics.\\n\\n\\n=== Analytic number theory ===\\n\\nAnalytic number theory may be defined\\n\\nin terms of its tools, as the study of the integers by means of tools from real and complex analysis; or\\nin terms of its concerns, as the study within number theory of estimates on size and density, as opposed to identities.Some subjects generally considered to be part of analytic number theory, for example, sieve theory, are better covered by the second rather than the first definition: some of sieve theory, for instance, uses little analysis, yet it does belong to analytic number theory.\\nThe following are examples of problems in analytic number theory: the prime number theorem, the Goldbach conjecture (or the twin prime conjecture, or the Hardy–Littlewood conjectures), the Waring problem and the Riemann hypothesis. Some of the most important tools of analytic number theory are the circle method, sieve methods and L-functions (or, rather, the study of their properties). The theory of modular forms (and, more generally, automorphic forms) also occupies an increasingly central place in the toolbox of analytic number theory.One may ask analytic questions about algebraic numbers, and use analytic means to answer such questions; it is thus that algebraic and analytic number theory intersect. For example, one may define prime ideals (generalizations of prime numbers in the field of algebraic numbers) and ask how many prime ideals there are up to a certain size. This question can be answered by means of an examination of Dedekind zeta functions, which are generalizations of the Riemann zeta function, a key analytic object at the roots of the subject. This is an example of a general procedure in analytic number theory: deriving information about the distribution of a sequence (here, prime ideals or prime numbers) from the analytic behavior of an appropriately constructed complex-valued function.\\n\\n\\n=== Algebraic number theory ===\\n\\nAn algebraic number is any complex number that is a solution to some polynomial equation \\n  \\n    \\n      \\n        f\\n        (\\n        x\\n        )\\n        =\\n        0\\n      \\n    \\n    {\\\\displaystyle f(x)=0}\\n   with rational coefficients; for example, every solution \\n  \\n    \\n      \\n        x\\n      \\n    \\n    {\\\\displaystyle x}\\n   of \\n  \\n    \\n      \\n        \\n          x\\n          \\n            5\\n          \\n        \\n        +\\n        (\\n        11\\n        \\n          /\\n        \\n        2\\n        )\\n        \\n          x\\n          \\n            3\\n          \\n        \\n        −\\n        7\\n        \\n          x\\n          \\n            2\\n          \\n        \\n        +\\n        9\\n        =\\n        0\\n      \\n    \\n    {\\\\displaystyle x^{5}+(11/2)x^{3}-7x^{2}+9=0}\\n   (say) is an algebraic number. Fields of algebraic numbers are also called algebraic number fields, or shortly number fields. Algebraic number theory studies algebraic number fields. Thus, analytic and algebraic number theory can and do overlap: the former is defined by its methods, the latter by its objects of study.\\nIt could be argued that the simplest kind of number fields (viz., quadratic fields) were already studied by Gauss, as the discussion of quadratic forms in Disquisitiones arithmeticae can be restated in terms of ideals and\\nnorms in quadratic fields. (A quadratic field consists of all\\nnumbers of the form \\n  \\n    \\n      \\n        a\\n        +\\n        b\\n        \\n          \\n            d\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle a+b{\\\\sqrt {d}}}\\n  , where\\n\\n  \\n    \\n      \\n        a\\n      \\n    \\n    {\\\\displaystyle a}\\n   and \\n  \\n    \\n      \\n        b\\n      \\n    \\n    {\\\\displaystyle b}\\n   are rational numbers and \\n  \\n    \\n      \\n        d\\n      \\n    \\n    {\\\\displaystyle d}\\n  \\nis a fixed rational number whose square root is not rational.)\\nFor that matter, the 11th-century chakravala method amounts—in modern terms—to an algorithm for finding the units of a real quadratic number field. However, neither Bhāskara nor Gauss knew of number fields as such.\\nThe grounds of the subject as we know it were set in the late nineteenth century, when ideal numbers, the theory of ideals and valuation theory were developed; these are three complementary ways of dealing with the lack of unique factorisation in algebraic number fields. (For example, in the field generated by the rationals\\nand \\n  \\n    \\n      \\n        \\n          \\n            −\\n            5\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\sqrt {-5}}}\\n  , the number \\n  \\n    \\n      \\n        6\\n      \\n    \\n    {\\\\displaystyle 6}\\n   can be factorised both as \\n  \\n    \\n      \\n        6\\n        =\\n        2\\n        ⋅\\n        3\\n      \\n    \\n    {\\\\displaystyle 6=2\\\\cdot 3}\\n   and\\n\\n  \\n    \\n      \\n        6\\n        =\\n        (\\n        1\\n        +\\n        \\n          \\n            −\\n            5\\n          \\n        \\n        )\\n        (\\n        1\\n        −\\n        \\n          \\n            −\\n            5\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle 6=(1+{\\\\sqrt {-5}})(1-{\\\\sqrt {-5}})}\\n  ; all of \\n  \\n    \\n      \\n        2\\n      \\n    \\n    {\\\\displaystyle 2}\\n  , \\n  \\n    \\n      \\n        3\\n      \\n    \\n    {\\\\displaystyle 3}\\n  , \\n  \\n    \\n      \\n        1\\n        +\\n        \\n          \\n            −\\n            5\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle 1+{\\\\sqrt {-5}}}\\n   and\\n\\n  \\n    \\n      \\n        1\\n        −\\n        \\n          \\n            −\\n            5\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle 1-{\\\\sqrt {-5}}}\\n  \\nare irreducible, and thus, in a naïve sense, analogous to primes among the integers.) The initial impetus for the development of ideal numbers (by Kummer) seems to have come from the study of higher reciprocity laws, that is, generalisations of quadratic reciprocity.\\nNumber fields are often studied as extensions of smaller number fields: a field L is said to be an extension of a field K if L contains K.\\n(For example, the complex numbers C are an extension of the reals R, and the reals R are an extension of the rationals Q.)\\nClassifying the possible extensions of a given number field is a difficult and partially open problem. Abelian extensions—that is, extensions L of K such that the Galois group Gal(L/K) of L over K is an abelian group—are relatively well understood.\\nTheir classification was the object of the programme of class field theory, which was initiated in the late 19th century (partly by Kronecker and Eisenstein) and carried out largely in 1900–1950.\\nAn example of an active area of research in algebraic number theory is Iwasawa theory. The Langlands program, one of the main current large-scale research plans in mathematics, is sometimes described as an attempt to generalise class field theory to non-abelian extensions of number fields.\\n\\n\\n=== Diophantine geometry ===\\n\\nThe central problem of Diophantine geometry is to determine when a Diophantine equation has solutions, and if it does, how many. The approach taken is to think of the solutions of an equation as a geometric object.\\nFor example, an equation in two variables defines a curve in the plane. More generally, an equation, or system of equations, in two or more variables defines a curve, a surface or some other such object in n-dimensional space. In Diophantine geometry, one asks whether there are any rational points (points all of whose coordinates are rationals) or\\nintegral points (points all of whose coordinates are integers) on the curve or surface. If there are any such points, the next step is to ask how many there are and how they are distributed. A basic question in this direction is if there are finitely\\nor infinitely many rational points on a given curve (or surface).\\nIn the Pythagorean equation \\n  \\n    \\n      \\n        \\n          x\\n          \\n            2\\n          \\n        \\n        +\\n        \\n          y\\n          \\n            2\\n          \\n        \\n        =\\n        1\\n        ,\\n      \\n    \\n    {\\\\displaystyle x^{2}+y^{2}=1,}\\n  \\nwe would like to study its rational solutions, that is, its solutions\\n\\n  \\n    \\n      \\n        (\\n        x\\n        ,\\n        y\\n        )\\n      \\n    \\n    {\\\\displaystyle (x,y)}\\n   such that\\nx and y are both rational. This is the same as asking for all integer solutions\\nto \\n  \\n    \\n      \\n        \\n          a\\n          \\n            2\\n          \\n        \\n        +\\n        \\n          b\\n          \\n            2\\n          \\n        \\n        =\\n        \\n          c\\n          \\n            2\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle a^{2}+b^{2}=c^{2}}\\n  ; any solution to the latter equation gives\\nus a solution \\n  \\n    \\n      \\n        x\\n        =\\n        a\\n        \\n          /\\n        \\n        c\\n      \\n    \\n    {\\\\displaystyle x=a/c}\\n  , \\n  \\n    \\n      \\n        y\\n        =\\n        b\\n        \\n          /\\n        \\n        c\\n      \\n    \\n    {\\\\displaystyle y=b/c}\\n   to the former. It is also the\\nsame as asking for all points with rational coordinates on the curve\\ndescribed by \\n  \\n    \\n      \\n        \\n          x\\n          \\n            2\\n          \\n        \\n        +\\n        \\n          y\\n          \\n            2\\n          \\n        \\n        =\\n        1\\n      \\n    \\n    {\\\\displaystyle x^{2}+y^{2}=1}\\n  . (This curve happens to be a circle of radius 1 around the origin.)\\n\\nThe rephrasing of questions on equations in terms of points on curves turns out to be felicitous. The finiteness or not of the number of rational or integer points on an algebraic curve—that is, rational or integer solutions to an equation \\n  \\n    \\n      \\n        f\\n        (\\n        x\\n        ,\\n        y\\n        )\\n        =\\n        0\\n      \\n    \\n    {\\\\displaystyle f(x,y)=0}\\n  , where \\n  \\n    \\n      \\n        f\\n      \\n    \\n    {\\\\displaystyle f}\\n   is a polynomial in two variables—turns out to depend crucially on the genus of the curve. The genus can be defined as follows: allow the variables in \\n  \\n    \\n      \\n        f\\n        (\\n        x\\n        ,\\n        y\\n        )\\n        =\\n        0\\n      \\n    \\n    {\\\\displaystyle f(x,y)=0}\\n   to be complex numbers; then \\n  \\n    \\n      \\n        f\\n        (\\n        x\\n        ,\\n        y\\n        )\\n        =\\n        0\\n      \\n    \\n    {\\\\displaystyle f(x,y)=0}\\n   defines a 2-dimensional surface in (projective) 4-dimensional space (since two complex variables can be decomposed into four real variables, that is, four dimensions). If we count the number of (doughnut) holes in the surface; we call this number the genus of \\n  \\n    \\n      \\n        f\\n        (\\n        x\\n        ,\\n        y\\n        )\\n        =\\n        0\\n      \\n    \\n    {\\\\displaystyle f(x,y)=0}\\n  . Other geometrical notions turn out to be just as crucial.\\nThere is also the closely linked area of Diophantine approximations: given a number \\n  \\n    \\n      \\n        x\\n      \\n    \\n    {\\\\displaystyle x}\\n  , then finding how well can it be approximated by rationals. (We are looking for approximations that are good relative to the amount of space that it takes to write the rational: call \\n  \\n    \\n      \\n        a\\n        \\n          /\\n        \\n        q\\n      \\n    \\n    {\\\\displaystyle a/q}\\n   (with \\n  \\n    \\n      \\n        gcd\\n        (\\n        a\\n        ,\\n        q\\n        )\\n        =\\n        1\\n      \\n    \\n    {\\\\displaystyle \\\\gcd(a,q)=1}\\n  ) a good approximation to \\n  \\n    \\n      \\n        x\\n      \\n    \\n    {\\\\displaystyle x}\\n   if \\n  \\n    \\n      \\n        \\n          |\\n        \\n        x\\n        −\\n        a\\n        \\n          /\\n        \\n        q\\n        \\n          |\\n        \\n        <\\n        \\n          \\n            1\\n            \\n              q\\n              \\n                c\\n              \\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle |x-a/q|<{\\\\frac {1}{q^{c}}}}\\n  , where \\n  \\n    \\n      \\n        c\\n      \\n    \\n    {\\\\displaystyle c}\\n   is large.) This question is of special interest if \\n  \\n    \\n      \\n        x\\n      \\n    \\n    {\\\\displaystyle x}\\n   is an algebraic number. If \\n  \\n    \\n      \\n        x\\n      \\n    \\n    {\\\\displaystyle x}\\n   cannot be well approximated, then some equations do not have integer or rational solutions. Moreover, several concepts (especially that of height) turn out to be critical both in Diophantine geometry and in the study of Diophantine approximations. This question is also of special interest in transcendental number theory: if a number can be better approximated than any algebraic number, then it is a transcendental number. It is by this argument that π and e have been shown to be transcendental.\\nDiophantine geometry should not be confused with the geometry of numbers, which is a collection of graphical methods for answering certain questions in algebraic number theory. Arithmetic geometry, however, is a contemporary term\\nfor much the same domain as that covered by the term Diophantine geometry. The term arithmetic geometry is arguably used\\nmost often when one wishes to emphasise the connections to modern algebraic geometry (as in, for instance, Faltings\\'s theorem) rather than to techniques in Diophantine approximations.\\n\\n\\n== Other subfields ==\\nThe areas below date from no earlier than the mid-twentieth century, even if they are based on older material. For example, as is explained below, the matter of algorithms in number theory is very old, in some sense older than the concept of proof; at the same time, the modern study of computability dates only from the 1930s and 1940s, and computational complexity theory from the 1970s.\\n\\n\\n=== Probabilistic number theory ===\\n\\nMuch of probabilistic number theory can be seen as an important special case of the study of variables that are almost, but not quite, mutually independent. For example, the event that a random integer between one and a million be divisible by two and the event that it be divisible by three are almost independent, but not quite.\\nIt is sometimes said that probabilistic combinatorics uses the fact that whatever happens with probability greater than \\n  \\n    \\n      \\n        0\\n      \\n    \\n    {\\\\displaystyle 0}\\n   must happen sometimes; one may say with equal justice that many applications of probabilistic number theory hinge on the fact that whatever is unusual must be rare. If certain algebraic objects (say, rational or integer solutions to certain equations) can be shown to be in the tail of certain sensibly defined distributions, it follows that there must be few of them; this is a very concrete non-probabilistic statement following from a probabilistic one.\\nAt times, a non-rigorous, probabilistic approach leads to a number of heuristic algorithms and open problems, notably Cramér\\'s conjecture.\\n\\n\\n=== Arithmetic combinatorics ===\\n\\nIf we begin from a fairly \"thick\" infinite set \\n  \\n    \\n      \\n        A\\n      \\n    \\n    {\\\\displaystyle A}\\n  , does it contain many elements in arithmetic progression: \\n  \\n    \\n      \\n        a\\n      \\n    \\n    {\\\\displaystyle a}\\n  ,\\n\\n  \\n    \\n      \\n        a\\n        +\\n        b\\n        ,\\n        a\\n        +\\n        2\\n        b\\n        ,\\n        a\\n        +\\n        3\\n        b\\n        ,\\n        …\\n        ,\\n        a\\n        +\\n        10\\n        b\\n      \\n    \\n    {\\\\displaystyle a+b,a+2b,a+3b,\\\\ldots ,a+10b}\\n  , say? Should it be possible to write large integers as sums of elements of \\n  \\n    \\n      \\n        A\\n      \\n    \\n    {\\\\displaystyle A}\\n  ?\\nThese questions are characteristic of arithmetic combinatorics. This is a presently coalescing field; it subsumes additive number theory (which concerns itself with certain very specific sets \\n  \\n    \\n      \\n        A\\n      \\n    \\n    {\\\\displaystyle A}\\n   of arithmetic significance, such as the primes or the squares) and, arguably, some of the geometry of numbers,\\ntogether with some rapidly developing new material. Its focus on issues of growth and distribution accounts in part for its developing links with ergodic theory, finite group theory, model theory, and other fields. The term additive combinatorics is also used; however, the sets \\n  \\n    \\n      \\n        A\\n      \\n    \\n    {\\\\displaystyle A}\\n   being studied need not be sets of integers, but rather subsets of non-commutative groups, for which the multiplication symbol, not the addition symbol, is traditionally used; they can also be subsets of rings, in which case the growth of \\n  \\n    \\n      \\n        A\\n        +\\n        A\\n      \\n    \\n    {\\\\displaystyle A+A}\\n   and \\n  \\n    \\n      \\n        A\\n      \\n    \\n    {\\\\displaystyle A}\\n  ·\\n  \\n    \\n      \\n        A\\n      \\n    \\n    {\\\\displaystyle A}\\n   may be\\ncompared.\\n\\n\\n=== Computational number theory ===\\nWhile the word algorithm goes back only to certain readers of al-Khwārizmī, careful descriptions of methods of solution are older than proofs: such methods (that is, algorithms) are as old as any recognisable mathematics—ancient Egyptian, Babylonian, Vedic, Chinese—whereas proofs appeared only with the Greeks of the classical period.\\nAn early case is that of what we now call the Euclidean algorithm. In its basic form (namely, as an algorithm for computing the greatest common divisor) it appears as Proposition 2 of Book VII in Elements, together with a proof of correctness. However, in the form that is often used in number theory (namely, as an algorithm for finding integer solutions to an equation \\n  \\n    \\n      \\n        a\\n        x\\n        +\\n        b\\n        y\\n        =\\n        c\\n      \\n    \\n    {\\\\displaystyle ax+by=c}\\n  ,\\nor, what is the same, for finding the quantities whose existence is assured by the Chinese remainder theorem) it first appears in the works of Āryabhaṭa (5th–6th century CE) as an algorithm called\\nkuṭṭaka (\"pulveriser\"), without a proof of correctness.\\nThere are two main questions: \"Can we compute this?\" and \"Can we compute it rapidly?\" Anyone can test whether a number is prime or, if it is not, split it into prime factors; doing so rapidly is another matter. We now know fast algorithms for testing primality, but, in spite of much work (both theoretical and practical), no truly fast algorithm for factoring.\\nThe difficulty of a computation can be useful: modern protocols for encrypting messages (for example, RSA) depend on functions that are known to all, but whose inverses are known only to a chosen few, and would take one too long a time to figure out on one\\'s own. For example, these functions can be such that their inverses can be computed only if certain large integers are factorized. While many difficult computational problems outside number theory are known, most working encryption protocols nowadays are based on the difficulty of a few number-theoretical problems.\\nSome things may not be computable at all; in fact, this can be proven in some instances. For instance, in 1970, it was proven, as a solution to Hilbert\\'s 10th problem, that there is no Turing machine which can solve all Diophantine equations. In particular, this means that, given a computably enumerable set of axioms, there are Diophantine equations for which there is no proof, starting from the axioms, of whether the set of equations has or does not have integer solutions. (We would necessarily be speaking of Diophantine equations for which there are no integer solutions, since, given a Diophantine equation with at least one solution, the solution itself provides a proof of the fact that a solution exists. We cannot prove that a particular Diophantine equation is of this kind, since this would imply that it has no solutions.)\\n\\n\\n== Applications ==\\nThe number-theorist Leonard Dickson (1874–1954) said \"Thank God that number theory is unsullied by any application\". Such a view is no longer applicable to number theory. In 1974, Donald Knuth said \"...virtually every theorem in elementary number theory arises in a natural, motivated way in connection with the problem of making computers do high-speed numerical calculations\".\\nElementary number theory is taught in discrete mathematics courses for computer scientists; on the other hand, number theory also has applications to the continuous in numerical analysis. As well as the well-known applications to cryptography, there are also applications to many other areas of mathematics.\\n\\n\\n== Prizes ==\\nThe American Mathematical Society awards the Cole Prize in Number Theory. Moreover number theory is one of the three mathematical subdisciplines rewarded by the Fermat Prize.\\n\\n\\n== See also ==\\nAlgebraic function field\\nFinite field\\np-adic number\\n\\n\\n== Notes ==\\n\\n\\n== References ==\\n\\n\\n== Sources ==\\n\\nThis article incorporates material from the Citizendium article \"Number theory\", which is licensed under the Creative Commons Attribution-ShareAlike 3.0 Unported License but not under the GFDL.\\n\\n\\n== Further reading ==\\nTwo of the most popular introductions to the subject are:\\n\\nG.H. Hardy; E.M. Wright (2008) [1938]. An introduction to the theory of numbers (rev. by D.R. Heath-Brown and J.H. Silverman, 6th ed.). Oxford University Press. ISBN 978-0-19-921986-5. Retrieved 2016-03-02.\\nVinogradov, I.M. (2003) [1954]. Elements of Number Theory (reprint of the 1954 ed.). Mineola, NY: Dover Publications.Hardy and Wright\\'s book is a comprehensive classic, though its clarity sometimes suffers due to the authors\\' insistence on elementary methods (Apostol n.d.).\\nVinogradov\\'s main attraction consists in its set of problems, which quickly lead to Vinogradov\\'s own research interests; the text itself is very basic and close to minimal. Other popular first introductions are:\\n\\nIvan M. Niven; Herbert S. Zuckerman; Hugh L. Montgomery (2008) [1960]. An introduction to the theory of numbers (reprint of the 5th edition 1991 ed.). John Wiley & Sons. ISBN 978-81-265-1811-1. Retrieved 2016-02-28.\\nKenneth H. Rosen (2010). Elementary Number Theory (6th ed.). Pearson Education. ISBN 978-0-321-71775-7. Retrieved 2016-02-28.Popular choices for a second textbook include:\\n\\nBorevich, A. I.; Shafarevich, Igor R. (1966). Number theory. Pure and Applied Mathematics. 20. Boston, MA: Academic Press. ISBN 978-0-12-117850-5. MR 0195803.\\nSerre, Jean-Pierre (1996) [1973]. A course in arithmetic. Graduate texts in mathematics. 7. Springer. ISBN 978-0-387-90040-7.\\n\\n\\n== External links ==\\n Media related to Number theory at Wikimedia Commons\\nNumber Theory entry in the Encyclopedia of Mathematics\\nNumber Theory Web', 'An integer (; from the Latin integer meaning \"whole\") is colloquially defined as a number that can be written without a fractional component. For example, 21, 4, 0, and −2048 are integers, while 9.75, 5+1/2, and √2 are not.\\nThe set of integers consists of zero (0), the positive natural numbers (1, 2, 3, ...), also called whole numbers or counting numbers, and their additive inverses (the negative integers, i.e., −1, −2, −3, ...). The set of integers is often denoted by the boldface (Z) or blackboard bold \\n  \\n    \\n      \\n        (\\n        \\n          Z\\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle (\\\\mathbb {Z} )}\\n   letter \"Z\"—standing originally for the German word Zahlen (\"numbers\").\\n  \\n    \\n      \\n        \\n          Z\\n        \\n      \\n    \\n    {\\\\displaystyle \\\\mathbb {Z} }\\n   is a subset of the set of all rational numbers \\n  \\n    \\n      \\n        \\n          Q\\n        \\n      \\n    \\n    {\\\\displaystyle \\\\mathbb {Q} }\\n  , which in turn is a subset of the real numbers \\n  \\n    \\n      \\n        \\n          R\\n        \\n      \\n    \\n    {\\\\displaystyle \\\\mathbb {R} }\\n  .  Like the natural numbers, \\n  \\n    \\n      \\n        \\n          Z\\n        \\n      \\n    \\n    {\\\\displaystyle \\\\mathbb {Z} }\\n   is countably infinite.\\nThe integers form the smallest group and the smallest ring containing the natural numbers. In algebraic number theory, the integers are sometimes qualified as rational integers to distinguish them from the more general algebraic integers. In fact, (rational) integers are algebraic integers that are also rational numbers.\\n\\n\\n== Symbol ==\\nThe symbol \\n  \\n    \\n      \\n        \\n          Z\\n        \\n      \\n    \\n    {\\\\displaystyle \\\\mathbb {Z} }\\n   can be annotated to denote various sets, with varying usage amongst different authors: \\n  \\n    \\n      \\n        \\n          \\n            Z\\n          \\n          \\n            +\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle \\\\mathbb {Z} ^{+}}\\n  ,\\n  \\n    \\n      \\n        \\n          \\n            Z\\n          \\n          \\n            +\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle \\\\mathbb {Z} _{+}}\\n   or \\n  \\n    \\n      \\n        \\n          \\n            Z\\n          \\n          \\n            >\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle \\\\mathbb {Z} ^{>}}\\n   for the positive integers, \\n  \\n    \\n      \\n        \\n          \\n            Z\\n          \\n          \\n            0\\n            +\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle \\\\mathbb {Z} ^{0+}}\\n   or \\n  \\n    \\n      \\n        \\n          \\n            Z\\n          \\n          \\n            ≥\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle \\\\mathbb {Z} ^{\\\\geq }}\\n   for non-negative integers, and \\n  \\n    \\n      \\n        \\n          \\n            Z\\n          \\n          \\n            ≠\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle \\\\mathbb {Z} ^{\\\\neq }}\\n   for non-zero integers. Some authors use \\n  \\n    \\n      \\n        \\n          \\n            Z\\n          \\n          \\n            ∗\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle \\\\mathbb {Z} ^{*}}\\n   for non-zero integers, while others use it for non-negative integers, or for {–1, 1}. Additionally, \\n  \\n    \\n      \\n        \\n          \\n            Z\\n          \\n          \\n            p\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle \\\\mathbb {Z} _{p}}\\n   is used to denote either the set of integers modulo p (i.e., the set of congruence classes of integers), or the set of p-adic integers.\\n\\n\\n== Algebraic properties ==\\n\\nLike the natural numbers, \\n  \\n    \\n      \\n        \\n          Z\\n        \\n      \\n    \\n    {\\\\displaystyle \\\\mathbb {Z} }\\n   is closed under the operations of addition and multiplication, that is, the sum and product of any two integers is an integer. However, with the inclusion of the negative natural numbers (and importantly, 0), \\n  \\n    \\n      \\n        \\n          Z\\n        \\n      \\n    \\n    {\\\\displaystyle \\\\mathbb {Z} }\\n  , unlike the natural numbers, is also closed under subtraction.The integers form a unital ring which is the most basic one, in the following sense: for any unital ring, there is a unique ring homomorphism from the integers into this ring. This universal property, namely to be an initial object in the category of rings, characterizes the ring \\n  \\n    \\n      \\n        \\n          Z\\n        \\n      \\n    \\n    {\\\\displaystyle \\\\mathbb {Z} }\\n  .\\n\\n  \\n    \\n      \\n        \\n          Z\\n        \\n      \\n    \\n    {\\\\displaystyle \\\\mathbb {Z} }\\n   is not closed under division, since the quotient of two integers (e.g., 1 divided by 2) need not be an integer. Although the natural numbers are closed under exponentiation, the integers are not (since the result can be a fraction when the exponent is negative).\\nThe following table lists some of the basic properties of addition and multiplication for any integers a, b and c:\\n\\nThe first five properties listed above for addition say that \\n  \\n    \\n      \\n        \\n          Z\\n        \\n      \\n    \\n    {\\\\displaystyle \\\\mathbb {Z} }\\n  , under addition, is an abelian group. It is also a cyclic group, since every non-zero integer can be written as a finite sum 1 + 1 + ... + 1 or (−1) + (−1) + ... + (−1). In fact, \\n  \\n    \\n      \\n        \\n          Z\\n        \\n      \\n    \\n    {\\\\displaystyle \\\\mathbb {Z} }\\n   under addition is the only infinite cyclic group—in the sense that any infinite cyclic group is isomorphic to \\n  \\n    \\n      \\n        \\n          Z\\n        \\n      \\n    \\n    {\\\\displaystyle \\\\mathbb {Z} }\\n  .\\nThe first four properties listed above for multiplication say that \\n  \\n    \\n      \\n        \\n          Z\\n        \\n      \\n    \\n    {\\\\displaystyle \\\\mathbb {Z} }\\n   under multiplication is a commutative monoid. However, not every integer has a multiplicative inverse (as is the case of the number 2), which means that \\n  \\n    \\n      \\n        \\n          Z\\n        \\n      \\n    \\n    {\\\\displaystyle \\\\mathbb {Z} }\\n   under multiplication is not a group.\\nAll the rules from the above property table (except for the last), when taken together, say that \\n  \\n    \\n      \\n        \\n          Z\\n        \\n      \\n    \\n    {\\\\displaystyle \\\\mathbb {Z} }\\n   together with addition and multiplication is a commutative ring with unity. It is the prototype of all objects of such algebraic structure. Only those equalities of expressions are true in \\n  \\n    \\n      \\n        \\n          Z\\n        \\n      \\n    \\n    {\\\\displaystyle \\\\mathbb {Z} }\\n   for all values of variables, which are true in any unital commutative ring. Certain non-zero integers map to zero in certain rings.\\nThe lack of zero divisors in the integers (last property in the table) means that the commutative ring \\n  \\n    \\n      \\n        \\n          Z\\n        \\n      \\n    \\n    {\\\\displaystyle \\\\mathbb {Z} }\\n   is an integral domain.\\nThe lack of multiplicative inverses, which is equivalent to the fact that \\n  \\n    \\n      \\n        \\n          Z\\n        \\n      \\n    \\n    {\\\\displaystyle \\\\mathbb {Z} }\\n   is not closed under division, means that \\n  \\n    \\n      \\n        \\n          Z\\n        \\n      \\n    \\n    {\\\\displaystyle \\\\mathbb {Z} }\\n   is not a field. The smallest field containing the integers as a subring is the field of rational numbers. The process of constructing the rationals from the integers can be mimicked to form the field of fractions of any integral domain. And back, starting from an algebraic number field (an extension of rational numbers), its ring of integers can be extracted, which includes \\n  \\n    \\n      \\n        \\n          Z\\n        \\n      \\n    \\n    {\\\\displaystyle \\\\mathbb {Z} }\\n   as its subring.\\nAlthough ordinary division is not defined on \\n  \\n    \\n      \\n        \\n          Z\\n        \\n      \\n    \\n    {\\\\displaystyle \\\\mathbb {Z} }\\n  , the division \"with remainder\" is defined on them. It is called Euclidean division, and possesses the following important property: given two integers a and b with b ≠ 0, there exist unique integers q and r such that a = q × b + r and 0 ≤ r <  |b|, where |b| denotes the absolute value of b. The integer q is called the quotient and r is called the remainder of the division of a by b. The Euclidean algorithm for computing greatest common divisors works by a sequence of Euclidean divisions.\\nThe above says that \\n  \\n    \\n      \\n        \\n          Z\\n        \\n      \\n    \\n    {\\\\displaystyle \\\\mathbb {Z} }\\n   is a Euclidean domain. This implies that \\n  \\n    \\n      \\n        \\n          Z\\n        \\n      \\n    \\n    {\\\\displaystyle \\\\mathbb {Z} }\\n   is a principal ideal domain, and any positive integer can be written as the products of primes in an essentially unique way. This is the fundamental theorem of arithmetic.\\n\\n\\n== Order-theoretic properties ==\\n\\n  \\n    \\n      \\n        \\n          Z\\n        \\n      \\n    \\n    {\\\\displaystyle \\\\mathbb {Z} }\\n   is a totally ordered set without upper or lower bound. The ordering of \\n  \\n    \\n      \\n        \\n          Z\\n        \\n      \\n    \\n    {\\\\displaystyle \\\\mathbb {Z} }\\n   is given by:\\n:... −3 < −2 < −1 < 0 < 1 < 2 < 3 < ...\\nAn integer is positive if it is greater than zero, and negative if it is less than zero. Zero is defined as neither negative nor positive.\\nThe ordering of integers is compatible with the algebraic operations in the following way:\\n\\nif a < b and c < d, then a + c < b + d\\nif a < b and 0 < c, then ac < bc.Thus it follows that \\n  \\n    \\n      \\n        \\n          Z\\n        \\n      \\n    \\n    {\\\\displaystyle \\\\mathbb {Z} }\\n   together with the above ordering is an ordered ring.\\nThe integers are the only nontrivial totally ordered abelian group whose positive elements are well-ordered. This is equivalent to the statement that any Noetherian valuation ring is either a field—or a discrete valuation ring.\\n\\n\\n== Construction ==\\n\\nIn elementary school teaching, integers are often intuitively defined as the (positive) natural numbers, zero, and the negations of the natural numbers. However, this style of definition leads to many different cases (each arithmetic operation needs to be defined on each combination of types of integer) and makes it tedious to prove that integers obey the various laws of arithmetic. Therefore, in modern set-theoretic mathematics, a more abstract construction allowing one to define arithmetical operations without any case distinction is often used instead. The integers can thus be formally constructed as the equivalence classes of ordered pairs of natural numbers (a,b).The intuition is that (a,b) stands for the result of subtracting b from a. To confirm our expectation that 1 − 2 and 4 − 5 denote the same number, we define an equivalence relation ~ on these pairs with the following rule:\\n\\n  \\n    \\n      \\n        (\\n        a\\n        ,\\n        b\\n        )\\n        ∼\\n        (\\n        c\\n        ,\\n        d\\n        )\\n      \\n    \\n    {\\\\displaystyle (a,b)\\\\sim (c,d)}\\n  precisely when\\n\\n  \\n    \\n      \\n        a\\n        +\\n        d\\n        =\\n        b\\n        +\\n        c\\n        .\\n      \\n    \\n    {\\\\displaystyle a+d=b+c.}\\n  Addition and multiplication of integers can be defined in terms of the equivalent operations on the natural numbers; by using [(a,b)] to denote the equivalence class having (a,b) as a member, one has:\\n\\n  \\n    \\n      \\n        [\\n        (\\n        a\\n        ,\\n        b\\n        )\\n        ]\\n        +\\n        [\\n        (\\n        c\\n        ,\\n        d\\n        )\\n        ]\\n        :=\\n        [\\n        (\\n        a\\n        +\\n        c\\n        ,\\n        b\\n        +\\n        d\\n        )\\n        ]\\n        .\\n      \\n    \\n    {\\\\displaystyle [(a,b)]+[(c,d)]:=[(a+c,b+d)].}\\n  \\n\\n  \\n    \\n      \\n        [\\n        (\\n        a\\n        ,\\n        b\\n        )\\n        ]\\n        ⋅\\n        [\\n        (\\n        c\\n        ,\\n        d\\n        )\\n        ]\\n        :=\\n        [\\n        (\\n        a\\n        c\\n        +\\n        b\\n        d\\n        ,\\n        a\\n        d\\n        +\\n        b\\n        c\\n        )\\n        ]\\n        .\\n      \\n    \\n    {\\\\displaystyle [(a,b)]\\\\cdot [(c,d)]:=[(ac+bd,ad+bc)].}\\n  The negation (or additive inverse) of an integer is obtained by reversing the order of the pair:\\n\\n  \\n    \\n      \\n        −\\n        [\\n        (\\n        a\\n        ,\\n        b\\n        )\\n        ]\\n        :=\\n        [\\n        (\\n        b\\n        ,\\n        a\\n        )\\n        ]\\n        .\\n      \\n    \\n    {\\\\displaystyle -[(a,b)]:=[(b,a)].}\\n  Hence subtraction can be defined as the addition of the additive inverse:\\n\\n  \\n    \\n      \\n        [\\n        (\\n        a\\n        ,\\n        b\\n        )\\n        ]\\n        −\\n        [\\n        (\\n        c\\n        ,\\n        d\\n        )\\n        ]\\n        :=\\n        [\\n        (\\n        a\\n        +\\n        d\\n        ,\\n        b\\n        +\\n        c\\n        )\\n        ]\\n        .\\n      \\n    \\n    {\\\\displaystyle [(a,b)]-[(c,d)]:=[(a+d,b+c)].}\\n  The standard ordering on the integers is given by:\\n\\n  \\n    \\n      \\n        [\\n        (\\n        a\\n        ,\\n        b\\n        )\\n        ]\\n        <\\n        [\\n        (\\n        c\\n        ,\\n        d\\n        )\\n        ]\\n      \\n    \\n    {\\\\displaystyle [(a,b)]<[(c,d)]}\\n   if and only if \\n  \\n    \\n      \\n        a\\n        +\\n        d\\n        <\\n        b\\n        +\\n        c\\n        .\\n      \\n    \\n    {\\\\displaystyle a+d<b+c.}\\n  It is easily verified that these definitions are independent of the choice of representatives of the equivalence classes.\\nEvery equivalence class has a unique member that is of the form (n,0) or (0,n) (or both at once). The natural number n is identified with the class [(n,0)] (i.e., the natural numbers are embedded into the integers by map sending n to [(n,0)]), and the class [(0,n)] is denoted −n (this covers all remaining classes, and gives the class [(0,0)] a second time since −0 = 0.\\nThus, [(a,b)] is denoted by\\n\\n  \\n    \\n      \\n        \\n          \\n            {\\n            \\n              \\n                \\n                  a\\n                  −\\n                  b\\n                  ,\\n                \\n                \\n                  \\n                    \\n                      if \\n                    \\n                  \\n                  a\\n                  ≥\\n                  b\\n                \\n              \\n              \\n                \\n                  −\\n                  (\\n                  b\\n                  −\\n                  a\\n                  )\\n                  ,\\n                \\n                \\n                  \\n                    \\n                      if \\n                    \\n                  \\n                  a\\n                  <\\n                  b\\n                  .\\n                \\n              \\n            \\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\begin{cases}a-b,&{\\\\mbox{if }}a\\\\geq b\\\\\\\\-(b-a),&{\\\\mbox{if }}a<b.\\\\end{cases}}}\\n  If the natural numbers are identified with the corresponding integers (using the embedding mentioned above), this convention creates no ambiguity.\\nThis notation recovers the familiar representation of the integers as {..., −2, −1, 0, 1, 2, ...} .\\nSome examples are:\\n\\n  \\n    \\n      \\n        \\n          \\n            \\n              \\n                0\\n              \\n              \\n                \\n                =\\n                [\\n                (\\n                0\\n                ,\\n                0\\n                )\\n                ]\\n              \\n              \\n                =\\n                [\\n                (\\n                1\\n                ,\\n                1\\n                )\\n                ]\\n              \\n              \\n                \\n                =\\n                ⋯\\n              \\n              \\n              \\n                \\n                =\\n                [\\n                (\\n                k\\n                ,\\n                k\\n                )\\n                ]\\n              \\n            \\n            \\n              \\n                1\\n              \\n              \\n                \\n                =\\n                [\\n                (\\n                1\\n                ,\\n                0\\n                )\\n                ]\\n              \\n              \\n                =\\n                [\\n                (\\n                2\\n                ,\\n                1\\n                )\\n                ]\\n              \\n              \\n                \\n                =\\n                ⋯\\n              \\n              \\n              \\n                \\n                =\\n                [\\n                (\\n                k\\n                +\\n                1\\n                ,\\n                k\\n                )\\n                ]\\n              \\n            \\n            \\n              \\n                −\\n                1\\n              \\n              \\n                \\n                =\\n                [\\n                (\\n                0\\n                ,\\n                1\\n                )\\n                ]\\n              \\n              \\n                =\\n                [\\n                (\\n                1\\n                ,\\n                2\\n                )\\n                ]\\n              \\n              \\n                \\n                =\\n                ⋯\\n              \\n              \\n              \\n                \\n                =\\n                [\\n                (\\n                k\\n                ,\\n                k\\n                +\\n                1\\n                )\\n                ]\\n              \\n            \\n            \\n              \\n                2\\n              \\n              \\n                \\n                =\\n                [\\n                (\\n                2\\n                ,\\n                0\\n                )\\n                ]\\n              \\n              \\n                =\\n                [\\n                (\\n                3\\n                ,\\n                1\\n                )\\n                ]\\n              \\n              \\n                \\n                =\\n                ⋯\\n              \\n              \\n              \\n                \\n                =\\n                [\\n                (\\n                k\\n                +\\n                2\\n                ,\\n                k\\n                )\\n                ]\\n              \\n            \\n            \\n              \\n                −\\n                2\\n              \\n              \\n                \\n                =\\n                [\\n                (\\n                0\\n                ,\\n                2\\n                )\\n                ]\\n              \\n              \\n                =\\n                [\\n                (\\n                1\\n                ,\\n                3\\n                )\\n                ]\\n              \\n              \\n                \\n                =\\n                ⋯\\n              \\n              \\n              \\n                \\n                =\\n                [\\n                (\\n                k\\n                ,\\n                k\\n                +\\n                2\\n                )\\n                ]\\n                .\\n              \\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\begin{aligned}0&=[(0,0)]&=[(1,1)]&=\\\\cdots &&=[(k,k)]\\\\\\\\1&=[(1,0)]&=[(2,1)]&=\\\\cdots &&=[(k+1,k)]\\\\\\\\-1&=[(0,1)]&=[(1,2)]&=\\\\cdots &&=[(k,k+1)]\\\\\\\\2&=[(2,0)]&=[(3,1)]&=\\\\cdots &&=[(k+2,k)]\\\\\\\\-2&=[(0,2)]&=[(1,3)]&=\\\\cdots &&=[(k,k+2)].\\\\end{aligned}}}\\n  In theoretical computer science, other approaches for the construction of integers are used by automated theorem provers and term rewrite engines.\\nIntegers are represented as algebraic terms built using a few basic operations (e.g.,  zero, succ, pred) and, possibly, using natural numbers, which are assumed to be already constructed (using, say, the Peano approach).\\nThere exist at least ten such constructions of signed integers. These constructions differ in several ways: the number of basic operations used for the construction, the number (usually, between 0 and 2) and the types of arguments accepted by these operations; the presence or absence of natural numbers as arguments of some of these operations, and the fact that these operations are free constructors or not, i.e., that the same integer can be represented using only one or many algebraic terms.\\nThe technique for the construction of integers presented above in this section corresponds to the particular case where there is a single basic operation pair\\n  \\n    \\n      \\n        (\\n        x\\n        ,\\n        y\\n        )\\n      \\n    \\n    {\\\\displaystyle (x,y)}\\n   that takes as arguments two natural numbers \\n  \\n    \\n      \\n        x\\n      \\n    \\n    {\\\\displaystyle x}\\n   and \\n  \\n    \\n      \\n        y\\n      \\n    \\n    {\\\\displaystyle y}\\n  , and returns an integer (equal to \\n  \\n    \\n      \\n        x\\n        −\\n        y\\n      \\n    \\n    {\\\\displaystyle x-y}\\n  ). This operation is not free since the integer 0 can be written pair(0,0), or pair(1,1), or pair(2,2), etc. This technique of construction is used by the proof assistant Isabelle; however, many other tools use alternative construction techniques, notable those based upon free constructors, which are simpler and can be implemented more efficiently in computers.\\n\\n\\n== Computer science ==\\n\\nAn integer is often a primitive data type in computer languages. However, integer data types can only represent a subset of all integers, since practical computers are of finite capacity. Also, in the common two\\'s complement representation, the inherent definition of sign distinguishes between \"negative\" and \"non-negative\" rather than \"negative, positive, and 0\". (It is, however, certainly possible for a computer to determine whether an integer value is truly positive.) Fixed length integer approximation data types (or subsets) are denoted int or Integer in several programming languages (such as Algol68, C, Java, Delphi, etc.).\\nVariable-length representations of integers, such as bignums, can store any integer that fits in the computer\\'s memory. Other integer data types are implemented with a fixed size, usually a number of bits which is a power of 2 (4, 8, 16, etc.) or a memorable number of decimal digits (e.g., 9 or 10).\\n\\n\\n== Cardinality ==\\nThe cardinality of the set of integers is equal to ℵ0 (aleph-null). This is readily demonstrated by the construction of a bijection, that is, a function that is injective and surjective from \\n  \\n    \\n      \\n        \\n          Z\\n        \\n      \\n    \\n    {\\\\displaystyle \\\\mathbb {Z} }\\n   to \\n  \\n    \\n      \\n        \\n          N\\n        \\n        =\\n        {\\n        0\\n        ,\\n        1\\n        ,\\n        2\\n        ,\\n        .\\n        .\\n        .\\n        }\\n        .\\n      \\n    \\n    {\\\\displaystyle \\\\mathbb {N} =\\\\{0,1,2,...\\\\}.}\\n   Such a function may be defined as\\n\\n  \\n    \\n      \\n        f\\n        (\\n        x\\n        )\\n        =\\n        \\n          \\n            {\\n            \\n              \\n                \\n                  −\\n                  2\\n                  x\\n                  ,\\n                \\n                \\n                  \\n                    \\n                      if \\n                    \\n                  \\n                  x\\n                  ≤\\n                  0\\n                \\n              \\n              \\n                \\n                  2\\n                  x\\n                  −\\n                  1\\n                  ,\\n                \\n                \\n                  \\n                    \\n                      if \\n                    \\n                  \\n                  x\\n                  >\\n                  0\\n                  ,\\n                \\n              \\n            \\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle f(x)={\\\\begin{cases}-2x,&{\\\\mbox{if }}x\\\\leq 0\\\\\\\\2x-1,&{\\\\mbox{if }}x>0,\\\\end{cases}}}\\n  with graph (set of the pairs \\n  \\n    \\n      \\n        (\\n        x\\n        ,\\n        f\\n        (\\n        x\\n        )\\n        )\\n      \\n    \\n    {\\\\displaystyle (x,f(x))}\\n   is\\n\\n{... (−4,8), (−3,6), (−2,4), (−1,2), (0,0), (1,1), (2,3), (3,5), ...}.Its inverse function is defined by\\n\\n  \\n    \\n      \\n        \\n          \\n            {\\n            \\n              \\n                \\n                  g\\n                  (\\n                  2\\n                  x\\n                  )\\n                  =\\n                  −\\n                  x\\n                \\n              \\n              \\n                \\n                  g\\n                  (\\n                  2\\n                  x\\n                  −\\n                  1\\n                  )\\n                  =\\n                  x\\n                  ,\\n                \\n              \\n            \\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\begin{cases}g(2x)=-x\\\\\\\\g(2x-1)=x,\\\\end{cases}}}\\n  with graph\\n\\n{(0, 0), (1, 1), (2, −1), (3, 2), (4, −2), (5, −3), ...}.\\n\\n\\n== See also ==\\n\\nCanonical factorization of a positive integer\\nHyperinteger\\nInteger complexity\\nInteger lattice\\nInteger part\\nInteger sequence\\nInteger-valued function\\nMathematical symbols\\nParity (mathematics)\\nProfinite integer\\n\\n\\n== Footnotes ==\\n\\n\\n== References ==\\n\\n\\n== Sources ==\\nBell, E.T., Men of Mathematics. New York: Simon & Schuster, 1986. (Hardcover; ISBN 0-671-46400-0)/(Paperback; ISBN 0-671-62818-6)\\nHerstein, I.N., Topics in Algebra, Wiley; 2 edition (June 20, 1975), ISBN 0-471-01090-1.\\nMac Lane, Saunders, and Garrett Birkhoff; Algebra, American Mathematical Society; 3rd edition (1999). ISBN 0-8218-1646-2.\\n\\n\\n== External links ==\\n\"Integer\", Encyclopedia of Mathematics, EMS Press, 2001 [1994]\\nThe Positive Integers – divisor tables and numeral representation tools\\nOn-Line Encyclopedia of Integer Sequences cf OEIS\\nWeisstein, Eric W. \"Integer\". MathWorld.This article incorporates material from Integer on PlanetMath, which is licensed under the Creative Commons Attribution/Share-Alike License.', 'Cryptography, or cryptology (from Ancient Greek: κρυπτός, romanized: kryptós \"hidden, secret\"; and γράφειν graphein, \"to write\", or -λογία -logia, \"study\", respectively), is the practice and study of techniques for secure communication in the presence of adversarial behavior. More generally, cryptography is about constructing and analyzing protocols that prevent third parties or the public from reading private messages; various aspects in information security such as data confidentiality, data integrity, authentication, and non-repudiation are central to modern cryptography. Modern cryptography exists at the intersection of the disciplines of mathematics, computer science, electrical engineering, communication science, and physics. Applications of cryptography include electronic commerce, chip-based payment cards, digital currencies, computer passwords, and military communications.\\nCryptography prior to the modern age was effectively synonymous with encryption, converting information from a readable state to unintelligible nonsense. The sender of an encrypted message shares the decoding technique only with intended recipients to preclude access from adversaries. The cryptography literature often uses the names Alice (\"A\") for the sender, Bob (\"B\") for the intended recipient, and Eve (\"eavesdropper\") for the adversary. Since the development of rotor cipher machines in World War I and the advent of computers in World War II, cryptography methods have become increasingly complex and its applications more varied.\\nModern cryptography is heavily based on mathematical theory and computer science practice; cryptographic algorithms are designed around computational hardness assumptions, making such algorithms hard to break in actual practice by any adversary. While it is theoretically possible to break into a well-designed system, it is infeasible in actual practice to do so. Such schemes, if well designed, are therefore termed \"computationally secure\"; theoretical advances, e.g., improvements in integer factorization algorithms, and faster computing technology require these designs to be continually reevaluated, and if necessary, adapted. There exist information-theoretically secure schemes that provably cannot be broken even with unlimited computing power, such as the one-time pad, but these schemes are much more difficult to use in practice than the best theoretically breakable but computationally secure schemes.\\nThe growth of cryptographic technology has raised a number of legal issues in the information age. Cryptography\\'s potential for use as a tool for espionage and sedition has led many governments to classify it as a weapon and to limit or even prohibit its use and export. In some jurisdictions where the use of cryptography is legal, laws permit investigators to compel the disclosure of encryption keys for documents relevant to an investigation. Cryptography also plays a major role in digital rights management and copyright infringement disputes in regard to digital media.\\n\\n\\n== Terminology ==\\n\\nThe first use of the term cryptograph (as opposed to cryptogram) dates back to the 19th century—originating from The Gold-Bug, a story by Edgar Allan Poe.Until modern times, cryptography referred almost exclusively to encryption, which is the process of converting ordinary information (called plaintext) into unintelligible form (called ciphertext). Decryption is the reverse, in other words, moving from the unintelligible ciphertext back to plaintext. A cipher (or cypher) is a pair of algorithms that carry out the encryption and the reversing decryption. The detailed operation of a cipher is controlled both by the algorithm and, in each instance, by a \"key\". The key is a secret (ideally known only to the communicants), usually a string of characters (ideally short so it can be remembered by the user), which is needed to decrypt the ciphertext.  In formal mathematical terms, a \"cryptosystem\" is the ordered list of elements of finite possible plaintexts, finite possible cyphertexts, finite possible keys, and the encryption and decryption algorithms which correspond to each key.  Keys are important both formally and in actual practice, as ciphers without variable keys can be trivially broken with only the knowledge of the cipher used and are therefore useless (or even counter-productive) for most purposes.\\nHistorically, ciphers were often used directly for encryption or decryption without additional procedures such as authentication or integrity checks. There are, generally, two kinds of cryptosystems: symmetric and asymmetric. In symmetric systems, the only ones known until the 1970s, the same key (the secret key) is used to encrypt and decrypt a message. Data manipulation in symmetric systems is faster than asymmetric systems in part because they generally use shorter key lengths. Asymmetric systems use a \"public key\" to encrypt a message and a related \"private key\" to decrypt it. The use of asymmetric systems enhances the security of communication, largely because the relation between the two keys is very hard to discover. Examples of asymmetric systems include RSA (Rivest–Shamir–Adleman), and ECC (Elliptic Curve Cryptography). Quality symmetric algorithms include the commonly used AES (Advanced Encryption Standard) which replaced the older DES (Data Encryption Standard).  Not very high quality symmetric algorithms include the assorted children\\'s language tangling schemes such as Pig Latin or other cant, and indeed effectively all cryptographic schemes, however seriously intended, from any source prior to the invention of the one-time pad early in the 20th century.\\nIn colloquial use, the term \"code\" is often used to mean any method of encryption or concealment of meaning. However, in cryptography, code has a more specific meaning: the replacement of a unit of plaintext (i.e., a meaningful word or phrase) with a code word (for example, \"wallaby\" replaces \"attack at dawn\").  A cypher, in contrast, is a scheme for changing or substituting an element below such a level (a letter, or a syllable or a pair of letters or ...) in order to produce a cyphertext.\\nCryptanalysis is the term used for the study of methods for obtaining the meaning of encrypted information without access to the key normally required to do so; i.e., it is the study of how to \"crack\" encryption algorithms or their implementations.\\nSome use the terms cryptography and cryptology interchangeably in English, while others (including US military practice generally) use cryptography to refer specifically to the use and practice of cryptographic techniques and cryptology to refer to the combined study of cryptography and cryptanalysis. English is more flexible than several other languages in which cryptology (done by cryptologists) is always used in the second sense above. RFC 2828 advises that steganography is sometimes included in cryptology.The study of characteristics of languages that have some application in cryptography or cryptology (e.g. frequency data, letter combinations, universal patterns, etc.) is called cryptolinguistics.\\n\\n\\n== History of cryptography and cryptanalysis ==\\n\\nBefore the modern era, cryptography focused on message confidentiality (i.e., encryption)—conversion of messages from a comprehensible form into an incomprehensible one and back again at the other end, rendering it unreadable by interceptors or eavesdroppers without secret knowledge (namely the key needed for decryption of that message). Encryption attempted to ensure secrecy in communications, such as those of spies, military leaders, and diplomats. In recent decades, the field has expanded beyond confidentiality concerns to include techniques for message integrity checking, sender/receiver identity authentication, digital signatures, interactive proofs and secure computation, among others.\\n\\n\\n=== Classic cryptography ===\\n\\nThe main classical cipher types are transposition ciphers, which rearrange the order of letters in a message (e.g., \\'hello world\\' becomes \\'ehlol owrdl\\' in a trivially simple rearrangement scheme), and substitution ciphers, which systematically replace letters or groups of letters with other letters or groups of letters (e.g., \\'fly at once\\' becomes \\'gmz bu podf\\' by replacing each letter with the one following it in the Latin alphabet). Simple versions of either have never offered much confidentiality from enterprising opponents. An early substitution cipher was the Caesar cipher, in which each letter in the plaintext was replaced by a letter some fixed number of positions further down the alphabet. Suetonius reports that Julius Caesar used it with a shift of three to communicate with his generals. Atbash is an example of an early Hebrew cipher. The earliest known use of cryptography is some carved ciphertext on stone in Egypt (ca 1900 BCE), but this may have been done for the amusement of literate observers rather than as a way of concealing information.\\nThe Greeks of Classical times are said to have known of ciphers (e.g., the scytale transposition cipher claimed to have been used by the Spartan military). Steganography (i.e., hiding even the existence of a message so as to keep it confidential) was also first developed in ancient times. An early example, from Herodotus, was a message tattooed on a slave\\'s shaved head and concealed under the regrown hair. More modern examples of steganography include the use of invisible ink, microdots, and digital watermarks to conceal information.\\nIn India, the 2000-year-old Kamasutra of Vātsyāyana speaks of two different kinds of ciphers called Kautiliyam and Mulavediya. In the Kautiliyam, the cipher letter substitutions are based on phonetic relations, such as vowels becoming consonants. In the Mulavediya, the cipher alphabet consists of pairing letters and using the reciprocal ones.In Sassanid Persia, there were two secret scripts, according to the Muslim author Ibn al-Nadim: the šāh-dabīrīya (literally \"King\\'s script\") which was used for official correspondence, and the rāz-saharīya which was used to communicate secret messages with other countries.David Kahn notes in The Codebreakers that modern cryptology originated among the Arabs, the first people to systematically document cryptanalytic methods. Al-Khalil (717–786) wrote the Book of Cryptographic Messages, which contains the first use of permutations and combinations to list all possible Arabic words with and without vowels.\\n\\nCiphertexts produced by a classical cipher (and some modern ciphers) will reveal statistical information about the plaintext, and that information can often be used to break the cipher. After the discovery of frequency analysis, perhaps by the Arab mathematician and polymath Al-Kindi (also known as Alkindus) in the 9th century, nearly all such ciphers could be broken by an informed attacker. Such classical ciphers still enjoy popularity today, though mostly as puzzles (see cryptogram). Al-Kindi wrote a book on cryptography entitled Risalah fi Istikhraj al-Mu\\'amma (Manuscript for the Deciphering Cryptographic Messages), which described the first known use of frequency analysis cryptanalysis techniques.\\n\\nLanguage letter frequencies may offer little help for some extended historical encryption techniques such as homophonic cipher that tend to flatten the frequency distribution. For those ciphers, language letter group (or n-gram) frequencies may provide an attack.\\nEssentially all ciphers remained vulnerable to cryptanalysis using the frequency analysis technique until the development of the polyalphabetic cipher, most clearly by Leon Battista Alberti around the year 1467, though there is some indication that it was already known to Al-Kindi. Alberti\\'s innovation was to use different ciphers (i.e., substitution alphabets) for various parts of a message (perhaps for each successive plaintext letter at the limit). He also invented what was probably the first automatic cipher device, a wheel which implemented a partial realization of his invention. In the Vigenère cipher, a polyalphabetic cipher, encryption uses a key word, which controls letter substitution depending on which letter of the key word is used. In the mid-19th century Charles Babbage showed that the Vigenère cipher was vulnerable to Kasiski examination, but this was first published about ten years later by Friedrich Kasiski.Although frequency analysis can be a powerful and general technique against many ciphers, encryption has still often been effective in practice, as many a would-be cryptanalyst was unaware of the technique. Breaking a message without using frequency analysis essentially required knowledge of the cipher used and perhaps of the key involved, thus making espionage, bribery, burglary, defection, etc., more attractive approaches to the cryptanalytically uninformed. It was finally explicitly recognized in the 19th century that secrecy of a cipher\\'s algorithm is not a sensible nor practical safeguard of message security; in fact, it was further realized that any adequate cryptographic scheme (including ciphers) should remain secure even if the adversary fully understands the cipher algorithm itself. Security of the key used should alone be sufficient for a good cipher to maintain confidentiality under an attack. This fundamental principle was first explicitly stated in 1883 by Auguste Kerckhoffs and is generally called Kerckhoffs\\'s Principle; alternatively and more bluntly, it was restated by Claude Shannon, the inventor of information theory and the fundamentals of theoretical cryptography, as Shannon\\'s Maxim—\\'the enemy knows the system\\'.\\nDifferent physical devices and aids have been used to assist with ciphers. One of the earliest may have been the scytale of ancient Greece, a rod supposedly used by the Spartans as an aid for a transposition cipher. In medieval times, other aids were invented such as the cipher grille, which was also used for a kind of steganography. With the invention of polyalphabetic ciphers came more sophisticated aids such as Alberti\\'s own cipher disk, Johannes Trithemius\\' tabula recta scheme, and Thomas Jefferson\\'s wheel cypher (not publicly known, and reinvented independently by Bazeries around 1900). Many mechanical encryption/decryption devices were invented early in the 20th century, and several patented, among them rotor machines—famously including the Enigma machine used by the German government and military from the late 1920s and during World War II. The ciphers implemented by better quality examples of these machine designs brought about a substantial increase in cryptanalytic difficulty after WWI.\\n\\n\\n=== Computer era ===\\nPrior to the early 20th century, cryptography was mainly concerned with linguistic and lexicographic patterns. Since then the emphasis has shifted, and cryptography now makes extensive use of mathematics, including aspects of information theory, computational complexity, statistics, combinatorics, abstract algebra, number theory, and finite mathematics generally. Cryptography is also a branch of engineering, but an unusual one since it deals with active, intelligent, and malevolent opposition; other kinds of engineering (e.g., civil or chemical engineering) need deal only with neutral natural forces. There is also active research examining the relationship between cryptographic problems and quantum physics.\\nJust as the development of digital computers and electronics helped in cryptanalysis, it made possible much more complex ciphers. Furthermore, computers allowed for the encryption of any kind of data representable in any binary format, unlike classical ciphers which only encrypted written language texts; this was new and significant. Computer use has thus supplanted linguistic cryptography, both for cipher design and cryptanalysis. Many computer ciphers can be characterized by their operation on binary bit sequences (sometimes in groups or blocks), unlike classical and mechanical schemes, which generally manipulate traditional characters (i.e., letters and digits) directly. However, computers have also assisted cryptanalysis, which has compensated to some extent for increased cipher complexity. Nonetheless, good modern ciphers have stayed ahead of cryptanalysis; it is typically the case that use of a quality cipher is very efficient (i.e., fast and requiring few resources, such as memory or CPU capability), while breaking it requires an effort many orders of magnitude larger, and vastly larger than that required for any classical cipher, making cryptanalysis so inefficient and impractical as to be effectively impossible.\\n\\n\\n=== Advent of modern cryptography ===\\nCryptanalysis of the new mechanical devices proved to be both difficult and laborious. In the United Kingdom, cryptanalytic efforts at Bletchley Park during WWII spurred the development of more efficient means for carrying out repetitious tasks. This culminated in the development of the Colossus, the world\\'s first fully electronic, digital, programmable computer, which assisted in the decryption of ciphers generated by the German Army\\'s Lorenz SZ40/42 machine.\\nExtensive open academic research into cryptography is relatively recent; it began only in the mid-1970s. In recent times, IBM personnel designed the algorithm that became the Federal (i.e., US) Data Encryption Standard; Whitfield Diffie and Martin Hellman published their key agreement algorithm; and the RSA algorithm was published in Martin Gardner\\'s Scientific American column. Since then, cryptography has become a widely used tool in communications, computer networks, and computer security generally.\\nSome modern cryptographic techniques can only keep their keys secret if certain mathematical problems are intractable, such as the integer factorization or the discrete logarithm problems, so there are deep connections with abstract mathematics. There are very few cryptosystems that are proven to be unconditionally secure. The one-time pad is one, and was proven to be so by Claude Shannon. There are a few important algorithms that have been proven secure under certain assumptions. For example, the infeasibility of factoring extremely large integers is the basis for believing that RSA is secure, and some other systems, but even so, proof of unbreakability is unavailable since the underlying mathematical problem remains open. In practice, these are widely used, and are believed unbreakable in practice by most competent observers.  There are systems similar to RSA, such as one by Michael O. Rabin that are provably secure provided factoring  n = pq is impossible;  it is quite unusable in practice. The discrete logarithm problem is the basis for believing some other cryptosystems are secure, and again, there are related, less practical systems that are provably secure relative to the solvability or insolvability discrete log problem.As well as being aware of cryptographic history, cryptographic algorithm and system designers must also sensibly consider probable future developments while working on their designs. For instance, continuous improvements in computer processing power have increased the scope of brute-force attacks, so when specifying key lengths, the required key lengths are similarly advancing. The potential effects of quantum computing are already being considered by some cryptographic system designers developing post-quantum cryptography; the announced imminence of small implementations of these machines may be making the need for preemptive caution rather more than merely speculative.\\n\\n\\n== Modern cryptography ==\\n\\n\\n=== Symmetric-key cryptography ===\\n\\nSymmetric-key cryptography refers to encryption methods in which both the sender and receiver share the same key (or, less commonly, in which their keys are different, but related in an easily computable way). This was the only kind of encryption publicly known until June 1976.\\n\\nSymmetric key ciphers are implemented as either block ciphers or stream ciphers. A block cipher enciphers input in blocks of plaintext as opposed to individual characters, the input form used by a stream cipher.\\nThe Data Encryption Standard (DES) and the Advanced Encryption Standard (AES) are block cipher designs that have been designated cryptography standards by the US government (though DES\\'s designation was finally withdrawn after the AES was adopted). Despite its deprecation as an official standard, DES (especially its still-approved and much more secure triple-DES variant) remains quite popular; it is used across a wide range of applications, from ATM encryption to e-mail privacy and secure remote access. Many other block ciphers have been designed and released, with considerable variation in quality. Many, even some designed by capable practitioners, have been thoroughly broken, such as FEAL.Stream ciphers, in contrast to the \\'block\\' type, create an arbitrarily long stream of key material, which is combined with the plaintext bit-by-bit or character-by-character, somewhat like the one-time pad. In a stream cipher, the output stream is created based on a hidden internal state that changes as the cipher operates. That internal state is initially set up using the secret key material. RC4 is a widely used stream cipher. Block ciphers can be used as stream ciphers by generating blocks of a keystream (in place of a Pseudorandom number generator) and applying an XOR operation to each bit of the plaintext with each bit of the keystream.Message authentication codes (MACs) are much like cryptographic hash functions, except that a secret key can be used to authenticate the hash value upon receipt; this additional complication blocks an attack scheme against bare digest algorithms, and so has been thought worth the effort. Cryptographic hash functions are a third type of cryptographic algorithm. They take a message of any length as input, and output a short, fixed-length hash, which can be used in (for example) a digital signature. For good hash functions, an attacker cannot find two messages that produce the same hash. MD4 is a long-used hash function that is now broken; MD5, a strengthened variant of MD4, is also widely used but broken in practice. The US National Security Agency developed the Secure Hash Algorithm series of MD5-like hash functions: SHA-0 was a flawed algorithm that the agency withdrew; SHA-1 is widely deployed and more secure than MD5, but cryptanalysts have identified attacks against it; the SHA-2 family improves on SHA-1, but is vulnerable to clashes as of 2011; and the US standards authority thought it \"prudent\" from a security perspective to develop a new standard to \"significantly improve the robustness of NIST\\'s overall hash algorithm toolkit.\" Thus, a hash function design competition was meant to select a new U.S. national standard, to be called SHA-3, by 2012. The competition ended on October 2, 2012, when the NIST announced that Keccak would be the new SHA-3 hash algorithm. Unlike block and stream ciphers that are invertible, cryptographic hash functions produce a hashed output that cannot be used to retrieve the original input data. Cryptographic hash functions are used to verify the authenticity of data retrieved from an untrusted source or to add a layer of security.\\nMessage authentication codes (MACs) are much like cryptographic hash functions, except that a secret key can be used to authenticate the hash value upon receipt; this additional complication blocks an attack scheme against bare digest algorithms, and so has been thought worth the effort.\\n\\n\\n=== Public-key cryptography ===\\n\\nSymmetric-key cryptosystems use the same key for encryption and decryption of a message, although a message or group of messages can have a different key than others. A significant disadvantage of symmetric ciphers is the key management necessary to use them securely. Each distinct pair of communicating parties must, ideally, share a different key, and perhaps for each ciphertext exchanged as well. The number of keys required increases as the square of the number of network members, which very quickly requires complex key management schemes to keep them all consistent and secret.\\n\\nIn a groundbreaking 1976 paper, Whitfield Diffie and Martin Hellman proposed the notion of public-key (also, more generally, called asymmetric key) cryptography in which two different but mathematically related keys are used—a public key and a private key. A public key system is so constructed that calculation of one key (the \\'private key\\') is computationally infeasible from the other (the \\'public key\\'), even though they are necessarily related. Instead, both keys are generated secretly, as an interrelated pair. The historian David Kahn described public-key cryptography as \"the most revolutionary new concept in the field since polyalphabetic substitution emerged in the Renaissance\".In public-key cryptosystems, the public key may be freely distributed, while its paired private key must remain secret. In a public-key encryption system, the public key is used for encryption, while the private or secret key is used for decryption. While Diffie and Hellman could not find such a system, they showed that public-key cryptography was indeed possible by presenting the Diffie–Hellman key exchange protocol, a solution that is now widely used in secure communications to allow two parties to secretly agree on a shared encryption key.\\nThe X.509 standard defines the most commonly used format for public key certificates.Diffie and Hellman\\'s publication sparked widespread academic efforts in finding a practical public-key encryption system. This race was finally won in 1978 by Ronald Rivest, Adi Shamir, and Len Adleman, whose solution has since become known as the RSA algorithm.The Diffie–Hellman and RSA algorithms, in addition to being the first publicly known examples of high-quality public-key algorithms, have been among the most widely used. Other asymmetric-key algorithms include the Cramer–Shoup cryptosystem, ElGamal encryption, and various elliptic curve techniques.A document published in 1997 by the Government Communications Headquarters (GCHQ), a British intelligence organization, revealed that cryptographers at GCHQ had anticipated several academic developments. Reportedly, around 1970, James H. Ellis had conceived the principles of asymmetric key cryptography. In 1973, Clifford Cocks invented a solution that was very similar in design rationale to RSA. In 1974, Malcolm J. Williamson is claimed to have developed the Diffie–Hellman key exchange.\\n\\nPublic-key cryptography is also used for implementing digital signature schemes. A digital signature is reminiscent of an ordinary signature; they both have the characteristic of being easy for a user to produce, but difficult for anyone else to forge. Digital signatures can also be permanently tied to the content of the message being signed; they cannot then be \\'moved\\' from one document to another, for any attempt will be detectable. In digital signature schemes, there are two algorithms: one for signing, in which a secret key is used to process the message (or a hash of the message, or both), and one for verification, in which the matching public key is used with the message to check the validity of the signature. RSA and DSA are two of the most popular digital signature schemes. Digital signatures are central to the operation of public key infrastructures and many network security schemes (e.g., SSL/TLS, many VPNs, etc.).Public-key algorithms are most often based on the computational complexity of \"hard\" problems, often from number theory. For example, the hardness of RSA is related to the integer factorization problem, while Diffie–Hellman and DSA are related to the discrete logarithm problem. The security of elliptic curve cryptography is based on number theoretic problems involving elliptic curves. Because of the difficulty of the underlying problems, most public-key algorithms involve operations such as modular multiplication and exponentiation, which are much more computationally expensive than the techniques used in most block ciphers, especially with typical key sizes. As a result, public-key cryptosystems are commonly hybrid cryptosystems, in which a fast high-quality symmetric-key encryption algorithm is used for the message itself, while the relevant symmetric key is sent with the message, but encrypted using a public-key algorithm. Similarly, hybrid signature schemes are often used, in which a cryptographic hash function is computed, and only the resulting hash is digitally signed.\\n\\n\\n=== Cryptographic Hash Functions ===\\nCryptographic Hash Functions are cryptographic algorithms that are ways to generate and utilize specific keys to encrypt data for either symmetric or asymmetric encryption, and such functions may be viewed as keys themselves. They take a message of any length as input, and output a short, fixed-length hash, which can be used in (for example) a digital signature. For good hash functions, an attacker cannot find two messages that produce the same hash. MD4 is a long-used hash function that is now broken; MD5, a strengthened variant of MD4, is also widely used but broken in practice. The US National Security Agency developed the Secure Hash Algorithm series of MD5-like hash functions: SHA-0 was a flawed algorithm that the agency withdrew; SHA-1 is widely deployed and more secure than MD5, but cryptanalysts have identified attacks against it; the SHA-2 family improves on SHA-1, but is vulnerable to clashes as of 2011; and the US standards authority thought it \"prudent\" from a security perspective to develop a new standard to \"significantly improve the robustness of NIST\\'s overall hash algorithm toolkit.\" Thus, a hash function design competition was meant to select a new U.S. national standard, to be called SHA-3, by 2012. The competition ended on October 2, 2012, when the NIST announced that Keccak would be the new SHA-3 hash algorithm. Unlike block and stream ciphers that are invertible, cryptographic hash functions produce a hashed output that cannot be used to retrieve the original input data. Cryptographic hash functions are used to verify the authenticity of data retrieved from an untrusted source or to add a layer of security.\\n\\n\\n=== Cryptanalysis ===\\n\\nThe goal of cryptanalysis is to find some weakness or insecurity in a cryptographic scheme, thus permitting its subversion or evasion.\\nIt is a common misconception that every encryption method can be broken. In connection with his WWII work at Bell Labs, Claude Shannon proved that the one-time pad cipher is unbreakable, provided the key material is truly random, never reused, kept secret from all possible attackers, and of equal or greater length than the message. Most ciphers, apart from the one-time pad, can be broken with enough computational effort by brute force attack, but the amount of effort needed may be exponentially dependent on the key size, as compared to the effort needed to make use of the cipher. In such cases, effective security could be achieved if it is proven that the effort required (i.e., \"work factor\", in Shannon\\'s terms) is beyond the ability of any adversary. This means it must be shown that no efficient method (as opposed to the time-consuming brute force method) can be found to break the cipher. Since no such proof has been found to date, the one-time-pad remains the only theoretically unbreakable cipher. Although well-implemented one-time-pad encryption cannot be broken, traffic analysis is still possible.\\nThere are a wide variety of cryptanalytic attacks, and they can be classified in any of several ways. A common distinction turns on what Eve (an attacker) knows and what capabilities are available. In a ciphertext-only attack, Eve has access only to the ciphertext (good modern cryptosystems are usually effectively immune to ciphertext-only attacks). In a known-plaintext attack, Eve has access to a ciphertext and its corresponding plaintext (or to many such pairs). In a chosen-plaintext attack, Eve may choose a plaintext and learn its corresponding ciphertext (perhaps many times); an example is gardening, used by the British during WWII. In a chosen-ciphertext attack, Eve may be able to choose ciphertexts and learn their corresponding plaintexts. Finally in a man-in-the-middle attack Eve gets in between Alice (the sender) and Bob (the recipient), accesses and modifies the traffic and then forwards it to the recipient. Also important, often overwhelmingly so, are mistakes (generally in the design or use of one of the protocols involved).\\n\\nCryptanalysis of symmetric-key ciphers typically involves looking for attacks against the block ciphers or stream ciphers that are more efficient than any attack that could be against a perfect cipher. For example, a simple brute force attack against DES requires one known plaintext and 255 decryptions, trying approximately half of the possible keys, to reach a point at which chances are better than even that the key sought will have been found. But this may not be enough assurance; a linear cryptanalysis attack against DES requires 243 known plaintexts (with their corresponding ciphertexts) and approximately 243 DES operations. This is a considerable improvement over brute force attacks.\\nPublic-key algorithms are based on the computational difficulty of various problems. The most famous of these are the difficulty of integer factorization of semiprimes and the difficulty of calculating discrete logarithms, both of which are not yet proven to be solvable in polynomial time using only a classical Turing-complete computer. Much public-key cryptanalysis concerns designing algorithms in P that can solve these problems, or using other technologies, such as quantum computers. For instance, the best-known algorithms for solving the elliptic curve-based version of discrete logarithm are much more time-consuming than the best-known algorithms for factoring, at least for problems of more or less equivalent size. Thus, other things being equal, to achieve an equivalent strength of attack resistance, factoring-based encryption techniques must use larger keys than elliptic curve techniques. For this reason, public-key cryptosystems based on elliptic curves have become popular since their invention in the mid-1990s.\\nWhile pure cryptanalysis uses weaknesses in the algorithms themselves, other attacks on cryptosystems are based on actual use of the algorithms in real devices, and are called side-channel attacks. If a cryptanalyst has access to, for example, the amount of time the device took to encrypt a number of plaintexts or report an error in a password or PIN character, he may be able to use a timing attack to break a cipher that is otherwise resistant to analysis. An attacker might also study the pattern and length of messages to derive valuable information; this is known as traffic analysis and can be quite useful to an alert adversary. Poor administration of a cryptosystem, such as permitting too short keys, will make any system vulnerable, regardless of other virtues. Social engineering and other attacks against humans (e.g., bribery, extortion, blackmail, espionage, torture, ...) are usually employed due to being more cost-effective and feasible to perform in a reasonable amount of time compared to pure cryptanalysis by a high margin.\\n\\n\\n=== Cryptographic primitives ===\\nMuch of the theoretical work in cryptography concerns cryptographic primitives—algorithms with basic cryptographic properties—and their relationship to other cryptographic problems. More complicated cryptographic tools are then built from these basic primitives. These primitives provide fundamental properties, which are used to develop more complex tools called cryptosystems or cryptographic protocols, which guarantee one or more high-level security properties. Note, however, that the distinction between cryptographic primitives and cryptosystems, is quite arbitrary; for example, the RSA algorithm is sometimes considered a cryptosystem, and sometimes a primitive. Typical examples of cryptographic primitives include pseudorandom functions, one-way functions, etc.\\n\\n\\n=== Cryptosystems ===\\n\\nOne or more cryptographic primitives are often used to develop a more complex algorithm, called a cryptographic system, or cryptosystem. Cryptosystems (e.g., El-Gamal encryption) are designed to provide particular functionality (e.g., public key encryption) while guaranteeing certain security properties (e.g., chosen-plaintext attack (CPA) security in the random oracle model). Cryptosystems use the properties of the underlying cryptographic primitives to support the system\\'s security properties. As the distinction between primitives and cryptosystems is somewhat arbitrary, a sophisticated cryptosystem can be derived from a combination of several more primitive cryptosystems. In many cases, the cryptosystem\\'s structure involves back and forth communication among two or more parties in space (e.g., between the sender of a secure message and its receiver) or across time (e.g., cryptographically protected backup data). Such cryptosystems are sometimes called cryptographic protocols.\\nSome widely known cryptosystems include RSA encryption, Schnorr signature, El-Gamal encryption, PGP, etc. More complex cryptosystems include electronic cash systems, signcryption systems, etc. Some more \\'theoretical\\' cryptosystems include interactive proof systems, (like zero-knowledge proofs), systems for secret sharing, etc.\\n\\n\\n=== Lightweight cryptography ===\\nLightweight cryptography (LWC) concerns cryptographic algorithms developed for a strictly constrained environment. The growth of Internet of Things (IoT) has spiked research into the development of lightweight algorithms that are better suited for the environment. An IoT environment requires strict constraints on power consumption, processing power, and security. Algorithms such as PRESENT, AES, and SPECK are examples of the many LWC algorithms that have been developed to achieve the standard set by the National Institute of Standards and Technology.\\n\\n\\n== Legal issues ==\\n\\n\\n=== Prohibitions ===\\nCryptography has long been of interest to intelligence gathering and law enforcement agencies. Secret communications may be criminal or even treasonous. Because of its facilitation of privacy, and the diminution of privacy attendant on its prohibition, cryptography is also of considerable interest to civil rights supporters. Accordingly, there has been a history of controversial legal issues surrounding cryptography, especially since the advent of inexpensive computers has made widespread access to high-quality cryptography possible.\\nIn some countries, even the domestic use of cryptography is, or has been, restricted. Until 1999, France significantly restricted the use of cryptography domestically, though it has since relaxed many of these rules. In China and Iran, a license is still required to use cryptography. Many countries have tight restrictions on the use of cryptography. Among the more restrictive are laws in Belarus, Kazakhstan, Mongolia, Pakistan, Singapore, Tunisia, and Vietnam.In the United States, cryptography is legal for domestic use, but there has been much conflict over legal issues related to cryptography. One particularly important issue has been the export of cryptography and cryptographic software and hardware. Probably because of the importance of cryptanalysis in World War II and an expectation that cryptography would continue to be important for national security, many Western governments have, at some point, strictly regulated export of cryptography. After World War II, it was illegal in the US to sell or distribute encryption technology overseas; in fact, encryption was designated as auxiliary military equipment and put on the United States Munitions List. Until the development of the personal computer, asymmetric key algorithms (i.e., public key techniques), and the Internet, this was not especially problematic. However, as the Internet grew and computers became more widely available, high-quality encryption techniques became well known around the globe.\\n\\n\\n=== Export controls ===\\n\\nIn the 1990s, there were several challenges to US export regulation of cryptography. After the source code for Philip Zimmermann\\'s Pretty Good Privacy (PGP) encryption program found its way onto the Internet in June 1991, a complaint by RSA Security (then called RSA Data Security, Inc.) resulted in a lengthy criminal investigation of Zimmermann by the US Customs Service and the FBI, though no charges were ever filed. Daniel J. Bernstein, then a graduate student at UC Berkeley, brought a lawsuit against the US government challenging some aspects of the restrictions based on free speech grounds. The 1995 case Bernstein v. United States ultimately resulted in a 1999 decision that printed source code for cryptographic algorithms and systems was protected as free speech by the United States Constitution.In 1996, thirty-nine countries signed the Wassenaar Arrangement, an arms control treaty that deals with the export of arms and \"dual-use\" technologies such as cryptography. The treaty stipulated that the use of cryptography with short key-lengths (56-bit for symmetric encryption, 512-bit for RSA) would no longer be export-controlled. Cryptography exports from the US became less strictly regulated as a consequence of a major relaxation in 2000; there are no longer very many restrictions on key sizes in US-exported mass-market software. Since this relaxation in US export restrictions, and because most personal computers connected to the Internet include US-sourced web browsers such as Firefox or Internet Explorer, almost every Internet user worldwide has potential access to quality cryptography via their browsers (e.g., via Transport Layer Security). The Mozilla Thunderbird and Microsoft Outlook E-mail client programs similarly can transmit and receive emails via TLS, and can send and receive email encrypted with S/MIME. Many Internet users don\\'t realize that their basic application software contains such extensive cryptosystems. These browsers and email programs are so ubiquitous that even governments whose intent is to regulate civilian use of cryptography generally don\\'t find it practical to do much to control distribution or use of cryptography of this quality, so even when such laws are in force, actual enforcement is often effectively impossible.\\n\\n\\n=== NSA involvement ===\\n\\nAnother contentious issue connected to cryptography in the United States is the influence of the National Security Agency on cipher development and policy. The NSA was involved with the design of DES during its development at IBM and its consideration by the National Bureau of Standards as a possible Federal Standard for cryptography. DES was designed to be resistant to differential cryptanalysis, a powerful and general cryptanalytic technique known to the NSA and IBM, that became publicly known only when it was rediscovered in the late 1980s. According to Steven Levy, IBM discovered differential cryptanalysis, but kept the technique secret at the NSA\\'s request. The technique became publicly known only when Biham and Shamir re-discovered and announced it some years later. The entire affair illustrates the difficulty of determining what resources and knowledge an attacker might actually have.\\nAnother instance of the NSA\\'s involvement was the 1993 Clipper chip affair, an encryption microchip intended to be part of the Capstone cryptography-control initiative. Clipper was widely criticized by cryptographers for two reasons. The cipher algorithm (called Skipjack) was then classified (declassified in 1998, long after the Clipper initiative lapsed). The classified cipher caused concerns that the NSA had deliberately made the cipher weak in order to assist its intelligence efforts. The whole initiative was also criticized based on its violation of Kerckhoffs\\'s Principle, as the scheme included a special escrow key held by the government for use by law enforcement (i.e. wiretapping).\\n\\n\\n=== Digital rights management ===\\n\\nCryptography is central to digital rights management (DRM), a group of techniques for technologically controlling use of copyrighted material, being widely implemented and deployed at the behest of some copyright holders. In 1998, U.S. President Bill Clinton signed the Digital Millennium Copyright Act (DMCA), which criminalized all production, dissemination, and use of certain cryptanalytic techniques and technology (now known or later discovered); specifically, those that could be used to circumvent DRM technological schemes. This had a noticeable impact on the cryptography research community since an argument can be made that any cryptanalytic research violated the DMCA. Similar statutes have since been enacted in several countries and regions, including the implementation in the EU Copyright Directive. Similar restrictions are called for by treaties signed by World Intellectual Property Organization member-states.\\nThe United States Department of Justice and FBI have not enforced the DMCA as rigorously as had been feared by some, but the law, nonetheless, remains a controversial one. Niels Ferguson, a well-respected cryptography researcher, has publicly stated that he will not release some of his research into an Intel security design for fear of prosecution under the DMCA. Cryptologist Bruce Schneier has argued that the DMCA encourages vendor lock-in, while inhibiting actual measures toward cyber-security. Both Alan Cox (longtime Linux kernel developer) and Edward Felten (and some of his students at Princeton) have encountered problems related to the Act. Dmitry Sklyarov was arrested during a visit to the US from Russia, and jailed for five months pending trial for alleged violations of the DMCA arising from work he had done in Russia, where the work was legal. In 2007, the cryptographic keys responsible for Blu-ray and HD DVD content scrambling were discovered and released onto the Internet. In both cases, the Motion Picture Association of America sent out numerous DMCA takedown notices, and there was a massive Internet backlash triggered by the perceived impact of such notices on fair use and free speech.\\n\\n\\n=== Forced disclosure of encryption keys ===\\n\\nIn the United Kingdom, the Regulation of Investigatory Powers Act gives UK police the powers to force suspects to decrypt files or hand over passwords that protect encryption keys. Failure to comply is an offense in its own right, punishable on conviction by a two-year jail sentence or up to five years in cases involving national security. Successful prosecutions have occurred under the Act; the first, in 2009, resulted in a term of 13 months\\' imprisonment. Similar forced disclosure laws in Australia, Finland, France, and India compel individual suspects under investigation to hand over encryption keys or passwords during a criminal investigation.\\nIn the United States, the federal criminal case of United States v. Fricosu addressed whether a search warrant can compel a person to reveal an encryption passphrase or password. The Electronic Frontier Foundation (EFF) argued that this is a violation of the protection from self-incrimination given by the Fifth Amendment. In 2012, the court ruled that under the All Writs Act, the defendant was required to produce an unencrypted hard drive for the court.In many jurisdictions, the legal status of forced disclosure remains unclear.\\nThe 2016 FBI–Apple encryption dispute concerns the ability of courts in the United States to compel manufacturers\\' assistance in unlocking cell phones whose contents are cryptographically protected.\\nAs a potential counter-measure to forced disclosure some cryptographic software supports plausible deniability, where the encrypted data is indistinguishable from unused random data (for example such as that of a drive which has been securely wiped).\\n\\n\\n== See also ==\\nOutline of cryptography – Overview of and topical guide to cryptography\\nList of cryptographers – Wikipedia list article\\nList of important publications in cryptography – Wikipedia list article\\nList of multiple discoveries – Wikipedia list article\\nList of unsolved problems in computer science – Wikipedia list article\\nSyllabical and Steganographical Table – Eighteenth-century work believed to be the first cryptography chart – first cryptography chart\\nComparison of cryptography libraries\\nCrypto Wars\\nEncyclopedia of Cryptography and Security – Book by Technische Universiteit Eindhoven\\nGlobal surveillance – Mass surveillance across national borders\\nIndistinguishability obfuscation – Cryptographic algorithm\\nInformation theory – Scientific study of digital information\\nStrong cryptography\\nWorld Wide Web Consortium\\'s Web Cryptography API – World Wide Web Consortium cryptography standard\\nCollision attack\\n\\n\\n== References ==\\n\\n\\n== Further reading ==\\n\\n\\n== External links ==\\n The dictionary definition of cryptography at Wiktionary\\n Media related to Cryptography at Wikimedia Commons\\nCryptography on In Our Time at the BBC\\nCrypto Glossary and Dictionary of Technical Cryptography\\nNSA\\'s CryptoKids.\\nOverview and Applications of Cryptology by the CrypTool Team; PDF; 3.8 MB. July 2008\\nA Course in Cryptography by Raphael Pass & Abhi Shelat – offered at Cornell in the form of lecture notes.\\nFor more on the use of cryptographic elements in fiction, see: Dooley, John F., William and Marilyn Ingersoll Professor of Computer Science, Knox College (23 August 2012). \"Cryptology in Fiction\". Archived from the original on 29 July 2020. Retrieved 20 February 2015.\\nThe George Fabyan Collection at the Library of Congress has early editions of works of seventeenth-century English literature, publications relating to cryptography.', 'Artificial intelligence (AI) is intelligence demonstrated by machines, as opposed to natural intelligence displayed by animals including humans.\\nLeading AI textbooks define the field as the study of \"intelligent agents\": any system that perceives its environment and takes actions that maximize its chance of achieving its goals.\\nSome popular accounts use the term \"artificial intelligence\" to describe machines that mimic \"cognitive\" functions that humans associate with the human mind, such as \"learning\" and \"problem solving\", however, this definition is rejected by major AI researchers.AI applications include advanced web search engines (i.e. Google), recommendation systems (used by YouTube, Amazon and Netflix), understanding human speech (such as Siri and Alexa), self-driving cars (e.g. Tesla), automated decision-making and competing at the highest level in strategic game systems (such as chess and Go).\\nAs machines become increasingly capable, tasks considered to require \"intelligence\" are often removed from the definition of AI, a phenomenon known as the AI effect.  For instance, optical character recognition is frequently excluded from things considered to be AI, having become a routine technology.Artificial intelligence was founded as an academic discipline in 1956, and in the years since has experienced several waves of optimism, followed by disappointment and the loss of funding (known as an \"AI winter\"), followed by new approaches, success and renewed funding. AI research has tried and discarded many different approaches since its founding, including simulating the brain, modeling human problem solving, formal logic, large databases of knowledge and imitating animal behavior. In the first decades of the 21st century, highly mathematical statistical machine learning has dominated the field, and this technique has proved highly successful, helping to solve many challenging problems throughout industry and academia.The various sub-fields of AI research are centered around particular goals and the use of particular tools. The traditional goals of AI research include reasoning, knowledge representation, planning, learning, natural language processing, perception, and the ability to move and manipulate objects. General intelligence (the ability to solve an arbitrary problem) is among the field\\'s long-term goals. To solve these problems, AI researchers have adapted and integrated a wide range of problem-solving techniques -- including search and mathematical optimization, formal logic, artificial neural networks, and methods based on statistics, probability and economics. AI also draws upon computer science, psychology, linguistics, philosophy, and many other fields.\\nThe field was founded on the assumption that human intelligence \"can be so precisely described that a machine can be made to simulate it\".\\nThis raises philosophical arguments about the mind and the ethics of creating artificial beings endowed with human-like intelligence. These issues have been explored by myth, fiction, and philosophy since antiquity.Science fiction and futurology have also suggested that, with its enormous potential and power, AI may become an existential risk to humanity.\\n\\n\\n== History ==\\n\\nArtificial beings with intelligence appeared as storytelling devices in antiquity,\\nand have been common in fiction, as in Mary Shelley\\'s Frankenstein or Karel Čapek\\'s R.U.R. These characters and their fates raised many of the same issues now discussed in the ethics of artificial intelligence.The study of mechanical or \"formal\" reasoning began with philosophers and mathematicians in antiquity. The study of mathematical logic led directly to Alan Turing\\'s theory of computation, which suggested that a machine, by shuffling symbols as simple as \"0\" and \"1\", could simulate any conceivable act of mathematical deduction. This insight that digital computers can simulate any process of formal reasoning is known as the Church–Turing thesis.The Church-Turing thesis, along with concurrent discoveries in neurobiology, information theory and cybernetics, led researchers to consider the possibility of building an electronic brain.\\nThe first work that is now generally recognized as AI was McCullouch and Pitts\\' 1943 formal design for Turing-complete \"artificial neurons\".When access to digital computers became possible in the mid-1950s, AI research began to explore the possibility that human intelligence could be reduced to step-by-step symbol manipulation, known as Symbolic AI or GOFAI. Approaches based on cybernetics or artificial neural networks were abandoned or pushed into the background.\\nThe field of AI research was born at a workshop at Dartmouth College in 1956.\\nThe attendees became the founders and leaders of AI research.\\nThey and their students produced programs that the press described as \"astonishing\":\\ncomputers were learning checkers strategies, solving word problems in algebra, proving logical theorems and speaking English.\\nBy the middle of the 1960s, research in the U.S. was heavily funded by the Department of Defense\\nand laboratories had been established around the world.Researchers in the 1960s and the 1970s were convinced that symbolic approaches would eventually succeed in creating a machine with artificial general intelligence and considered this the goal of their field.Herbert Simon predicted, \"machines will be capable, within twenty years, of doing any work a man can do\".Marvin Minsky agreed, writing, \"within a generation ... the problem of creating \\'artificial intelligence\\' will substantially be solved\".They failed to recognize the difficulty of some of the remaining tasks. Progress slowed and in 1974, in response to the criticism of Sir James Lighthill\\nand ongoing pressure from the US Congress to fund more productive projects, both the U.S. and British governments cut off exploratory research in AI. The next few years would later be called an \"AI winter\", a period when obtaining funding for AI projects was difficult.\\nIn the early 1980s, AI research was revived by the commercial success of expert systems,\\na form of AI program that simulated the knowledge and analytical skills of human experts. By 1985, the market for AI had reached over a billion dollars. At the same time, Japan\\'s fifth generation computer project inspired the U.S and British governments to restore funding for academic research.\\nHowever, beginning with the collapse of the Lisp Machine market in 1987, AI once again fell into disrepute, and a second, longer-lasting winter began.Many researchers began to doubt that the symbolic approach would be able to imitate all the processes of human cognition, especially perception, robotics, learning and pattern recognition. A number of researchers began to look into \"sub-symbolic\" approaches to specific AI problems. Robotics researchers, such as Rodney Brooks, rejected symbolic AI and focused on the basic engineering problems that would allow robots to move, survive, and learn their environment.\\nInterest in neural networks and \"connectionism\" was revived by Geoffrey Hinton, David Rumelhart and others in the middle of the 1980s.Soft computing tools were developed in the 80s, such as neural networks, fuzzy systems, Grey system theory, evolutionary computation and many tools drawn from statistics or mathematical optimization.\\nAI gradually restored its reputation in the late 1990s and early 21st century by finding specific solutions to specific problems. The narrow focus allowed researchers to produce verifiable results, exploit more  mathematical methods, and collaborate with other fields (such as statistics, economics and mathematics).\\nBy 2000, solutions developed by AI researchers were being widely used, although in the 1990s they were rarely described as \"artificial intelligence\".Faster computers, algorithmic improvements, and access to large amounts of data enabled advances in machine learning and perception; data-hungry deep learning methods started to dominate accuracy benchmarks around 2012.\\nAccording to Bloomberg\\'s Jack Clark, 2015 was a landmark year for artificial intelligence, with the number of software projects that use AI within Google increased from a \"sporadic usage\" in 2012 to more than 2,700 projects. He attributes this to an increase in affordable neural networks, due to a rise in cloud computing infrastructure and to an increase in research tools and datasets. In a 2017 survey, one in five companies reported they had \"incorporated AI in some offerings or processes\". The amount of research into AI (measured by total publications) increased by 50% in the years 2015–2019.Numerous academic researchers became concerned that AI was no longer pursuing the original goal of creating versatile, fully intelligent machines. Much of current research involves statistical AI, which is overwhelmingly used to solve specific problems, even highly successful techniques such as deep learning. This concern has led to the subfield artificial general intelligence (or \"AGI\"), which had several well-funded institutions by the 2010s.\\n\\n\\n== Goals ==\\nThe general problem of simulating (or creating) intelligence has been broken down into sub-problems. These consist of particular traits or capabilities that researchers expect an intelligent system to display. The traits described below have received the most attention.\\n\\n\\n=== Reasoning, problem solving ===\\nEarly researchers developed algorithms that imitated step-by-step reasoning that humans use when they solve puzzles or make logical deductions.\\nBy the late 1980s and 1990s, AI research had developed methods for dealing with uncertain or incomplete information, employing concepts from probability and economics.Many of these algorithms proved to be insufficient for solving large reasoning problems because they experienced a \"combinatorial explosion\": they became exponentially slower as the problems grew larger.\\nEven humans rarely use the step-by-step deduction that early AI research could model. They solve most of their problems using fast, intuitive judgments.\\n\\n\\n=== Knowledge representation ===\\n\\nKnowledge representation and knowledge engineering\\nallow AI programs to answer questions intelligently and make deductions about real world facts.\\nA representation of \"what exists\" is an ontology: the set of objects, relations, concepts, and properties formally described so that software agents can interpret them.\\nThe most general ontologies are called upper ontologies, which attempt to provide a foundation for all other knowledge and act as mediators between domain ontologies that cover specific knowledge about a particular knowledge domain (field of interest or area of concern). A truly intelligent program would also need access to commonsense knowledge; the set of facts that an average person knows. The semantics of an ontology is typically represented in a description logic, such as the Web Ontology Language.AI research has developed tools to represent specific domains, such as: objects, properties, categories and relations between objects;\\nsituations, events, states and time;\\ncauses and effects;\\nknowledge about knowledge (what we know about what other people know);.default reasoning (things that humans assume are true until they are told differently and will remain true even when other facts are changing);\\n\\nas well as other domains. Among the most difficult problems in AI are: the breadth of commonsense knowledge (the number of atomic facts that the average person knows is enormous);\\nand the sub-symbolic form of most commonsense knowledge (much of what people know is not represented as \"facts\" or \"statements\" that they could express verbally).Formal knowledge representations are used in content-based indexing and retrieval,\\nscene interpretation,\\nclinical decision support,\\nknowledge discovery (mining \"interesting\" and actionable inferences from large databases),\\nand other areas.\\n\\n\\n=== Planning ===\\n\\nAn intelligent agent that can plan makes a representation of the state of the world, makes predictions about how their actions will change it and makes choices that maximize the utility (or \"value\") of the available choices.\\nIn classical planning problems, the agent can assume that it is the only system acting in the world, allowing the agent to be certain of the consequences of its actions.\\nHowever, if the agent is not the only actor, then it requires that the agent reason under uncertainty, and continuously re-assess its environment and adapt.Multi-agent planning uses the cooperation and competition of many agents to achieve a given goal. Emergent behavior such as this is used by evolutionary algorithms and swarm intelligence.\\n\\n\\n=== Learning ===\\n\\nMachine learning (ML), a fundamental concept of AI research since the field\\'s inception,\\nis the study of computer algorithms that improve automatically through experience.Unsupervised learning finds patterns in a stream of input. Supervised learning requires a human to label the input data first, and comes in two main varieties: classification and numerical regression. Classification is used to determine what category something belongs in -- the program sees a number of examples of things from several categories and will learn to classify new inputs. Regression is the attempt to produce a function that describes the relationship between inputs and outputs and predicts how the outputs should change as the inputs change. Both classifiers and regression learners can be viewed as \"function approximators\" trying to learn an unknown (possibly implicit) function; for example, a spam classifier can be viewed as learning a function that maps from the text of an email to one of two categories, \"spam\" or \"not spam\".\\nIn reinforcement learning the agent is rewarded for good responses and punished for bad ones. The agent classifies its responses to form a strategy for operating in its problem space.Transfer learning is when knowledge gained from one problem is applied to a new problem.Computational learning theory can assess learners by computational complexity, by sample complexity (how much data is required), or by other notions of optimization.\\n\\n\\n=== Natural language processing ===\\n\\nNatural language processing (NLP)\\nallows machines to read and understand human language. A sufficiently powerful natural language processing system would enable natural-language user interfaces and the acquisition of knowledge directly from human-written sources, such as newswire texts. Some straightforward applications of NLP include information retrieval, question answering and machine translation.\\nSymbolic AI used formal syntax to translate the deep structure of sentences into logic. This failed to produce useful applications, due to the intractability of logic and the breadth of commonsense knowledge. Modern statistical techniques include co-occurrence frequencies (how often one word appears near another), \"Keyword spotting\" (searching for a particular word to retrieve information), transformer-based deep learning (which finds patterns in text), and others. They have achieved acceptable accuracy at the page or paragraph level, and, by 2019, could generate coherent text.\\n\\n\\n=== Perception ===\\n\\nMachine perception\\nis the ability to use input from sensors (such as cameras, microphones, wireless signals, and active lidar, sonar, radar, and tactile sensors) to deduce aspects of the world. Applications include speech recognition,facial recognition, and object recognition.\\nComputer vision is the ability to analyze visual input.\\n\\n\\n=== Motion and manipulation ===\\n\\nAI is heavily used in robotics.Localization is how a robot knows its location and map its environment. When given a small, static, and visible environment, this is easy; however, dynamic environments, such as (in endoscopy) the interior of a patient\\'s breathing body, pose a greater challenge.\\nMotion planning is the process of breaking down a movement task into \"primitives\" such as individual joint movements. Such movement often involves compliant motion, a process where movement requires maintaining physical contact with an object. Robots can learn from experience how to move efficiently despite the presence of friction and gear slippage.\\n\\n\\n=== Social intelligence ===\\n\\nAffective computing is an interdisciplinary umbrella that comprises systems which recognize, interpret, process, or simulate human feeling, emotion and mood. \\nFor example, some virtual assistants are programmed to speak conversationally or even to banter humorously; it makes them appear more sensitive to the emotional dynamics of human interaction, or to otherwise facilitate human–computer interaction.\\nHowever, this tends to give naïve users an unrealistic conception of how intelligent existing computer agents actually are.\\nModerate successes related to affective computing include textual sentiment analysis and, more recently, multimodal sentiment analysis), wherein AI classifies the affects displayed by a videotaped subject.\\n\\n\\n=== General intelligence ===\\n\\nA machine with general intelligence can solve a wide variety of problems with a breadth and versatility similar to human intelligence. There are several competing ideas about how to develop artificial general intelligence. Hans Moravec and Marvin Minsky argue that work in different individual domains can be incorporated into an advanced multi-agent system or cognitive architecture with general intelligence.Pedro Domingos hopes that there is a conceptually straightforward, but mathematically difficult, \"master algorithm\" that could lead to AGI.\\nOthers believe that anthropomorphic features like an artificial brain\\nor simulated child development\\nwill someday reach a critical point where general intelligence emerges.\\n\\n\\n== Tools ==\\n\\n\\n=== Search and optimization ===\\n\\nMany problems in AI can be solved theoretically by intelligently searching through many possible solutions:Reasoning can be reduced to performing a search. For example, logical proof can be viewed as searching for a path that leads from premises to conclusions, where each step is the application of an inference rule.Planning algorithms search through trees of goals and subgoals, attempting to find a path to a target goal, a process called means-ends analysis.Robotics algorithms for moving limbs and grasping objects use local searches in configuration space.Simple exhaustive searches\\nare rarely sufficient for most real-world problems: the search space (the number of places to search) quickly grows to astronomical numbers. The result is a search that is too slow or never completes. The solution, for many problems, is to use \"heuristics\" or \"rules of thumb\" that prioritize choices in favor of those more likely to reach a goal and to do so in a shorter number of steps. In some search methodologies heuristics can also serve to eliminate some choices unlikely to lead to a goal (called \"pruning the search tree\"). Heuristics supply the program with a \"best guess\" for the path on which the solution lies.\\nHeuristics limit the search for solutions into a smaller sample size.\\n\\nA very different kind of search came to prominence in the 1990s, based on the mathematical theory of optimization. For many problems, it is possible to begin the search with some form of a guess and then refine the guess incrementally until no more refinements can be made. These algorithms can be visualized as blind hill climbing: we begin the search at a random point on the landscape, and then, by jumps or steps, we keep moving our guess uphill, until we reach the top. Other optimization algorithms are simulated annealing, beam search and random optimization.Evolutionary computation uses a form of optimization search. For example, they may begin with a population of organisms (the guesses) and then allow them to mutate and recombine, selecting only the fittest to survive each generation (refining the guesses). Classic evolutionary algorithms include genetic algorithms, gene expression programming, and genetic programming.\\nAlternatively, distributed search processes can coordinate via swarm intelligence algorithms. Two popular swarm algorithms used in search are particle swarm optimization (inspired by bird flocking) and ant colony optimization (inspired by ant trails).\\n\\n\\n=== Logic ===\\n\\nLogic\\nis used for knowledge representation and problem solving, but it can be applied to other problems as well. For example, the satplan algorithm uses logic for planning\\nand inductive logic programming is a method for learning.Several different forms of logic are used in AI research. Propositional logic involves truth functions such as \"or\" and \"not\". First-order logic\\nadds quantifiers and predicates, and can express facts about objects, their properties, and their relations with each other. Fuzzy logic assigns a \"degree of truth\" (between 0 and 1) to vague statements such as \"Alice is old\" (or rich, or tall, or hungry), that are too linguistically imprecise to be completely true or false.Default logics, non-monotonic logics and circumscription are forms of logic designed to help with default reasoning and the qualification problem.\\nSeveral extensions of logic have been designed to handle specific domains of knowledge, such as: description logics;situation calculus, event calculus and fluent calculus (for representing events and time);causal calculus;belief calculus (belief revision); and modal logics.\\nLogics to model contradictory or inconsistent statements arising in multi-agent systems have also been designed, such as paraconsistent logics.\\n\\n\\n=== Probabilistic methods for uncertain reasoning ===\\n\\nMany problems in AI (in reasoning, planning, learning, perception, and robotics) require the agent to operate with incomplete or uncertain information. AI researchers have devised a number of powerful tools to solve these problems using methods from probability theory and economics.Bayesian networks\\nare a very general tool that can be used for various problems: reasoning (using the Bayesian inference algorithm),learning (using the expectation-maximization algorithm),planning (using decision networks) and perception (using dynamic Bayesian networks).\\nProbabilistic algorithms can also be used for filtering, prediction, smoothing and finding explanations for streams of data, helping perception systems to analyze processes that occur over time (e.g., hidden Markov models or Kalman filters).A key concept from the science of economics is \"utility\": a measure of how valuable something is to an intelligent agent. Precise mathematical tools have been developed that analyze how an agent can make choices and plan, using decision theory, decision analysis,\\nand information value theory. These tools include models such as Markov decision processes, dynamic decision networks, game theory and mechanism design.\\n\\n\\n=== Classifiers and statistical learning methods ===\\n\\nThe simplest AI applications can be divided into two types: classifiers (\"if shiny then diamond\") and controllers (\"if diamond then pick up\"). Controllers do, however, also classify conditions before inferring actions, and therefore classification forms a central part of many AI systems. Classifiers are functions that use pattern matching to determine a closest match. They can be tuned according to examples, making them very attractive for use in AI. These examples are known as observations or patterns. In supervised learning, each pattern belongs to a certain predefined class. A class is a decision that has to be made. All the observations combined with their class labels are known as a data set. When a new observation is received, that observation is classified based on previous experience.A classifier can be trained in various ways; there are many statistical and machine learning approaches.\\nThe decision tree is the simplest and most widely used symbolic machine learning algorithm.K-nearest neighbor algorithm was the most widely used analogical AI until the mid-1990s.Kernel methods such as the support vector machine (SVM) displaced k-nearest neighbor in the 1990s.\\nThe naive Bayes classifier is reportedly the \"most widely used learner\" at Google, due in part to its scalability.Neural networks are also used for classification.Classifier performance depends greatly on the characteristics of the data to be classified, such as the dataset size, distribution of samples across classes, the dimensionality, and the level of noise. Model-based classifiers perform well if the assumed model is an extremely good fit for the actual data. Otherwise, if no matching model is available, and if accuracy (rather than speed or scalability) is the sole concern, conventional wisdom is that discriminative classifiers (especially SVM) tend to be more accurate than model-based classifiers such as \"naive Bayes\" on most practical data sets.\\n\\n\\n=== Artificial neural networks ===\\n\\nNeural networks\\nwere inspired by the architecture of neurons in the human brain. A simple \"neuron\" N accepts input from other neurons, each of which, when activated (or \"fired\"), casts a weighted \"vote\" for or against whether neuron N should itself activate. Learning requires an algorithm to adjust these weights based on the training data; one simple algorithm (dubbed \"fire together, wire together\") is to increase the weight between two connected neurons when the activation of one triggers the successful activation of another. Neurons have a continuous spectrum of activation; in addition, neurons can process inputs in a nonlinear way rather than weighing straightforward votes.\\nModern neural networks model complex relationships between inputs and outputs or and find patterns in data. They can learn continuous functions and even digital logical operations. Neural networks can be viewed a type of mathematical optimization — they perform a gradient descent on a multi-dimensional topology that was created by training the network. The most common training technique is the backpropagation algorithm.\\nOther learning techniques for neural networks are Hebbian learning (\"fire together, wire together\"), GMDH or competitive learning.The main categories of networks are acyclic or feedforward neural networks (where the signal passes in only one direction) and recurrent neural networks (which allow feedback and short-term memories of previous input events). Among the most popular feedforward networks are perceptrons, multi-layer perceptrons and radial basis networks.\\n\\n\\n==== Deep learning ====\\n \\nDeep learning\\nuses several layers of neurons between the network\\'s inputs and outputs. The multiple layers can progressively extract higher-level features from the raw input. For example, in image processing, lower layers may identify edges, while higher layers may identify the concepts relevant to a human such as digits or letters or faces. Deep learning has drastically improved the performance of programs in many important subfields of artificial intelligence, including computer vision, speech recognition, image classification and others.\\nDeep learning often uses convolutional neural networks for many or all of its layers. In a convolutional layer, each neuron receives input from only a restricted area of the previous layer called the neuron\\'s receptive field. This can substantially reduce the number of weighted connections between neurons, and creates a hierarchy similar to the organization of the animal visual cortex.In a recurrent neural network the signal will propagate through a layer more than once; \\nthus, an RNN is an example of deep learning.\\nRNNs can be trained by gradient descent,\\nhowever long-term gradients which are back-propagated can \"vanish\" (that is, they can tend to zero) or \"explode\" (that is, they can tend to infinity), known as the vanishing gradient problem.\\nThe long short term memory (LSTM) technique can prevent this in most cases.\\n\\n\\n=== Specialized languages and hardware ===\\n\\nSpecialized languages for artificial intelligence have been developed, such as Lisp, Prolog, TensorFlow and many others. Hardware developed for AI includes AI accelerators and neuromorphic computing.\\n\\n\\n== Applications ==\\n\\nAI is relevant to any intellectual task.\\nModern artificial intelligence techniques are pervasive and are too numerous to list here.\\nFrequently, when a technique reaches mainstream use, it is no longer considered artificial intelligence; this phenomenon is described as the AI effect.In the 2010s, AI applications were at the heart of the most commercially successful areas of computing, and have become a ubiquitous feature of daily life. AI is used in search engines (such as Google Search),\\ntargeting online advertisements,recommendation systems (offered by Netflix, YouTube or Amazon),\\ndriving internet traffic,targeted advertising (AdSense, Facebook),\\nvirtual assistants (such as Siri or Alexa),autonomous vehicles (including drones and self-driving cars),\\nautomatic language translation (Microsoft Translator, Google Translate),\\nfacial recognition (Apple\\'s Face ID or Microsoft\\'s DeepFace),\\nimage labeling (used by Facebook, Apple\\'s iPhoto and TikTok)\\nand spam filtering.\\nThere are also thousands of successful AI applications used to solve problems for specific industries or institutions. A few examples are: \\nenergy storage,deepfakes,\\nmedical diagnosis, \\nmilitary logistics, or \\nsupply chain management.\\nGame playing has been a test of AI\\'s strength since the 1950s. Deep Blue became the first computer chess-playing system to beat a reigning world chess champion, Garry Kasparov, on 11 May 1997. \\nIn 2011, in a Jeopardy! quiz show exhibition match, IBM\\'s question answering system, Watson, defeated the two greatest Jeopardy! champions, Brad Rutter and Ken Jennings, by a significant margin. \\nIn March 2016, AlphaGo won 4 out of 5 games of Go in a match with Go champion Lee Sedol, becoming the first computer Go-playing system to beat a professional Go player without handicaps.\\nOther programs handle imperfect-information games; such as for poker at a superhuman level, Pluribus\\nand Cepheus.DeepMind in the 2010s developed a \"generalized artificial intelligence\" that could learn many diverse Atari games on its own.By 2020, Natural Language Processing systems such as the enormous GPT-3 (then by far the largest artificial neural network) were matching human performance on pre-existing benchmarks, albeit without the system attaining commonsense understanding of the contents of the benchmarks.\\nDeepMind\\'s AlphaFold 2 (2020) demonstrated the ability to determine, in hours rather than months, the 3D structure of a protein.\\nOther applications predict the result of judicial decisions, create art (such as poetry or painting) and prove mathematical theorems.\\n\\n\\n== Philosophy ==\\n\\n\\n=== Defining artificial intelligence ===\\n\\n\\n==== Thinking vs. acting: the Turing test ====\\n\\nAlan Turing wrote in 1950 \"I propose to consider the question \\'can machines think\\'?\"\\nHe advised changing the question from whether a machine \"thinks\", to \"whether or not it is possible for machinery to show intelligent behaviour\". \\nThe only thing visible is the behavior of the machine, so it does not matter if the machine is conscious, or has a mind, or whether the intelligence is merely a \"simulation\" and not \"the real thing\".  He noted that we also don\\'t know these things about other people, but that we extend a \"polite convention\" that they are actually \"thinking\". This idea forms the basis of the Turing test.\\n\\n\\n==== Acting humanly vs. acting intelligently: intelligent agents ====\\n\\nAI founder John McCarthy said: \"Artificial intelligence is not, by definition, simulation of human intelligence\". Russell and Norvig agree and criticize the Turing test. They wrote: \"Aeronautical engineering texts do not define the goal of their field as \\'making machines that fly so exactly like pigeons that they can fool other pigeons.\\'\" Other researchers and analysts disagree and have argued that AI should simulate natural intelligence by studying psychology or neurobiology.The intelligent agent paradigm\\ndefines intelligent behavior in general, without reference to human beings. An intelligent agent is a system that perceives its environment and takes actions that maximize its chances of success. Any system that has goal-directed behavior can be analyzed as an intelligent agent: something as simple as a thermostat, as complex as a human being, as well as large systems such as firms, biomes or nations. The intelligent agent paradigm became widely accepted during the 1990s, and currently serves as the definition of the field.The paradigm has other advantages for AI. It provides a reliable and scientific way to test programs; researchers can directly compare or even combine different approaches to isolated problems, by asking which agent is best at maximizing a given \"goal function\".  It also gives them a common language to communicate with other fields — such as mathematical optimization (which is defined in terms of \"goals\") or economics (which uses the same definition of a \"rational agent\").\\n\\n\\n=== Evaluating approaches to AI ===\\nNo established unifying theory or paradigm has guided AI research for most of its history. The unprecedented success of statistical machine learning in the 2010s eclipsed all other approaches (so much so that some sources, especially in the business world, use the term \"artificial intelligence\" to mean \"machine learning with neural networks\"). This approach is mostly sub-symbolic, neat, soft and narrow (see below). Critics argue that these questions may have to be revisited by future generations of AI researchers. \\n\\n\\n==== Symbolic AI and its limits ====\\n\\nSymbolic AI (or \"GOFAI\") simulated the high-level conscious reasoning that people use when they solve puzzles, express legal reasoning and do mathematics. They were highly successful at \"intelligent\" tasks such as algebra or IQ tests. In the 1960s, Newell and Simon proposed the physical symbol systems hypothesis: \"A physical symbol system has the necessary and sufficient means of general intelligent action.\"However, the symbolic approach failed dismally on many tasks that humans solve easily, such as learning, recognizing an object or commonsense reasoning. Moravec\\'s paradox is the discovery that high-level \"intelligent\" tasks were easy for AI, but low level \"instinctive\" tasks were extremely difficult.\\nPhilosopher Hubert Dreyfus had argued since the 1960s that human expertise depends on unconscious instinct rather than conscious symbol manipulation, and on having a \"feel\" for the situation, rather than explicit symbolic knowledge.\\nAlthough his arguments had been ridiculed and ignored when they were first presented, eventually AI research came to agree.The issue is not resolved: sub-symbolic reasoning can make many of the same inscrutable mistakes that human intuition does, such as algorithmic bias. Critics such Noam Chomsky argue continuing research into symbolic AI will still be necessary to attain general intelligence, in part because sub-symbolic AI is a move away from explainable AI: it can be difficult or impossible to understand why a modern statistical AI program made a particular decision.\\n\\n\\n==== Neat vs. scruffy ====\\n\\n\"Neats\" hope that intelligent behavior be described using simple, elegant principles (such as logic, optimization, or neural networks). \"Scruffies\" expect that it necessarily requires solving a large number of unrelated problems. This issue was actively discussed in the 70s and 80s,\\nbut in the 1990s mathematical methods and solid scientific standards became the norm, a transition that Russell and Norvig termed \"the victory of the neats\".\\n\\n\\n==== Soft vs. hard computing ====\\n\\nFinding a provably correct or optimal solution is intractable for many important problems. Soft computing is a set of techniques, including genetic algorithms, fuzzy logic and neural networks, that are tolerant of imprecision, uncertainty, partial truth and approximation. Soft computing was introduced in the late 80s and most successful AI programs in the 21st century are examples of soft computing with neural networks.\\n\\n\\n==== Narrow vs. general AI ====\\n\\nAI researchers are divided as to whether to pursue the goals of artificial general intelligence and superintelligence (general AI) directly, or to solve as many specific problems as possible (narrow AI) in hopes these solutions will lead indirectly to the field\\'s long-term goals\\nGeneral intelligence is difficult to define and difficult to measure, and modern AI has had more verifiable successes by focussing on specific problems with specific solutions. The experimental sub-field of artificial general intelligence studies this area exclusively.\\n\\n\\n=== Machine consciousness, sentience and mind ===\\n\\nThe philosophy of mind does not know whether a machine can have a mind, consciousness and mental states, in the same sense that human beings do. This issue considers the internal experiences of the machine, rather than its external behavior. Mainstream AI research considers this issue irrelevant, because it does not effect the goals of the field. Stuart Russell and Peter Norvig observe that most AI researchers \"don\\'t care about the [philosophy of AI] — as long as the program works, they don\\'t care whether you call it a simulation of intelligence or real intelligence.\" However, the question has become central to the philosophy of mind. It is also typically the central question at issue in artificial intelligence in fiction.\\n\\n\\n==== Consciousness ====\\n\\nDavid Chalmers identified two problems in understanding the mind, which he named the \"hard\" and \"easy\" problems of consciousness. The easy problem is understanding how the brain processes signals, makes plans and controls behavior. The hard problem is explaining how this feels or why it should feel like anything at all. Human information processing is easy to explain, however human subjective experience is difficult to explain. For example, it is easy to imagine a color blind person who has learned to identify which objects in their field of view are red, but it is not clear what would be required for the person to know what red looks like.\\n\\n\\n==== Computationalism and functionalism ====\\n\\nComputationalism is the position in the philosophy of mind that the human mind is an information processing system and that thinking is a form of computing. Computationalism argues that the relationship between mind and body is similar or identical to the relationship between software and hardware and thus may be a solution to the mind-body problem. This philosophical position was inspired by the work of AI researchers and cognitive scientists in the 1960s and was originally proposed by philosophers Jerry Fodor and Hilary Putnam.Philosopher John Searle characterized this position as \"strong AI\": \"The appropriately programmed computer with the right inputs and outputs would thereby have a mind in exactly the same sense human beings have minds.\"\\nSearle counters this assertion with his Chinese room argument, which attempts to show that, even if a machine perfectly simulates human behavior, there is still no reason to suppose it also has a mind.\\n\\n\\n==== Robot rights ====\\n\\nIf a machine has a mind and subjective experience, then it may also have sentience (the ability to feel), and if so, then it could also suffer, and thus it would be entitled to certain rights.\\nAny hypothetical robot rights would lie on a spectrum with animal rights and human rights.\\nThis issue has been considered in fiction for centuries,\\nand is now being considered by, for example, California\\'s Institute for the Future, however critics argue that the discussion is premature.\\n\\n\\n== Future ==\\n\\n\\n=== Superintelligence ===\\n\\nA superintelligence, hyperintelligence, or superhuman intelligence, is a hypothetical agent that would possess intelligence far surpassing that of the brightest and most gifted human mind. Superintelligence may also refer to the form or degree of intelligence possessed by such an agent.If research into artificial general intelligence produced sufficiently intelligent software, it might be able to reprogram and improve itself. The improved software would be even better at improving itself, leading to recursive self-improvement.\\nIts intelligence would increase exponentially in an intelligence explosion and could dramatically surpass humans. Science fiction writer Vernor Vinge named this scenario the \"singularity\".\\nBecause it is difficult or impossible to know the limits of intelligence or the capabilities of superintelligent machines, the technological singularity is an occurrence beyond which events are unpredictable or even unfathomable.Robot designer Hans Moravec, cyberneticist Kevin Warwick, and inventor Ray Kurzweil have predicted that humans and machines will merge in the future into cyborgs that are more capable and powerful than either. This idea, called transhumanism, has roots in Aldous Huxley and Robert Ettinger.Edward Fredkin argues that \"artificial intelligence is the next stage in evolution\", an idea first proposed by Samuel Butler\\'s \"Darwin among the Machines\" as far back as 1863, and expanded upon by George Dyson in his book of the same name in 1998.\\n\\n\\n=== Risks ===\\n\\n\\n==== Technological unemployment ====\\n\\nIn the past technology has tended to increase rather than reduce total employment, but economists acknowledge that \"we\\'re in uncharted territory\" with AI.\\nA survey of economists showed disagreement about whether the increasing use of robots and AI will cause a substantial increase in long-term unemployment, but they generally agree that it could be a net benefit, if productivity gains are redistributed.\\nSubjective estimates of the risk vary widely; for example, Michael Osborne and Carl Benedikt Frey estimate 47% of U.S. jobs are at \"high risk\" of potential automation, while an OECD report classifies only 9% of U.S. jobs as \"high risk\".Unlike previous waves of automation, many middle-class jobs may be eliminated by artificial intelligence; The Economist states that \"the worry that AI could do to white-collar jobs what steam power did to blue-collar ones during the Industrial Revolution\" is \"worth taking seriously\".\\nJobs at extreme risk range from paralegals to fast food cooks, while job demand is likely to increase for care-related professions ranging from personal healthcare to the clergy.\\n\\n\\n==== Bad actors and weaponized AI ====\\n\\nAI provides a number of tools that are particularly useful for authoritarian governments: smart spyware, face recognition and voice recognition allow widespread surveillance; such surveillance allows machine learning to classify potential enemies of the state and can prevent them from hiding; recommendation systems can precisely target propaganda and misinformation for maximum effect; deepfakes aid in producing misinformation; advanced AI can make centralized decision making more competitive with liberal and decentralized systems such as markets.Terrorists, criminals and rogue states may use other forms of weaponized AI such as advanced digital warfare and lethal autonomous weapons. By 2015, over fifty countries were reported to be researching battlefield robots.\\n\\n\\n==== Algorithmic bias ====\\n\\nAI programs can become biased after learning from real world data. It is not typically introduced by the system designers, but is learned by the program, and thus the programmers are often unaware that the bias exists.\\nBias can be inadvertently introduced by the way training data is selected.\\nIt can also emerge from correlations: AI is used to classify individuals into groups and then make predictions assuming that the individual will resemble other members of the group. In some cases, this assumption may be unfair.\\nAn example of this is COMPAS, a commercial program widely used by U.S. courts to assess the likelihood of a defendant becoming a recidivist. ProPublica claims that the COMPAS-assigned recidivism risk level of black defendants is far more likely to be an overestimate than that of white defendants, despite the fact that the program was not told the races of the defendants. Other examples where algorithmic bias can lead to unfair outcomes are when AI is used for credit rating or hiring.\\n\\n\\n==== Existential risk ====\\n\\nSuperintelligent AI may be able to improve itself to the point that humans could not control it. This could, as physicist Stephen Hawking puts it, \"spell the end of the human race\". Philosopher Nick Bostrom argues that sufficiently intelligent AI, if it chooses actions based on achieving some goal, will exhibit convergent behavior such as acquiring resources or protecting itself from being shut down. If this AI\\'s goals do not fully reflect humanity\\'s, it might need to harm humanity in order to acquire more resources or prevent itself from being shut down, ultimately to better achieve its goal. He concludes that AI poses a risk to mankind, however humble or \"friendly\" its stated goals might be.\\nPolitical scientist Charles T. Rubin argues that \"any sufficiently advanced benevolence may be indistinguishable from malevolence.\" Humans should not assume machines or robots would treat us favorably because there is no a priori reason to believe that they would share our system of morality.The opinion of experts and industry insiders is mixed, with sizable fractions both concerned and unconcerned by risk from eventual superhumanly-capable AI.Stephen Hawking, Microsoft founder Bill Gates, history professor Yuval Noah Harari, and SpaceX founder Elon Musk have all expressed serious misgivings about the future of AI.\\nProminent tech titans including Peter Thiel (Amazon Web Services) and Musk have committed more than $1 billion to nonprofit companies that champion responsible AI development, such as OpenAI and the Future of Life Institute.Mark Zuckerberg (CEO, Facebook) has said that artificial intelligence is helpful in its current form and will continue to assist humans.\\nOther experts argue is that the risks are far enough in the future to not be worth researching,\\nor that humans will be valuable from the perspective of a superintelligent machine.Rodney Brooks, in particular, has said that \"malevolent\" AI is still centuries away.\\n\\n\\n=== Ethical machines ===\\n\\nFriendly AI are machines that have been designed from the beginning to minimize risks and to make choices that benefit humans. Eliezer Yudkowsky, who coined the term, argues that developing friendly AI should be a higher research priority: it may require a large investment and it must be completed before AI becomes an existential risk.Machines with intelligence have the potential to use their intelligence to make ethical decisions. The field of machine ethics provides machines with ethical principles and procedures for resolving ethical dilemmas.\\nMachine ethics is also called machine morality, computational ethics or computational morality,\\nand was founded at an AAAI symposium in 2005.Others approaches include Wendell Wallach\\'s \"artificial moral agents\"\\nand Stuart J. Russell\\'s three principles for developing provably beneficial machines.\\n\\n\\n=== Regulation ===\\n\\nThe regulation of artificial intelligence is the development of public sector policies and laws for promoting and regulating artificial intelligence (AI); it is therefore related to the broader regulation of algorithms.\\nThe regulatory and  policy landscape for AI is an emerging issue in jurisdictions globally.\\nBetween 2016 and 2020, more than 30 countries adopted dedicated strategies for AI.\\nMost EU member states had released national AI strategies, as had Canada, China, India, Japan, Mauritius, the Russian Federation, Saudi Arabia, United Arab Emirates, USA and Viet Nam. Others were in the process of elaborating their own AI strategy, including Bangladesh, Malaysia and Tunisia.\\nThe Global Partnership on Artificial Intelligence was launched in June 2020, stating a need for AI to be developed in accordance with human rights and democratic values, to ensure public confidence and trust in the technology.\\n\\n\\n== In fiction ==\\n\\nThought-capable artificial beings have appeared as storytelling devices since antiquity,\\nand have been a persistent theme in science fiction.A common trope in these works began with Mary Shelley\\'s Frankenstein, where a human creation becomes a threat to its masters. This includes such works as Arthur C. Clarke\\'s and Stanley Kubrick\\'s 2001: A Space Odyssey (both 1968), with HAL 9000, the murderous computer in charge of the Discovery One spaceship, as well as The Terminator (1984) and The Matrix (1999). In contrast, the rare loyal robots such as Gort from The Day the Earth Stood Still (1951) and Bishop from Aliens (1986) are less prominent in popular culture.Isaac Asimov introduced the Three Laws of Robotics in many books and stories, most notably the \"Multivac\" series about a super-intelligent computer of the same name. Asimov\\'s laws are often brought up during lay discussions of machine ethics;\\nwhile almost all artificial intelligence researchers are familiar with Asimov\\'s laws through popular culture, they generally consider the laws useless for many reasons, one of which is their ambiguity.Transhumanism (the merging of humans and machines) is explored in the manga Ghost in the Shell and the science-fiction series Dune.\\nSeveral works use AI to force us to confront the fundamental question of what makes us human, showing us artificial beings that have the ability to feel, and thus to suffer. This appears in Karel Čapek\\'s R.U.R., the films A.I. Artificial Intelligence and Ex Machina, as well as the novel Do Androids Dream of Electric Sheep?, by Philip K. Dick. Dick considers the idea that our understanding of human subjectivity is altered by technology created with artificial intelligence.\\n\\n\\n== See also ==\\n\\n\\n== Explanatory notes ==\\n\\n\\n== Citations ==\\n\\n\\n== References ==\\n\\n\\n=== AI textbooks ===\\nThese were the four the most widely used AI textbooks in 2008.\\n\\n\\n=== History of AI ===\\n\\n\\n=== Other sources ===\\nWerbos, P. J. (1988), \"Generalization of backpropagation with application to a recurrent gas market model\", Neural Networks, 1 (4): 339–356, doi:10.1016/0893-6080(88)90007-X\\nGers, Felix A.; Schraudolph, Nicol N.; Schraudolph, Jürgen (2002). \"Learning Precise Timing with LSTM Recurrent Networks\" (PDF). Journal of Machine Learning Research. 3: 115–143. Retrieved 13 June 2017.\\nDeng, L.; Yu, D. (2014). \"Deep Learning: Methods and Applications\" (PDF). Foundations and Trends in Signal Processing. 7 (3–4): 1–199. doi:10.1561/2000000039. Archived (PDF) from the original on 14 March 2016. Retrieved 18 October 2014.\\nSchulz, Hannes; Behnke, Sven (1 November 2012). \"Deep Learning\". KI - Künstliche Intelligenz. 26 (4): 357–363. doi:10.1007/s13218-012-0198-z. ISSN 1610-1987. S2CID 220523562.\\nFukushima, K. (2007). \"Neocognitron\". Scholarpedia. 2 (1): 1717. Bibcode:2007SchpJ...2.1717F. doi:10.4249/scholarpedia.1717.</ref> was introduced by Kunihiko Fukushima in 1980.\\nHabibi, Aghdam, Hamed (30 May 2017). Guide to convolutional neural networks : a practical application to traffic-sign detection and classification. Heravi, Elnaz Jahani. Cham, Switzerland. ISBN 9783319575490. OCLC 987790957.\\nCiresan, D.; Meier, U.; Schmidhuber, J. (2012). \"Multi-column deep neural networks for image classification\". 2012 IEEE Conference on Computer Vision and Pattern Recognition. pp. 3642–3649. arXiv:1202.2745. doi:10.1109/cvpr.2012.6248110. ISBN 978-1-4673-1228-8. S2CID 2161592.\\nSchmidhuber, J. (2015). \"Deep Learning in Neural Networks: An Overview\". Neural Networks. 61: 85–117. arXiv:1404.7828. doi:10.1016/j.neunet.2014.09.003. PMID 25462637. S2CID 11715509.\\n\"From not working to neural networking\". The Economist. 2016. Archived from the original on 31 December 2016. Retrieved 26 April 2018.\\nThompson, Derek (23 January 2014). \"What Jobs Will the Robots Take?\". The Atlantic. Archived from the original on 24 April 2018. Retrieved 24 April 2018.\\nScassellati, Brian (2002). \"Theory of mind for a humanoid robot\". Autonomous Robots. 12 (1): 13–24. doi:10.1023/A:1013298507114. S2CID 1979315.\\nSample, Ian (14 March 2017). \"Google\\'s DeepMind makes AI program that can learn like a human\". The Guardian. Archived from the original on 26 April 2018. Retrieved 26 April 2018.\\nHeath, Nick (11 December 2020). \"What is AI? Everything you need to know about Artificial Intelligence\". ZDNet. Retrieved 1 March 2021.\\nBowling, Michael; Burch, Neil; Johanson, Michael; Tammelin, Oskari (9 January 2015). \"Heads-up limit hold\\'em poker is solved\". Science. 347 (6218): 145–149. Bibcode:2015Sci...347..145B. doi:10.1126/science.1259433. ISSN 0036-8075. PMID 25574016. S2CID 3796371.\\nSolly, Meilan (15 July 2019). \"This Poker-Playing A.I. Knows When to Hold \\'Em and When to Fold \\'Em\". Smithsonian.\\n\"Artificial intelligence: Google\\'s AlphaGo beats Go master Lee Se-dol\". BBC News. 12 March 2016. Archived from the original on 26 August 2016. Retrieved 1 October 2016.</ref>\\nRowinski, Dan (15 January 2013). \"Virtual Personal Assistants & The Future Of Your Smartphone [Infographic]\". ReadWrite. Archived from the original on 22 December 2015.\\nMarkoff, John (16 February 2011). \"Computer Wins on \\'Jeopardy!\\': Trivial, It\\'s Not\". The New York Times. Archived from the original on 22 October 2014. Retrieved 25 October 2014.\\nAnadiotis, George (1 October 2020). \"The state of AI in 2020: Democratization, industrialization, and the way to artificial general intelligence\". ZDNet. Retrieved 1 March 2021.\\nGoertzel, Ben; Lian, Ruiting; Arel, Itamar; de Garis, Hugo; Chen, Shuo (December 2010). \"A world survey of artificial brain projects, Part II: Biologically inspired cognitive architectures\". Neurocomputing. 74 (1–3): 30–49. doi:10.1016/j.neucom.2010.08.012.\\nRobinson, A. J.; Fallside, F. (1987), \"The utility driven dynamic error propagation network.\", Technical Report CUED/F-INFENG/TR.1, Cambridge University Engineering Department\\nHochreiter, Sepp (1991). Untersuchungen zu dynamischen neuronalen Netzen (PDF) (diploma thesis). Munich: Institut f. Informatik, Technische Univ. Archived 6 March 2015 at the Wayback Machine\\nWilliams, R. J.; Zipser, D. (1994), \"Gradient-based learning algorithms for recurrent networks and their computational complexity\", Back-propagation: Theory, Architectures and Applications, Hillsdale, NJ: Erlbaum\\nHochreiter, Sepp; Schmidhuber, Jürgen (1997), \"Long Short-Term Memory\", Neural Computation, 9 (8): 1735–1780, doi:10.1162/neco.1997.9.8.1735, PMID 9377276, S2CID 1915014Goodfellow, Ian; Bengio, Yoshua; Courville, Aaron (2016), Deep Learning, MIT Press. Archived 16 April 2016 at the Wayback Machine\\nHinton, G.; Deng, L.; Yu, D.; Dahl, G.; Mohamed, A.; Jaitly, N.; Senior, A.; Vanhoucke, V.; Nguyen, P.; Sainath, T.; Kingsbury, B. (2012). \"Deep Neural Networks for Acoustic Modeling in Speech Recognition – The shared views of four research groups\". IEEE Signal Processing Magazine. 29 (6): 82–97. Bibcode:2012ISPM...29...82H. doi:10.1109/msp.2012.2205597. S2CID 206485943.\\nSchmidhuber, J. (2015a). \"Deep Learning in Neural Networks: An Overview\". Neural Networks. 61: 85–117. arXiv:1404.7828. doi:10.1016/j.neunet.2014.09.003. PMID 25462637. S2CID 11715509.\\nLinnainmaa, Seppo (1970). The representation of the cumulative rounding error of an algorithm as a Taylor expansion of the local rounding errors (Thesis) (in Finnish). Univ. Helsinki, 6–7.|\\nGriewank, Andreas (2012). \"Who Invented the Reverse Mode of Differentiation? Optimization Stories\". Documenta Matematica, Extra Volume ISMP: 389–400.\\nWerbos, Paul (1974). Beyond Regression: New Tools for Prediction and Analysis in the Behavioral Sciences (Ph.D. thesis). Harvard University.\\nWerbos, Paul (1982). \"Beyond Regression: New Tools for Prediction and Analysis in the Behavioral Sciences\" (PDF). System Modeling and Optimization. Applications of advances in nonlinear sensitivity analysis. Berlin, Heidelberg: Springer. Archived 14 April 2016 at the Wayback Machine\\n\"What is \\'fuzzy logic\\'? Are there computers that are inherently fuzzy and do not apply the usual binary logic?\". Scientific American. 21 October 1999. Retrieved 5 May 2018.\\nMerkle, Daniel; Middendorf, Martin (2013). \"Swarm Intelligence\".  In Burke, Edmund K.; Kendall, Graham (eds.). Search Methodologies: Introductory Tutorials in Optimization and Decision Support Techniques. Springer Science & Business Media. ISBN 978-1-4614-6940-7.\\nvan der Walt, Christiaan; Bernard, Etienne (2006). \"Data characteristics that determine classifier performance\" (PDF). Archived from the original (PDF) on 25 March 2009. Retrieved 5 August 2009.\\nHutter, Marcus (2005). Universal Artificial Intelligence. Berlin: Springer. ISBN 978-3-540-22139-5.\\nHowe, J. (November 1994). \"Artificial Intelligence at Edinburgh University: a Perspective\". Archived from the original on 15 May 2007. Retrieved 30 August 2007.\\nGalvan, Jill (1 January 1997). \"Entering the Posthuman Collective in Philip K. Dick\\'s \"Do Androids Dream of Electric Sheep?\"\". Science Fiction Studies. 24 (3): 413–429. JSTOR 4240644.\\nMcCauley, Lee (2007). \"AI armageddon and the three laws of robotics\". Ethics and Information Technology. 9 (2): 153–164. CiteSeerX 10.1.1.85.8904. doi:10.1007/s10676-007-9138-2. S2CID 37272949.\\nButtazzo, G. (July 2001). \"Artificial consciousness: Utopia or real possibility?\". Computer. 34 (7): 24–30. doi:10.1109/2.933500.\\nAnderson, Susan Leigh (2008). \"Asimov\\'s \"three laws of robotics\" and machine metaethics\". AI & Society. 22 (4): 477–493. doi:10.1007/s00146-007-0094-5. S2CID 1809459.\\nYudkowsky, E (2008), \"Artificial Intelligence as a Positive and Negative Factor in Global Risk\" (PDF), Global Catastrophic Risks, Oxford University Press, 2008, Bibcode:2008gcr..book..303Y\\nMcGaughey, E (2018), Will Robots Automate Your Job Away? Full Employment, Basic Income, and Economic Democracy, p. SSRN part 2(3), SSRN 3044448 Archived 24 May 2018 at the Wayback Machine\\nIGM Chicago (30 June 2017). \"Robots and Artificial Intelligence\". www.igmchicago.org. Archived from the original on 1 May 2019. Retrieved 3 July 2019.\\nLohr, Steve (2017). \"Robots Will Take Jobs, but Not as Fast as Some Fear, New Report Says\". The New York Times. Archived from the original on 14 January 2018. Retrieved 13 January 2018.\\nFrey, Carl Benedikt; Osborne, Michael A (1 January 2017). \"The future of employment: How susceptible are jobs to computerisation?\". Technological Forecasting and Social Change. 114: 254–280. CiteSeerX 10.1.1.395.416. doi:10.1016/j.techfore.2016.08.019. ISSN 0040-1625.\\nArntz, Melanie; Gregory, Terry; Zierahn, Ulrich (2016), \"The risk of automation for jobs in OECD countries: A comparative analysis\", OECD Social, Employment, and Migration Working Papers 189\\nMorgenstern, Michael (9 May 2015). \"Automation and anxiety\". The Economist. Archived from the original on 12 January 2018. Retrieved 13 January 2018.\\nMahdawi, Arwa (26 June 2017). \"What jobs will still be around in 20 years? Read this to prepare your future\". The Guardian. Archived from the original on 14 January 2018. Retrieved 13 January 2018.\\nRubin, Charles (Spring 2003). \"Artificial Intelligence and Human Nature\". The New Atlantis. 1: 88–100. Archived from the original on 11 June 2012.\\nBostrom, Nick (2014). Superintelligence: Paths, Dangers, Strategies. Oxford University Press.\\nBrooks, Rodney (10 November 2014). \"artificial intelligence is a tool, not a threat\". Archived from the original on 12 November 2014.\\nSainato, Michael (19 August 2015). \"Stephen Hawking, Elon Musk, and Bill Gates Warn About Artificial Intelligence\". Observer. Archived from the original on 30 October 2015. Retrieved 30 October 2015.\\nHarari, Yuval Noah (October 2018). \"Why Technology Favors Tyranny\". The Atlantic.\\nRobitzski, Dan (5 September 2018). \"Five experts share what scares them the most about AI\". Archived from the original on 8 December 2019. Retrieved 8 December 2019.\\nGoffrey, Andrew (2008). \"Algorithm\".  In Fuller, Matthew (ed.). Software studies: a lexicon. Cambridge, Mass.: MIT Press. pp. 15–20. ISBN 978-1-4356-4787-9.\\nLipartito, Kenneth (6 January 2011). \"The Narrative and the Algorithm: Genres of Credit Reporting from the Nineteenth Century to Today\" (PDF) (Submitted manuscript). doi:10.2139/ssrn.1736283. S2CID 166742927. \\nGoodman, Bryce; Flaxman, Seth (2017). \"EU regulations on algorithmic decision-making and a \"right to explanation\"\". AI Magazine. 38 (3): 50. arXiv:1606.08813. doi:10.1609/aimag.v38i3.2741. S2CID 7373959.\\nCNA (12 January 2019). \"Commentary: Bad news. Artificial intelligence is biased\". CNA. Archived from the original on 12 January 2019. Retrieved 19 June 2020.\\nLarson, Jeff; Angwin, Julia (23 May 2016). \"How We Analyzed the COMPAS Recidivism Algorithm\". ProPublica. Archived from the original on 29 April 2019. Retrieved 19 June 2020.\\nMüller, Vincent C.; Bostrom, Nick (2014). \"Future Progress in Artificial Intelligence: A Poll Among Experts\" (PDF). AI Matters. 1 (1): 9–11. doi:10.1145/2639475.2639478. S2CID 8510016. Archived (PDF) from the original on 15 January 2016.\\nCellan-Jones, Rory (2 December 2014). \"Stephen Hawking warns artificial intelligence could end mankind\". BBC News. Archived from the original on 30 October 2015. Retrieved 30 October 2015.\\nRawlinson, Kevin (29 January 2015). \"Microsoft\\'s Bill Gates insists AI is a threat\". BBC News. Archived from the original on 29 January 2015. Retrieved 30 January 2015.\\nHolley, Peter (28 January 2015). \"Bill Gates on dangers of artificial intelligence: \\'I don\\'t understand why some people are not concerned\\'\". The Washington Post. ISSN 0190-8286. Archived from the original on 30 October 2015. Retrieved 30 October 2015.\\nGibbs, Samuel (27 October 2014). \"Elon Musk: artificial intelligence is our biggest existential threat\". The Guardian. Archived from the original on 30 October 2015. Retrieved 30 October 2015.\\nChurm, Philip Andrew (14 May 2019). \"Yuval Noah Harari talks politics, technology and migration\". euronews. Archived from the original on 14 May 2019. Retrieved 15 November 2020.\\nBostrom, Nick (2015). \"What happens when our computers get smarter than we are?\". TED (conference). Archived from the original on 25 July 2020. Retrieved 30 January 2020.\\nPost, Washington (2015). \"Tech titans like Elon Musk are spending $1 billion to save you from terminators\". Chicago Tribune. Archived from the original on 7 June 2016.\\nDel Prado, Guia Marie (9 October 2015). \"The mysterious artificial intelligence company Elon Musk invested in is developing game-changing smart computers\". Tech Insider. Archived from the original on 30 October 2015. Retrieved 30 October 2015.\\nFastCompany (15 January 2015). \"Elon Musk Is Donating $10M Of His Own Money To Artificial Intelligence Research\". Fast Company. Archived from the original on 30 October 2015. Retrieved 30 October 2015.\\nThibodeau, Patrick (25 March 2019). \"Oracle CEO Mark Hurd sees no reason to fear ERP AI\". SearchERP. Archived from the original on 6 May 2019. Retrieved 6 May 2019.\\nBhardwaj, Prachi (24 May 2018). \"Mark Zuckerberg responds to Elon Musk\\'s paranoia about AI: \\'AI is going to... help keep our communities safe.\\'\". Business Insider. Archived from the original on 6 May 2019. Retrieved 6 May 2019.\\nGeist, Edward Moore (9 August 2015). \"Is artificial intelligence really an existential threat to humanity?\". Bulletin of the Atomic Scientists. Archived from the original on 30 October 2015. Retrieved 30 October 2015.\\nMadrigal, Alexis C. (27 February 2015). \"The case against killer robots, from a guy actually working on artificial intelligence\". Fusion.net. Archived from the original on 4 February 2016. Retrieved 31 January 2016.\\nLee, Timothy B. (22 August 2014). \"Will artificial intelligence destroy humanity? Here are 5 reasons not to worry\". Vox. Archived from the original on 30 October 2015. Retrieved 30 October 2015.\\nLaw Library of Congress (U.S.). Global Legal Research Directorate, issuing body. (2019). Regulation of artificial intelligence in selected jurisdictions. LCCN 2019668143. OCLC 1110727808.\\nUNESCO Science Report: the Race Against Time for Smarter Development. Paris: UNESCO. 11 June 2021. ISBN 978-92-3-100450-6.\\nBerryhill, Jamie; Heang, Kévin Kok; Clogher, Rob; McBride, Keegan (2019). Hello, World: Artificial Intelligence and its Use in the Public Sector (PDF). Paris: OECD Observatory of Public Sector Innovation. Archived (PDF) from the original on 20 December 2019. Retrieved 9 August 2020.\\nBarfield, Woodrow; Pagallo, Ugo (2018). Research handbook on the law of artificial intelligence. Cheltenham, UK. ISBN 978-1-78643-904-8. OCLC 1039480085.\\nIphofen, Ron; Kritikos, Mihalis (3 January 2019). \"Regulating artificial intelligence and robotics: ethics by design in a digital society\". Contemporary Social Science. 16 (2): 170–184. doi:10.1080/21582041.2018.1563803. ISSN 2158-2041. S2CID 59298502.\\nWirtz, Bernd W.; Weyerer, Jan C.; Geyer, Carolin (24 July 2018). \"Artificial Intelligence and the Public Sector – Applications and Challenges\". International Journal of Public Administration. 42 (7): 596–615. doi:10.1080/01900692.2018.1498103. ISSN 0190-0692. S2CID 158829602. Archived from the original on 18 August 2020. Retrieved 22 August 2020.\\nBuiten, Miriam C (2019). \"Towards Intelligent Regulation of Artificial Intelligence\". European Journal of Risk Regulation. 10 (1): 41–59. doi:10.1017/err.2019.8. ISSN 1867-299X.\\nWallach, Wendell (2010). Moral Machines. Oxford University Press.\\nBrown, Eileen (5 November 2019). \"Half of Americans do not believe deepfake news could target them online\". ZDNet. Archived from the original on 6 November 2019. Retrieved 3 December 2019.\\nFrangoul, Anmar (14 June 2019). \"A Californian business is using A.I. to change the way we think about energy storage\". CNBC. Archived from the original on 25 July 2020. Retrieved 5 November 2019.\\n\"The Economist Explains: Why firms are piling into artificial intelligence\". The Economist. 31 March 2016. Archived from the original on 8 May 2016. Retrieved 19 May 2016.\\nLohr, Steve (28 February 2016). \"The Promise of Artificial Intelligence Unfolds in Small Steps\". The New York Times. Archived from the original on 29 February 2016. Retrieved 29 February 2016.\\nSmith, Mark (22 July 2016). \"So you think you chose to read this article?\". BBC News. Archived from the original on 25 July 2016.\\nAletras, N.; Tsarapatsanis, D.; Preotiuc-Pietro, D.; Lampos, V. (2016). \"Predicting judicial decisions of the European Court of Human Rights: a Natural Language Processing perspective\". PeerJ Computer Science. 2: e93. doi:10.7717/peerj-cs.93.\\nCadena, Cesar; Carlone, Luca; Carrillo, Henry; Latif, Yasir; Scaramuzza, Davide; Neira, Jose; Reid, Ian; Leonard, John J. (December 2016). \"Past, Present, and Future of Simultaneous Localization and Mapping: Toward the Robust-Perception Age\". IEEE Transactions on Robotics. 32 (6): 1309–1332. arXiv:1606.05830. Bibcode:2016arXiv160605830C. doi:10.1109/TRO.2016.2624754. S2CID 2596787.\\nCambria, Erik; White, Bebo (May 2014). \"Jumping NLP Curves: A Review of Natural Language Processing Research [Review Article]\". IEEE Computational Intelligence Magazine. 9 (2): 48–57. doi:10.1109/MCI.2014.2307227. S2CID 206451986.\\nVincent, James (7 November 2019). \"OpenAI has published the text-generating AI it said was too dangerous to share\". The Verge. Archived from the original on 11 June 2020. Retrieved 11 June 2020.\\nJordan, M. I.; Mitchell, T. M. (16 July 2015). \"Machine learning: Trends, perspectives, and prospects\". Science. 349 (6245): 255–260. Bibcode:2015Sci...349..255J. doi:10.1126/science.aaa8415. PMID 26185243. S2CID 677218.\\nMaschafilm (2010). \"Content: Plug & Pray Film – Artificial Intelligence – Robots -\". plugandpray-film.de. Archived from the original on 12 February 2016.\\nEvans, Woody (2015). \"Posthuman Rights: Dimensions of Transhuman Worlds\". Teknokultura. 12 (2). doi:10.5209/rev_TK.2015.v12.n2.49072.\\nWaddell, Kaveh (2018). \"Chatbots Have Entered the Uncanny Valley\". The Atlantic. Archived from the original on 24 April 2018. Retrieved 24 April 2018.\\nPoria, Soujanya; Cambria, Erik; Bajpai, Rajiv; Hussain, Amir (September 2017). \"A review of affective computing: From unimodal analysis to multimodal fusion\". Information Fusion. 37: 98–125. doi:10.1016/j.inffus.2017.02.003. hdl:1893/25490.\\n\"Robots could demand legal rights\". BBC News. 21 December 2006. Archived from the original on 15 October 2019. Retrieved 3 February 2011.\\nHorst, Steven (2005). \"The Computational Theory of Mind\". The Stanford Encyclopedia of Philosophy.\\nOmohundro, Steve (2008). The Nature of Self-Improving Artificial Intelligence. presented and distributed at the 2007 Singularity Summit, San Francisco, CA.\\nFord, Martin; Colvin, Geoff (6 September 2015). \"Will robots create more jobs than they destroy?\". The Guardian. Archived from the original on 16 June 2018. Retrieved 13 January 2018.\\nWhite Paper: On Artificial Intelligence – A European approach to excellence and trust (PDF). Brussels: European Commission. 2020. Archived (PDF) from the original on 20 February 2020. Retrieved 20 February 2020.\\nAnderson, Michael; Anderson, Susan Leigh (2011). Machine Ethics. Cambridge University Press.\\n\"Machine Ethics\". aaai.org. Archived from the original on 29 November 2014.\\nRussell, Stuart (8 October 2019). Human Compatible: Artificial Intelligence and the Problem of Control. United States: Viking. ISBN 978-0-525-55861-3. OCLC 1083694322.\\n\"AI set to exceed human brain power\". CNN. 9 August 2006. Archived from the original on 19 February 2008.\\n\"Robots could demand legal rights\". BBC News. 21 December 2006. Archived from the original on 15 October 2019. Retrieved 3 February 2011.\\n\"Kismet\". MIT Artificial Intelligence Laboratory, Humanoid Robotics Group. Archived from the original on 17 October 2014. Retrieved 25 October 2014.\\nSmoliar, Stephen W.; Zhang, HongJiang (1994). \"Content based video indexing and retrieval\". IEEE Multimedia. 1 (2): 62–72. doi:10.1109/93.311653. S2CID 32710913.\\nNeumann, Bernd; Möller, Ralf (January 2008). \"On scene interpretation with description logics\". Image and Vision Computing. 26 (1): 82–101. doi:10.1016/j.imavis.2007.08.013.\\nKuperman, G. J.; Reichley, R. M.; Bailey, T. C. (1 July 2006). \"Using Commercial Knowledge Bases for Clinical Decision Support: Opportunities, Hurdles, and Recommendations\". Journal of the American Medical Informatics Association. 13 (4): 369–371. doi:10.1197/jamia.M2055. PMC 1513681. PMID 16622160.\\nMcGarry, Ken (1 December 2005). \"A survey of interestingness measures for knowledge discovery\". The Knowledge Engineering Review. 20 (1): 39–61. doi:10.1017/S0269888905000408. S2CID 14987656.\\nBertini, M; Del Bimbo, A; Torniai, C (2006). \"Automatic annotation and semantic retrieval of video sequences using multimedia ontologies\". MM \\'06 Proceedings of the 14th ACM international conference on Multimedia. 14th ACM international conference on Multimedia. Santa Barbara: ACM. pp. 679–682.\\nKahneman, Daniel (25 October 2011). Thinking, Fast and Slow. Macmillan. ISBN 978-1-4299-6935-2. Retrieved 8 April 2012.\\nTuring, Alan (1948), \"Machine Intelligence\",  in Copeland, B. Jack (ed.), The Essential Turing: The ideas that gave birth to the computer age, Oxford: Oxford University Press, p. 412, ISBN 978-0-19-825080-7\\nDomingos, Pedro (22 September 2015). The Master Algorithm: How the Quest for the Ultimate Learning Machine Will Remake Our World. Basic Books. ISBN 978-0465065707.\\nMinsky, Marvin (1986), The Society of Mind, Simon and Schuster\\nPinker, Steven (4 September 2007) [1994], The Language Instinct, Perennial Modern Classics, Harper, ISBN 978-0-06-133646-1\\nChalmers, David (1995). \"Facing up to the problem of consciousness\". Journal of Consciousness Studies. 2 (3): 200–219. Archived from the original on 8 March 2005. Retrieved 11 October 2018.\\nRoberts, Jacob (2016). \"Thinking Machines: The Search for Artificial Intelligence\". Distillations. Vol. 2 no. 2. pp. 14–23. Archived from the original on 19 August 2018. Retrieved 20 March 2018.\\nPennachin, C.; Goertzel, B. (2007). \"Contemporary Approaches to Artificial General Intelligence\". Artificial General Intelligence. Cognitive Technologies. Berlin, Heidelberg: Springer. doi:10.1007/978-3-540-68677-4_1. ISBN 978-3-540-23733-4.\\n\"Ask the AI experts: What\\'s driving today\\'s progress in AI?\". McKinsey & Company. Archived from the original on 13 April 2018. Retrieved 13 April 2018.\\n\"Reshaping Business With Artificial Intelligence\". MIT Sloan Management Review. Archived from the original on 19 May 2018. Retrieved 2 May 2018.\\nLorica, Ben (18 December 2017). \"The state of AI adoption\". O\\'Reilly Media. Archived from the original on 2 May 2018. Retrieved 2 May 2018.\\n\"AlphaGo – Google DeepMind\". Archived from the original on 20 October 2021.Asada, M.; Hosoda, K.; Kuniyoshi, Y.; Ishiguro, H.; Inui, T.; Yoshikawa, Y.; Ogino, M.; Yoshida, C. (2009). \"Cognitive developmental robotics: a survey\". IEEE Transactions on Autonomous Mental Development. 1 (1): 12–34. doi:10.1109/tamd.2009.2021702. S2CID 10168773.\\nAshok83 (10 September 2019). \"How AI Is Getting Groundbreaking Changes In Talent Management And HR Tech\". Hackernoon. Archived from the original on 11 September 2019. Retrieved 14 February 2020.\\nBerlinski, David (2000). The Advent of the Algorithm. Harcourt Books. ISBN 978-0-15-601391-8. OCLC 46890682. Archived from the original on 26 July 2020. Retrieved 22 August 2020.\\nBrooks, Rodney (1990). \"Elephants Don\\'t Play Chess\" (PDF). Robotics and Autonomous Systems. 6 (1–2): 3–15. CiteSeerX 10.1.1.588.7539. doi:10.1016/S0921-8890(05)80025-9. Archived (PDF) from the original on 9 August 2007.\\nButler, Samuel (13 June 1863). \"Darwin among the Machines\". Letters to the Editor. The Press. Christchurch, New Zealand. Archived from the original on 19 September 2008. Retrieved 16 October 2014 – via Victoria University of Wellington.\\nClark, Jack (1 July 2015a). \"Musk-Backed Group Probes Risks Behind Artificial Intelligence\". Bloomberg.com. Archived from the original on 30 October 2015. Retrieved 30 October 2015.\\nClark, Jack (8 December 2015b). \"Why 2015 Was a Breakthrough Year in Artificial Intelligence\". Bloomberg.com. Archived from the original on 23 November 2016. Retrieved 23 November 2016.\\nDennett, Daniel (1991). Consciousness Explained. The Penguin Press. ISBN 978-0-7139-9037-9.\\nDreyfus, Hubert (1972). What Computers Can\\'t Do. New York: MIT Press. ISBN 978-0-06-011082-6.\\nDreyfus, Hubert; Dreyfus, Stuart (1986). Mind over Machine: The Power of Human Intuition and Expertise in the Era of the Computer. Oxford, UK: Blackwell. ISBN 978-0-02-908060-3. Archived from the original on 26 July 2020. Retrieved 22 August 2020.\\nDyson, George (1998). Darwin among the Machines. Allan Lane Science. ISBN 978-0-7382-0030-9. Archived from the original on 26 July 2020. Retrieved 22 August 2020.\\nEdelson, Edward (1991). The Nervous System. New York: Chelsea House. ISBN 978-0-7910-0464-7. Archived from the original on 26 July 2020. Retrieved 18 November 2019.\\nFearn, Nicholas (2007). The Latest Answers to the Oldest Questions: A Philosophical Adventure with the World\\'s Greatest Thinkers. New York: Grove Press. ISBN 978-0-8021-1839-4.\\nHaugeland, John (1985). Artificial Intelligence: The Very Idea. Cambridge, Mass.: MIT Press. ISBN 978-0-262-08153-5.\\nHawkins, Jeff; Blakeslee, Sandra (2005). On Intelligence. New York: Owl Books. ISBN 978-0-8050-7853-4.\\nHenderson, Mark (24 April 2007). \"Human rights for robots? We\\'re getting carried away\". The Times Online. London. Archived from the original on 31 May 2014. Retrieved 31 May 2014.\\nKahneman, Daniel; Slovic, D.; Tversky, Amos (1982). Judgment under uncertainty: Heuristics and biases. Science. 185. New York: Cambridge University Press. pp. 1124–1131. doi:10.1126/science.185.4157.1124. ISBN 978-0-521-28414-1. PMID 17835457. S2CID 143452957.\\nKatz, Yarden (1 November 2012). \"Noam Chomsky on Where Artificial Intelligence Went Wrong\". The Atlantic. Archived from the original on 28 February 2019. Retrieved 26 October 2014.\\nKurzweil, Ray (2005). The Singularity is Near. Penguin Books. ISBN 978-0-670-03384-3.\\nLangley, Pat (2011). \"The changing science of machine learning\". Machine Learning. 82 (3): 275–279. doi:10.1007/s10994-011-5242-y.\\nLegg, Shane; Hutter, Marcus (15 June 2007). A Collection of Definitions of Intelligence (Technical report). IDSIA. arXiv:0706.3639. Bibcode:2007arXiv0706.3639L. 07-07.\\nLenat, Douglas; Guha, R. V. (1989). Building Large Knowledge-Based Systems. Addison-Wesley. ISBN 978-0-201-51752-1.\\nLighthill, James (1973). \"Artificial Intelligence: A General Survey\". Artificial Intelligence: a paper symposium. Science Research Council.\\nLombardo, P; Boehm, I; Nairz, K (2020). \"RadioComics – Santa Claus and the future of radiology\". Eur J Radiol. 122 (1): 108771. doi:10.1016/j.ejrad.2019.108771. PMID 31835078.\\nLungarella, M.; Metta, G.; Pfeifer, R.; Sandini, G. (2003). \"Developmental robotics: a survey\". Connection Science. 15 (4): 151–190. CiteSeerX 10.1.1.83.7615. doi:10.1080/09540090310001655110. S2CID 1452734.\\nMaker, Meg Houston (2006). \"AI@50: AI Past, Present, Future\". Dartmouth College. Archived from the original on 3 January 2007. Retrieved 16 October 2008.\\nMcCarthy, John; Minsky, Marvin; Rochester, Nathan; Shannon, Claude (1955). \"A Proposal for the Dartmouth Summer Research Project on Artificial Intelligence\". Archived from the original on 26 August 2007. Retrieved 30 August 2007.\\nMinsky, Marvin (1967). Computation: Finite and Infinite Machines. Englewood Cliffs, N.J.: Prentice-Hall. ISBN 978-0-13-165449-5. Archived from the original on 26 July 2020. Retrieved 18 November 2019.\\nMoravec, Hans (1988). Mind Children. Harvard University Press. ISBN 978-0-674-57616-2. Archived from the original on 26 July 2020. Retrieved 18 November 2019.\\nNRC (United States National Research Council) (1999). \"Developments in Artificial Intelligence\". Funding a Revolution: Government Support for Computing Research. National Academy Press.\\nNewell, Allen; Simon, H. A. (1976). \"Computer Science as Empirical Inquiry: Symbols and Search\". Communications of the ACM. 19 (3): 113–126. doi:10.1145/360018.360022..\\nNilsson, Nils (1983). \"Artificial Intelligence Prepares for 2001\" (PDF). AI Magazine. 1 (1). Archived (PDF) from the original on 17 August 2020. Retrieved 22 August 2020. Presidential Address to the Association for the Advancement of Artificial Intelligence.\\nOudeyer, P-Y. (2010). \"On the impact of robotics in behavioral and cognitive sciences: from insect navigation to human cognitive development\" (PDF). IEEE Transactions on Autonomous Mental Development. 2 (1): 2–16. doi:10.1109/tamd.2009.2039057. S2CID 6362217. Archived (PDF) from the original on 3 October 2018. Retrieved 4 June 2013.\\nSchank, Roger C. (1991). \"Where\\'s the AI\". AI magazine. Vol. 12 no. 4.\\nSearle, John (1980). \"Minds, Brains and Programs\" (PDF). Behavioral and Brain Sciences. 3 (3): 417–457. doi:10.1017/S0140525X00005756. Archived (PDF) from the original on 17 March 2019. Retrieved 22 August 2020.\\nSearle, John (1999). Mind, language and society. New York: Basic Books. ISBN 978-0-465-04521-1. OCLC 231867665. Archived from the original on 26 July 2020. Retrieved 22 August 2020.\\nSimon, H. A. (1965). The Shape of Automation for Men and Management. New York: Harper & Row. Archived from the original on 26 July 2020. Retrieved 18 November 2019.\\nSolomonoff, Ray (1956). An Inductive Inference Machine (PDF). Dartmouth Summer Research Conference on Artificial Intelligence. Archived (PDF) from the original on 26 April 2011. Retrieved 22 March 2011 – via std.com, pdf scanned copy of the original. Later published asSolomonoff, Ray (1957). \"An Inductive Inference Machine\". IRE Convention Record. Section on Information Theory, part 2. pp. 56–62.\\nSpadafora, Anthony (21 October 2016). \"Stephen Hawking believes AI could be mankind\\'s last accomplishment\". BetaNews. Archived from the original on 28 August 2017.\\nTao, Jianhua; Tan, Tieniu (2005). Affective Computing and Intelligent Interaction. Affective Computing: A Review. LNCS 3784. Springer. pp. 981–995. doi:10.1007/11573548.\\nTecuci, Gheorghe (March–April 2012). \"Artificial Intelligence\". Wiley Interdisciplinary Reviews: Computational Statistics. 4 (2): 168–180. doi:10.1002/wics.200.\\nThro, Ellen (1993). Robotics: The Marriage of Computers and Machines. New York: Facts on File. ISBN 978-0-8160-2628-9. Archived from the original on 26 July 2020. Retrieved 22 August 2020.\\nTuring, Alan (October 1950), \"Computing Machinery and Intelligence\", Mind, LIX (236): 433–460, doi:10.1093/mind/LIX.236.433, ISSN 0026-4423.\\nVinge, Vernor (1993). \"The Coming Technological Singularity: How to Survive in the Post-Human Era\". Vision 21: Interdisciplinary Science and Engineering in the Era of Cyberspace: 11. Bibcode:1993vise.nasa...11V. Archived from the original on 1 January 2007. Retrieved 14 November 2011.\\nWason, P. C.; Shapiro, D. (1966). \"Reasoning\".  In Foss, B. M. (ed.). New horizons in psychology. Harmondsworth: Penguin. Archived from the original on 26 July 2020. Retrieved 18 November 2019.\\nWeng, J.; McClelland; Pentland, A.; Sporns, O.; Stockman, I.; Sur, M.; Thelen, E. (2001). \"Autonomous mental development by robots and animals\" (PDF). Science. 291 (5504): 599–600. doi:10.1126/science.291.5504.599. PMID 11229402. S2CID 54131797. Archived (PDF) from the original on 4 September 2013. Retrieved 4 June 2013 – via msu.edu.\\n\\n\\n== Further reading ==\\n\\n\\n== External links ==\\n\\n\"Artificial Intelligence\". Internet Encyclopedia of Philosophy.\\nThomason, Richmond. \"Logic and Artificial Intelligence\".  In Zalta, Edward N. (ed.). Stanford Encyclopedia of Philosophy.\\nArtificial Intelligence, BBC Radio 4 discussion with John Agar, Alison Adam & Igor Aleksander (In Our Time, Dec. 8, 2005)\\n\\n\\n== Sources ==\\n This article incorporates text from a free content work.  Licensed under C-BY-SA 3.0 IGO Text taken from UNESCO Science Report: the Race Against Time for Smarter Development.,  Schneegans, S., T. Straza and J. Lewis (eds), UNESCO. To learn how to add open license text to Wikipedia articles, please see this how-to page. For information on reusing text from Wikipedia, please see the terms of use.', 'In mathematics and computer science, an algorithm ( (listen)) is a finite sequence of well-defined instructions, typically used to solve a class of specific problems or to perform a computation. Algorithms are used as specifications for performing calculations, data processing, automated reasoning, automated decision-making and other tasks. In contrast, a heuristic is a  technique used in problem solving that uses practical methods and/or various estimates in order to produce solutions that may not be optimal but are sufficient given the circumstances.As an effective method, an algorithm can be expressed within a finite amount of space and time, and in a well-defined formal language for calculating a function. Starting from an initial state and initial input (perhaps empty), the instructions describe a computation that, when executed, proceeds through a finite number of well-defined successive states, eventually producing \"output\" and terminating at a final ending state. The transition from one state to the next is not necessarily deterministic; some algorithms, known as randomized algorithms, incorporate random input.\\n\\n\\n== History ==\\nThe concept of algorithm has existed since antiquity. Arithmetic algorithms, such as a division algorithm, were used by ancient Babylonian mathematicians c. 2500 BC and Egyptian mathematicians c. 1550 BC. Greek mathematicians later used algorithms in 240 BC in the sieve of Eratosthenes for finding prime numbers, and the Euclidean algorithm for finding the greatest common divisor of two numbers. Arabic mathematicians such as al-Kindi in the 9th century used cryptographic algorithms for code-breaking, based on frequency analysis.The word algorithm is derived from the name of the 9th-century Persian mathematician Muḥammad ibn Mūsā al-Khwārizmī, whose nisba (identifying him as from Khwarazm) was Latinized as Algoritmi (Arabized Persian الخوارزمی c. 780–850).Muḥammad ibn Mūsā al-Khwārizmī was a mathematician, astronomer, geographer, and scholar in the House of Wisdom in Baghdad, whose name means \\'the native of Khwarazm\\', a region that was part of Greater Iran and is now in Uzbekistan. About 825, al-Khwarizmi wrote an Arabic language treatise on the Hindu–Arabic numeral system, which was translated into Latin during the 12th century. The manuscript starts with the phrase Dixit Algorizmi (\\'Thus spake Al-Khwarizmi\\'), where \"Algorizmi\" was the translator\\'s Latinization of Al-Khwarizmi\\'s name. Al-Khwarizmi was the most widely read mathematician in Europe in the late Middle Ages, primarily through another of his books, the Algebra. In late medieval Latin, algorismus, English \\'algorism\\', the corruption of his name, simply meant the \"decimal number system\". In the 15th century, under the influence of the Greek word ἀριθμός (arithmos), \\'number\\' (cf. \\'arithmetic\\'), the Latin word was altered to algorithmus, and the corresponding English term \\'algorithm\\' is first attested in the 17th century; the modern sense was introduced in the 19th century.In English, it was first used in about 1230 and then by Chaucer in 1391. English adopted the French term, but it was not until the late 19th century that \"algorithm\" took on the meaning that it has in modern English.Another early use of the word is from 1240, in a manual titled Carmen de Algorismo composed by Alexandre de Villedieu. It begins with:\\n\\nHaec algorismus ars praesens dicitur, in qua / Talibus Indorum fruimur bis quinque figuris.\\nwhich translates to:\\n\\nAlgorism is the art by which at present we use those Indian figures, which number two times five.\\nThe poem is a few hundred lines long and summarizes the art of calculating with the new styled Indian dice (Tali Indorum), or Hindu numerals.A partial formalization of the modern concept of algorithm began with attempts to solve the Entscheidungsproblem  (decision problem) posed by David Hilbert in 1928. Later formalizations were framed as attempts to define \"effective calculability\" or \"effective method\". Those formalizations included the Gödel–Herbrand–Kleene recursive functions of 1930, 1934 and 1935, Alonzo Church\\'s lambda calculus of 1936, Emil Post\\'s Formulation 1 of 1936, and Alan Turing\\'s Turing machines of 1936–37 and 1939.\\n\\n\\n== Informal definition ==\\n\\nAn informal definition could be \"a set of rules that precisely defines a sequence of operations\", which would include all computer programs (including programs that do not perform numeric calculations), and (for example) any prescribed bureaucratic procedure\\nor cook-book recipe.In general, a program is only an algorithm if it stops eventually—even though infinite loops may sometimes prove desirable.\\nA prototypical example of an algorithm is the Euclidean algorithm, which is used to determine the maximum common divisor of two integers; an example (there are others) is described by the flowchart above and as an example in a later section.\\nBoolos, Jeffrey & 1974, 1999 offer an informal meaning of the word \"algorithm\" in the following quotation:\\n\\nNo human being can write fast enough, or long enough, or small enough† ( †\"smaller and smaller without limit … you\\'d be trying to write on molecules, on atoms, on electrons\") to list all members of an enumerably infinite set by writing out their names, one after another, in some notation. But humans can do something equally useful, in the case of certain enumerably infinite sets: They can give explicit instructions for determining the nth member of the set, for arbitrary finite n. Such instructions are to be given quite explicitly, in a form in which they could be followed by a computing machine, or by a human who is capable of carrying out only very elementary operations on symbols.\\nAn \"enumerably infinite set\" is one whose elements can be put into one-to-one correspondence with the integers. Thus Boolos and Jeffrey are saying that an algorithm implies instructions for a process that \"creates\" output integers from an arbitrary \"input\" integer or integers that, in theory, can be arbitrarily large. For example, an algorithm can be an algebraic equation such as y = m + n (i.e., two arbitrary \"input variables\" m and n that produce an output y), but various authors\\' attempts to define the notion indicate that the word implies much more than this, something on the order of (for the addition example):\\n\\nPrecise instructions (in a language understood by \"the computer\") for a fast, efficient, \"good\" process that specifies the \"moves\" of \"the computer\" (machine or human, equipped with the necessary internally contained information and capabilities) to find, decode, and then process arbitrary input integers/symbols m and n, symbols + and = … and \"effectively\" produce, in a \"reasonable\" time, output-integer y at a specified place and in a specified format.The concept of algorithm is also used to define the notion of decidability—a notion that is central for explaining how formal systems come into being starting from a small set of axioms and rules. In logic, the time that an algorithm requires to complete cannot be measured, as it is not apparently related to the customary physical dimension. From such uncertainties, that characterize ongoing work, stems the unavailability of a definition of algorithm that suits both concrete (in some sense) and abstract usage of the term.\\n\\n\\n== Formalization ==\\nAlgorithms are essential to the way computers process data. Many computer programs contain algorithms that detail the specific instructions a computer should perform—in a specific order—to carry out a specified task, such as calculating employees\\' paychecks or printing students\\' report cards. Thus, an algorithm can be considered to be any sequence of operations that can be simulated by a Turing-complete system. Authors who assert this thesis include Minsky (1967), Savage (1987) and Gurevich (2000):\\n\\n Minsky: \"But we will also maintain, with Turing … that any procedure which could \"naturally\" be called effective, can, in fact, be realized by a (simple) machine. Although this may seem extreme, the arguments … in its favor are hard to refute\".\\n Gurevich: \"… Turing\\'s informal argument in favor of his thesis justifies a stronger thesis: every algorithm can be simulated by a Turing machine … according to Savage [1987], an algorithm is a computational process defined by a Turing machine\".Turing machines can define computational processes that do not terminate. The informal definitions of algorithms generally require that the algorithm always terminates. This requirement renders the task of deciding whether a formal procedure is an algorithm impossible in the general case—due to a major theorem of computability theory known as the halting problem.\\nTypically, when an algorithm is associated with processing information, data can be read from an input source, written to an output device and stored for further processing. Stored data are regarded as part of the internal state of the entity performing the algorithm. In practice, the state is stored in one or more data structures.\\nFor some of these computational processes, the algorithm must be rigorously defined: specified in the way it applies in all possible circumstances that could arise. This means that any conditional steps must be systematically dealt with, case-by-case; the criteria for each case must be clear (and computable).\\nBecause an algorithm is a precise list of precise steps, the order of computation is always crucial to the functioning of the algorithm. Instructions are usually assumed to be listed explicitly, and are described as starting \"from the top\" and going \"down to the bottom\"—an idea that is described more formally by flow of control.\\nSo far, the discussion on the formalization of an algorithm has assumed the premises of imperative programming. This is the most common conception—one which attempts to describe a task in discrete, \"mechanical\" means. Unique to this conception of formalized algorithms is the assignment operation, which sets the value of a variable. It derives from the intuition of \"memory\" as a scratchpad. An example of such an assignment can be found below.\\nFor some alternate conceptions of what constitutes an algorithm, see functional programming and logic programming.\\n\\n\\n== Expressing algorithms ==\\nAlgorithms can be expressed in many kinds of notation, including natural languages, pseudocode, flowcharts, drakon-charts, programming languages or control tables (processed by interpreters). Natural language expressions of algorithms tend to be verbose and ambiguous, and are rarely used for complex or technical algorithms. Pseudocode, flowcharts, drakon-charts and control tables are structured ways to express algorithms that avoid many of the ambiguities common in the statements based on natural language. Programming languages are primarily intended for expressing algorithms in a form that can be executed by a computer, but are also often used as a way to define or document algorithms.\\nThere is a wide variety of representations possible and one can express a given Turing machine program as a sequence of machine tables (see finite-state machine, state transition table and control table for more), as flowcharts and drakon-charts (see state diagram for more), or as a form of rudimentary machine code or assembly code called \"sets of quadruples\" (see Turing machine for more).\\nRepresentations of algorithms can be classed into three accepted levels of Turing machine description, as follows:\\n1 High-level description\\n\"…prose to describe an algorithm, ignoring the implementation details. At this level, we do not need to mention how the machine manages its tape or head.\"\\n2 Implementation description\\n\"…prose used to define the way the Turing machine uses its head and the way that it stores data on its tape. At this level, we do not give details of states or transition function.\"\\n3 Formal description\\nMost detailed, \"lowest level\", gives the Turing machine\\'s \"state table\".For an example of the simple algorithm \"Add m+n\" described in all three levels, see Algorithm#Examples.\\n\\n\\n== Design ==\\n\\nAlgorithm design refers to a method or a mathematical process for problem-solving and engineering algorithms. The design of algorithms is part of many solution theories of operation research, such as dynamic programming and divide-and-conquer. Techniques for designing and implementing algorithm designs are also called algorithm design patterns, with examples including the template method pattern and the decorator pattern.\\nOne of the most important aspects of algorithm design is resource (run-time, memory usage) efficiency; the big O notation is used to describe e.g. an algorithm\\'s run-time growth as the size of its input increases.\\nTypical steps in the development of algorithms:\\n\\nProblem definition\\nDevelopment of a model\\nSpecification of the algorithm\\nDesigning an algorithm\\nChecking the correctness of the algorithm\\nAnalysis of algorithm\\nImplementation of algorithm\\nProgram testing\\nDocumentation preparation\\n\\n\\n== Implementation ==\\n\\nMost algorithms are intended to be implemented as computer programs. However, algorithms are also implemented by other means, such as in a biological neural network (for example, the human brain implementing arithmetic or an insect looking for food), in an electrical circuit, or in a mechanical device.\\n\\n\\n== Computer algorithms ==\\n\\nIn computer systems, an algorithm is basically an instance of logic written in software by software developers, to be effective for the intended \"target\" computer(s) to produce output from given (perhaps null) input. An optimal algorithm, even running in old hardware, would produce faster results than a non-optimal (higher time complexity) algorithm for the same purpose, running in more efficient hardware; that is why algorithms, like computer hardware, are considered technology.\\n\"Elegant\" (compact) programs, \"good\" (fast) programs : The notion of \"simplicity and elegance\" appears informally in Knuth and precisely in Chaitin:\\n\\nKnuth: \" … we want good algorithms in some loosely defined aesthetic sense. One criterion … is the length of time taken to perform the algorithm …. Other criteria are adaptability of the algorithm to computers, its simplicity and elegance, etc\"Chaitin: \" … a program is \\'elegant,\\' by which I mean that it\\'s the smallest possible program for producing the output that it does\"Chaitin prefaces his definition with: \"I\\'ll show you can\\'t prove that a program is \\'elegant\\'\"—such a proof would solve the Halting problem (ibid).\\nAlgorithm versus function computable by an algorithm: For a given function multiple algorithms may exist. This is true, even without expanding the available instruction set available to the programmer. Rogers observes that \"It is ... important to distinguish between the notion of algorithm, i.e. procedure and the notion of function computable by algorithm, i.e. mapping yielded by procedure. The same function may have several different algorithms\".Unfortunately, there may be a tradeoff between goodness (speed) and elegance (compactness)—an elegant program may take more steps to complete a computation than one less elegant. An example that uses Euclid\\'s algorithm appears below.\\nComputers (and computors), models of computation: A computer (or human \"computor\") is a restricted type of machine, a \"discrete deterministic mechanical device\" that blindly follows its instructions. Melzak\\'s and Lambek\\'s primitive models reduced this notion to four elements: (i) discrete, distinguishable locations, (ii) discrete, indistinguishable counters (iii) an agent, and (iv) a list of instructions that are effective relative to the capability of the agent.Minsky describes a more congenial variation of Lambek\\'s \"abacus\" model in his \"Very Simple Bases for Computability\". Minsky\\'s machine proceeds sequentially through its five (or six, depending on how one counts) instructions unless either a conditional IF-THEN GOTO or an unconditional GOTO changes program flow out of sequence. Besides HALT, Minsky\\'s machine includes three assignment (replacement, substitution) operations: ZERO (e.g. the contents of location replaced by 0: L ← 0), SUCCESSOR (e.g. L ← L+1), and DECREMENT (e.g. L ← L − 1). Rarely must a programmer write \"code\" with such a limited instruction set. But Minsky shows (as do Melzak and Lambek) that his machine is Turing complete with only four general types of instructions: conditional GOTO, unconditional GOTO, assignment/replacement/substitution, and HALT.  However, a few different assignment instructions (e.g. DECREMENT, INCREMENT, and ZERO/CLEAR/EMPTY for a Minsky machine) are also required for Turing-completeness; their exact specification is somewhat up to the designer. The unconditional GOTO is a convenience; it can be constructed by initializing a dedicated location to zero e.g. the instruction \" Z ← 0 \"; thereafter the instruction IF Z=0 THEN GOTO xxx is unconditional.\\nSimulation of an algorithm: computer (computor) language: Knuth advises the reader that \"the best way to learn an algorithm is to try it . . . immediately take pen and paper and work through an example\". But what about a simulation or execution of the real thing? The programmer must translate the algorithm into a language that the simulator/computer/computor can effectively execute. Stone gives an example of this: when computing the roots of a quadratic equation the computor must know how to take a square root. If they don\\'t, then the algorithm, to be effective, must provide a set of rules for extracting a square root.This means that the programmer must know a \"language\" that is effective relative to the target computing agent (computer/computor).\\nBut what model should be used for the simulation? Van Emde Boas observes \"even if we base complexity theory on abstract instead of concrete machines, arbitrariness of the choice of a model remains. It is at this point that the notion of simulation enters\". When speed is being measured, the instruction set matters. For example, the subprogram in Euclid\\'s algorithm to compute the remainder would execute much faster if the programmer had a \"modulus\" instruction available rather than just subtraction (or worse: just Minsky\\'s \"decrement\").\\nStructured programming, canonical structures: Per the Church–Turing thesis, any algorithm can be computed by a model known to be Turing complete, and per Minsky\\'s demonstrations, Turing completeness requires only four instruction types—conditional GOTO, unconditional GOTO, assignment, HALT. Kemeny and Kurtz observe that, while \"undisciplined\" use of unconditional GOTOs and conditional IF-THEN GOTOs can result in \"spaghetti code\", a programmer can write structured programs using only these instructions; on the other hand \"it is also possible, and not too hard, to write badly structured programs in a structured language\". Tausworthe augments the three Böhm-Jacopini canonical structures: SEQUENCE, IF-THEN-ELSE, and WHILE-DO, with two more: DO-WHILE and CASE. An additional benefit of a structured program is that it lends itself to proofs of correctness using mathematical induction.Canonical flowchart symbols: The graphical aide called a flowchart, offers a way to describe and document an algorithm (and a computer program of one). Like the program flow of a Minsky machine, a flowchart always starts at the top of a page and proceeds down. Its primary symbols are only four: the directed arrow showing program flow, the rectangle (SEQUENCE, GOTO), the diamond (IF-THEN-ELSE), and the dot (OR-tie). The Böhm–Jacopini canonical structures are made of these primitive shapes. Sub-structures can \"nest\" in rectangles, but only if a single exit occurs from the superstructure. The symbols, and their use to build the canonical structures are shown in the diagram.\\n\\n\\n== Examples ==\\n\\n\\n=== Algorithm example ===\\nOne of the simplest algorithms is to find the largest number in a list of numbers of random order. Finding the solution requires looking at every number in the list. From this follows a simple algorithm, which can be stated in a high-level description in English prose, as:\\nHigh-level description:\\n\\nIf there are no numbers in the set then there is no highest number.\\nAssume the first number in the set is the largest number in the set.\\nFor each remaining number in the set: if this number is larger than the current largest number, consider this number to be the largest number in the set.\\nWhen there are no numbers left in the set to iterate over, consider the current largest number to be the largest number of the set.(Quasi-)formal description:\\nWritten in prose but much closer to the high-level language of a computer program, the following is the more formal coding of the algorithm in pseudocode or pidgin code:\\n\\n\\n=== Euclid\\'s algorithm ===\\nIn mathematics, the Euclidean algorithm, or Euclid\\'s algorithm, is an efficient method for computing the greatest common divisor (GCD) of two integers (numbers), the largest number that divides them both without a remainder. It is named after the ancient Greek mathematician Euclid, who first described it in his Elements (c. 300 BC). It is one of the oldest algorithms in common use. It can be used to reduce fractions to their simplest form, and is a part of many other number-theoretic and cryptographic calculations. \\n\\nEuclid poses the problem thus: \"Given two numbers not prime to one another, to find their greatest common measure\". He defines \"A number [to be] a multitude composed of units\": a counting number, a positive integer not including zero. To \"measure\" is to place a shorter measuring length s successively (q times) along longer length l until the remaining portion r is less than the shorter length s. In modern words, remainder r = l − q×s, q being the quotient, or remainder r is the \"modulus\", the integer-fractional part left over after the division.For Euclid\\'s method to succeed, the starting lengths must satisfy two requirements: (i) the lengths must not be zero, AND (ii) the subtraction must be \"proper\"; i.e., a test must guarantee that the smaller of the two numbers is subtracted from the larger (or the two can be equal so their subtraction yields zero).\\nEuclid\\'s original proof adds a third requirement: the two lengths must not be prime to one another. Euclid stipulated this so that he could construct a reductio ad absurdum proof that the two numbers\\' common measure is in fact the greatest. While Nicomachus\\' algorithm is the same as Euclid\\'s, when the numbers are prime to one another, it yields the number \"1\" for their common measure. So, to be precise, the following is really Nicomachus\\' algorithm.\\n\\n\\n==== Computer language for Euclid\\'s algorithm ====\\nOnly a few instruction types are required to execute Euclid\\'s algorithm—some logical tests (conditional GOTO), unconditional GOTO, assignment (replacement), and subtraction.\\n\\nA location is symbolized by upper case letter(s), e.g. S, A, etc.\\nThe varying quantity (number) in a location is written in lower case letter(s) and (usually) associated with the location\\'s name. For example, location L at the start might contain the number l = 3009.\\n\\n\\n==== An inelegant program for Euclid\\'s algorithm ====\\n\\nThe following algorithm is framed as Knuth\\'s four-step version of Euclid\\'s and Nicomachus\\', but, rather than using division to find the remainder, it uses successive subtractions of the shorter length s from the remaining length r until r is less than s. The high-level description, shown in boldface, is adapted from Knuth 1973:2–4:\\nINPUT:\\n\\n1 [Into two locations L and S put the numbers l and s that represent the two lengths]:\\n  INPUT L, S\\n2 [Initialize R: make the remaining length r equal to the starting/initial/input length l]:\\n  R ← L\\n\\nE0: [Ensure r ≥ s.]\\n\\n3 [Ensure the smaller of the two numbers is in S and the larger in R]:\\n  IF R > S THEN\\n    the contents of L is the larger number so skip over the exchange-steps 4, 5 and 6:\\n    GOTO step 7\\n  ELSE\\n    swap the contents of R and S.\\n4   L ← R (this first step is redundant, but is useful for later discussion).\\n5   R ← S\\n6   S ← L\\n\\nE1: [Find remainder]: Until the remaining length r in R is less than the shorter length s in S, repeatedly subtract the measuring number s in S from the remaining length r in R.\\n\\n7 IF S > R THEN\\n    done measuring so\\n    GOTO 10\\n  ELSE\\n    measure again,\\n8   R ← R − S\\n9   [Remainder-loop]:\\n    GOTO 7.\\n\\nE2: [Is the remainder zero?]: EITHER (i) the last measure was exact, the remainder in R is zero, and the program can halt, OR (ii) the algorithm must continue: the last measure left a remainder in R less than measuring number in S.\\n\\n10 IF R = 0 THEN\\n     done so\\n     GOTO step 15\\n   ELSE\\n     CONTINUE TO step 11,\\n\\nE3: [Interchange s and r]: The nut of Euclid\\'s algorithm. Use remainder r to measure what was previously smaller number s; L serves as a temporary location.\\n\\n11  L ← R\\n12  R ← S\\n13  S ← L\\n14  [Repeat the measuring process]:\\n    GOTO 7\\n\\nOUTPUT:\\n\\n15 [Done. S contains the greatest common divisor]:\\n   PRINT S\\n\\nDONE:\\n\\n16 HALT, END, STOP.\\n\\n\\n==== An elegant program for Euclid\\'s algorithm ====\\nThe following version of Euclid\\'s algorithm requires only six core instructions to do what thirteen are required to do by \"Inelegant\"; worse, \"Inelegant\" requires more types of instructions. The flowchart of \"Elegant\" can be found at the top of this article. In the (unstructured) Basic language, the steps are numbered, and the instruction LET [] = [] is the assignment instruction symbolized by ←.\\n\\nHow \"Elegant\" works: In place of an outer \"Euclid loop\", \"Elegant\" shifts back and forth between two \"co-loops\", an A > B loop that computes A ← A − B, and a B ≤ A loop that computes B ← B − A. This works because, when at last the minuend M is less than or equal to the subtrahend S (Difference = Minuend − Subtrahend), the minuend can become s (the new measuring length) and the subtrahend can become the new r (the length to be measured); in other words the \"sense\" of the subtraction reverses.\\nThe following version can be used with programming languages from the C-family:\\n\\n\\n=== Testing the Euclid algorithms ===\\nDoes an algorithm do what its author wants it to do? A few test cases usually give some confidence in the core functionality. But tests are not enough. For test cases, one source uses 3009 and 884. Knuth suggested 40902, 24140. Another interesting case is the two relatively prime numbers 14157 and 5950.\\nBut \"exceptional cases\" must be identified and tested. Will \"Inelegant\" perform properly when R > S, S > R, R = S? Ditto for \"Elegant\": B > A, A > B, A = B? (Yes to all). What happens when one number is zero, both numbers are zero? (\"Inelegant\" computes forever in all cases; \"Elegant\" computes forever when A = 0.) What happens if negative numbers are entered? Fractional numbers? If the input numbers, i.e. the domain of the function computed by the algorithm/program, is to include only positive integers including zero, then the failures at zero indicate that the algorithm (and the program that instantiates it) is a partial function rather than a total function. A notable failure due to exceptions is the Ariane 5 Flight 501 rocket failure (June 4, 1996).\\nProof of program correctness by use of mathematical induction: Knuth demonstrates the application of mathematical induction to an \"extended\" version of Euclid\\'s algorithm, and he proposes \"a general method applicable to proving the validity of any algorithm\". Tausworthe proposes that a measure of the complexity of a program be the length of its correctness proof.\\n\\n\\n=== Measuring and improving the Euclid algorithms ===\\nElegance (compactness) versus goodness (speed): With only six core instructions, \"Elegant\" is the clear winner, compared to \"Inelegant\" at thirteen instructions. However, \"Inelegant\" is faster (it arrives at HALT in fewer steps). Algorithm analysis indicates why this is the case: \"Elegant\" does two conditional tests in every subtraction loop, whereas \"Inelegant\" only does one. As the algorithm (usually) requires many loop-throughs, on average much time is wasted doing a \"B = 0?\" test that is needed only after the remainder is computed.\\nCan the algorithms be improved?: Once the programmer judges a program \"fit\" and \"effective\"—that is, it computes the function intended by its author—then the question becomes, can it be improved?\\nThe compactness of \"Inelegant\" can be improved by the elimination of five steps. But Chaitin proved that compacting an algorithm cannot be automated by a generalized algorithm; rather, it can only be done heuristically; i.e., by exhaustive search (examples to be found at Busy beaver), trial and error, cleverness, insight, application of inductive reasoning, etc. Observe that steps 4, 5 and 6 are repeated in steps 11, 12 and 13. Comparison with \"Elegant\" provides a hint that these steps, together with steps 2 and 3, can be eliminated. This reduces the number of core instructions from thirteen to eight, which makes it \"more elegant\" than \"Elegant\", at nine steps.\\nThe speed of \"Elegant\" can be improved by moving the \"B=0?\" test outside of the two subtraction loops. This change calls for the addition of three instructions (B = 0?, A = 0?, GOTO). Now \"Elegant\" computes the example-numbers faster; whether this is always the case for any given A, B, and R, S would require a detailed analysis.\\n\\n\\n== Algorithmic analysis ==\\n\\nIt is frequently important to know how much of a particular resource (such as time or storage) is theoretically required for a given algorithm. Methods have been developed for the analysis of algorithms to obtain such quantitative answers (estimates); for example, the sorting algorithm above has a time requirement of O(n), using the big O notation with n as the length of the list. At all times the algorithm only needs to remember two values: the largest number found so far, and its current position in the input list. Therefore, it is said to have a space requirement of O(1), if the space required to store the input numbers is not counted, or O(n) if it is counted.\\nDifferent algorithms may complete the same task with a different set of instructions in less or more time, space, or \\'effort\\' than others. For example, a binary search algorithm (with cost O(log n) ) outperforms a sequential search (cost O(n) ) when used for table lookups on sorted lists or arrays.\\n\\n\\n=== Formal versus empirical ===\\n\\nThe analysis, and study of algorithms is a discipline of computer science, and is often practiced abstractly without the use of a specific programming language or implementation. In this sense, algorithm analysis resembles other mathematical disciplines in that it focuses on the underlying properties of the algorithm and not on the specifics of any particular implementation. Usually pseudocode is used for analysis as it is the simplest and most general representation. However, ultimately, most algorithms are usually implemented on particular hardware/software platforms and their algorithmic efficiency is eventually put to the test using real code. For the solution of a \"one off\" problem, the efficiency of a particular algorithm may not have significant consequences (unless n is extremely large) but for algorithms designed for fast interactive, commercial or long life scientific usage it may be critical. Scaling from small n to large n frequently exposes inefficient algorithms that are otherwise benign.\\nEmpirical testing is useful because it may uncover unexpected interactions that affect performance. Benchmarks may be used to compare before/after potential improvements to an algorithm after program optimization.\\nEmpirical tests cannot replace formal analysis, though, and are not trivial to perform in a fair manner.\\n\\n\\n=== Execution efficiency ===\\n\\nTo illustrate the potential improvements possible even in well-established algorithms, a recent significant innovation, relating to FFT algorithms (used heavily in the field of image processing), can decrease processing time up to 1,000 times for applications like medical imaging. In general, speed improvements depend on special properties of the problem, which are very common in practical applications. Speedups of this magnitude enable computing devices that make extensive use of image processing (like digital cameras and medical equipment) to consume less power.\\n\\n\\n== Classification ==\\nThere are various ways to classify algorithms, each with its own merits.\\n\\n\\n=== By implementation ===\\nOne way to classify algorithms is by implementation means.\\n\\nRecursion\\nA recursive algorithm is one that invokes (makes reference to) itself repeatedly until a certain condition (also known as termination condition) matches, which is a method common to functional programming. Iterative algorithms use repetitive constructs like loops and sometimes additional data structures like stacks to solve the given problems. Some problems are naturally suited for one implementation or the other. For example, towers of Hanoi is well understood using recursive implementation. Every recursive version has an equivalent (but possibly more or less complex) iterative version, and vice versa.\\nLogical\\nAn algorithm may be viewed as controlled logical deduction. This notion may be expressed as: Algorithm = logic + control. The logic component expresses the axioms that may be used in the computation and the control component determines the way in which deduction is applied to the axioms. This is the basis for the logic programming paradigm. In pure logic programming languages, the control component is fixed and algorithms are specified by supplying only the logic component. The appeal of this approach is the elegant semantics: a change in the axioms produces a well-defined change in the algorithm.\\nSerial, parallel or distributed\\nAlgorithms are usually discussed with the assumption that computers execute one instruction of an algorithm at a time. Those computers are sometimes called serial computers. An algorithm designed for such an environment is called a serial algorithm, as opposed to parallel algorithms or distributed algorithms. Parallel algorithms take advantage of computer architectures where several processors can work on a problem at the same time, whereas distributed algorithms utilize multiple machines connected with a computer network. Parallel or distributed algorithms divide the problem into more symmetrical or asymmetrical subproblems and collect the results back together. The resource consumption in such algorithms is not only processor cycles on each processor but also the communication overhead between the processors. Some sorting algorithms can be parallelized efficiently, but their communication overhead is expensive. Iterative algorithms are generally parallelizable. Some problems have no parallel algorithms and are called inherently serial problems.\\nDeterministic or non-deterministic\\nDeterministic algorithms solve the problem with exact decision at every step of the algorithm whereas non-deterministic algorithms solve problems via guessing although typical guesses are made more accurate through the use of heuristics.\\nExact or approximate\\nWhile many algorithms reach an exact solution, approximation algorithms seek an approximation that is closer to the true solution. The approximation can be reached by either using a deterministic or a random strategy. Such algorithms have practical value for many hard problems. One of the examples of an approximate algorithm is the Knapsack problem, where there is a set of given items. Its goal is to pack the knapsack to get the maximum total value. Each item has some weight and some value. Total weight that can be carried is no more than some fixed number X. So, the solution must consider weights of items as well as their value.\\nQuantum algorithm\\nThey run on a realistic model of quantum computation. The term is usually used for those algorithms which seem inherently quantum, or use some essential feature of Quantum computing such as quantum superposition or quantum entanglement.\\n\\n\\n=== By design paradigm ===\\nAnother way of classifying algorithms is by their design methodology or paradigm. There is a certain number of paradigms, each different from the other. Furthermore, each of these categories includes many different types of algorithms. Some common paradigms are:\\n\\nBrute-force or exhaustive search\\nThis is the naive method of trying every possible solution to see which is best.\\nDivide and conquer\\nA divide and conquer algorithm repeatedly reduces an instance of a problem to one or more smaller instances of the same problem (usually recursively) until the instances are small enough to solve easily. One such example of divide and conquer is merge sorting. Sorting can be done on each segment of data after dividing data into segments and sorting of entire data can be obtained in the conquer phase by merging the segments. A simpler variant of divide and conquer is called a decrease and conquer algorithm, which solves an identical subproblem and uses the solution of this subproblem to solve the bigger problem. Divide and conquer divides the problem into multiple subproblems and so the conquer stage is more complex than decrease and conquer algorithms. An example of a decrease and conquer algorithm is the binary search algorithm.\\nSearch and enumeration\\nMany problems (such as playing chess) can be modeled as problems on graphs. A graph exploration algorithm specifies rules for moving around a graph and is useful for such problems. This category also includes search algorithms, branch and bound enumeration and backtracking.\\nRandomized algorithm\\nSuch algorithms make some choices randomly (or pseudo-randomly). They can be very useful in finding approximate solutions for problems where finding exact solutions can be impractical (see heuristic method below). For some of these problems, it is known that the fastest approximations must involve some randomness. Whether randomized algorithms with polynomial time complexity can be the fastest algorithms for some problems is an open question known as the P versus NP problem. There are two large classes of such algorithms:Monte Carlo algorithms return a correct answer with high-probability. E.g. RP is the subclass of these that run in polynomial time.\\nLas Vegas algorithms always return the correct answer, but their running time is only probabilistically bound, e.g. ZPP.Reduction of complexity\\nThis technique involves solving a difficult problem by transforming it into a better-known problem for which we have (hopefully) asymptotically optimal algorithms. The goal is to find a reducing algorithm whose complexity is not dominated by the resulting reduced algorithm\\'s. For example, one selection algorithm for finding the median in an unsorted list involves first sorting the list (the expensive portion) and then pulling out the middle element in the sorted list (the cheap portion). This technique is also known as transform and conquer.\\nBack tracking\\nIn this approach, multiple solutions are built incrementally and abandoned when it is determined that they cannot lead to a valid full solution.\\n\\n\\n=== Optimization problems ===\\nFor optimization problems there is a more specific classification of algorithms; an algorithm for such problems may fall into one or more of the general categories described above as well as into one of the following:\\n\\nLinear programming\\nWhen searching for optimal solutions to a linear function bound to linear equality and inequality constraints, the constraints of the problem can be used directly in producing the optimal solutions. There are algorithms that can solve any problem in this category, such as the popular simplex algorithm. Problems that can be solved with linear programming include the maximum flow problem for directed graphs. If a problem additionally requires that one or more of the unknowns must be an integer then it is classified in integer programming. A linear programming algorithm can solve such a problem if it can be proved that all restrictions for integer values are superficial, i.e., the solutions satisfy these restrictions anyway. In the general case, a specialized algorithm or an algorithm that finds approximate solutions is used, depending on the difficulty of the problem.\\nDynamic programming\\nWhen a problem shows optimal substructures—meaning the optimal solution to a problem can be constructed from optimal solutions to subproblems—and overlapping subproblems, meaning the same subproblems are used to solve many different problem instances, a quicker approach called dynamic programming avoids recomputing solutions that have already been computed. For example, Floyd–Warshall algorithm, the shortest path to a goal from a vertex in a weighted graph can be found by using the shortest path to the goal from all adjacent vertices. Dynamic programming and memoization go together. The main difference between dynamic programming and divide and conquer is that subproblems are more or less independent in divide and conquer, whereas subproblems overlap in dynamic programming. The difference between dynamic programming and straightforward recursion is in caching or memoization of recursive calls. When subproblems are independent and there is no repetition, memoization does not help; hence dynamic programming is not a solution for all complex problems. By using memoization or maintaining a table of subproblems already solved, dynamic programming reduces the exponential nature of many problems to polynomial complexity.\\nThe greedy method\\nA greedy algorithm is similar to a dynamic programming algorithm in that it works by examining substructures, in this case not of the problem but of a given solution. Such algorithms start with some solution, which may be given or have been constructed in some way, and improve it by making small modifications. For some problems they can find the optimal solution while for others they stop at local optima, that is, at solutions that cannot be improved by the algorithm but are not optimum. The most popular use of greedy algorithms is for finding the minimal spanning tree where finding the optimal solution is possible with this method. Huffman Tree, Kruskal, Prim, Sollin are greedy algorithms that can solve this optimization problem.\\nThe heuristic method\\nIn optimization problems, heuristic algorithms can be used to find a solution close to the optimal solution in cases where finding the optimal solution is impractical. These algorithms work by getting closer and closer to the optimal solution as they progress. In principle, if run for an infinite amount of time, they will find the optimal solution. Their merit is that they can find a solution very close to the optimal solution in a relatively short time. Such algorithms include local search, tabu search, simulated annealing, and genetic algorithms. Some of them, like simulated annealing, are non-deterministic algorithms while others, like tabu search, are deterministic. When a bound on the error of the non-optimal solution is known, the algorithm is further categorized as an approximation algorithm.\\n\\n\\n=== By field of study ===\\n\\nEvery field of science has its own problems and needs efficient algorithms. Related problems in one field are often studied together. Some example classes are search algorithms, sorting algorithms, merge algorithms, numerical algorithms, graph algorithms, string algorithms, computational geometric algorithms, combinatorial algorithms, medical algorithms, machine learning, cryptography, data compression algorithms and parsing techniques.\\nFields tend to overlap with each other, and algorithm advances in one field may improve those of other, sometimes completely unrelated, fields. For example, dynamic programming was invented for optimization of resource consumption in industry but is now used in solving a broad range of problems in many fields.\\n\\n\\n=== By complexity ===\\n\\nAlgorithms can be classified by the amount of time they need to complete compared to their input size:\\n\\nConstant time: if the time needed by the algorithm is the same, regardless of the input size. E.g. an access to an array element.\\nLogarithmic time: if the time is a logarithmic function of the input size. E.g. binary search algorithm.\\nLinear time: if the time is proportional to the input size. E.g. the traverse of a list.\\nPolynomial time: if the time is a power of the input size. E.g. the bubble sort algorithm has quadratic time complexity.\\nExponential time: if the time is an exponential function of the input size. E.g. Brute-force search.Some problems may have multiple algorithms of differing complexity, while other problems might have no algorithms or no known efficient algorithms. There are also mappings from some problems to other problems. Owing to this, it was found to be more suitable to classify the problems themselves instead of the algorithms into equivalence classes based on the complexity of the best possible algorithms for them.\\n\\n\\n== Continuous algorithms ==\\nThe adjective \"continuous\" when applied to the word \"algorithm\" can mean:\\n\\nAn algorithm operating on data that represents continuous quantities, even though this data is represented by discrete approximations—such algorithms are studied in numerical analysis; or\\nAn algorithm in the form of a differential equation that operates continuously on the data, running on an analog computer.\\n\\n\\n== Legal issues ==\\n\\nAlgorithms, by themselves, are not usually patentable. In the United States, a claim consisting solely of simple manipulations of abstract concepts, numbers, or signals does not constitute \"processes\" (USPTO 2006), and hence algorithms are not patentable (as in Gottschalk v. Benson). However practical applications of algorithms are sometimes patentable. For example, in Diamond v. Diehr, the application of a simple feedback algorithm to aid in the curing of synthetic rubber was deemed patentable. The patenting of software is highly controversial, and there are highly criticized patents involving algorithms, especially data compression algorithms, such as Unisys\\' LZW patent.\\nAdditionally, some cryptographic algorithms have export restrictions (see export of cryptography).\\n\\n\\n== History: Development of the notion of \"algorithm\" ==\\n\\n\\n=== Ancient Near East ===\\nThe earliest  evidence of algorithms is found in the Babylonian mathematics of ancient Mesopotamia (modern Iraq). A Sumerian clay tablet found in Shuruppak near Baghdad and dated to circa 2500 BC described the earliest division algorithm. During the Hammurabi dynasty circa 1800-1600 BC, Babylonian clay tablets described algorithms for computing formulas. Algorithms were also used in Babylonian astronomy. Babylonian clay tablets describe and employ algorithmic procedures to compute the time and place of significant astronomical events.Algorithms for arithmetic are also found in ancient Egyptian mathematics, dating back to the Rhind Mathematical Papyrus circa 1550 BC. Algorithms were later used in ancient Hellenistic mathematics. Two examples are the Sieve of Eratosthenes, which was described in the Introduction to Arithmetic by Nicomachus,:\\u200aCh 9.2\\u200a and the Euclidean algorithm, which was first described in Euclid\\'s Elements (c. 300 BC).:\\u200aCh 9.1\\u200a\\n\\n\\n=== Discrete and distinguishable symbols ===\\nTally-marks: To keep track of their flocks, their sacks of grain and their money the ancients used tallying: accumulating stones or marks scratched on sticks or making discrete symbols in clay. Through the Babylonian and Egyptian use of marks and symbols, eventually Roman numerals and the abacus evolved (Dilson, p. 16–41). Tally marks appear prominently in unary numeral system arithmetic used in Turing machine and Post–Turing machine computations.\\n\\n\\n=== Manipulation of symbols as \"place holders\" for numbers: algebra ===\\nMuhammad ibn Mūsā al-Khwārizmī, a Persian mathematician, wrote the Al-jabr in the 9th century. The terms \"algorism\" and \"algorithm\" are derived from the name al-Khwārizmī, while the term \"algebra\" is derived from the book Al-jabr. In Europe, the word \"algorithm\" was originally used to refer to the sets of rules and techniques used by Al-Khwarizmi to solve algebraic equations, before later being generalized to refer to any set of rules or techniques. This eventually culminated in Leibniz\\'s notion of the calculus ratiocinator (ca 1680):\\n\\nA good century and a half ahead of his time, Leibniz proposed an algebra of logic, an algebra that would specify the rules for manipulating logical concepts in the manner that ordinary algebra specifies the rules for manipulating numbers.\\n\\n\\n=== Cryptographic algorithms ===\\nThe first cryptographic algorithm for deciphering encrypted code was developed by Al-Kindi, a 9th-century Arab mathematician, in A Manuscript On Deciphering Cryptographic Messages. He gave the first description of cryptanalysis by frequency analysis, the earliest codebreaking algorithm.\\n\\n\\n=== Mechanical contrivances with discrete states ===\\nThe clock: Bolter credits the invention of the weight-driven clock as \"The key invention [of Europe in the Middle Ages]\", in particular, the verge escapement that provides us with the tick and tock of a mechanical clock. \"The accurate automatic machine\" led immediately to \"mechanical automata\" beginning in the 13th century and finally to \"computational machines\"—the difference engine and analytical engines of Charles Babbage and Countess Ada Lovelace, mid-19th century. Lovelace is credited with the first creation of an algorithm intended for processing on a computer—Babbage\\'s analytical engine, the first device considered a real Turing-complete computer instead of just a calculator—and is sometimes called \"history\\'s first programmer\" as a result, though a full implementation of Babbage\\'s second device would not be realized until decades after her lifetime.\\nLogical machines 1870 – Stanley Jevons\\' \"logical abacus\" and \"logical machine\": The technical problem was to reduce Boolean equations when presented in a form similar to what is now known as Karnaugh maps. Jevons (1880) describes first a simple \"abacus\" of \"slips of wood furnished with pins, contrived so that any part or class of the [logical] combinations can be picked out mechanically ... More recently, however, I have reduced the system to a completely mechanical form, and have thus embodied the whole of the indirect process of inference in what may be called a Logical Machine\" His machine came equipped with \"certain moveable wooden rods\" and \"at the foot are 21 keys like those of a piano [etc] ...\". With this machine he could analyze a \"syllogism or any other simple logical argument\".This machine he displayed in 1870 before the Fellows of the Royal Society. Another logician John Venn, however, in his 1881 Symbolic Logic, turned a jaundiced eye to this effort: \"I have no high estimate myself of the interest or importance of what are sometimes called logical machines ... it does not seem to me that any contrivances at present known or likely to be discovered really deserve the name of logical machines\"; see more at Algorithm characterizations. But not to be outdone he too presented \"a plan somewhat analogous, I apprehend, to Prof. Jevon\\'s abacus ... [And] [a]gain, corresponding to Prof. Jevons\\'s logical machine, the following contrivance may be described. I prefer to call it merely a logical-diagram machine ... but I suppose that it could do very completely all that can be rationally expected of any logical machine\".Jacquard loom, Hollerith punch cards, telegraphy and telephony – the electromechanical relay: Bell and Newell (1971) indicate that the Jacquard loom (1801), precursor to Hollerith cards (punch cards, 1887), and \"telephone switching technologies\" were the roots of a tree leading to the development of the first computers. By the mid-19th century the telegraph, the precursor of the telephone, was in use throughout the world, its discrete and distinguishable encoding of letters as \"dots and dashes\" a common sound. By the late 19th century the ticker tape (ca 1870s) was in use, as was the use of Hollerith cards in the 1890 U.S. census. Then came the teleprinter (ca. 1910) with its punched-paper use of Baudot code on tape.\\nTelephone-switching networks of electromechanical relays (invented 1835) was behind the work of George Stibitz (1937), the inventor of the digital adding device. As he worked in Bell Laboratories, he observed the \"burdensome\\' use of mechanical calculators with gears. \"He went home one evening in 1937 intending to test his idea... When the tinkering was over, Stibitz had constructed a binary adding device\".Davis (2000) observes the particular importance of the electromechanical relay (with its two \"binary states\" open and closed):\\n\\nIt was only with the development, beginning in the 1930s, of electromechanical calculators using electrical relays, that machines were built having the scope Babbage had envisioned.\"\\n\\n\\n=== Mathematics during the 19th century up to the mid-20th century ===\\nSymbols and rules: In rapid succession, the mathematics of George Boole (1847, 1854), Gottlob Frege (1879), and Giuseppe Peano (1888–1889) reduced arithmetic to a sequence of symbols manipulated by rules. Peano\\'s The principles of arithmetic, presented by a new method (1888) was \"the first attempt at an axiomatization of mathematics in a symbolic language\".But Heijenoort gives Frege (1879) this kudos: Frege\\'s is \"perhaps the most important single work ever written in logic. ... in which we see a \" \\'formula language\\', that is a lingua characterica, a language written with special symbols, \"for pure thought\", that is, free from rhetorical embellishments ... constructed from specific symbols that are manipulated according to definite rules\". The work of Frege was further simplified and amplified by Alfred North Whitehead and Bertrand Russell in their Principia Mathematica (1910–1913).\\nThe paradoxes: At the same time a number of disturbing paradoxes appeared in the literature, in particular, the Burali-Forti paradox (1897), the Russell paradox (1902–03), and the Richard Paradox. The resultant considerations led to Kurt Gödel\\'s paper (1931)—he specifically cites the paradox of the liar—that completely reduces rules of recursion to numbers.\\nEffective calculability: In an effort to solve the Entscheidungsproblem defined precisely by Hilbert in 1928, mathematicians first set about to define what was meant by an \"effective method\" or \"effective calculation\" or \"effective calculability\" (i.e., a calculation that would succeed). In rapid succession the following appeared: Alonzo Church, Stephen Kleene and J.B. Rosser\\'s λ-calculus a finely honed definition of \"general recursion\" from the work of Gödel acting on suggestions of Jacques Herbrand (cf. Gödel\\'s Princeton lectures of 1934) and subsequent simplifications by Kleene. Church\\'s proof that the Entscheidungsproblem was unsolvable, Emil Post\\'s definition of effective calculability as a worker mindlessly following a list of instructions to move left or right through a sequence of rooms and while there either mark or erase a paper or observe the paper and make a yes-no decision about the next instruction. Alan Turing\\'s proof of that the Entscheidungsproblem was unsolvable by use of his \"a- [automatic-] machine\"—in effect almost identical to Post\\'s \"formulation\", J. Barkley Rosser\\'s definition of \"effective method\" in terms of \"a machine\". Kleene\\'s proposal of a precursor to \"Church thesis\" that he called \"Thesis I\", and a few years later Kleene\\'s renaming his Thesis \"Church\\'s Thesis\" and proposing \"Turing\\'s Thesis\".\\n\\n\\n=== Emil Post (1936) and Alan Turing (1936–37, 1939) ===\\nEmil Post (1936) described the actions of a \"computer\" (human being) as follows:\\n\\n\"...two concepts are involved: that of a symbol space in which the work leading from problem to answer is to be carried out, and a fixed unalterable set of directions.His symbol space would be\\n\\n\"a two-way infinite sequence of spaces or boxes... The problem solver or worker is to move and work in this symbol space, being capable of being in, and operating in but one box at a time.... a box is to admit of but two possible conditions, i.e., being empty or unmarked, and having a single mark in it, say a vertical stroke.\"One box is to be singled out and called the starting point. ...a specific problem is to be given in symbolic form by a finite number of boxes [i.e., INPUT] being marked with a stroke. Likewise, the answer [i.e., OUTPUT] is to be given in symbolic form by such a configuration of marked boxes...\"A set of directions applicable to a general problem sets up a deterministic process when applied to each specific problem. This process terminates only when it comes to the direction of type (C ) [i.e., STOP]\". See more at Post–Turing machine\\nAlan Turing\\'s work preceded that of Stibitz (1937); it is unknown whether Stibitz knew of the work of Turing. Turing\\'s biographer believed that Turing\\'s use of a typewriter-like model derived from a youthful interest: \"Alan had dreamt of inventing typewriters as a boy; Mrs. Turing had a typewriter, and he could well have begun by asking himself what was meant by calling a typewriter \\'mechanical\\'\". Given the prevalence of Morse code and telegraphy, ticker tape machines, and teletypewriters we might conjecture that all were influences.\\nTuring—his model of computation is now called a Turing machine—begins, as did Post, with an analysis of a human computer that he whittles down to a simple set of basic motions and \"states of mind\". But he continues a step further and creates a machine as a model of computation of numbers.\\n\"Computing is normally done by writing certain symbols on paper. We may suppose this paper is divided into squares like a child\\'s arithmetic book...I assume then that the computation is carried out on one-dimensional paper, i.e., on a tape divided into squares. I shall also suppose that the number of symbols which may be printed is finite...\"The behavior of the computer at any moment is determined by the symbols which he is observing, and his \"state of mind\" at that moment. We may suppose that there is a bound B to the number of symbols or squares which the computer can observe at one moment. If he wishes to observe more, he must use successive observations. We will also suppose that the number of states of mind which need be taken into account is finite...\"Let us imagine that the operations performed by the computer to be split up into \\'simple operations\\' which are so elementary that it is not easy to imagine them further divided.\"Turing\\'s reduction yields the following:\\n\\n\"The simple operations must therefore include:\\n\"(a) Changes of the symbol on one of the observed squares\\n\"(b) Changes of one of the squares observed to another square within L squares of one of the previously observed squares.\"It may be that some of these change necessarily invoke a change of state of mind. The most general single operation must, therefore, be taken to be one of the following:\\n\\n\"(A) A possible change (a) of symbol together with a possible change of state of mind.\\n\"(B) A possible change (b) of observed squares, together with a possible change of state of mind\"\"We may now construct a machine to do the work of this computer.\"A few years later, Turing expanded his analysis (thesis, definition) with this forceful expression of it:\\n\\n\"A function is said to be \"effectively calculable\" if its values can be found by some purely mechanical process. Though it is fairly easy to get an intuitive grasp of this idea, it is nevertheless desirable to have some more definite, mathematical expressible definition ... [he discusses the history of the definition pretty much as presented above with respect to Gödel, Herbrand, Kleene, Church, Turing, and Post] ... We may take this statement literally, understanding by a purely mechanical process one which could be carried out by a machine. It is possible to give a mathematical description, in a certain normal form, of the structures of these machines. The development of these ideas leads to the author\\'s definition of a computable function, and to an identification of computability † with effective calculability ... .\\n\"† We shall use the expression \"computable function\" to mean a function calculable by a machine, and we let \"effectively calculable\" refer to the intuitive idea without particular identification with any one of these definitions\".\\n\\n\\n=== J.B. Rosser (1939) and S.C. Kleene (1943) ===\\nJ. Barkley Rosser defined an \\'effective [mathematical] method\\' in the following manner (italicization added):\\n\\n\"\\'Effective method\\' is used here in the rather special sense of a method each step of which is precisely determined and which is certain to produce the answer in a finite number of steps. With this special meaning, three different precise definitions have been given to date. [his footnote #5; see discussion immediately below]. The simplest of these to state (due to Post and Turing) says essentially that an effective method of solving certain sets of problems exists if one can build a machine which will then solve any problem of the set with no human intervention beyond inserting the question and (later) reading the answer. All three definitions are equivalent, so it doesn\\'t matter which one is used. Moreover, the fact that all three are equivalent is a very strong argument for the correctness of any one.\" (Rosser 1939:225–226)Rosser\\'s footnote No. 5 references the work of (1) Church and Kleene and their definition of λ-definability, in particular Church\\'s use of it in his An Unsolvable Problem of Elementary Number Theory (1936); (2) Herbrand and Gödel and their use of recursion in particular Gödel\\'s use in his famous paper On Formally Undecidable Propositions of Principia Mathematica and Related Systems I (1931); and (3) Post (1936) and Turing (1936–37) in their mechanism-models of computation.\\nStephen C. Kleene defined as his now-famous \"Thesis I\" known as the Church–Turing thesis. But he did this in the following context (boldface in original):\\n\\n\"12. Algorithmic theories... In setting up a complete algorithmic theory, what we do is to describe a procedure, performable for each set of values of the independent variables, which procedure necessarily terminates and in such manner that from the outcome we can read a definite answer, \"yes\" or \"no,\" to the question, \"is the predicate value true?\"\" (Kleene 1943:273)\\n\\n\\n=== History after 1950 ===\\nA number of efforts have been directed toward further refinement of the definition of \"algorithm\", and activity is on-going because of issues surrounding, in particular, foundations of mathematics (especially the Church–Turing thesis) and philosophy of mind (especially arguments about artificial intelligence). For more, see Algorithm characterizations.\\n\\n\\n== See also ==\\n\\n\\n== Notes ==\\n\\n\\n== Bibliography ==\\n\\n\\n== Further reading ==\\n\\n\\n== External links ==\\n\"Algorithm\", Encyclopedia of Mathematics, EMS Press, 2001 [1994]\\nAlgorithms at Curlie\\nWeisstein, Eric W. \"Algorithm\". MathWorld.\\nDictionary of Algorithms and Data Structures – National Institute of Standards and TechnologyAlgorithm repositoriesThe Stony Brook Algorithm Repository – State University of New York at Stony Brook\\nCollected Algorithms of the ACM – Association for Computing Machinery\\nThe Stanford GraphBase – Stanford University', 'In computer science, a data structure is a data organization, management, and storage format that enables efficient access and modification. More precisely, a data structure is a collection of data values, the relationships among them, and the functions or operations that can be applied to the data, i.e., it is an algebraic structure about data.\\n\\n\\n== Usage ==\\nData structures serve as the basis for abstract data types (ADT). The ADT defines the logical form of the data type. The data structure implements the physical form of the data type.Different types of data structures are suited to different kinds of applications, and some are highly specialized to specific tasks. For example, relational databases commonly use B-tree indexes for data retrieval, while compiler implementations usually use hash tables to look up identifiers.Data structures provide a means to manage large amounts of data efficiently for uses such as large databases and internet indexing services. Usually, efficient data structures are key to designing efficient algorithms. Some formal design methods and programming languages emphasize data structures, rather than algorithms, as the key organizing factor in software design. Data structures can be used to organize the storage and retrieval of information stored in both main memory and secondary memory.\\n\\n\\n== Implementation ==\\nData structures are generally based on the ability of a computer to fetch and store data at any place in its memory, specified by a pointer—a bit string, representing a memory address, that can be itself stored in memory and manipulated by the program. Thus, the array and record data structures are based on computing the addresses of data items with arithmetic operations, while the linked data structures are based on storing addresses of data items within the structure itself. \\nThe implementation of a data structure usually requires writing a set of procedures that create and manipulate instances of that structure. The efficiency of a data structure cannot be analyzed separately from those operations. This observation motivates the theoretical concept of an abstract data type, a data structure that is defined indirectly by the operations that may be performed on it, and the mathematical properties of those operations (including their space and time cost).\\n\\n\\n== Examples ==\\n\\nThere are numerous types of data structures, generally built upon simpler primitive data types:\\nA byte is the smallest amount of data that a Computer CPU can copy from memory to a register or back in a single instruction and a byte stream is the most efficient way to copy big data through a computer. Ref.\\nAn array is a number of elements in a specific order, typically all of the same type (depending on the language, individual elements may either all be forced to be the same type, or may be of almost any type). Elements are accessed using an integer index to specify which element is required. Typical implementations allocate contiguous memory words for the elements of arrays (but this is not always a necessity). Arrays may be fixed-length or resizable.\\nA linked list (also just called list) is a linear collection of data elements of any type, called nodes, where each node has itself a value, and points to the next node in the linked list. The principal advantage of a linked list over an array is that values can always be efficiently inserted and removed without relocating the rest of the list. Certain other operations, such as random access to a certain element, are however slower on lists than on arrays.\\nA record (also called tuple or struct) is an aggregate data structure. A record is a value that contains other values, typically in fixed number and sequence and typically indexed by names. The elements of records are usually called fields or members.\\nA union is a data structure that specifies which of a number of permitted primitive types may be stored in its instances, e.g. float or long integer. Contrast with a record, which could be defined to contain a float and an integer; whereas in a union, there is only one value at a time. Enough space is allocated to contain the widest member data-type.\\nA tagged union (also called variant, variant record, discriminated union, or disjoint union) contains an additional field indicating its current type, for enhanced type safety.\\nAn object is a data structure that contains data fields, like a record does, as well as various methods which operate on the data contents. An object is an in-memory instance of a class from a taxonomy. In the context of object-oriented programming, records are known as plain old data structures to distinguish them from objects.In addition, hashes, graphs and binary trees are other commonly used data structures.\\n\\n\\n== Language support ==\\nMost assembly languages and some low-level languages, such as BCPL (Basic Combined Programming Language), lack built-in support for data structures. On the other hand, many high-level programming languages and some higher-level assembly languages, such as MASM, have special syntax or other built-in support for certain data structures, such as records and arrays. For example, the C (a direct descendant of BCPL) and Pascal languages support structs and records, respectively, in addition to vectors (one-dimensional arrays) and multi-dimensional arrays.Most programming languages feature some sort of library mechanism that allows data structure implementations to be reused by different programs. Modern languages usually come with standard libraries that implement the most common data structures. Examples are the C++ Standard Template Library, the Java Collections Framework, and the Microsoft .NET Framework.\\nModern languages also generally support modular programming, the separation between the interface of a library module and its implementation. Some provide opaque data types that allow clients to hide implementation details. Object-oriented programming languages, such as C++, Java, and Smalltalk, typically use classes for this purpose.\\nMany known data structures have concurrent versions which allow multiple computing threads to access a single concrete instance of a data structure simultaneously.\\n\\n\\n== See also ==\\n\\n\\n== References ==\\n\\n\\n== Bibliography ==\\nPeter Brass, Advanced Data Structures, Cambridge University Press, 2008, ISBN 978-0521880374\\nDonald Knuth, The Art of Computer Programming, vol. 1. Addison-Wesley, 3rd edition, 1997, ISBN 978-0201896831\\nDinesh Mehta and Sartaj Sahni, Handbook of Data Structures and Applications, Chapman and Hall/CRC Press, 2004, ISBN 1584884355\\nNiklaus Wirth, Algorithms and Data Structures, Prentice Hall, 1985, ISBN 978-0130220059\\n\\n\\n== Further reading ==\\nAlfred Aho, John Hopcroft, and Jeffrey Ullman, Data Structures and Algorithms, Addison-Wesley, 1983, ISBN 0-201-00023-7\\nG. H. Gonnet and R. Baeza-Yates, Handbook of Algorithms and Data Structures - in Pascal and C, second edition, Addison-Wesley, 1991, ISBN 0-201-41607-7\\nEllis Horowitz and Sartaj Sahni, Fundamentals of Data Structures in Pascal, Computer Science Press, 1984, ISBN 0-914894-94-3\\n\\n\\n== External links ==\\n\\nDescriptions from the Dictionary of Algorithms and Data Structures\\nData structures course\\nAn Examination of Data Structures from .NET perspective\\nSchaffer, C. Data Structures and Algorithm Analysis', 'In computer science, in particular in knowledge representation and reasoning and metalogic, the area of automated reasoning is dedicated to understanding different aspects of reasoning. The study of automated reasoning helps produce computer programs that allow computers to reason completely, or nearly completely, automatically. Although automated reasoning is considered a sub-field of artificial intelligence, it also has connections with theoretical computer science and philosophy.\\nThe most developed subareas of automated reasoning are automated theorem proving (and the less automated but more pragmatic subfield of interactive theorem proving) and automated proof checking (viewed as guaranteed correct reasoning under fixed assumptions). Extensive work has also been done in reasoning by analogy using induction and abduction.Other important topics include reasoning under uncertainty and non-monotonic reasoning. An important part of the uncertainty field is that of argumentation, where further constraints of minimality and consistency are applied on top of the more standard automated deduction. John Pollock\\'s OSCAR system is an example of an automated argumentation system that is more specific than being just an automated theorem prover.\\nTools and techniques of automated reasoning include the classical logics and calculi, fuzzy logic, Bayesian inference, reasoning with maximal entropy and many less formal ad hoc techniques.\\n\\n\\n== Early years ==\\nThe development of formal logic played a big role in the field of automated reasoning, which itself led to the development of artificial intelligence. A formal proof is a proof in which every logical inference has been checked back to the fundamental axioms of mathematics. All the intermediate logical steps are supplied, without exception. No appeal is made to intuition, even if the translation from intuition to logic is routine. Thus, a formal proof is less intuitive and less susceptible to logical errors.Some consider the Cornell Summer meeting of 1957, which brought together many logicians and computer scientists, as the origin of automated reasoning, or automated deduction. Others say that it began before that with the 1955 Logic Theorist program of Newell, Shaw and Simon, or with Martin Davis’ 1954 implementation of Presburger\\'s decision procedure (which proved that the sum of two even numbers is even).Automated reasoning, although a significant and popular area of research, went through an \"AI winter\" in the eighties and early nineties. The field subsequently revived, however. For example, in 2005, Microsoft started using verification technology in many of their internal projects and is planning to include a logical specification and checking language in their 2012 version of Visual C.\\n\\n\\n== Significant contributions ==\\nPrincipia Mathematica was a milestone work in formal logic written by Alfred North Whitehead and Bertrand Russell. Principia Mathematica - also meaning Principles of Mathematics - was written with a purpose to derive all or some of the mathematical expressions, in terms of symbolic logic. Principia Mathematica was initially published in three volumes in 1910, 1912 and 1913.Logic Theorist (LT) was the first ever program developed in 1956 by Allen Newell, Cliff Shaw and Herbert A. Simon to \"mimic human reasoning\" in proving theorems and was demonstrated on fifty-two theorems from chapter two of Principia Mathematica, proving thirty-eight of them. In addition to proving the theorems, the program found a proof for one of the theorems that was more elegant than the one provided by Whitehead and Russell. After an unsuccessful attempt at publishing their results, Newell, Shaw, and Herbert reported in their publication in 1958, The Next Advance in Operation Research:\\n\\n\"There are now in the world machines that think, that learn and that create. Moreover, their ability to do these things is going to increase rapidly until (in a visible future) the range of problems they can handle will be co- extensive with the range to which the human mind has been applied.\" Examples of Formal Proofs\\n\\n\\n== Proof systems ==\\nBoyer-Moore Theorem Prover (NQTHM)\\nThe design of NQTHM was influenced by John McCarthy and Woody Bledsoe. Started in 1971 at Edinburgh, Scotland, this was a fully automatic theorem prover built using Pure Lisp. The main aspects of NQTHM were:\\nthe use of Lisp as a working logic.\\nthe reliance on a principle of definition for total recursive functions.\\nthe extensive use of rewriting and \"symbolic evaluation\".\\nan induction heuristic based the failure of symbolic evaluation.HOL Light\\nWritten in OCaml, HOL Light is designed to have a simple and clean logical foundation and an uncluttered implementation. It is essentially another proof assistant for classical higher order logic.Coq\\nDeveloped in France, Coq is another automated proof assistant, which can automatically extract executable programs from specifications, as either Objective CAML or Haskell source code. Properties, programs and proofs are formalized in the same language called the Calculus of Inductive Constructions (CIC).\\n\\n\\n== Applications ==\\nAutomated reasoning has been most commonly used to build automated theorem provers. Oftentimes, however, theorem provers require some human guidance to be effective and so more generally qualify as proof assistants. In some cases such provers have come up with new approaches to proving a theorem. Logic Theorist is a good example of this. The program came up with a proof for one of the theorems in Principia Mathematica that was more efficient (requiring fewer steps) than the proof provided by Whitehead and Russell. Automated reasoning programs are being applied to solve a growing number of problems in formal logic, mathematics and computer science, logic programming, software and hardware verification, circuit design, and many others. The TPTP (Sutcliffe and Suttner 1998) is a library of such problems that is updated on a regular basis. There is also a competition among automated theorem provers held regularly at the CADE conference (Pelletier, Sutcliffe and Suttner 2002); the problems for the competition are selected from the TPTP library.\\n\\n\\n== See also ==\\nAutomated machine learning (AutoML)\\nAutomated theorem proving\\nReasoning system\\nSemantic reasoner\\nProgram analysis (computer science)\\nApplications of artificial intelligence\\nOutline of artificial intelligence\\nCasuistry • Case-based reasoning\\nAbductive reasoning\\nInference engine\\nCommonsense reasoning\\n\\n\\n=== Conferences and workshops ===\\nInternational Joint Conference on Automated Reasoning (IJCAR)\\nConference on Automated Deduction (CADE)\\nInternational Conference on Automated Reasoning with Analytic Tableaux and Related Methods\\n\\n\\n=== Journals ===\\nJournal of Automated Reasoning\\n\\n\\n=== Communities ===\\nAssociation for Automated Reasoning (AAR)\\n\\n\\n== References ==\\n\\n\\n== External links ==\\nInternational Workshop on the Implementation of Logics\\nWorkshop Series on Empirically Successful Topics in Automated Reasoning', 'A prologue or prolog (from Greek πρόλογος prólogos, from πρό pró, \"before\" and λόγος lógos, \"word\") is an opening to a story that establishes the context and gives background details, often some earlier story that ties into the main one, and other miscellaneous information. The Ancient Greek prólogos included the modern meaning of prologue, but was of wider significance, more like the meaning of preface. The importance, therefore, of the prologue in Greek drama was very great; it sometimes almost took the place of a romance, to which, or to an episode in which, the play itself succeeded.\\n\\n\\n== Latin ==\\n\\nOn the Latin stage the prologue was often more elaborate than it was in Athens, and in the careful composition of the poems which Plautus prefixes to his plays we see what importance he gave to this portion of the entertainment; sometimes, as in the preface to the Rudens, Plautus rises to the height of his genius in his adroit and romantic prologues, usually placed in the mouths of persons who make no appearance in the play itself.\\nMolière revived the Plautian prologue in the introduction to his Amphitryon. Racine introduced Piety as the speaker of a prologue which opened his choral tragedy of Esther.\\nThe tradition of the ancients vividly affected our own early dramatists. Not only was the mystery plays and miracles of the Middle Ages begun by a homily, but when the drama in its modern sense was inaugurated in the reign of Elizabeth, the prologue came with it, directly adapted from the practice of Euripides and Terence. Sackville, Lord Buckhurst, prepared a sort of prologue in the dumb show for his Gorboduc of 1562; and he also wrote a famous Induction, which is, practically, a prologue, to a miscellany of short romantic epics by diverse hands.\\n\\n\\n== Elizabethan ==\\nPrologues of Renaissance drama often served a specific function of transition and clarification for the audience. A direct address made by one actor, the prologue acted as an appeal to the audience\\'s attention and sympathy, providing historical context, a guide to themes of the play, and occasionally, a disclaimer.:\\u200a17\\u200a  In this mode, a prologue, like any scripted performance, would exist as the text, the actor who speaks that text, and the presentation of the language as it is spoken.:\\u200a1\\u200a In ushering the audience from reality into the world of the play, the prologue straddles boundaries between audience, actors, characters, playwrights—basically, it creates a distinction between the imaginary space within the play and the outside world.:\\u200a2\\u200a Ben Jonson has often been noted as using the prologue to remind the audience of the complexities between themselves and all aspects of the performance.The actor reciting the prologue would appear dressed in black, a stark contrast to the elaborate costumes used during the play. The prologue removed his hat and wore no makeup. He may have carried a book, scroll, or placard displaying the title of the play.:\\u200a24\\u200a He was introduced by three short trumpet calls, on the third of which he entered and took a position downstage. He made three bows in the current fashion of the court, and then addressed the audience.:\\u200a26–27\\u200a\\nThe Elizabethan prologue was unique in incorporating aspects of both classical and medieval traditions.:\\u200a13\\u200a In the classical tradition, the prologue conformed to one of four subgenres: the sustatikos, which recommends either the play or the poet; the epitimetikos, in which a curse is given against a rival, or thanks given to the audience; dramatikos, in which the plot of the play is explained; and mixtos, which contains all of these things.:\\u200a13\\u200a In the medieval tradition, expressions of morality and modesty are seen,:\\u200a14\\u200a as well as a meta-theatrical self-consciousness, and an unabashed awareness of the financial contract engaged upon by paid actors and playwrights, and a paying audience.:\\u200a58\\u200a\\n\\n\\n== Use in fiction ==\\nPrologues have long been used in non-dramatic fiction, since at least the time of Geoffrey Chaucer\\'s Canterbury Tales, although Chaucer had prologues to many of the tales, rather than one at the front of the book.\\nThe Museum of Eterna\\'s Novel by the Argentine writer Macedonio Fernandez has over 50 prologues by the author. Their style varies between metaphysical, humoristic, psychological, discussions about the art of the novel, etc.\\n\\n\\n== See also ==\\nEpigraph\\nEpilogue\\nForeword\\nInterlude\\nIntroduction\\nLoa\\nPreface\\nProlegomena\\n\\n\\n== References ==\\n\\n\\n=== Attribution ===\\nThis article incorporates text from a publication now in the public domain: Chisholm, Hugh, ed. (1911). \"Prologue\". Encyclopædia Britannica (11th ed.). Cambridge University Press.', 'Automated theorem proving (also known as ATP or automated deduction) is a subfield of automated reasoning and mathematical logic dealing with proving mathematical theorems by computer programs.  Automated reasoning over mathematical proof was a major impetus for the development of computer science.\\n\\n\\n== Logical foundations ==\\nWhile the roots of formalised logic go back to Aristotle, the end of the 19th and early 20th centuries saw the development of modern logic and formalised mathematics. Frege\\'s Begriffsschrift (1879) introduced both a complete propositional calculus and what is essentially modern predicate logic.  His Foundations of Arithmetic, published 1884, expressed (parts of) mathematics in formal logic. This approach was continued by Russell and Whitehead in their influential Principia Mathematica, first published 1910–1913, and with a revised second edition in 1927. Russell and Whitehead thought they could derive all mathematical truth using axioms and inference rules of formal logic, in principle opening up the process to automatisation. In 1920, Thoralf Skolem simplified a previous result by Leopold Löwenheim, leading to the Löwenheim–Skolem theorem and, in 1930, to the notion of a Herbrand universe and a Herbrand interpretation that allowed (un)satisfiability of first-order formulas (and hence the validity of a theorem) to be reduced to (potentially infinitely many) propositional satisfiability problems.In 1929, Mojżesz Presburger showed that the theory of natural numbers with addition and equality (now called Presburger arithmetic in his honor) is decidable and gave an algorithm that could determine if a given sentence in the language was true or false.\\nHowever, shortly after this positive result, Kurt Gödel published On Formally Undecidable Propositions of Principia Mathematica and Related Systems (1931), showing that in any sufficiently strong axiomatic system there are true statements which cannot be proved in the system. This topic was further developed in the 1930s by Alonzo Church and Alan Turing, who on the one hand gave two independent but equivalent definitions of computability, and on the other gave concrete examples for undecidable questions.\\n\\n\\n== First implementations ==\\nShortly after World War II, the first general purpose computers became available. In 1954, Martin Davis programmed Presburger\\'s algorithm for a JOHNNIAC vacuum tube computer at the Institute for Advanced Study in Princeton, New Jersey. According to Davis, \"Its great triumph was to prove that the sum of two even numbers is even\". More ambitious was the Logic Theory Machine in 1956, a deduction system for the propositional logic of the Principia Mathematica, developed by Allen Newell, Herbert A. Simon and J. C. Shaw. Also running on a JOHNNIAC, the Logic Theory Machine constructed proofs from a small set of propositional axioms and three deduction rules: modus ponens, (propositional) variable substitution, and the replacement of formulas by their definition. The system used heuristic guidance, and managed to prove 38 of the first 52 theorems of the Principia.The \"heuristic\" approach of the Logic Theory Machine tried to emulate human mathematicians, and could not guarantee that a proof could be found for every valid theorem even in principle.  In contrast, other, more systematic algorithms achieved, at least theoretically, completeness for first-order logic. Initial approaches relied on the results of Herbrand and Skolem to convert a first-order formula into successively larger sets of propositional formulae by instantiating variables with terms from the Herbrand universe. The propositional formulas could then be checked for unsatisfiability using a number of methods. Gilmore\\'s program used conversion to disjunctive normal form, a form in which the satisfiability of a formula is obvious.\\n\\n\\n== Decidability of the problem ==\\nDepending on the underlying logic, the problem of deciding the validity of a formula varies from trivial to impossible. For the frequent case of propositional logic, the problem is decidable but co-NP-complete, and hence only exponential-time algorithms are believed to exist for general proof tasks. For a first order predicate calculus, Gödel\\'s completeness theorem states that the theorems (provable statements) are exactly the logically valid well-formed formulas, so identifying valid formulas is recursively enumerable: given unbounded resources, any valid formula can eventually be proven. However, invalid formulas (those that are not entailed by a given theory), cannot always be recognized.\\nThe above applies to first order theories, such as Peano arithmetic. However, for a specific model that may be described by a first order theory, some statements may be true but undecidable in the theory used to describe the model. For example, by Gödel\\'s incompleteness theorem, we know that any theory whose proper axioms are true for the natural numbers cannot prove all first order statements true for the natural numbers, even if the list of proper axioms is allowed to be infinite enumerable. It follows that an automated theorem prover will fail to terminate while searching for a proof precisely when the statement being investigated is undecidable in the theory being used, even if it is true in the model of interest. Despite this theoretical limit, in practice, theorem provers can solve many hard problems, even in models that are not fully described by any first order theory (such as the integers).\\n\\n\\n== Related problems ==\\nA simpler, but related, problem is proof verification, where an existing proof for a theorem is certified valid. For this, it is generally required that each individual proof step can be verified by a primitive recursive function or program, and hence the problem is always decidable.\\nSince the proofs generated by automated theorem provers are typically very large, the problem of proof compression is crucial and various techniques aiming at making the prover\\'s output smaller, and consequently more easily understandable and checkable, have been developed.\\nProof assistants require a human user to give hints to the system. Depending on the degree of automation, the prover can essentially be reduced to a proof checker, with the user providing the proof in a formal way, or significant proof tasks can be performed automatically. Interactive provers are used for a variety of tasks, but even fully automatic systems have proved a number of interesting and hard theorems, including at least one that has eluded human mathematicians for a long time, namely the Robbins conjecture. However, these successes are sporadic, and work on hard problems usually requires a proficient user.\\nAnother distinction is sometimes drawn between theorem proving and other techniques, where a process is considered to be theorem proving if it consists of a traditional proof, starting with axioms and producing new inference steps using rules of inference.  Other techniques would include model checking, which, in the simplest case, involves brute-force enumeration of many possible states (although the actual implementation of model checkers requires much cleverness, and does not simply reduce to brute force).\\nThere are hybrid theorem proving systems which use model checking as an inference rule. There are also programs which were written to prove a particular theorem, with a (usually informal) proof that if the program finishes with a certain result, then the theorem is true. A good example of this was the machine-aided proof of the four color theorem, which was very controversial as the first claimed mathematical proof which was essentially impossible to verify by humans due to the enormous size of the program\\'s calculation (such proofs are called non-surveyable proofs).  Another example of a program-assisted proof is the one that shows that the game of Connect Four can always be won by the first player.\\n\\n\\n== Industrial uses ==\\nCommercial use of automated theorem proving is mostly concentrated in integrated circuit design and verification.  Since the Pentium FDIV bug, the complicated floating point units of modern microprocessors have been designed with extra scrutiny. AMD, Intel and others use automated theorem proving to verify that division and other operations are correctly implemented in their processors.\\n\\n\\n== First-order theorem proving ==\\nIn the late 1960s agencies funding research in automated deduction began to emphasize the need for practical applications. One of the first fruitful areas was that of program verification whereby first-order theorem provers were applied to the problem of verifying the correctness of computer programs in languages such as Pascal, Ada, etc. Notable among early program verification systems was the Stanford Pascal Verifier developed by David Luckham at Stanford University. This was based on the Stanford Resolution Prover also developed at Stanford using John Alan Robinson\\'s resolution principle. This was the first automated deduction system to demonstrate an ability to solve mathematical problems that were announced in the Notices of the American Mathematical Society before solutions were formally published.First-order theorem proving is one of the most mature subfields of automated theorem proving. The logic is expressive enough to allow the specification of arbitrary problems, often in a reasonably natural and intuitive way. On the other hand, it is still semi-decidable, and a number of sound and complete calculi have been developed, enabling fully automated systems. More expressive logics, such as Higher-order logics, allow the convenient expression of a wider range of problems than first order logic, but theorem proving for these logics is less well developed.\\n\\n\\n== Benchmarks, competitions, and sources ==\\nThe quality of implemented systems has benefited from the existence of a large library of standard benchmark examples — the Thousands of Problems for Theorem Provers (TPTP) Problem Library  — as well as from the CADE ATP System Competition (CASC), a yearly competition of first-order systems for many important classes of first-order problems.\\nSome important systems (all have won at least one CASC competition division) are listed below.\\n\\nE is a high-performance prover for full first-order logic, but built on a purely equational calculus, originally developed in the automated reasoning group of Technical University of Munich under the direction of Wolfgang Bibel, and now at Baden-Württemberg Cooperative State University in Stuttgart.\\nOtter, developed at the Argonne National Laboratory, is based on first-order resolution and paramodulation. Otter has since been replaced by Prover9, which is paired with Mace4.\\nSETHEO is a high-performance system based on the goal-directed model elimination calculus, originally developed by a team under direction of Wolfgang Bibel. E and SETHEO have been combined (with other systems) in the composite theorem prover E-SETHEO.\\nVampire was originally developed and implemented at Manchester University by Andrei Voronkov and Krystof Hoder. It is now developed by a growing international team. It has won the FOF division (among other divisions) at the CADE ATP System Competition regularly since 2001.\\nWaldmeister is a specialized system for unit-equational first-order logic developed by Arnim Buch and Thomas Hillenbrand. It won the CASC UEQ division for fourteen consecutive years (1997–2010).\\nSPASS is a first order logic theorem prover with equality. This is developed by the research group Automation of Logic, Max Planck Institute for Computer Science.The Theorem Prover Museum is an initiative to conserve the sources of theorem prover systems for future analysis, since they are important cultural/scientific artefacts. It has the sources of many of the systems mentioned above.\\n\\n\\n== Popular techniques ==\\nFirst-order resolution with unification\\nModel elimination\\nMethod of analytic tableaux\\nSuperposition and term rewriting\\nModel checking\\nMathematical induction\\nBinary decision diagrams\\nDPLL\\nHigher-order unification\\n\\n\\n== Software systems ==\\n\\n\\n=== Free software ===\\nAlt-Ergo\\nAutomath\\nCVC\\nE\\nGKC\\nGödel machine\\niProver\\nIsaPlanner\\nKED theorem prover\\nleanCoP\\nLeo II\\nLCF\\nLogictools online theorem prover\\nLoTREC\\nMetaPRL\\nMizar\\nNuPRL\\nParadox\\nProver9\\nSimplify\\nSPARK (programming language)\\nTwelf\\nZ3 Theorem Prover\\n\\n\\n=== Proprietary software ===\\nAcumen RuleManager (commercial product)\\nALLIGATOR (CC BY-NC-SA 2.0 UK)\\nCARINE\\nKIV (freely available as a plugin for Eclipse)\\nProver Plug-In (commercial proof engine product)\\nProverBox\\nWolfram Mathematica\\nResearchCyc\\nSpear modular arithmetic theorem prover\\n\\n\\n== See also ==\\n\\n\\n== Notes ==\\n\\n\\n== References ==\\nChin-Liang Chang; Richard Char-Tung Lee (1973). Symbolic Logic and Mechanical Theorem Proving. Academic Press.\\nLoveland, Donald W. (1978). Automated Theorem Proving: A Logical Basis. Fundamental Studies in Computer Science Volume 6. North-Holland Publishing.\\nLuckham, David (1990). Programming with Specifications: An Introduction to Anna, A Language for Specifying Ada Programs. Springer-Verlag Texts and Monographs in Computer Science, 421 pp. ISBN 978-1461396871.Gallier, Jean H. (1986). Logic for Computer Science: Foundations of Automatic Theorem Proving. Harper & Row Publishers (Available for free download).\\nDuffy, David A. (1991). Principles of Automated Theorem Proving. John Wiley & Sons.\\nWos, Larry; Overbeek, Ross; Lusk, Ewing; Boyle, Jim (1992). Automated Reasoning: Introduction and Applications (2nd ed.). McGraw–Hill.\\nAlan Robinson; Andrei Voronkov, eds. (2001). Handbook of Automated Reasoning Volume I & II. Elsevier and MIT Press.\\nFitting, Melvin (1996). First-Order Logic and Automated Theorem Proving (2nd ed.). Springer.\\n\\n\\n== External links ==\\nA list of theorem proving tools', 'A theory is a rational type of abstract thinking about a phenomenon, or the results of such thinking. The process of contemplative and rational thinking is often associated with such processes as observational study or research. Theories may be scientific, belong to a non-scientific discipline, or no discipline at all. Depending on the context, a theory\\'s assertions might, for example, include generalized explanations of how nature works. The word has its roots in ancient Greek, but in modern use it has taken on several related meanings.\\nIn modern science, the term \"theory\" refers to scientific theories, a well-confirmed type of explanation of nature, made in a way consistent with the scientific method, and fulfilling the criteria required by modern science. Such theories are described in such a way that scientific tests should be able to provide empirical support for it, or empirical contradiction (\"falsify\") of it. Scientific theories are the most reliable, rigorous, and comprehensive form of scientific knowledge, in contrast to more common uses of the word \"theory\" that imply that something is unproven or speculative (which in formal terms is better characterized by the word hypothesis). Scientific theories are distinguished from hypotheses, which are individual empirically testable conjectures, and from scientific laws, which are descriptive accounts of the way nature behaves under certain conditions.\\nTheories guide the enterprise of finding facts rather than of reaching goals, and are neutral concerning alternatives among values.:\\u200a131\\u200a A theory can be a body of knowledge, which may or may not be associated with particular explanatory models. To theorize is to develop this body of knowledge.:\\u200a46\\u200aThe word theory or \"in theory\" is sometimes used erroneously by people to explain something which they individually did not experience or test before. In those instances, semantically, it is being substituted for another concept, a hypothesis. Instead of using the word \"hypothetically\", it is replaced by a phrase: \"in theory\". In some instances the theory\\'s credibility could be contested by calling it \"just a theory\" (implying that the idea has not even been tested). Hence, that word \"theory\" is very often contrasted to \"practice\" (from Greek praxis, πρᾶξις) a Greek term for doing, which is opposed to theory. A \"classical example\" of the distinction between \"theoretical\" and \"practical\" uses the discipline of medicine: medical theory involves trying to understand the causes and nature of health and sickness, while the practical side of medicine is trying to make people healthy. These two things are related but can be independent, because it is possible to research health and sickness without curing specific patients, and it is possible to cure a patient without knowing how the cure worked.\\n\\n\\n== Ancient usage ==\\nThe English word theory derives from a technical term in philosophy in Ancient Greek. As an everyday word, theoria, θεωρία, meant \"looking at, viewing, beholding\", but in more technical contexts it came to refer to contemplative or speculative understandings of natural things, such as those of natural philosophers, as opposed to more practical ways of knowing things, like that of skilled orators or artisans. English-speakers have used the word theory since at least the late 16th century. Modern uses of the word theory derive from the original definition, but have taken on new shades of meaning, still based on the idea of a theory as a thoughtful and rational explanation of the general nature of things.\\nAlthough it has more mundane meanings in Greek, the word θεωρία apparently developed special uses early in the recorded history of the Greek language. In the book From Religion to Philosophy, Francis Cornford suggests that the Orphics used the word theoria to mean \"passionate sympathetic contemplation\". Pythagoras changed the word to mean \"the passionless contemplation of rational, unchanging truth\" of mathematical knowledge, because he considered this intellectual pursuit the way to reach the highest plane of existence. Pythagoras emphasized subduing emotions and bodily desires to help the intellect function at the higher plane of theory. Thus, it was Pythagoras who gave the word theory the specific meaning that led to the classical and modern concept of a distinction between theory (as uninvolved, neutral thinking) and practice.Aristotle\\'s terminology, as already mentioned, contrasts theory with praxis or practice, and this contrast exists till today. For Aristotle, both practice and theory involve thinking, but the aims are different. Theoretical contemplation considers things humans do not move or change, such as nature, so it has no human aim apart from itself and the knowledge it helps create. On the other hand, praxis involves thinking, but always with an aim to desired actions, whereby humans cause change or movement themselves for their own ends. Any human movement that involves no conscious choice and thinking could not be an example of praxis or doing.\\n\\n\\n== Formality ==\\n\\nTheories are analytical tools for understanding, explaining, and making predictions about a given subject matter. There are theories in many and varied fields of study, including the arts and sciences. A formal theory is syntactic in nature and is only meaningful when given a semantic component by applying it to some content (e.g., facts and relationships of the actual historical world as it is unfolding). Theories in various fields of study are expressed in natural language, but are always constructed in such a way that their general form is identical to a theory as it is expressed in the formal language of mathematical logic. Theories may be expressed mathematically, symbolically, or in common language, but are generally expected to follow principles of rational thought or logic.\\nTheory is constructed of a set of sentences that are entirely true statements about the subject under consideration. However, the truth of any one of these statements is always relative to the whole theory. Therefore, the same statement may be true with respect to one theory, and not true with respect to another. This is, in ordinary language, where statements such as \"He is a terrible person\" cannot be judged as true or false without reference to some interpretation of who \"He\" is and for that matter what a \"terrible person\" is under the theory.Sometimes two theories have exactly the same explanatory power because they make the same predictions. A pair of such theories is called indistinguishable or observationally equivalent, and the choice between them reduces to convenience or philosophical preference.\\nThe form of theories is studied formally in mathematical logic, especially in model theory. When theories are studied in mathematics, they are usually expressed in some formal language and their statements are closed under application of certain procedures called rules of inference. A special case of this, an axiomatic theory, consists of axioms (or axiom schemata) and rules of inference. A theorem is a statement that can be derived from those axioms by application of these rules of inference. Theories used in applications are abstractions of observed phenomena and the resulting theorems provide solutions to real-world problems. Obvious examples include arithmetic (abstracting concepts of number), geometry (concepts of space), and probability (concepts of randomness and likelihood).\\nGödel\\'s incompleteness theorem shows that no consistent, recursively enumerable theory (that is, one whose theorems form a recursively enumerable set) in which the concept of natural numbers can be expressed, can include all true statements about them. As a result, some domains of knowledge cannot be formalized, accurately and completely, as mathematical theories. (Here, formalizing accurately and completely means that all true propositions—and only true propositions—are derivable within the mathematical system.) This limitation, however, in no way precludes the construction of mathematical theories that formalize large bodies of scientific knowledge.\\n\\n\\n=== Underdetermination ===\\n\\nA theory is underdetermined (also called indeterminacy of data to theory) if a rival, inconsistent theory is at least as consistent with the evidence. Underdetermination is an epistemological issue about the relation of evidence to conclusions.\\nA theory that lacks supporting evidence is generally, more properly, referred to as a hypothesis.\\n\\n\\n=== Intertheoretic reduction and elimination ===\\n\\nIf a new theory better explains and predicts a phenomenon than an old theory (i.e., it has more explanatory power), we are justified in believing that the newer theory describes reality more correctly. This is called an intertheoretic reduction because the terms of the old theory can be reduced to the terms of the new one. For instance, our historical understanding about sound, \"light\" and heat have been reduced to wave compressions and rarefactions, electromagnetic waves, and molecular kinetic energy, respectively. These terms, which are identified with each other, are called intertheoretic identities. When an old and new theory are parallel in this way, we can conclude that the new one describes the same reality, only more completely.\\nWhen a new theory uses new terms that do not reduce to terms of an older theory, but rather replace them because they misrepresent reality, it is called an intertheoretic elimination. For instance, the obsolete scientific theory that put forward an understanding of heat transfer in terms of the movement of caloric fluid was eliminated when a theory of heat as energy replaced it. Also, the theory that phlogiston is a substance released from burning and rusting material was eliminated with the new understanding of the reactivity of oxygen.\\n\\n\\n=== Versus theorems ===\\nTheories are distinct from theorems. A theorem is derived deductively from axioms (basic assumptions) according to a formal system of rules, sometimes as an end in itself and sometimes as a first step toward being tested or applied in a concrete situation; theorems are said to be true in the sense that the conclusions of a theorem are logical consequences of the axioms. Theories are abstract and conceptual, and are supported or challenged by observations in the world. They are \\'rigorously tentative\\', meaning that they are proposed as true and expected to satisfy careful examination to account for the possibility of faulty inference or incorrect observation. Sometimes theories are incorrect, meaning that an explicit set of observations contradicts some fundamental objection or application of the theory, but more often theories are corrected to conform to new observations, by restricting the class of phenomena the theory applies to or changing the assertions made. An example of the former is the restriction of classical mechanics to phenomena involving macroscopic length scales and particle speeds much lower than the speed of light.\\n\\n\\n== The theory-practice gap ==\\nTheory is often distinguished from practice. The question of whether theoretical models of work are relevant to work itself is of interest to scholars of professions such as medicine, engineering, and law, and management.:\\u200a802\\u200aThis gap between theory and practice has been framed as a knowledge transfer where there is a task of translating research knowledge to be application in practice, and ensuring that practictioners are made aware of it academics have been criticized for not attempting to transfer the knowledge they produce to practitioners.:\\u200a804\\u200a  Another framing supposes that theory and knowledge seek to understand different problems and model the world in different words (using different ontologies and epistemologies) . Another framing says that research does not produce theory that is relevant to practice.:\\u200a803\\u200aIn the context of management, Van de Van and Johnson propose a form of engaged scholarship where scholars examine problems that occur in practice, in an interdisciplinary fashion, producing results that create both new practical results as well as new theoretical models, but targeting theoretical results shared in an academic fashion.:\\u200a815\\u200a They use a metaphor of \"arbitrage\" of ideas between disciplines, distinguishing it from collaboration.:\\u200a803\\u200a\\n\\n\\n== Scientific ==\\n\\nIn science, the term \"theory\" refers to \"a well-substantiated explanation of some aspect of the natural world, based on a body of facts that have been repeatedly confirmed through observation and experiment.\" Theories must also meet further requirements, such as the ability to make falsifiable predictions with consistent accuracy across a broad area of scientific inquiry, and production of strong evidence in favor of the theory from multiple independent sources (consilience).\\nThe strength of a scientific theory is related to the diversity of phenomena it can explain, which is measured by its ability to make falsifiable predictions with respect to those phenomena. Theories are improved (or replaced by better theories) as more evidence is gathered, so that accuracy in prediction improves over time; this increased accuracy corresponds to an increase in scientific knowledge. Scientists use theories as a foundation to gain further scientific knowledge, as well as to accomplish goals such as inventing technology or curing diseases.\\n\\n\\n=== Definitions from scientific organizations ===\\nThe United States National Academy of Sciences defines scientific theories as follows:The formal scientific definition of \"theory\" is quite different from the everyday meaning of the word. It refers to a comprehensive explanation of some aspect of nature that is supported by a vast body of evidence. Many scientific theories are so well established that no new evidence is likely to alter them substantially. For example, no new evidence will demonstrate that the Earth does not orbit around the sun (heliocentric theory), or that living things are not made of cells (cell theory), that matter is not composed of atoms, or that the surface of the Earth is not divided into solid plates that have moved over geological timescales (the theory of plate tectonics) ... One of the most useful properties of scientific theories is that they can be used to make predictions about natural events or phenomena that have not yet been observed.\\nFrom the American Association for the Advancement of Science:\\n\\nA scientific theory is a well-substantiated explanation of some aspect of the natural world, based on a body of facts that have been repeatedly confirmed through observation and experiment. Such fact-supported theories are not \"guesses\" but reliable accounts of the real world. The theory of biological evolution is more than \"just a theory.\" It is as factual an explanation of the universe as the atomic theory of matter or the germ theory of disease. Our understanding of gravity is still a work in progress. But the phenomenon of gravity, like evolution, is an accepted fact.\\nThe term theory is not appropriate for describing scientific models or untested, but intricate hypotheses.\\n\\n\\n=== Philosophical views ===\\nThe logical positivists thought of scientific theories as deductive theories—that a theory\\'s content is based on some formal system of logic and on basic axioms. In a deductive theory, any sentence which is a logical consequence of one or more of the axioms is also a sentence of that theory. This is called the received view of theories.\\nIn the semantic view of theories,  which has largely replaced the received view, theories are viewed as scientific models. A model is a logical framework intended to represent reality (a \"model of reality\"), similar to the way that a map is a graphical model that represents the territory of a city or country. In this approach, theories are a specific category of models that fulfill the necessary criteria. (See Theories as models for further discussion.)\\n\\n\\n=== In physics ===\\nIn physics the term theory is generally used for a mathematical framework—derived from a small set of basic postulates (usually symmetries, like equality of locations in space or in time, or identity of electrons, etc.)—which is capable of producing experimental predictions for a given category of physical systems. One good example is classical electromagnetism, which encompasses results derived from gauge symmetry (sometimes called gauge invariance) in a form of a few equations called Maxwell\\'s equations. The specific mathematical aspects of classical electromagnetic theory are termed \"laws of electromagnetism\", reflecting the level of consistent and reproducible evidence that supports them. Within electromagnetic theory generally, there are numerous hypotheses about how electromagnetism applies to specific situations. Many of these hypotheses are already considered adequately tested, with new ones always in the making and perhaps untested.\\n\\n\\n=== Regarding the term \"theoretical\" ===\\nCertain tests may be infeasible or technically difficult. As a result, theories may make predictions that have not been confirmed or proven incorrect. These predictions may be described informally as \"theoretical\". They can be tested later, and if they are incorrect, this may lead to revision, invalidation, or rejection of the theory.\\n\\n\\n== Mathematical ==\\n\\nIn mathematics the use of the term theory is different, necessarily so, since mathematics contains no explanations of natural phenomena, per se, even though it may help provide insight into natural systems or be inspired by them. In the general sense, a mathematical theory is a branch of or topic in mathematics, such as Set theory, Number theory, Group theory, Probability theory, Game theory, Control theory, Perturbation theory, etc., such as might be appropriate for a single textbook.\\nIn the same sense, but more specifically, the word theory is an extensive, structured collection of theorems, organized so that the proof of each theorem only requires the theorems and axioms that preceded it (no circular proofs), occurs as early as feasible in sequence (no postponed proofs), and the whole is as succinct as possible (no redundant proofs). Ideally, the sequence in which the theorems are presented is as easy to understand as possible, although illuminating a branch of mathematics is the purpose of textbooks, rather than the mathematical theory they might be written to cover.\\n\\n\\n== Philosophical ==\\n\\nA theory can be either descriptive as in science, or prescriptive (normative) as in philosophy. The latter are those whose subject matter consists not of empirical data, but rather of ideas. At least some of the elementary theorems of a philosophical theory are statements whose truth cannot necessarily be scientifically tested through empirical observation.\\nA field of study is sometimes named a \"theory\" because its basis is some initial set of assumptions describing the field\\'s approach to the subject. These assumptions are the elementary theorems of the particular theory, and can be thought of as the axioms of that field. Some commonly known examples include set theory and number theory; however literary theory, critical theory, and music theory are also of the same form.\\n\\n\\n=== Metatheory ===\\n\\nOne form of philosophical theory is a metatheory or meta-theory. A metatheory is a theory whose subject matter is some other theory or set of theories. In other words, it is a theory about theories. Statements made in the metatheory about the theory are called metatheorems.\\n\\n\\n== Political ==\\n\\nA political theory is an ethical theory about the law and government. Often the term \"political theory\" refers to a general view, or specific ethic, political belief or attitude, thought about politics.\\n\\n\\n== Jurisprudential ==\\n\\nIn social science, jurisprudence is the philosophical theory of law. Contemporary philosophy of law addresses problems internal to law and legal systems, and problems of law as a particular social institution.\\n\\n\\n== Examples ==\\nMost of the following are scientific theories. Some are not, but rather encompass a body of knowledge or art, such as Music theory and Visual Arts Theories.\\n\\nAnthropology: Carneiro\\'s circumscription theory\\nAstronomy: Alpher–Bethe–Gamow theory — B2FH Theory — Copernican theory — Giant impact hypothesis — Newton\\'s theory of gravitation — Hubble\\'s law — Kepler\\'s laws of planetary motion — Nebular hypothesis — Ptolemaic theory\\nCosmology: Big Bang Theory — Cosmic inflation — Loop quantum gravity — Superstring theory — Supergravity — Supersymmetric theory — Multiverse theory — Holographic principle — Quantum gravity — M-theory\\nBiology: Cell theory — Evolution — Germ theory\\nChemistry: Molecular theory — Kinetic theory of gases — Molecular orbital theory — Valence bond theory — Transition state theory — RRKM theory — Chemical graph theory — Flory–Huggins solution theory — Marcus theory — Lewis theory (successor to Brønsted–Lowry acid–base theory) — HSAB theory — Debye–Hückel theory — Thermodynamic theory of polymer elasticity — Reptation theory — Polymer field theory — Møller–Plesset perturbation theory — density functional theory — Frontier molecular orbital theory — Polyhedral skeletal electron pair theory — Baeyer strain theory — Quantum theory of atoms in molecules — Collision theory — Ligand field theory (successor to Crystal field theory) — Variational transition-state theory — Benson group increment theory — Specific ion interaction theory\\nClimatology: Climate change theory (general study of climate changes) and anthropogenic climate change (ACC)/ global warming (AGW) theories (due to human activity)\\nEconomics: Macroeconomic theory — Microeconomic theory — Law of Supply and demand\\nEducation: Constructivist theory — Critical pedagogy theory — Education theory — Multiple intelligence theory — Progressive education theory\\nEngineering: Circuit theory — Control theory — Signal theory — Systems theory — Information theory\\nFilm: Film Theory\\nGeology: Plate tectonics\\nHumanities: Critical theory\\nJurisprudence or \\'Legal theory\\': Natural law — Legal positivism — Legal realism — Critical legal studies\\nLaw: see Jurisprudence; also Case theory\\nLinguistics: X-bar theory — Government and Binding — Principles and parameters — Universal grammar\\nLiterature: Literary theory\\nMathematics: Approximation theory — Arakelov theory — Asymptotic theory — Bifurcation theory — Catastrophe theory — Category theory — Chaos theory — Choquet theory — Coding theory — Combinatorial game theory — Computability theory — Computational complexity theory — Deformation theory — Dimension theory — Ergodic theory — Field theory — Galois theory — Game theory — Graph theory — Group theory — Hodge theory — Homology theory — Homotopy theory — Ideal theory — Intersection theory — Invariant theory — Iwasawa theory — K-theory — KK-theory — Knot theory — L-theory — Lie theory — Littlewood–Paley theory — Matrix theory — Measure theory — Model theory — Morse theory — Nevanlinna theory — Number theory — Obstruction theory — Operator theory — PCF theory — Perturbation theory — Potential theory — Probability theory — Ramsey theory — Rational choice theory — Representation theory — Ring theory — Set theory — Shape theory — Small cancellation theory — Spectral theory — Stability theory — Stable theory — Sturm–Liouville theory — Twistor theory\\nMusic: Music theory\\nPhilosophy: Proof theory — Speculative reason — Theory of truth — Type theory — Value theory — Virtue theory\\nPhysics: Acoustic theory — Antenna theory — Atomic theory — BCS theory — Dirac hole theory — Dynamo theory — Landau theory — M-theory — Perturbation theory — Theory of relativity (successor to classical mechanics) — Quantum field theory — Scattering theory — String theory — Quantum information theory\\nPsychology: Theory of mind — Cognitive dissonance theory — Attachment theory — Object permanence — Poverty of stimulus — Attribution theory — Self-fulfilling prophecy — Stockholm syndrome\\nPublic Budgeting: Incrementalism — Zero-based budgeting\\nPublic Administration: Organizational theory\\nSemiotics: Intertheoricity - Transferogenesis\\nSociology: Critical theory — Engaged theory — Social theory — Sociological theory - Social capital theory\\nStatistics: Extreme value theory\\nTheatre: Performance theory\\nVisual Art: Aesthetics — Art educational theory — Architecture — Composition — Anatomy — Color theory — Perspective — Visual perception — Geometry — Manifolds\\nOther: Obsolete scientific theories\\n\\n\\n== See also ==\\n\\n\\n== Notes ==\\n\\n\\n== References ==\\n\\n\\n=== Citations ===\\n\\n\\n=== Sources ===\\n\\n\\n== External links ==\\n\\n\"How science works: Even theories change\", Understanding Science by the University of California Museum of Paleontology.\\nWhat is a Theory?', 'Soft computing is a set of algorithms, \\nincluding neural networks, fuzzy logic, and genetic algorithms.\\nThese algorithms are tolerant of imprecision, uncertainty, partial truth and approximation.\\nIt is contrasted with hard computing: algorithms which finds provably correct and optimal solutions to problems. \\n\\n\\n== History ==\\nThe theory and techniques related to soft computing were first introduced in 1980s. The term \"soft computing\" was coined by Lotfi A. Zadeh.\\n\\n\\n== See also ==\\nEmergence\\nSynthetic intelligence\\nWatson (computer)\\n\\n\\n== Notable journals ==\\nSoft Computing\\nApplied Soft Computing\\n\\n\\n== References ==', 'In computer science, evolutionary computation is a family of algorithms for global optimization inspired by biological evolution, and the subfield of artificial intelligence and soft computing studying these algorithms. In technical terms, they are a family of population-based trial and error problem solvers with a metaheuristic or stochastic optimization character.\\nIn evolutionary computation, an initial set of candidate solutions is generated and iteratively updated. Each new generation is produced by stochastically removing less desired solutions, and introducing small random changes. In biological terminology, a population of solutions is subjected to natural selection (or artificial selection) and mutation. As a result, the population will gradually evolve to increase in fitness, in this case the chosen fitness function of the algorithm.\\nEvolutionary computation techniques can produce highly optimized solutions in a wide range of problem settings, making them popular in computer science. Many variants and extensions exist, suited to more specific families of problems and data structures. Evolutionary computation is also sometimes used in evolutionary biology as an in silico experimental procedure to study common aspects of general evolutionary processes.\\n\\n\\n== History ==\\nThe use of evolutionary principles for automated problem solving originated in the 1950s. It was not until the 1960s that three distinct interpretations of this idea started to be developed in three different places.\\nEvolutionary programming was introduced by Lawrence J. Fogel in the US, while John Henry Holland called his method a genetic algorithm. In Germany Ingo Rechenberg and Hans-Paul Schwefel introduced evolution strategies. These areas developed separately for about 15 years. From the early nineties on they are unified as different representatives (\"dialects\") of one technology, called evolutionary computing. Also in the early nineties, a fourth stream following the general ideas had emerged – genetic programming. Since the 1990s, nature-inspired algorithms are becoming an increasingly significant part of the evolutionary computation.\\nThese terminologies denote the field of evolutionary computing and consider evolutionary programming, evolution strategies, genetic algorithms, and genetic programming as sub-areas.\\nThe earliest computational simulations of evolution using evolutionary algorithms and artificial life techniques were performed by Nils Aall Barricelli in 1953, with first results published in 1954. Another pioneer in the 1950s was Alex Fraser, who published a series of papers on simulation of artificial selection. Artificial evolution became a widely recognised optimisation method as a result of the work of Ingo Rechenberg in the 1960s and early 1970s, who used evolution strategies to solve complex engineering problems. Genetic algorithms in particular became popular through the writing of John Holland. As academic interest grew, dramatic increases in the power of computers allowed practical applications, including the automatic evolution of computer programs. Evolutionary algorithms are now used to solve multi-dimensional problems more efficiently than software produced by human designers, and also to optimise the design of systems.\\n\\n\\n== Techniques ==\\nEvolutionary computing techniques mostly involve metaheuristic optimization algorithms. Broadly speaking, the field includes:\\n\\nAgent-based modeling\\nAnt colony optimization\\nArtificial immune systems\\nArtificial life (also see digital organism)\\nCultural algorithms\\nDifferential evolution\\nDual-phase evolution\\nEstimation of distribution algorithms\\nEvolutionary algorithms\\nEvolutionary programming\\nEvolution strategy\\nGene expression programming\\nGenetic algorithm\\nGenetic programming\\nGrammatical evolution\\nLearnable evolution model\\nLearning classifier systems\\nMemetic algorithms\\nNeuroevolution\\nParticle swarm optimization\\nSelf-organization such as self-organizing maps, competitive learning\\nSwarm intelligence\\n\\n\\n== Evolutionary algorithms ==\\n\\nEvolutionary algorithms form a subset of evolutionary computation in that they generally only involve techniques implementing mechanisms inspired by biological evolution such as reproduction, mutation, recombination, natural selection and survival of the fittest. Candidate solutions to the optimization problem play the role of individuals in a population, and the cost function determines the environment within which the solutions \"live\" (see also fitness function). Evolution of the population then takes place after the repeated application of the above operators.\\nIn this process, there are two main forces that form the basis of evolutionary systems:  Recombination mutation and crossover create the necessary diversity and thereby facilitate novelty, while selection acts as a force increasing quality.\\nMany aspects of such an evolutionary process are stochastic. Changed pieces of information due to recombination and mutation are randomly chosen. On the other hand, selection operators can be either deterministic, or stochastic. In the latter case, individuals with a higher fitness have a higher chance to be selected than individuals with a lower fitness, but typically even the weak individuals have a chance to become a parent or to survive.\\n\\n\\n== Evolutionary algorithms and biology ==\\n\\nGenetic algorithms deliver methods to model biological systems and systems biology that are linked to the theory of dynamical systems, since they are used to predict the future states of the system. This is just a vivid (but perhaps misleading) way of drawing attention to the orderly, well-controlled and highly structured character of development in biology.\\nHowever, the use of algorithms and informatics, in particular of computational theory, beyond the analogy to dynamical systems, is also relevant to understand evolution itself.\\nThis view has the merit of recognizing that there is no central control of development; organisms develop as a result of local interactions within and between cells. The most promising ideas about program-development parallels seem to us to be ones that point to an apparently close analogy between processes within cells, and the low-level operation of modern computers. Thus, biological systems are like computational machines that process input information to compute next states, such that biological systems are closer to a computation than classical dynamical system.Furthermore, following concepts from computational theory, micro processes in biological organisms are fundamentally incomplete and undecidable (completeness (logic)), implying that “there is more than a crude metaphor behind the analogy between cells and computers.The analogy to computation extends also to the relationship between inheritance systems and biological structure, which is often thought to reveal one of the most pressing problems in explaining the origins of life.\\nEvolutionary automata, a generalization of Evolutionary Turing machines, have been introduced in order to investigate more precisely properties of biological and evolutionary computation. In particular, they allow to obtain new results on expressiveness of evolutionary computation. This confirms the initial result about undecidability of natural evolution and evolutionary algorithms and processes. Evolutionary finite automata, the simplest subclass of Evolutionary automata working in terminal mode can accept arbitrary languages over a given alphabet, including non-recursively enumerable (e.g., diagonalization language) and recursively enumerable but not recursive languages (e.g., language of the universal Turing machine). \\n\\n\\n== Notable practitioners ==\\nThe list of active researchers is naturally dynamic and non-exhaustive. A network analysis of the community was published in 2007.\\nKalyanmoy Deb\\nKenneth A De Jong\\nPeter J. Fleming\\nDavid B. Fogel\\nStephanie Forrest\\nDavid E. Goldberg\\nJohn Henry Holland\\nTheo Jansen\\nJohn Koza\\nZbigniew Michalewicz\\nMelanie Mitchell\\nPeter Nordin\\nRiccardo Poli\\nIngo Rechenberg\\nHans-Paul Schwefel\\n\\n\\n== Conferences ==\\nThe main conferences in the evolutionary computation area include \\n\\nACM Genetic and Evolutionary Computation Conference (GECCO),\\nIEEE Congress on Evolutionary Computation (CEC),\\nEvoStar, which comprises four conferences: EuroGP, EvoApplications, EvoCOP and EvoMUSART,\\nParallel Problem Solving from Nature (PPSN).\\n\\n\\n== See also ==\\n\\n\\n== External links ==\\nArticle in the Stanford Encyclopedia of Philosophy about Biological Information (English)\\n\\n\\n== Bibliography ==\\nTh. Bäck, D.B. Fogel, and Z. Michalewicz (Editors), Handbook of Evolutionary Computation, 1997, ISBN 0750303921\\nTh. Bäck and H.-P. Schwefel. An overview of evolutionary algorithms for parameter optimization. Evolutionary Computation, 1(1):1–23, 1993.\\nW. Banzhaf, P. Nordin, R.E. Keller, and F.D. Francone. Genetic Programming — An Introduction. Morgan Kaufmann, 1998.\\nS. Cagnoni, et al., Real-World Applications of Evolutionary Computing, Springer-Verlag Lecture Notes in Computer Science, Berlin, 2000.\\nR. Chiong, Th. Weise, Z. Michalewicz (Editors), Variants of Evolutionary Algorithms for Real-World Applications, Springer, 2012, ISBN 3642234232\\nK. A. De Jong, Evolutionary computation: a unified approach. MIT Press, Cambridge MA, 2006\\nA. E. Eiben and J.E. Smith, From evolutionary computation to the evolution of things, Nature, 521:476-482, doi:10.1038/nature14544, 2015\\nA. E. Eiben and J.E. Smith, Introduction to Evolutionary Computing, Springer, First edition, 2003; Second edition, 2015\\nD. B. Fogel. Evolutionary Computation. Toward a New Philosophy of Machine Intelligence. IEEE Press, Piscataway, NJ, 1995.\\nL. J. Fogel, A. J. Owens, and M. J. Walsh. Artificial Intelligence through Simulated Evolution. New York: John Wiley, 1966.\\nD. E. Goldberg. Genetic algorithms in search, optimization and machine learning. Addison Wesley, 1989.\\nJ. H. Holland. Adaptation in natural and artificial systems. University of Michigan Press, Ann Arbor, 1975.\\nP. Hingston, L. Barone, and Z. Michalewicz (Editors), Design by Evolution, Natural Computing Series, 2008, Springer, ISBN 3540741097\\nJ. R. Koza. Genetic Programming: On the Programming of Computers by means of Natural Evolution. MIT Press, Massachusetts, 1992.\\nF.J. Lobo, C.F. Lima, Z. Michalewicz (Editors), Parameter Setting in Evolutionary Algorithms, Springer, 2010, ISBN 3642088929\\nZ. Michalewicz, Genetic Algorithms + Data Structures – Evolution Programs, 1996, Springer, ISBN 3540606769\\nZ. Michalewicz and D.B. Fogel, How to Solve It: Modern Heuristics, Springer, 2004, ISBN 978-3-540-22494-5\\nI. Rechenberg. Evolutionstrategie: Optimierung Technischer Systeme nach Prinzipien des Biologischen Evolution. Fromman-Hozlboog Verlag, Stuttgart, 1973. (in German)\\nH.-P. Schwefel. Numerical Optimization of Computer Models. John Wiley & Sons, New-York, 1981. 1995 – 2nd edition.\\nD. Simon. Evolutionary Optimization Algorithms. Wiley, 2013.\\nM. Sipper, W. Fu, K. Ahuja, and J. H. Moore (2018). \"Investigating the parameter space of evolutionary algorithms\". BioData Mining. 11: 2. doi:10.1186/s13040-018-0164-x. PMC 5816380. PMID 29467825.CS1 maint: uses authors parameter (link)\\nY. Zhang and S. Li. (2017). \"PSA: A novel optimization algorithm based on survival rules of porcellio scaber\". arXiv:1709.09840 [cs.NE].CS1 maint: uses authors parameter (link)\\n\\n\\n== References ==', 'Natural language processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language, in particular how to program computers to process and analyze large amounts of natural language data.  The goal is a computer capable of \"understanding\" the contents of documents, including the contextual nuances of the language within them. The technology can then accurately extract information and insights contained in the documents as well as categorize and organize the documents themselves.\\nChallenges in natural language processing frequently involve speech recognition, natural language understanding, and natural language generation.\\n\\n\\n== History ==\\n\\nNatural language processing has its roots in the 1950s. Already in 1950, Alan Turing published an article titled \"Computing Machinery and Intelligence\" which proposed what is now called the Turing test as a criterion of intelligence, a task that involves the automated interpretation and generation of natural language, but at the time not articulated as a problem separate from artificial intelligence.\\n\\n\\n=== Symbolic NLP (1950s – early 1990s) ===\\nThe premise of symbolic NLP is well-summarized by John Searle\\'s Chinese room experiment: Given a collection of rules (e.g., a Chinese phrasebook, with questions and matching answers), the computer emulates natural language understanding (or other NLP tasks) by applying those rules to the data it is confronted with.\\n\\n1950s: The Georgetown experiment in 1954 involved fully automatic translation of more than sixty Russian sentences into English. The authors claimed that within three or five years, machine translation would be a solved problem.  However, real progress was much slower, and after the ALPAC report in 1966, which found that ten-year-long research had failed to fulfill the expectations, funding for machine translation was dramatically reduced.  Little further research in machine translation was conducted until the late 1980s when the first statistical machine translation systems were developed.\\n1960s: Some notably successful natural language processing systems developed in the 1960s were SHRDLU, a natural language system working in restricted \"blocks worlds\" with restricted vocabularies, and ELIZA, a simulation of a Rogerian psychotherapist, written by Joseph Weizenbaum between 1964 and 1966.  Using almost no information about human thought or emotion, ELIZA sometimes provided a startlingly human-like interaction. When the \"patient\" exceeded the very small knowledge base, ELIZA might provide a generic response, for example, responding to \"My head hurts\" with \"Why do you say your head hurts?\".\\n1970s: During the 1970s, many programmers began to write \"conceptual ontologies\", which structured real-world information into computer-understandable data.  Examples are MARGIE (Schank, 1975), SAM (Cullingford, 1978), PAM (Wilensky, 1978), TaleSpin (Meehan, 1976), QUALM (Lehnert, 1977), Politics (Carbonell, 1979), and Plot Units (Lehnert 1981).  During this time, the first many chatterbots were written (e.g., PARRY).\\n1980s: The 1980s and early 1990s mark the hey-day of symbolic methods in NLP. Focus areas of the time included research on rule-based parsing (e.g., the development of HPSG as a computational operationalization of generative grammar), morphology (e.g., two-level morphology), semantics (e.g., Lesk algorithm), reference (e.g., within Centering Theory) and other areas of natural language understanding (e.g., in the Rhetorical Structure Theory). Other lines of research were continued, e.g., the development of chatterbots with Racter and Jabberwacky. An important development (that eventually led to the statistical turn in the 1990s) was the rising importance of quantitative evaluation in this period.\\n\\n\\n=== Statistical NLP (1990s–2010s) ===\\nUp to the 1980s, most natural language processing systems were based on complex sets of hand-written rules.  Starting in the late 1980s, however, there was a revolution in natural language processing with the introduction of machine learning algorithms for language processing.  This was due to both the steady increase in computational power (see Moore\\'s law) and the gradual lessening of the dominance of Chomskyan theories of linguistics (e.g. transformational grammar), whose theoretical underpinnings discouraged the sort of corpus linguistics that underlies the machine-learning approach to language processing.\\n1990s: Many of the notable early successes on statistical methods in NLP occurred in the field of machine translation, due especially to work at IBM Research.  These systems were able to take advantage of existing multilingual textual corpora that had been produced by the Parliament of Canada and the European Union as a result of laws calling for the translation of all governmental proceedings into all official languages of the corresponding systems of government.  However, most other systems depended on corpora specifically developed for the tasks implemented by these systems, which was (and often continues to be) a major limitation in the success of these systems. As a result, a great deal of research has gone into methods of more effectively learning from limited amounts of data.\\n2000s: With the growth of the web, increasing amounts of raw (unannotated) language data has become available since the mid-1990s. Research has thus increasingly focused on unsupervised and semi-supervised learning algorithms.  Such algorithms can learn from data that has not been hand-annotated with the desired answers or using a combination of annotated and non-annotated data.  Generally, this task is much more difficult than supervised learning, and typically produces less accurate results for a given amount of input data.  However, there is an enormous amount of non-annotated data available (including, among other things, the entire content of the World Wide Web), which can often make up for the inferior results if the algorithm used has a low enough time complexity to be practical.\\n\\n\\n=== Neural NLP (present) ===\\nIn the 2010s, representation learning and deep neural network-style machine learning methods became widespread in natural language processing, due in part to a flurry of results showing that such techniques can achieve state-of-the-art results in many natural language tasks, for example in language modeling, parsing, and many others. This is increasingly important in medicine and healthcare, where NLP is being used to analyze notes and text in electronic health records that would otherwise be inaccessible for study when seeking to improve care.\\n\\n\\n== Methods: Rules, statistics, neural networks ==\\nIn the early days, many language-processing systems were designed by symbolic methods, i.e., the hand-coding of a set of rules, coupled with a dictionary lookup: such as by writing grammars or devising heuristic rules for stemming.\\nMore recent systems based on machine-learning algorithms have many advantages over hand-produced rules: \\n\\nThe learning procedures used during machine learning automatically focus on the most common cases, whereas when writing rules by hand it is often not at all obvious where the effort should be directed.\\nAutomatic learning procedures can make use of statistical inference algorithms to produce models that are robust to unfamiliar input (e.g. containing words or structures that have not been seen before) and to erroneous input (e.g. with misspelled words or words accidentally omitted). Generally, handling such input gracefully with handwritten rules, or, more generally, creating systems of handwritten rules that make soft decisions, is extremely difficult, error-prone and time-consuming.\\nSystems based on automatically learning the rules can be made more accurate simply by supplying more input data. However, systems based on handwritten rules can only be made more accurate by increasing the complexity of the rules, which is a much more difficult task. In particular, there is a limit to the complexity of systems based on handwritten rules, beyond which the systems become more and more unmanageable. However, creating more data to input to machine-learning systems simply requires a corresponding increase in the number of man-hours worked, generally without significant increases in the complexity of the annotation process.Despite the popularity of machine learning in NLP research, symbolic methods are still (2020) commonly used:\\n\\nwhen the amount of training data is insufficient to successfully apply machine learning methods, e.g., for the machine translation of low-resource languages such as provided by the Apertium system,\\nfor preprocessing in NLP pipelines, e.g., tokenization, or\\nfor postprocessing and transforming the output of NLP pipelines, e.g., for knowledge extraction from syntactic parses.\\n\\n\\n=== Statistical methods ===\\nSince the so-called \"statistical revolution\" in the late 1980s and mid-1990s, much natural language processing research has relied heavily on machine learning. The machine-learning paradigm calls instead for using statistical inference to automatically learn such rules through the analysis of large corpora (the plural form of corpus, is a set of documents, possibly with human or computer annotations) of typical real-world examples.\\nMany different classes of machine-learning algorithms have been applied to natural-language-processing tasks. These algorithms take as input a large set of \"features\" that are generated from the input data. Increasingly, however, research has focused on statistical models, which make soft, probabilistic decisions based on attaching real-valued weights to each input feature (complex-valued embeddings, and neural networks in general have also been proposed, for e.g. speech). Such models have the advantage that they can express the relative certainty of many different possible answers rather than only one, producing more reliable results when such a model is included as a component of a larger system.\\nSome of the earliest-used machine learning algorithms, such as decision trees, produced systems of hard if-then rules similar to existing hand-written rules.  However, part-of-speech tagging introduced the use of hidden Markov models to natural language processing, and increasingly, research has focused on statistical models, which make soft, probabilistic decisions based on attaching real-valued weights to the features making up the input data. The cache language models upon which many speech recognition systems now rely are examples of such statistical models.  Such models are generally more robust when given unfamiliar input, especially input that contains errors (as is very common for real-world data), and produce more reliable results when integrated into a larger system comprising multiple subtasks.\\nSince the neural turn, statistical methods in NLP research have been largely replaced by neural networks. However, they continue to be relevant for contexts in which statistical interpretability and transparency is required.\\n\\n\\n=== Neural networks ===\\n\\nA major drawback of statistical methods is that they require elaborate feature engineering. Since 2015, the field has thus largely abandoned statistical methods and shifted to neural networks for machine learning. Popular techniques include the use of word embeddings to capture semantic properties of words, and an increase in end-to-end learning of a higher-level task (e.g., question answering) instead of relying on a pipeline of separate intermediate tasks (e.g., part-of-speech tagging and dependency parsing). In some areas, this shift has entailed substantial changes in how NLP systems are designed, such that deep neural network-based approaches may be viewed as a new paradigm distinct from statistical natural language processing. For instance, the term neural machine translation (NMT) emphasizes the fact that deep learning-based approaches to machine translation directly learn sequence-to-sequence transformations, obviating the need for intermediate steps such as word alignment and language modeling that was used in statistical machine translation (SMT). Latest works tend to use non-technical structure of a given task to build proper neural network.\\n\\n\\n== Common NLP tasks ==\\nThe following is a list of some of the most commonly researched tasks in natural language processing. Some of these tasks have direct real-world applications, while others more commonly serve as subtasks that are used to aid in solving larger tasks.\\nThough natural language processing tasks are closely intertwined, they can be subdivided into categories for convenience. A coarse division is given below.\\n\\n\\n=== Text and speech processing ===\\nOptical character recognition (OCR)\\nGiven an image representing printed text, determine the corresponding text.Speech recognition\\nGiven a sound clip of a person or people speaking, determine the textual representation of the speech.  This is the opposite of text to speech and is one of the extremely difficult problems colloquially termed \"AI-complete\" (see above).  In natural speech there are hardly any pauses between successive words, and thus speech segmentation is a necessary subtask of speech recognition (see below). In most spoken languages, the sounds representing successive letters blend into each other in a process termed coarticulation, so the conversion of the analog signal to discrete characters can be a very difficult process. Also, given that words in the same language are spoken by people with different accents, the speech recognition software must be able to recognize the wide variety of input as being identical to each other in terms of its textual equivalent.\\nSpeech segmentation\\nGiven a sound clip of a person or people speaking, separate it into words.  A subtask of speech recognition and typically grouped with it.Text-to-speech\\nGiven a text, transform those units and produce a spoken representation. Text-to-speech can be used to aid the visually impaired.Word segmentation (Tokenization)\\nSeparate a chunk of continuous text into separate words. For a language like English, this is fairly trivial, since words are usually separated by spaces. However, some written languages like Chinese, Japanese and Thai do not mark word boundaries in such a fashion, and in those languages text segmentation is a significant task requiring knowledge of the vocabulary and morphology of words in the language. Sometimes this process is also used in cases like bag of words (BOW) creation in data mining.\\n\\n\\n=== Morphological analysis ===\\nLemmatization\\nThe task of removing inflectional endings only and to return the base dictionary form of a word which is also known as a lemma. Lemmatization is another technique for reducing words to their normalized form. But in this case, the transformation actually uses a dictionary to map words to their actual form.\\nMorphological segmentation\\nSeparate words into individual morphemes and identify the class of the morphemes. The difficulty of this task depends greatly on the complexity of the morphology (i.e., the structure of words) of the language being considered. English has fairly simple morphology, especially inflectional morphology, and thus it is often possible to ignore this task entirely and simply model all possible forms of a word (e.g., \"open, opens, opened, opening\") as separate words. In languages such as Turkish or Meitei, a highly agglutinated Indian language, however, such an approach is not possible, as each dictionary entry has thousands of possible word forms.\\nPart-of-speech tagging\\nGiven a sentence, determine the part of speech (POS) for each word. Many words, especially common ones, can serve as multiple parts of speech. For example, \"book\" can be a noun (\"the book on the table\") or verb (\"to book a flight\"); \"set\" can be a noun, verb or adjective; and \"out\" can be any of at least five different parts of speech.Stemming\\nThe process of reducing inflected (or sometimes derived) words to a base form (e.g., \"close\" will be the root for \"closed\", \"closing\", \"close\", \"closer\" etc.). Stemming yields similar results as lemmatization, but does so on grounds of rules, not a dictionary.\\n\\n\\n=== Syntactic analysis ===\\nGrammar induction\\nGenerate a formal grammar that describes a language\\'s syntax.\\nSentence breaking (also known as \"sentence boundary disambiguation\")\\nGiven a chunk of text, find the sentence boundaries. Sentence boundaries are often marked by periods or other punctuation marks, but these same characters can serve other purposes (e.g., marking abbreviations).\\nParsing\\nDetermine the parse tree (grammatical analysis) of a given sentence. The grammar for natural languages is ambiguous and typical sentences have multiple possible analyses: perhaps surprisingly, for a typical sentence there may be thousands of potential parses (most of which will seem completely nonsensical to a human). There are two primary types of parsing: dependency parsing and constituency parsing. Dependency parsing focuses on the relationships between words in a sentence (marking things like primary objects and predicates), whereas constituency parsing focuses on building out the parse tree using a probabilistic context-free grammar (PCFG) (see also stochastic grammar).\\n\\n\\n=== Lexical semantics (of individual words in context) ===\\nLexical semantics\\nWhat is the computational meaning of individual words in context?\\nDistributional semantics\\nHow can we learn semantic representations from data?\\nNamed entity recognition (NER)\\nGiven a stream of text, determine which items in the text map to proper names, such as people or places, and what the type of each such name is (e.g. person, location, organization). Although capitalization can aid in recognizing named entities in languages such as English, this information cannot aid in determining the type of named entity, and in any case, is often inaccurate or insufficient.  For example, the first letter of a sentence is also capitalized, and named entities often span several words, only some of which are capitalized.  Furthermore, many other languages in non-Western scripts (e.g. Chinese or Arabic) do not have any capitalization at all, and even languages with capitalization may not consistently use it to distinguish names. For example, German capitalizes all nouns, regardless of whether they are names, and French and Spanish do not capitalize names that serve as adjectives.Sentiment analysis (see also Multimodal sentiment analysis)\\nExtract subjective information usually from a set of documents, often using online reviews to determine \"polarity\" about specific objects. It is especially useful for identifying trends of public opinion in social media, for marketing.Terminology extractionThe goal of terminology extraction is to automatically extract relevant terms from a given corpus.\\nWord sense disambiguation (WSD)\\nMany words have more than one meaning; we have to select the meaning which makes the most sense in context.  For this problem, we are typically given a list of words and associated word senses, e.g. from a dictionary or an online resource such as WordNet.\\nEntity linking\\nMany words - typically proper names - refer to named entities; here we have to select the entity (a famous individual, a location, a company, etc.) which is referred to in context.\\n\\n\\n=== Relational semantics (semantics of individual sentences) ===\\nRelationship extraction\\nGiven a chunk of text, identify the relationships among named entities (e.g. who is married to whom).\\nSemantic parsing\\nGiven a piece of text (typically a sentence), produce a formal representation of its semantics, either as a graph (e.g., in AMR parsing) or in accordance with a logical formalism (e.g., in DRT parsing). This challenge typically includes aspects of several more elementary NLP tasks from semantics (e.g., semantic role labelling, word sense disambiguation) and can be extended to include full-fledged discourse analysis (e.g., discourse analysis, coreference; see Natural language understanding below).\\nSemantic role labelling (see also implicit semantic role labelling below)\\nGiven a single sentence, identify and disambiguate semantic predicates (e.g., verbal frames), then identify and classify the frame elements (semantic roles).\\n\\n\\n=== Discourse (semantics beyond individual sentences) ===\\nCoreference resolution\\nGiven a sentence or larger chunk of text, determine which words (\"mentions\") refer to the same objects (\"entities\"). Anaphora resolution is a specific example of this task, and is specifically concerned with matching up pronouns with the nouns or names to which they refer. The more general task of coreference resolution also includes identifying so-called \"bridging relationships\" involving referring expressions. For example, in a sentence such as \"He entered John\\'s house through the front door\", \"the front door\" is a referring expression and the bridging relationship to be identified is the fact that the door being referred to is the front door of John\\'s house (rather than of some other structure that might also be referred to).\\nDiscourse analysis\\nThis rubric includes several related tasks.  One task is discourse parsing, i.e., identifying the discourse structure of a connected text, i.e. the nature of the discourse relationships between sentences (e.g. elaboration, explanation, contrast).  Another possible task is recognizing and classifying the speech acts in a chunk of text (e.g. yes-no question, content question, statement, assertion, etc.).Implicit semantic role labelling\\nGiven a single sentence, identify and disambiguate semantic predicates (e.g., verbal frames) and their explicit semantic roles in the current sentence (see Semantic role labelling above). Then, identify semantic roles that are not explicitly realized in the current sentence, classify them into arguments that are explicitly realized elsewhere in the text and those that are not specified, and resolve the former against the local text. A closely related task is zero anaphora resolution, i.e., the extension of coreference resolution to pro-drop languages.Recognizing textual entailment\\nGiven two text fragments, determine if one being true entails the other, entails the other\\'s negation, or allows the other to be either true or false.Topic segmentation and recognition\\nGiven a chunk of text, separate it into segments each of which is devoted to a topic, and identify the topic of the segment.Argument mining\\nThe goal of argument mining is the automatic extraction and identification of argumentative structures from natural language text with the aid of computer programs. Such argumentative structures include the premise, conclusions, the argument scheme and the relationship between the main and subsidiary argument, or the main and counter-argument within discourse.\\n\\n\\n=== Higher-level NLP applications ===\\nAutomatic summarization (text summarization)\\nProduce a readable summary of a chunk of text.  Often used to provide summaries of the text of a known type, such as research papers, articles in the financial section of a newspaper.\\nBook generation\\nNot an NLP task proper but an extension of natural language generation and other NLP tasks is the creation of full-fledged books. The first machine-generated book was created by a rule-based system in 1984 (Racter, The policeman\\'s beard is half-constructed). The first published work by a neural network was published in 2018, 1 the Road, marketed as a novel, contains sixty million words. Both these systems are basically elaborate but non-sensical (semantics-free) language models. The first machine-generated science book was published in 2019 (Beta Writer, Lithium-Ion Batteries, Springer, Cham). Unlike Racter and 1 the Road, this is grounded on factual knowledge and based on text summarization.\\nDialogue management\\nComputer systems intended to converse with a human.\\nDocument AI\\nA Document AI platform sits on top of the NLP technology enabling users with no prior experience of artificial intelligence, machine learning or NLP to quickly train a computer to extract the specific data they need from different document types. NLP-powered Document AI enables non-technical teams to quickly access information hidden in documents, for example, lawyers, business analysts and accountants.\\nGrammatical error correction\\nGrammatical error detection and correction involves a great band-width of problems on all levels of linguistic analysis (phonology/orthography, morphology, syntax, semantics, pragmatics). Grammatical error correction is impactful since it affects hundreds of millions of people that use or acquire English as a second language. It has thus been subject to a number of shared tasks since 2011. As far as orthography, morphology, syntax and certain aspects of semantics are concerned, and due to the development of powerful neural language models such as GPT-2, this can now (2019) be considered a largely solved problem and is being marketed in various commercial applications.\\nMachine translation\\nAutomatically translate text from one human language to another.  This is one of the most difficult problems, and is a member of a class of problems colloquially termed \"AI-complete\", i.e. requiring all of the different types of knowledge that humans possess (grammar, semantics, facts about the real world, etc.) to solve properly.\\nNatural language generation (NLG):\\nConvert information from computer databases or semantic intents into readable human language.\\nNatural language understanding (NLU)\\nConvert chunks of text into more formal representations such as first-order logic structures that are easier for computer programs to manipulate. Natural language understanding involves the identification of the intended semantic from the multiple possible semantics which can be derived from a natural language expression which usually takes the form of organized notations of natural language concepts. Introduction and creation of language metamodel and ontology are efficient however empirical solutions. An explicit formalization of natural language semantics without confusions with implicit assumptions such as closed-world assumption (CWA) vs. open-world assumption, or subjective Yes/No vs. objective True/False is expected for the construction of a basis of semantics formalization.\\nQuestion answering\\nGiven a human-language question, determine its answer. Typical questions have a specific right answer (such as \"What is the capital of Canada?\"), but sometimes open-ended questions are also considered (such as \"What is the meaning of life?\").\\n\\n\\n== General tendencies and (possible) future directions ==\\nBased on long-standing trends in the field, it is possible to extrapolate future directions of NLP. As of 2020, three trends among the topics of the long-standing series of CoNLL Shared Tasks can be observed:\\nInterest on increasingly abstract, \"cognitive\" aspects of natural language (1999-2001: shallow parsing, 2002-03: named entity recognition, 2006-09/2017-18: dependency syntax, 2004-05/2008-09 semantic role labelling, 2011-12 coreference, 2015-16: discourse parsing, 2019: semantic parsing).\\nIncreasing interest in multilinguality, and, potentially, multimodality (English since 1999; Spanish, Dutch since 2002; German since 2003; Bulgarian, Danish, Japanese, Portuguese, Slovenian, Swedish, Turkish since 2006; Basque, Catalan, Chinese, Greek, Hungarian, Italian, Turkish since 2007; Czech since 2009; Arabic since 2012; 2017: 40+ languages; 2018: 60+/100+ languages)\\nElimination of symbolic representations (rule-based over supervised towards weakly supervised methods, representation learning and end-to-end systems)\\n\\n\\n=== Cognition and NLP ===\\nMost higher-level NLP applications involve aspects that emulate intelligent behaviour and apparent comprehension of natural language. More broadly speaking, the technical operationalization of increasingly advanced aspects of cognitive behaviour represents one of the developmental trajectories of NLP (see trends among CoNLL shared tasks above).\\nCognition refers to \"the mental action or process of acquiring knowledge and understanding through thought, experience, and the senses.\" Cognitive science is the interdisciplinary, scientific study of the mind and its processes. Cognitive linguistics is an interdisciplinary branch of linguistics, combining knowledge and research from both psychology and linguistics. Especially during the age of symbolic NLP, the area of computational linguistics maintained strong ties with cognitive studies.\\nAs an example, George Lakoff offers a methodology to build natural language processing (NLP) algorithms through the perspective of cognitive science, along with the findings of cognitive linguistics, with two defining aspects:\\n\\nApply the theory of conceptual metaphor, explained by Lakoff as “the understanding of one idea, in terms of another” which provides an idea of the intent of the author. For example, consider the English word “big”. When used in a comparison (“That is a big tree”), the author\\'s intent is to imply that the tree is ”physically large” relative to other trees or the authors experience.  When used metaphorically (”Tomorrow is a big day”), the author’s intent to imply ”importance”.  The intent behind other usages, like in ”She is a big person” will remain somewhat ambiguous to a person and a cognitive NLP algorithm alike without additional information.\\nAssign relative measures of meaning to a word, phrase, sentence or piece of text based on the information presented before and after the piece of text being analyzed, e.g., by means of a probabilistic context-free grammar (PCFG). The mathematical equation for such algorithms is presented in US patent 9269353 :\\n  \\n    \\n      \\n        \\n          R\\n          M\\n          M\\n          (\\n          t\\n          o\\n          k\\n          e\\n          \\n            n\\n            \\n              N\\n            \\n          \\n          )\\n        \\n        =\\n        \\n          P\\n          M\\n          M\\n          (\\n          t\\n          o\\n          k\\n          e\\n          \\n            n\\n            \\n              N\\n            \\n          \\n          )\\n        \\n        ×\\n        \\n          \\n            1\\n            \\n              2\\n              d\\n            \\n          \\n        \\n        \\n          (\\n          \\n            \\n              ∑\\n              \\n                i\\n                =\\n                −\\n                d\\n              \\n              \\n                d\\n              \\n            \\n            \\n              (\\n              (\\n              P\\n              M\\n              M\\n              (\\n              t\\n              o\\n              k\\n              e\\n              \\n                n\\n                \\n                  N\\n                  −\\n                  1\\n                \\n              \\n              )\\n            \\n            ×\\n            \\n              P\\n              F\\n              (\\n              t\\n              o\\n              k\\n              e\\n              \\n                n\\n                \\n                  N\\n                \\n              \\n              ,\\n              t\\n              o\\n              k\\n              e\\n              \\n                n\\n                \\n                  N\\n                  −\\n                  1\\n                \\n              \\n              )\\n              \\n                )\\n                \\n                  i\\n                \\n              \\n            \\n          \\n          )\\n        \\n      \\n    \\n    {\\\\displaystyle {RMM(token_{N})}={PMM(token_{N})}\\\\times {\\\\frac {1}{2d}}\\\\left(\\\\sum _{i=-d}^{d}{((PMM(token_{N-1})}\\\\times {PF(token_{N},token_{N-1}))_{i}}\\\\right)}\\n  Where,\\nRMM, is the Relative Measure of Meaning\\ntoken, is any block of text, sentence, phrase or word\\nN, is the number of tokens being analyzed\\nPMM, is the Probable Measure of Meaning based on a corpora\\nd, is the location of the token along the sequence of N-1 tokens\\nPF, is the Probability Function specific to a languageTies with cognitive linguistics are part of the historical heritage of NLP, but they have been less frequently addressed since the statistical turn during the 1990s. Nevertheless, approaches to develop cognitive models towards technically operationalizable frameworks have been pursued in the context of various frameworks, e.g., of cognitive grammar, functional grammar, construction grammar, computational psycholinguistics and cognitive neuroscience (e.g., ACT-R), however, with limited uptake in mainstream NLP (as measured by presence on major conferences of the ACL). More recently, ideas of cognitive NLP have been revived as an approach to achieve explainability, e.g., under the notion of \"cognitive AI\". Likewise, ideas of cognitive NLP are inherent to neural models multimodal NLP (although rarely made explicit).\\n\\n\\n== See also ==\\n\\n\\n== References ==\\n\\n\\n== Further reading ==\\n\\n\\n== External link ==\\n Media related to Natural language processing at Wikimedia Commons', 'Robotics is an interdisciplinary branch of computer science and engineering. Robotics involves design, construction, operation, and use of robots. The goal of robotics is to design machines that can help and assist humans. Robotics integrates fields of mechanical engineering, electrical engineering, information engineering, mechatronics, electronics, bioengineering, computer engineering, control engineering, software engineering, mathematics, etc.\\nRobotics develops machines that can substitute for humans and replicate human actions. Robots can be used in many situations for many purposes, but today many are used in dangerous environments (including inspection of radioactive materials, bomb detection and deactivation), manufacturing processes, or where humans cannot survive (e.g. in space, underwater, in high heat, and clean up and containment of hazardous materials and radiation). Robots can take on any form, but some are made to resemble humans in appearance. This is claimed to help in the acceptance of robots in certain replicative behaviors which are usually performed by people. Such robots attempt to replicate walking, lifting, speech, cognition, or any other human activity. Many of today\\'s robots are inspired by nature, contributing to the field of bio-inspired robotics.\\nCertain robots require user input to operate while other robots function autonomously. The concept of creating robots that can operate autonomously dates back to classical times, but research into the functionality and potential uses of robots did not grow substantially until the 20th century. Throughout history, it has been frequently assumed by various scholars, inventors, engineers, and technicians that robots will one day be able to mimic human behavior and manage tasks in a human-like fashion. Today, robotics is a rapidly growing field, as technological advances continue; researching, designing, and building new robots serve various practical purposes, whether domestically, commercially, or militarily. Many robots are built to do jobs that are hazardous to people, such as defusing bombs, finding survivors in unstable ruins, and exploring mines and shipwrecks. Robotics is also used in STEM (science, technology, engineering, and mathematics) as a teaching aid.\\n\\n\\n== Etymology ==\\nThe word robotics was derived from the word robot, which was introduced to the public by Czech writer Karel Čapek in his play R.U.R. (Rossum\\'s Universal Robots), which was published in 1920. The word robot comes from the Slavic word robota, which means work/job. The play begins in a factory that makes artificial people called robots, creatures who can be mistaken for humans – very similar to the modern ideas of androids. Karel Čapek himself did not coin the word. He wrote a short letter in reference to an etymology in the Oxford English Dictionary in which he named his brother Josef Čapek as its actual originator.According to the Oxford English Dictionary, the word robotics was first used in print by Isaac Asimov, in his science fiction short story \"Liar!\", published in May 1941 in Astounding Science Fiction. Asimov was unaware that he was coining the term; since the science and technology of electrical devices is electronics, he assumed robotics already referred to the science and technology of robots. In some of Asimov\\'s other works, he states that the first use of the word robotics was in his short story Runaround (Astounding Science Fiction, March 1942), where he introduced his concept of The Three Laws of Robotics. However, the original publication of \"Liar!\" predates that of \"Runaround\" by ten months, so the former is generally cited as the word\\'s origin.\\n\\n\\n== History ==\\n\\nIn 1948, Norbert Wiener formulated the principles of cybernetics, the basis of practical robotics.\\nFully autonomous robots only appeared in the second half of the 20th century. The first digitally operated and programmable robot, the Unimate, was installed in 1961 to lift hot pieces of metal from a die casting machine and stack them. Commercial and industrial robots are widespread today and used to perform jobs more cheaply, more accurately and more reliably, than humans. They are also employed in some jobs which are too dirty, dangerous, or dull to be suitable for humans. Robots are widely used in manufacturing, assembly, packing and packaging, mining, transport, earth and space exploration, surgery, weaponry, laboratory research, safety, and the mass production of consumer and industrial goods.\\n\\n\\n== Robotic aspects ==\\n\\nThere are many types of robots; they are used in many different environments and for many different uses. Although being very diverse in application and form, they all share three basic similarities when it comes to their construction:\\n\\nRobots all have some kind of mechanical construction, a frame, form or shape designed to achieve a particular task. For example, a robot designed to travel across heavy dirt or mud, might use caterpillar tracks. The mechanical aspect is mostly the creator\\'s solution to completing the assigned task and dealing with the physics of the environment around it. Form follows function.\\nRobots have electrical components that power and control the machinery. For example, the robot with caterpillar tracks would need some kind of power to move the tracker treads. That power comes in the form of electricity, which will have to travel through a wire and originate from a battery, a basic electrical circuit. Even petrol powered machines that get their power mainly from petrol still require an electric current to start the combustion process which is why most petrol powered machines like cars, have batteries. The electrical aspect of robots is used for movement (through motors), sensing (where electrical signals are used to measure things like heat, sound, position, and energy status) and operation (robots need some level of electrical energy supplied to their motors and sensors in order to activate and perform basic operations)\\nAll robots contain some level of computer programming code. A program is how a robot decides when or how to do something. In the caterpillar track example, a robot that needs to move across a muddy road may have the correct mechanical construction and receive the correct amount of power from its battery, but would not go anywhere without a program telling it to move. Programs are the core essence of a robot, it could have excellent mechanical and electrical construction, but if its program is poorly constructed its performance will be very poor (or it may not perform at all). There are three different types of robotic programs: remote control, artificial intelligence and hybrid. A robot with remote control programming has a preexisting set of commands that it will only perform if and when it receives a signal from a control source, typically a human being with a remote control. It is perhaps more appropriate to view devices controlled primarily by human commands as falling in the discipline of automation rather than robotics. Robots that use artificial intelligence interact with their environment on their own without a control source, and can determine reactions to objects and problems they encounter using their preexisting programming. Hybrid is a form of programming that incorporates both AI and RC functions in them.\\n\\n\\n== Applications ==\\nAs more and more robots are designed for specific tasks, this method of classification becomes more relevant. For example, many robots are designed for assembly work, which may not be readily adaptable for other applications. They are termed as \"assembly robots\". For seam welding, some suppliers provide complete welding systems with the robot i.e. the welding equipment along with other material handling facilities like turntables, etc. as an integrated unit. Such an integrated robotic system is called a \"welding robot\" even though its discrete manipulator unit could be adapted to a variety of tasks. Some robots are specifically designed for heavy load manipulation, and are labeled as \"heavy-duty robots\".Current and potential applications include:\\n\\nMilitary robots.\\nIndustrial robots. Robots are increasingly used in manufacturing (since the 1960s). According to the Robotic Industries Association US data, in 2016 automotive industry was the main customer of industrial robots with 52% of total sales. In the auto industry, they can amount for more than half of the \"labor\". There are even \"lights off\" factories such as an IBM keyboard manufacturing factory in Texas that was fully automated as early as 2003.\\nCobots (collaborative robots).\\nConstruction robots. Construction robots can be separated into three types: traditional robots, robotic arm, and robotic exoskeleton.\\nAgricultural robots (AgRobots). The use of robots in agriculture is closely linked to the concept of AI-assisted precision agriculture and drone usage. 1996-1998 research also proved that robots can perform a herding task.\\nMedical robots of various types (such as da Vinci Surgical System and Hospi).\\nKitchen automation. Commercial examples of kitchen automation are Flippy (burgers), Zume Pizza (pizza), Cafe X (coffee), Makr Shakr (cocktails), Frobot (frozen yogurts) and Sally (salads). Home examples are Rotimatic (flatbreads baking) and Boris (dishwasher loading).\\nRobot combat for sport – hobby or sport event where two or more robots fight in an arena to disable each other. This has developed from a hobby in the 1990s to several TV series worldwide.\\nCleanup of contaminated areas, such as toxic waste or nuclear facilities.\\nDomestic robots.\\nNanorobots.\\nSwarm robotics.\\nAutonomous drones.\\nSports field line marking.\\n\\n\\n== Components ==\\n\\n\\n=== Power source ===\\n\\nAt present, mostly (lead–acid) batteries are used as a power source. Many different types of batteries can be used as a power source for robots. They range from lead–acid batteries, which are safe and have relatively long shelf lives but are rather heavy compared to silver–cadmium batteries that are much smaller in volume and are currently much more expensive. Designing a battery-powered robot needs to take into account factors such as safety, cycle lifetime and weight. Generators, often some type of internal combustion engine, can also be used. However, such designs are often mechanically complex and need a fuel, require heat dissipation and are relatively heavy. A tether connecting the robot to a power supply would remove the power supply from the robot entirely. This has the advantage of saving weight and space by moving all power generation and storage components elsewhere. However, this design does come with the drawback of constantly having a cable connected to the robot, which can be difficult to manage. Potential power sources could be:\\n\\npneumatic (compressed gases)\\nSolar power (using the sun\\'s energy and converting it into electrical power)\\nhydraulics (liquids)\\nflywheel energy storage\\norganic garbage (through anaerobic digestion)\\nnuclear\\n\\n\\n=== Actuation ===\\n\\nActuators are the \"muscles\" of a robot, the parts which convert stored energy into movement. By far the most popular actuators are electric motors that rotate a wheel or gear, and linear actuators that control industrial robots in factories. There are some recent advances in alternative types of actuators, powered by electricity, chemicals, or compressed air.\\n\\n\\n==== Electric motors ====\\n\\nThe vast majority of robots use electric motors, often brushed and brushless DC motors in portable robots or AC motors in industrial robots and CNC machines. These motors are often preferred in systems with lighter loads, and where the predominant form of motion is rotational.\\n\\n\\n==== Linear actuators ====\\n\\nVarious types of linear actuators move in and out instead of by spinning, and often have quicker direction changes, particularly when very large forces are needed such as with industrial robotics. They are typically powered by compressed and oxidized air (pneumatic actuator) or an oil (hydraulic actuator) Linear actuators can also be powered by electricity which usually consists of a motor and a leadscrew. Another common type is a mechanical linear actuator that is turned by hand, such as a rack and pinion on a car.\\n\\n\\n==== Series elastic actuators ====\\nSeries elastic actuation (SEA) relies on the idea of introducing intentional elasticity between the motor actuator and the load for robust force control. Due to the resultant lower reflected inertia, series elastic actuation improves safety when a robot interacts with the environment (e.g., humans or workpiece) or during collisions.  Furthermore, it also provides energy efficiency and shock absorption (mechanical filtering) while reducing excessive wear on the transmission and other mechanical components. This approach has successfully been employed in various robots, particularly advanced manufacturing robots and walking humanoid robots.The controller design of a series elastic actuator is most often performed within the passivity framework as it ensures the safety of interaction with unstructured environments. Despite its remarkable stability robustness, this framework suffers from the stringent limitations imposed on the controller which may trade-off performance. The reader is referred to the following survey which summarizes the common controller architectures for SEA along with the corresponding sufficient passivity conditions. One recent study has derived the necessary and sufficient passivity conditions for one of the most common impedance control architectures, namely velocity-sourced SEA. This work is of particular importance as it drives the non-conservative passivity bounds in an SEA scheme for the first time which allows a larger selection of control gains.\\n\\n\\n==== Air muscles ====\\n\\nPneumatic artificial muscles also known as air muscles, are special tubes that expand (typically up to 40%) when air is forced inside them. They are used in some robot applications.\\n\\n\\n==== Muscle wire ====\\n\\nMuscle wire, also known as shape memory alloy, Nitinol® or Flexinol® wire, is a material which contracts (under 5%) when electricity is applied. They have been used for some small robot applications.\\n\\n\\n==== Electroactive polymers ====\\n\\nEAPs or EPAMs are a plastic material that can contract substantially (up to 380% activation strain) from electricity, and have been used in facial muscles and arms of humanoid robots, and to enable new robots to float, fly, swim or walk.\\n\\n\\n==== Piezo motors ====\\n\\nRecent alternatives to DC motors are piezo motors or ultrasonic motors. These work on a fundamentally different principle, whereby tiny piezoceramic elements, vibrating many thousands of times per second, cause linear or rotary motion. There are different mechanisms of operation; one type uses the vibration of the piezo elements to step the motor in a circle or a straight line. Another type uses the piezo elements to cause a nut to vibrate or to drive a screw. The advantages of these motors are nanometer resolution, speed, and available force for their size. These motors are already available commercially, and being used on some robots.\\n\\n\\n==== Elastic nanotubes ====\\n\\nElastic nanotubes are a promising artificial muscle technology in early-stage experimental development. The absence of defects in carbon nanotubes enables these filaments to deform elastically by several percent, with energy storage levels of perhaps 10 J/cm3 for metal nanotubes. Human biceps could be replaced with an 8 mm diameter wire of this material. Such compact \"muscle\" might allow future robots to outrun and outjump humans.\\n\\n\\n=== Sensing ===\\n\\nSensors allow robots to receive information about a certain measurement of the environment, or internal components. This is essential for robots to perform their tasks, and act upon any changes in the environment to calculate the appropriate response. They are used for various forms of measurements, to give the robots warnings about safety or malfunctions, and to provide real-time information of the task it is performing.\\n\\n\\n==== Touch ====\\n\\nCurrent robotic and prosthetic hands receive far less tactile information than the human hand. Recent research has developed a tactile sensor array that mimics the mechanical properties and touch receptors of human fingertips. The sensor array is constructed as a rigid core surrounded by conductive fluid contained by an elastomeric skin. Electrodes are mounted on the surface of the rigid core and are connected to an impedance-measuring device within the core. When the artificial skin touches an object the fluid path around the electrodes is deformed, producing impedance changes that map the forces received from the object. The researchers expect that an important function of such artificial fingertips will be adjusting robotic grip on held objects.\\nScientists from several European countries and Israel developed a prosthetic hand in 2009, called SmartHand, which functions like a real one—allowing patients to write with it, type on a keyboard, play piano and perform other fine movements. The prosthesis has sensors which enable the patient to sense real feeling in its fingertips.\\n\\n\\n==== Vision ====\\n\\nComputer vision is the science and technology of machines that see. As a scientific discipline, computer vision is concerned with the theory behind artificial systems that extract information from images. The image data can take many forms, such as video sequences and views from cameras.\\nIn most practical computer vision applications, the computers are pre-programmed to solve a particular task, but methods based on learning are now becoming increasingly common.\\nComputer vision systems rely on image sensors which detect electromagnetic radiation which is typically in the form of either visible light or infra-red light. The sensors are designed using solid-state physics. The process by which light propagates and reflects off surfaces is explained using optics. Sophisticated image sensors even require quantum mechanics to provide a complete understanding of the image formation process. Robots can also be equipped with multiple vision sensors to be better able to compute the sense of depth in the environment. Like human eyes, robots\\' \"eyes\" must also be able to focus on a particular area of interest, and also adjust to variations in light intensities.\\nThere is a subfield within computer vision where artificial systems are designed to mimic the processing and behavior of biological system, at different levels of complexity. Also, some of the learning-based methods developed within computer vision have their background in biology.\\n\\n\\n==== Other ====\\nOther common forms of sensing in robotics use lidar, radar, and sonar. Lidar measures distance to a target by illuminating the target with laser light and measuring the reflected light with a sensor. Radar uses radio waves to determine the range, angle, or velocity of objects. Sonar uses sound propagation to navigate, communicate with or detect objects on or under the surface of the water.\\n\\n\\n=== Manipulation ===\\n\\nA definition of robotic manipulation has been provided by Matt Mason as: \"manipulation refers to an agent’s control of its environment through selective contact”.Robots need to manipulate objects; pick up, modify, destroy, or otherwise have an effect. Thus the functional end of a robot arm intended to make the effect (whether a hand, or tool) are often referred to as end effectors, while the \"arm\" is referred to as a manipulator. Most robot arms have replaceable end-effectors, each allowing them to perform some small range of tasks. Some have a fixed manipulator that cannot be replaced, while a few have one very general purpose manipulator, for example, a humanoid hand.\\n\\n\\n==== Mechanical grippers ====\\nOne of the most common types of end-effectors are \"grippers\". In its simplest manifestation, it consists of just two fingers that can open and close to pick up and let go of a range of small objects. Fingers can, for example, be made of a chain with a metal wire run through it. Hands that resemble and work more like a human hand include the Shadow Hand and the Robonaut hand. Hands that are of a mid-level complexity include the Delft hand. Mechanical grippers can come in various types, including friction and encompassing jaws. Friction jaws use all the force of the gripper to hold the object in place using friction. Encompassing jaws cradle the object in place, using less friction.\\n\\n\\n==== Suction end-effectors ====\\nSuction end-effectors, powered by vacuum generators, are very simple astrictive devices that can hold very large loads provided the prehension surface is smooth enough to ensure suction.\\nPick and place robots for electronic components and for large objects like car windscreens, often use very simple vacuum end-effectors.\\nSuction is a highly used type of end-effector in industry, in part because the natural compliance of soft suction end-effectors can enable a robot to be more robust in the presence of imperfect robotic perception.  As an example: consider the case of a robot vision system estimates the position of a water bottle, but has 1 centimeter of error.  While this may cause a rigid mechanical gripper to puncture the water bottle, the soft suction end-effector may just bend slightly and conform to the shape of the water bottle surface.\\n\\n\\n==== General purpose effectors ====\\nSome advanced robots are beginning to use fully humanoid hands, like the Shadow Hand, MANUS, and the Schunk hand. These are highly dexterous manipulators, with as many as 20 degrees of freedom and hundreds of tactile sensors.\\n\\n\\n=== Locomotion ===\\n\\n\\n==== Rolling robots ====\\n\\nFor simplicity, most mobile robots have four wheels or a number of continuous tracks. Some researchers have tried to create more complex wheeled robots with only one or two wheels. These can have certain advantages such as greater efficiency and reduced parts, as well as allowing a robot to navigate in confined places that a four-wheeled robot would not be able to.\\n\\n\\n===== Two-wheeled balancing robots =====\\nBalancing robots generally use a gyroscope to detect how much a robot is falling and then drive the wheels proportionally in the same direction, to counterbalance the fall at hundreds of times per second, based on the dynamics of an inverted pendulum. Many different balancing robots have been designed. While the Segway is not commonly thought of as a robot, it can be thought of as a component of a robot, when used as such Segway refer to them as RMP (Robotic Mobility Platform). An example of this use has been as NASA\\'s Robonaut that has been mounted on a Segway.\\n\\n\\n===== One-wheeled balancing robots =====\\n\\nA one-wheeled balancing robot is an extension of a two-wheeled balancing robot so that it can move in any 2D direction using a round ball as its only wheel. Several one-wheeled balancing robots have been designed recently, such as Carnegie Mellon University\\'s \"Ballbot\" that is the approximate height and width of a person, and Tohoku Gakuin University\\'s \"BallIP\". Because of the long, thin shape and ability to maneuver in tight spaces, they have the potential to function better than other robots in environments with people.\\n\\n\\n===== Spherical orb robots =====\\n\\nSeveral attempts have been made in robots that are completely inside a spherical ball, either by spinning a weight inside the ball, or by rotating the outer shells of the sphere. These have also been referred to as an orb bot or a ball bot.\\n\\n\\n===== Six-wheeled robots =====\\nUsing six wheels instead of four wheels can give better traction or grip in outdoor terrain such as on rocky dirt or grass.\\n\\n\\n===== Tracked robots =====\\n\\nTank tracks provide even more traction than a six-wheeled robot. Tracked wheels behave as if they were made of hundreds of wheels, therefore are very common for outdoor and military robots, where the robot must drive on very rough terrain. However, they are difficult to use indoors such as on carpets and smooth floors. Examples include NASA\\'s Urban Robot \"Urbie\".\\n\\n\\n==== Walking applied to robots ====\\nWalking is a difficult and dynamic problem to solve. Several robots have been made which can walk reliably on two legs, however, none have yet been made which are as robust as a human. There has been much study on human inspired walking, such as AMBER lab which was established in 2008 by the Mechanical Engineering Department at Texas A&M University. Many other robots have been built that walk on more than two legs, due to these robots being significantly easier to construct. Walking robots can be used for uneven terrains, which would provide better mobility and energy efficiency than other locomotion methods. Typically, robots on two legs can walk well on flat floors and can occasionally walk up stairs. None can walk over rocky, uneven terrain. Some of the methods which have been tried are:\\n\\n\\n===== ZMP technique =====\\n\\nThe zero moment point (ZMP) is the algorithm used by robots such as Honda\\'s ASIMO. The robot\\'s onboard computer tries to keep the total inertial forces (the combination of Earth\\'s gravity and the acceleration and deceleration of walking), exactly opposed by the floor reaction force (the force of the floor pushing back on the robot\\'s foot). In this way, the two forces cancel out, leaving no moment (force causing the robot to rotate and fall over). However, this is not exactly how a human walks, and the difference is obvious to human observers, some of whom have pointed out that ASIMO walks as if it needs the lavatory. ASIMO\\'s walking algorithm is not static, and some dynamic balancing is used (see below). However, it still requires a smooth surface to walk on.\\n\\n\\n===== Hopping =====\\nSeveral robots, built in the 1980s by Marc Raibert at the MIT Leg Laboratory, successfully demonstrated very dynamic walking. Initially, a robot with only one leg, and a very small foot could stay upright simply by hopping. The movement is the same as that of a person on a pogo stick. As the robot falls to one side, it would jump slightly in that direction, in order to catch itself. Soon, the algorithm was generalised to two and four legs. A bipedal robot was demonstrated running and even performing somersaults. A quadruped was also demonstrated which could trot, run, pace, and bound. For a full list of these robots, see the MIT Leg Lab Robots page.\\n\\n\\n===== Dynamic balancing (controlled falling) =====\\nA more advanced way for a robot to walk is by using a dynamic balancing algorithm, which is potentially more robust than the Zero Moment Point technique, as it constantly monitors the robot\\'s motion, and places the feet in order to maintain stability. This technique was recently demonstrated by Anybots\\' Dexter Robot, which is so stable, it can even jump. Another example is the TU Delft Flame.\\n\\n\\n===== Passive dynamics =====\\n\\nPerhaps the most promising approach utilizes passive dynamics where the momentum of swinging limbs is used for greater efficiency. It has been shown that totally unpowered humanoid mechanisms can walk down a gentle slope, using only gravity to propel themselves. Using this technique, a robot need only supply a small amount of motor power to walk along a flat surface or a little more to walk up a hill. This technique promises to make walking robots at least ten times more efficient than ZMP walkers, like ASIMO.\\n\\n\\n==== Other methods of locomotion ====\\n\\n\\n===== Flying =====\\nA modern passenger airliner is essentially a flying robot, with two humans to manage it. The autopilot can control the plane for each stage of the journey, including takeoff, normal flight, and even landing. Other flying robots are uninhabited and are known as unmanned aerial vehicles (UAVs). They can be smaller and lighter without a human pilot on board, and fly into dangerous territory for military surveillance missions. Some can even fire on targets under command. UAVs are also being developed which can fire on targets automatically, without the need for a command from a human. Other flying robots include cruise missiles, the Entomopter, and the Epson micro helicopter robot. Robots such as the Air Penguin, Air Ray, and Air Jelly have lighter-than-air bodies, propelled by paddles, and guided by sonar.\\n\\n\\n===== Snaking =====\\n\\nSeveral snake robots have been successfully developed. Mimicking the way real snakes move, these robots can navigate very confined spaces, meaning they may one day be used to search for people trapped in collapsed buildings. The Japanese ACM-R5 snake robot can even navigate both on land and in water.\\n\\n\\n===== Skating =====\\nA small number of skating robots have been developed, one of which is a multi-mode walking and skating device. It has four legs, with unpowered wheels, which can either step or roll. Another robot, Plen, can use a miniature skateboard or roller-skates, and skate across a desktop.\\n\\n\\n===== Climbing =====\\nSeveral different approaches have been used to develop robots that have the ability to climb vertical surfaces. One approach mimics the movements of a human climber on a wall with protrusions; adjusting the center of mass and moving each limb in turn to gain leverage. An example of this is Capuchin, built by Dr. Ruixiang Zhang at Stanford University, California. Another approach uses the specialized toe pad method of wall-climbing geckoes, which can run on smooth surfaces such as vertical glass. Examples of this approach include Wallbot and Stickybot.China\\'s Technology Daily reported on 15 November 2008, that Dr. Li Hiu Yeung and his research group of New Concept Aircraft (Zhuhai) Co., Ltd. had successfully developed a bionic gecko robot named \"Speedy Freelander\". According to Dr. Yeung, the gecko robot could rapidly climb up and down a variety of building walls, navigate through ground and wall fissures, and walk upside-down on the ceiling. It was also able to adapt to the surfaces of smooth glass, rough, sticky or dusty walls as well as various types of metallic materials. It could also identify and circumvent obstacles automatically. Its flexibility and speed were comparable to a natural gecko. A third approach is to mimic the motion of a snake climbing a pole.\\n\\n\\n===== Swimming (Piscine) =====\\nIt is calculated that when swimming some fish can achieve a propulsive efficiency greater than 90%. Furthermore, they can accelerate and maneuver far better than any man-made boat or submarine, and produce less noise and water disturbance. Therefore, many researchers studying underwater robots would like to copy this type of locomotion. Notable examples are the Essex University Computer Science Robotic Fish G9, and the Robot Tuna built by the Institute of Field Robotics, to analyze and mathematically model thunniform motion. The Aqua Penguin, designed and built by Festo of Germany, copies the streamlined shape and propulsion by front \"flippers\" of penguins. Festo have also built the Aqua Ray and Aqua Jelly, which emulate the locomotion of manta ray, and jellyfish, respectively.\\n\\nIn 2014 iSplash-II was developed by PhD student Richard James Clapham and Prof. Huosheng Hu at Essex University. It was the first robotic fish capable of outperforming real carangiform fish in terms of average maximum velocity (measured in body lengths/ second) and endurance, the duration that top speed is maintained. This build attained swimming speeds of 11.6BL/s (i.e. 3.7 m/s). The first build, iSplash-I (2014) was the first robotic platform to apply a full-body length carangiform swimming motion which was found to increase swimming speed by 27% over the traditional approach of a posterior confined waveform.\\n\\n\\n===== Sailing =====\\n\\nSailboat robots have also been developed in order to make measurements at the surface of the ocean. A typical sailboat robot is Vaimos built by IFREMER and ENSTA-Bretagne. Since the propulsion of sailboat robots uses the wind, the energy of the batteries is only used for the computer, for the communication and for the actuators (to tune the rudder and the sail). If the robot is equipped with solar panels, the robot could theoretically navigate forever. The two main competitions of sailboat robots are WRSC, which takes place every year in Europe, and Sailbot.\\n\\n\\n=== Environmental interaction and navigation ===\\n\\nThough a significant percentage of robots in commission today are either human controlled or operate in a static environment, there is an increasing interest in robots that can operate autonomously in a dynamic environment. These robots require some combination of navigation hardware and software in order to traverse their environment. In particular, unforeseen events (e.g. people and other obstacles that are not stationary) can cause problems or collisions. Some highly advanced robots such as ASIMO and Meinü robot have particularly good robot navigation hardware and software. Also, self-controlled cars, Ernst Dickmanns\\' driverless car, and the entries in the DARPA Grand Challenge, are capable of sensing the environment well and subsequently making navigational decisions based on this information, including by a swarm of autonomous robots. Most of these robots employ a GPS navigation device with waypoints, along with radar, sometimes combined with other sensory data such as lidar, video cameras, and inertial guidance systems for better navigation between waypoints.\\n\\n\\n=== Human-robot interaction ===\\n\\nThe state of the art in sensory intelligence for robots will have to progress through several orders of magnitude if we want the robots working in our homes to go beyond vacuum-cleaning the floors. If robots are to work effectively in homes and other non-industrial environments, the way they are instructed to perform their jobs, and especially how they will be told to stop will be of critical importance. The people who interact with them may have little or no training in robotics, and so any interface will need to be extremely intuitive. Science fiction authors also typically assume that robots will eventually be capable of communicating with humans through speech, gestures, and facial expressions, rather than a command-line interface. Although speech would be the most natural way for the human to communicate, it is unnatural for the robot. It will probably be a long time before robots interact as naturally as the fictional C-3PO, or Data of Star Trek, Next Generation. Even though the current state of robotics cannot meet the standards of these robots from science-fiction, robotic media characters (e.g., Wall-E, R2-D2) can elicit audience sympathies that increase people\\'s willingness to accept actual robots in the future. Acceptance of social robots is also likely to increase if people can meet a social robot under appropriate conditions. Studies have shown that interacting with a robot by looking at, touching, or even imagining interacting with the robot can reduce negative feelings that some people have about robots before interacting with them. However, if pre-existing negative sentiments are especially strong, interacting with a robot can increase those negative feelings towards robots.\\n\\n\\n==== Speech recognition ====\\n\\nInterpreting the continuous flow of sounds coming from a human, in real time, is a difficult task for a computer, mostly because of the great variability of speech. The same word, spoken by the same person may sound different depending on local acoustics, volume, the previous word, whether or not the speaker has a cold, etc.. It becomes even harder when the speaker has a different accent. Nevertheless, great strides have been made in the field since Davis, Biddulph, and Balashek designed the first \"voice input system\" which recognized \"ten digits spoken by a single user with 100% accuracy\" in 1952. Currently, the best systems can recognize continuous, natural speech, up to 160 words per minute, with an accuracy of 95%. With the help of artificial intelligence, machines nowadays can use people\\'s voice to identify their emotions such as satisfied or angry\\n\\n\\n==== Robotic voice ====\\nOther hurdles exist when allowing the robot to use voice for interacting with humans. For social reasons, synthetic voice proves suboptimal as a communication medium, making it necessary to develop the emotional component of robotic voice through various techniques. An advantage of diphonic branching is the emotion that the robot is programmed to project, can be carried on the voice tape, or phoneme, already pre-programmed onto the voice media. One of the earliest examples is a teaching robot named Leachim developed in 1974 by Michael J. Freeman. Leachim was able to convert digital memory to rudimentary verbal speech on pre-recorded computer discs. It was programmed to teach students in The Bronx, New York.\\n\\n\\n==== Gestures ====\\n\\nOne can imagine, in the future, explaining to a robot chef how to make a pastry, or asking directions from a robot police officer. In both of these cases, making hand gestures would aid the verbal descriptions. In the first case, the robot would be recognizing gestures made by the human, and perhaps repeating them for confirmation. In the second case, the robot police officer would gesture to indicate \"down the road, then turn right\". It is likely that gestures will make up a part of the interaction between humans and robots. A great many systems have been developed to recognize human hand gestures.\\n\\n\\n==== Facial expression ====\\n\\nFacial expressions can provide rapid feedback on the progress of a dialog between two humans, and soon may be able to do the same for humans and robots. Robotic faces have been constructed by Hanson Robotics using their elastic polymer called Frubber, allowing a large number of facial expressions due to the elasticity of the rubber facial coating and embedded subsurface motors (servos). The coating and servos are built on a metal skull. A robot should know how to approach a human, judging by their facial expression and body language. Whether the person is happy, frightened, or crazy-looking affects the type of interaction expected of the robot. Likewise, robots like Kismet and the more recent addition, Nexi can produce a range of facial expressions, allowing it to have meaningful social exchanges with humans.\\n\\n\\n==== Artificial emotions ====\\nArtificial emotions can also be generated, composed of a sequence of facial expressions or gestures. As can be seen from the movie Final Fantasy: The Spirits Within, the programming of these artificial emotions is complex and requires a large amount of human observation. To simplify this programming in the movie, presets were created together with a special software program. This decreased the amount of time needed to make the film. These presets could possibly be transferred for use in real-life robots. An example of a robot with artificial emotions is Robin the Robot developed by an Armenian IT company Expper Technologies, which uses AI-based peer-to-peer interaction. Its main task is achieving emotional well-being, i.e. overcome stress and anxiety. Robin was trained to analyze facial expressions and use his face to display his emotions given the context. The robot has been tested by kids in US clinics, and observations show that Robin increased the appetite and cheerfulness of children after meeting and talking.\\n\\n\\n==== Personality ====\\nMany of the robots of science fiction have a personality, something which may or may not be desirable in the commercial robots of the future. Nevertheless, researchers are trying to create robots which appear to have a personality: i.e. they use sounds, facial expressions, and body language to try to convey an internal state, which may be joy, sadness, or fear. One commercial example is Pleo, a toy robot dinosaur, which can exhibit several apparent emotions.\\n\\n\\n==== Social intelligence ====\\nThe Socially Intelligent Machines Lab of the Georgia Institute of Technology researches new concepts of guided teaching interaction with robots. The aim of the projects is a social robot that learns task and goals from human demonstrations without prior knowledge of high-level concepts. These new concepts are grounded from low-level continuous sensor data through unsupervised learning, and task goals are subsequently learned using a Bayesian approach. These concepts can be used to transfer knowledge to future tasks, resulting in faster learning of those tasks. The results are demonstrated by the robot Curi who can scoop some pasta from a pot onto a plate and serve the sauce on top.\\n\\n\\n== Control ==\\n\\nThe mechanical structure of a robot must be controlled to perform tasks. The control of a robot involves three distinct phases – perception, processing, and action (robotic paradigms). Sensors give information about the environment or the robot itself (e.g. the position of its joints or its end effector). This information is then processed to be stored or transmitted and to calculate the appropriate signals to the actuators (motors) which move the mechanical.\\nThe processing phase can range in complexity. At a reactive level, it may translate raw sensor information directly into actuator commands. Sensor fusion may first be used to estimate parameters of interest (e.g. the position of the robot\\'s gripper) from noisy sensor data. An immediate task (such as moving the gripper in a certain direction) is inferred from these estimates. Techniques from control theory convert the task into commands that drive the actuators.\\nAt longer time scales or with more sophisticated tasks, the robot may need to build and reason with a \"cognitive\" model. Cognitive models try to represent the robot, the world, and how they interact. Pattern recognition and computer vision can be used to track objects. Mapping techniques can be used to build maps of the world. Finally, motion planning and other artificial intelligence techniques may be used to figure out how to act. For example, a planner may figure out how to achieve a task without hitting obstacles, falling over, etc.\\n\\n\\n=== Autonomy levels ===\\n\\nControl systems may also have varying levels of autonomy.\\n\\nDirect interaction is used for haptic or teleoperated devices, and the human has nearly complete control over the robot\\'s motion.\\nOperator-assist modes have the operator commanding medium-to-high-level tasks, with the robot automatically figuring out how to achieve them.\\nAn autonomous robot may go without human interaction for extended periods of time . Higher levels of autonomy do not necessarily require more complex cognitive capabilities. For example, robots in assembly plants are completely autonomous but operate in a fixed pattern.Another classification takes into account the interaction between human control and the machine motions.\\n\\nTeleoperation. A human controls each movement, each machine actuator change is specified by the operator.\\nSupervisory. A human specifies general moves or position changes and the machine decides specific movements of its actuators.\\nTask-level autonomy. The operator specifies only the task and the robot manages itself to complete it.\\nFull autonomy. The machine will create and complete all its tasks without human interaction.\\n\\n\\n== Research ==\\n\\nMuch of the research in robotics focuses not on specific industrial tasks, but on investigations into new types of robots, alternative ways to think about or design robots, and new ways to manufacture them. Other investigations, such as MIT\\'s cyberflora project, are almost wholly academic.\\nA first particular new innovation in robot design is the open sourcing of robot-projects. To describe the level of advancement of a robot, the term \"Generation Robots\" can be used. This term is coined by Professor Hans Moravec, Principal Research Scientist at the Carnegie Mellon University Robotics Institute in describing the near future evolution of robot technology. First generation robots, Moravec predicted in 1997, should have an intellectual capacity comparable to perhaps a lizard and should become available by 2010. Because the first generation robot would be incapable of learning, however, Moravec predicts that the second generation robot would be an improvement over the first and become available by 2020, with the intelligence maybe comparable to that of a mouse. The third generation robot should have the intelligence comparable to that of a monkey. Though fourth generation robots, robots with human intelligence, professor Moravec predicts, would become possible, he does not predict this happening before around 2040 or 2050.The second is evolutionary robots. This is a methodology that uses evolutionary computation to help design robots, especially the body form, or motion and behavior controllers. In a similar way to natural evolution, a large population of robots is allowed to compete in some way, or their ability to perform a task is measured using a fitness function. Those that perform worst are removed from the population and replaced by a new set, which have new behaviors based on those of the winners. Over time the population improves, and eventually a satisfactory robot may appear. This happens without any direct programming of the robots by the researchers. Researchers use this method both to create better robots, and to explore the nature of evolution. Because the process often requires many generations of robots to be simulated, this technique may be run entirely or mostly in simulation, using a robot simulator software package, then tested on real robots once the evolved algorithms are good enough. Currently, there are about 10 million industrial robots toiling around the world, and Japan is the top country having high density of utilizing robots in its manufacturing industry.\\n\\n\\n=== Dynamics and kinematics ===\\n\\nThe study of motion can be divided into kinematics and dynamics. Direct kinematics or forward kinematics refers to the calculation of end effector position, orientation, velocity, and acceleration when the corresponding joint values are known. Inverse kinematics refers to the opposite case in which required joint values are calculated for given end effector values, as done in path planning. Some special aspects of kinematics include handling of redundancy (different possibilities of performing the same movement), collision avoidance, and singularity avoidance. Once all relevant positions, velocities, and accelerations have been calculated using kinematics, methods from the field of dynamics are used to study the effect of forces upon these movements. Direct dynamics refers to the calculation of accelerations in the robot once the applied forces are known. Direct dynamics is used in computer simulations of the robot. Inverse dynamics refers to the calculation of the actuator forces necessary to create a prescribed end-effector acceleration. This information can be used to improve the control algorithms of a robot.\\nIn each area mentioned above, researchers strive to develop new concepts and strategies, improve existing ones, and improve the interaction between these areas. To do this, criteria for \"optimal\" performance and ways to optimize design, structure, and control of robots must be developed and implemented.\\n\\n\\n=== Bionics and biomimetics ===\\nBionics and biomimetics apply the physiology and methods of locomotion of animals to the design of robots.  For example, the design of BionicKangaroo was based on the way kangaroos jump.\\n\\n\\n=== Quantum computing ===\\nThere has been some research into whether robotics algorithms can be run more quickly on quantum computers than they can be run on digital computers. This area has been referred to as quantum robotics.\\n\\n\\n== Education and training ==\\n\\nRobotics engineers design robots, maintain them, develop new applications for them, and conduct research to expand the potential of robotics. Robots have become a popular educational tool in some middle and high schools, particularly in parts of the USA, as well as in numerous youth summer camps, raising interest in programming, artificial intelligence, and robotics among students.\\n\\n\\n=== Career training ===\\nUniversities  like Worcester Polytechnic Institute (WPI) offer bachelors, masters, and doctoral degrees in the field of robotics. Vocational schools offer robotics training aimed at careers in robotics.\\n\\n\\n=== Certification ===\\nThe Robotics Certification Standards Alliance (RCSA) is an international robotics certification authority that confers various industry- and educational-related robotics certifications.\\n\\n\\n=== Summer robotics camp ===\\nSeveral national summer camp programs include robotics as part of their core curriculum. In addition, youth summer robotics programs are frequently offered by celebrated museums and institutions.\\n\\n\\n=== Robotics competitions ===\\n\\nThere are many competitions around the globe. The SeaPerch curriculum is aimed as students of all ages. This is a short list of competition examples; for a more complete list see Robot competition.\\n\\n\\n==== Competitions for Younger Children ====\\nThe FIRST organization offers the FIRST Lego League Jr. competitions for younger children. This competition\\'s goal is to offer younger children an opportunity to start learning about science and technology. Children in this competition build Lego models and have the option of using the Lego WeDo robotics kit.\\n\\n\\n==== Competitions for Children Ages 9-14 ====\\nOne of the most important competitions is the FLL or FIRST Lego League. The idea of this specific competition is that kids start developing knowledge and getting into robotics while playing with Lego since they are nine years old. This competition is associated with National Instruments. Children use Lego Mindstorms to solve autonomous robotics challenges in this competition.\\n\\n\\n==== Competitions for Teenagers ====\\n\\nThe FIRST Tech Challenge is designed for intermediate students, as a transition from the FIRST Lego League to the FIRST Robotics Competition.\\nThe FIRST Robotics Competition focuses more on mechanical design, with a specific game being played each year. Robots are built specifically for that year\\'s game. In match play, the robot moves autonomously during the first 15 seconds of the game (although certain years such as 2019\\'s Deep Space change this rule), and is manually operated for the rest of the match.\\n\\n\\n==== Competitions for Older Students ====\\nThe various RoboCup competitions include teams of teenagers and university students.  These competitions focus on soccer competitions with different types of robots, dance competitions, and urban search and rescue competitions. All of the robots in these competitions must be autonomous. Some of these competitions focus on simulated robots.\\nAUVSI runs competitions for flying robots, robot boats, and underwater robots.\\nThe Student AUV Competition Europe (SAUC-E) mainly attracts undergraduate and graduate student teams. As in the AUVSI competitions, the robots must be fully autonomous while they are participating in the competition.\\nThe Microtransat Challenge is a competition to sail a boat across the Atlantic Ocean.\\n\\n\\n==== Competitions Open to Anyone ====\\nRoboGames is open to anyone wishing to compete in their over 50 categories of robot competitions.\\nFederation of International Robot-soccer Association holds the FIRA World Cup competitions. There are flying robot competitions, robot soccer competitions, and other challenges, including weightlifting barbells made from dowels and CDs.\\n\\n\\n=== Robotics afterschool programs ===\\nMany schools across the country are beginning to add robotics programs to their after school curriculum. Some major programs for afterschool robotics include FIRST Robotics Competition, Botball and B.E.S.T. Robotics. Robotics competitions often include aspects of business and marketing as well as engineering and design.\\nThe Lego company began a program for children to learn and get excited about robotics at a young age.\\n\\n\\n=== Decolonial Educational Robotics ===\\nDecolonial Educational Robotics is a branch of Decolonial Technology, and Decolonial A.I., practiced in various places around the world. This methodology is summarized in pedagogical theories and practices such as Pedagogy of the Oppressed and Montessori methods. And it aims at teaching robotics from the local culture, to pluralize and mix technological knowledge.\\n\\n\\n== Employment ==\\n\\nRobotics is an essential component in many modern manufacturing environments. As factories increase their use of robots, the number of robotics–related jobs grow and have been observed to be steadily rising. The employment of robots in industries has increased productivity and efficiency savings and is typically seen as a long-term investment for benefactors. A paper by Michael Osborne and Carl Benedikt Frey found that 47 per cent of US jobs are at risk to automation \"over some unspecified number of years\". These claims have been criticized on the ground that social policy, not AI, causes unemployment. In a 2016 article in The Guardian, Stephen Hawking stated \"The automation of factories has already decimated jobs in traditional manufacturing, and the rise of artificial intelligence is likely to extend this job destruction deep into the middle classes, with only the most caring, creative or supervisory roles remaining\".According to a GlobalData September 2021 report, the robotics industry was worth $45bn in 2020, and by 2030, it will have grown at a compound annual growth rate (CAGR) of 29% to $568bn, driving jobs in robotics and related industries.\\n\\n\\n== Occupational safety and health implications ==\\n\\nA discussion paper drawn up by EU-OSHA highlights how the spread of robotics presents both opportunities and challenges for occupational safety and health (OSH).The greatest OSH benefits stemming from the wider use of robotics should be substitution for people working in unhealthy or dangerous environments. In space, defence, security, or the nuclear industry, but also in logistics, maintenance, and inspection, autonomous robots are particularly useful in replacing human workers performing dirty, dull or unsafe tasks, thus avoiding workers\\' exposures to hazardous agents and conditions and reducing physical, ergonomic and psychosocial risks. For example, robots are already used to perform repetitive and monotonous tasks, to handle radioactive material or to work in explosive atmospheres. In the future, many other highly repetitive, risky or unpleasant tasks will be performed by robots in a variety of sectors like agriculture, construction, transport, healthcare, firefighting or cleaning services.Despite these advances, there are certain skills to which humans will be better suited than machines for some time to come and the question is how to achieve the best combination of human and robot skills. The advantages of robotics include heavy-duty jobs with precision and repeatability, whereas the advantages of humans include creativity, decision-making, flexibility, and adaptability. This need to combine optimal skills has resulted in collaborative robots and humans sharing a common workspace more closely and led to the development of new approaches and standards to guarantee the safety of the \"man-robot merger\". Some European countries are including robotics in their national programmes and trying to promote a safe and flexible co-operation between robots and operators to achieve better productivity. For example, the German Federal Institute for Occupational Safety and Health (BAuA) organises annual workshops on the topic \"human-robot collaboration\".\\nIn the future, co-operation between robots and humans will be diversified, with robots increasing their autonomy and human-robot collaboration reaching completely new forms. Current approaches and technical standards aiming to protect employees from the risk of working with collaborative robots will have to be revised.\\n\\n\\n== See also ==\\n\\n\\n== References ==\\n\\n\\n== Further reading ==\\nR. Andrew Russell (1990). Robot Tactile Sensing. New York: Prentice Hall. ISBN 978-0-13-781592-0.\\nE McGaughey, \\'Will Robots Automate Your Job Away? Full Employment, Basic Income, and Economic Democracy\\' (2018) SSRN, part 2(3)\\nDH Autor, ‘Why Are There Still So Many Jobs? The History and Future of Workplace Automation’ (2015) 29(3) Journal of Economic Perspectives 3\\nTooze, Adam, \"Democracy and Its Discontents\", The New York Review of Books, vol. LXVI, no. 10 (6 June 2019), pp. 52–53, 56–57.  \"Democracy has no clear answer for the mindless operation of bureaucratic and technological power.  We may indeed be witnessing its extension in the form of artificial intelligence and robotics.  Likewise, after decades of dire warning, the environmental problem remains fundamentally unaddressed.... Bureaucratic overreach and environmental catastrophe are precisely the kinds of slow-moving existential challenges that democracies deal with very badly.... Finally, there is the threat du jour: corporations and the technologies they promote.\"  (pp. 56–57.)\\n\\n\\n== External links ==\\n\\nRobotics at Curlie\\nIEEE Robotics and Automation Society\\nInvestigation of social robots – Robots that mimic human behaviors and gestures.\\nWired\\'s guide to the \\'50 best robots ever\\', a mix of robots in fiction (Hal, R2D2, K9) to real robots (Roomba, Mobot, Aibo).', 'A computer network is a set of computers sharing resources located on or provided by network nodes. The computers use common communication protocols over digital interconnections to communicate with each other. These interconnections are made up of telecommunication network technologies, based on physically wired, optical, and wireless radio-frequency methods that may be arranged in a variety of network topologies.\\nThe nodes of a computer network may include personal computers, servers, networking hardware, or other specialised or general-purpose hosts. They are identified by network addresses, and may have  hostnames. Hostnames serve as memorable labels for the nodes, rarely changed after initial assignment. Network addresses serve for locating and identifying the nodes by communication protocols such as the Internet Protocol.\\nComputer networks may be classified by many criteria, including the transmission medium used to carry signals,  bandwidth, communications protocols to organize network traffic, the network size, the topology, traffic control mechanism, and organizational intent.\\nComputer networks support many applications and services, such as access to the World Wide Web, digital video, digital audio, shared use of application and storage servers, printers, and fax machines, and use of email and instant messaging applications.\\n\\n\\n== History ==\\nComputer networking may be considered a branch of computer science, computer engineering, and telecommunications, since it relies on the theoretical and practical application of the related disciplines. Computer networking was influenced by a wide array of technology developments and historical milestones.\\n\\nIn the late 1950s, a network of computers was built for the U.S. military Semi-Automatic Ground Environment (SAGE) radar system using the Bell 101 modem. It was the first commercial modem for computers, released by AT&T Corporation in 1958. The modem allowed digital data to be transmitted over regular unconditioned telephone lines at a speed of 110 bits per second (bit/s).\\nIn 1959, Christopher Strachey filed a patent application for time-sharing and John McCarthy initiated the first project to implement time-sharing of user programs at MIT. Stratchey passed the concept on to J. C. R. Licklider at the inaugural UNESCO Information Processing Conference in Paris that year. McCarthy was instrumental in the creation of three of the earliest time-sharing systems (Compatible Time-Sharing System in 1961, BBN Time-Sharing System in 1962, and Dartmouth Time Sharing System in 1963).\\nIn 1959, Anatoly Kitov proposed to the Central Committee of the Communist Party of the Soviet Union a detailed plan for the re-organisation of the control of the Soviet armed forces and of the Soviet economy on the basis of a network of computing centres.\\nIn 1960, the commercial airline reservation system semi-automatic business research environment (SABRE) went online with two connected mainframes.\\nIn 1963, J. C. R. Licklider sent a memorandum to office colleagues discussing the concept of the \"Intergalactic Computer Network\", a computer network intended to allow general communications among computer users.\\nThroughout the 1960s, Paul Baran and Donald Davies independently developed the concept of packet switching to transfer information between computers over a network. Davies pioneered the implementation of the concept. The NPL network, a local area network at the National Physical Laboratory (United Kingdom) used a line speed of 768 kbit/s and later high-speed T1 links (1.544 Mbit/s line rate).\\nIn 1965, Western Electric introduced the first widely used telephone switch that implemented computer control in the switching fabric.\\nIn 1969, the first four nodes of the ARPANET were connected using 50 kbit/s circuits between the University of California at Los Angeles, the Stanford Research Institute, the University of California at Santa Barbara, and the University of Utah. In the early 1970s, Leonard Kleinrock carried out mathematical work to model the performance of packet-switched networks, which underpinned the development of the ARPANET. His theoretical work on hierarchical routing in the late 1970s with student Farouk Kamoun remains critical to the operation of the Internet today.\\nIn 1972, commercial services were first deployed on public data networks in Europe, which began using X.25 in the late 1970s and spread across the globe. The underlying infrastructure was used for expanding TCP/IP networks in the 1980s.\\nIn 1973, the French CYCLADES network was the first to make the hosts responsible for the reliable delivery of data, rather than this being a centralized service of the network itself.\\nIn 1973, Robert Metcalfe wrote a formal memo at Xerox PARC describing Ethernet, a networking system that was based on the Aloha network, developed in the 1960s by Norman Abramson and colleagues at the University of Hawaii. In July 1976, Robert Metcalfe and David Boggs published their paper \"Ethernet: Distributed Packet Switching for Local Computer Networks\" and collaborated on several patents received in 1977 and 1978.\\nIn 1974, Vint Cerf, Yogen Dalal, and Carl Sunshine published the Transmission Control Protocol (TCP) specification, RFC 675, coining the term Internet as a shorthand for internetworking.\\nIn 1976, John Murphy of Datapoint Corporation created ARCNET, a token-passing network first used to share storage devices.\\nIn 1977, the first long-distance fiber network was deployed by GTE in Long Beach, California.\\nIn 1977, Xerox Network Systems (XNS) was developed by Robert Metcalfe and Yogen Dalal at Xerox.\\nIn 1979, Robert Metcalfe pursued making Ethernet an open standard.\\nIn 1980, Ethernet was upgraded from the original 2.94 Mbit/s protocol to the 10 Mbit/s protocol, which was developed by Ron Crane, Bob Garner, Roy Ogus, and Yogen Dalal.\\nIn 1995, the transmission speed capacity for Ethernet increased from 10 Mbit/s to 100 Mbit/s. By 1998, Ethernet supported transmission speeds of 1 Gbit/s. Subsequently, higher speeds of up to 400 Gbit/s were added (as of 2018). The scaling of Ethernet has been a contributing factor to its continued use.\\n\\n\\n== Use ==\\nA computer network extends interpersonal communications by electronic means with various technologies, such as email, instant messaging, online chat, voice and video telephone calls, and video conferencing. A network allows sharing of network and computing resources. Users may access and use resources provided by devices on the network, such as printing a document on a shared network printer or use of a shared storage device. A network allows sharing of files, data, and other types of information giving authorized users the ability to access information stored on other computers on the network. Distributed computing uses computing resources across a network to accomplish tasks.\\n\\n\\n== Network packet ==\\nMost modern computer networks use protocols based on packet-mode transmission. A network packet is a formatted unit of data carried by a packet-switched network.\\nPackets consist of two types of data: control information and user data (payload). The control information provides data the network needs to deliver the user data, for example, source and destination network addresses, error detection codes, and sequencing information. Typically, control information is found in packet headers and trailers, with payload data in between.\\nWith packets, the bandwidth of the transmission medium can be better shared among users than if the network were circuit switched. When one user is not sending packets, the link can be filled with packets from other users, and so the cost can be shared, with relatively little interference, provided the link isn\\'t overused. Often the route a packet needs to take through a network is not immediately available. In that case, the packet is queued and waits until a link is free.\\nThe physical link technologies of packet network typically limit the size of packets to a certain maximum transmission unit (MTU). A longer message may be fragmented before it is transferred and once the packets arrive, they are reassembled to construct the original message.\\n\\n\\n== Network topology ==\\n\\nNetwork topology is the layout, pattern, or organizational hierarchy of the interconnection of network hosts, in contrast to their physical or geographic location. Typically, most diagrams describing networks are arranged by their topology. The network topology can affect throughput, but reliability is often more critical. With many technologies, such as bus or star networks, a single failure can cause the network to fail entirely. In general, the more interconnections there are, the more robust the network is; but the more expensive it is to install.\\nCommon layouts are:\\n\\nBus network: all nodes are connected to a common medium along this medium. This was the layout used in the original Ethernet, called 10BASE5 and 10BASE2. This is still a common topology on the data link layer, although modern physical layer variants use point-to-point links instead.\\nStar network: all nodes are connected to a special central node. This is the typical layout found in a Wireless LAN, where each wireless client connects to the central Wireless access point.\\nRing network: each node is connected to its left and right neighbour node, such that all nodes are connected and that each node can reach each other node by traversing nodes left- or rightwards. The Fiber Distributed Data Interface (FDDI) made use of such a topology.\\nMesh network: each node is connected to an arbitrary number of neighbours in such a way that there is at least one traversal from any node to any other.\\nFully connected network: each node is connected to every other node in the network.\\nTree network: nodes are arranged hierarchically.The physical layout of the nodes in a network may not necessarily reflect the network topology. As an example, with FDDI, the network topology is a ring, but the physical topology is often a star, because all neighboring connections can be routed via a central physical location. Physical layout is not completely irrelevant, however, as common ducting and equipment locations can represent single points of failure due to issues like fires, power failures and flooding.\\n\\n\\n=== Overlay network ===\\n\\nAn overlay network is a virtual network that is built on top of another network. Nodes in the overlay network are connected by virtual or logical links.  Each link corresponds to a path, perhaps through many physical links, in the underlying network. The topology of the overlay network may (and often does) differ from that of the underlying one. For example, many peer-to-peer networks are overlay networks.  They are organized as nodes of a virtual system of links that run on top of the Internet.Overlay networks have been around since the invention of networking when computer systems were connected over telephone lines using modems before any data network existed.\\nThe most striking example of an overlay network is the Internet itself. The Internet itself was initially built as an overlay on the telephone network. Even today, each Internet node can communicate with virtually any other through an underlying mesh of sub-networks of wildly different topologies and technologies. Address resolution and routing are the means that allow mapping of a fully connected IP overlay network to its underlying network.\\nAnother example of an overlay network is a distributed hash table, which maps keys to nodes in the network. In this case, the underlying network is an IP network, and the overlay network is a table (actually a map) indexed by keys.\\nOverlay networks have also been proposed as a way to improve Internet routing, such as through quality of service guarantees achieve higher-quality streaming media. Previous proposals such as IntServ, DiffServ, and IP Multicast have not seen wide acceptance largely because they require modification of all routers in the network.  On the other hand, an overlay network can be incrementally deployed on end-hosts running the overlay protocol software, without cooperation from Internet service providers.  The overlay network has no control over how packets are routed in the underlying network between two overlay nodes, but it can control, for example, the sequence of overlay nodes that a message traverses before it reaches its destination.\\nFor example, Akamai Technologies manages an overlay network that provides reliable, efficient content delivery (a kind of multicast).  Academic research includes end system multicast, resilient routing and quality of service studies, among others.\\n\\n\\n== Network links ==\\n\\nThe transmission media (often referred to in the literature as the physical medium) used to link devices to form a computer network include electrical cable, optical fiber, and free space. In the OSI model, the software to handle the media is defined at layers 1 and 2 — the physical layer and the data link layer.\\nA widely adopted family that uses copper and fiber media in local area network (LAN) technology are collectively known as Ethernet. The media and protocol standards that enable communication between networked devices over Ethernet are defined by IEEE 802.3.  Wireless LAN standards use radio waves, others use infrared signals as a transmission medium. Power line communication uses a building\\'s power cabling to transmit data.\\n\\n\\n=== Wired ===\\n\\nThe following classes of wired technologies are used in computer networking.\\n\\nCoaxial cable is widely used for cable television systems, office buildings, and other work-sites for local area networks. Transmission speed ranges from 200 million bits per second to more than 500 million bits per second.\\nITU-T G.hn technology uses existing home wiring (coaxial cable, phone lines and power lines) to create a high-speed local area network.\\nTwisted pair cabling is used for wired Ethernet and other standards. It typically consists of 4 pairs of copper cabling that can be utilized for both voice and data transmission. The use of two wires twisted together helps to reduce crosstalk and electromagnetic induction. The transmission speed ranges from 2 Mbit/s to 10 Gbit/s. Twisted pair cabling comes in two forms: unshielded twisted pair (UTP) and shielded twisted-pair (STP). Each form comes in several category ratings, designed for use in various scenarios.\\nAn optical fiber is a glass fiber. It carries pulses of light that represent data via lasers and optical amplifiers. Some advantages of optical fibers over metal wires are very low transmission loss and immunity to electrical interference. Using dense wave division multiplexing, optical fibers can simultaneously carry multiple streams of data on different wavelengths of light, which greatly increases the rate that data can be sent to up to trillions of bits per second. Optic fibers can be used for long runs of cable carrying very high data rates, and are used for undersea cables to interconnect continents. There are two basic types of fiber optics, single-mode optical fiber (SMF) and multi-mode optical fiber (MMF).  Single-mode fiber has the advantage of being able to sustain a coherent signal for dozens or even a hundred kilometers. Multimode fiber is cheaper to terminate but is limited to a few hundred or even only a few dozens of meters, depending on the data rate and cable grade.\\n\\n\\n=== Wireless ===\\n\\nNetwork connections can be established wirelessly using radio or other electromagnetic means of communication.\\n\\n Terrestrial microwave – Terrestrial microwave communication uses Earth-based transmitters and receivers resembling satellite dishes. Terrestrial microwaves are in the low gigahertz range, which limits all communications to line-of-sight. Relay stations are spaced approximately 40 miles (64 km) apart.\\nCommunications satellites – Satellites also communicate via microwave. The satellites are stationed in space, typically in geosynchronous orbit 35,400 km (22,000 mi) above the equator. These Earth-orbiting systems are capable of receiving and relaying voice, data, and TV signals.\\nCellular networks use several radio communications technologies. The systems divide the region covered into multiple geographic areas. Each area is served by a low-power transceiver.\\nRadio and spread spectrum technologies – Wireless LANs use a high-frequency radio technology similar to digital cellular. Wireless LANs use spread spectrum technology to enable communication between multiple devices in a limited area. IEEE 802.11 defines a common flavor of open-standards wireless radio-wave technology known as Wi-Fi.\\nFree-space optical communication uses visible or invisible light for communications. In most cases, line-of-sight propagation is used, which limits the physical positioning of communicating devices.\\nExtending the Internet to interplanetary dimensions via radio waves and optical means, the Interplanetary Internet.\\nIP over Avian Carriers was a humorous April fool\\'s Request for Comments, issued as RFC 1149. It was implemented in real life in 2001.The last two cases have a large round-trip delay time, which gives slow two-way communication but doesn\\'t prevent sending large amounts of information (they can have high throughput).\\n\\n\\n== Network nodes ==\\n\\nApart from any physical transmission media, networks are built from additional basic system building blocks, such as network interface controllers (NICs), repeaters, hubs, bridges, switches, routers, modems, and firewalls. Any particular piece of equipment will frequently contain multiple building blocks and so may perform multiple functions.\\n\\n\\n=== Network interfaces ===\\n\\nA network interface controller (NIC) is computer hardware that connects the computer to the network media and has the ability to process low-level network information. For example, the NIC may have a connector for accepting a cable, or an aerial for wireless transmission and reception, and the associated circuitry.\\nIn Ethernet networks, each network interface controller has a unique Media Access Control (MAC) address—usually stored in the controller\\'s permanent memory. To avoid address conflicts between network devices, the Institute of Electrical and Electronics Engineers (IEEE) maintains and administers MAC address uniqueness. The size of an Ethernet MAC address is six octets. The three most significant octets are reserved to identify NIC manufacturers. These manufacturers, using only their assigned prefixes, uniquely assign the three least-significant octets of every Ethernet interface they produce.\\n\\n\\n=== Repeaters and hubs ===\\nA repeater is an electronic device that receives a network signal, cleans it of unnecessary noise and regenerates it. The signal is retransmitted at a higher power level, or to the other side of obstruction so that the signal can cover longer distances without degradation. In most twisted pair Ethernet configurations, repeaters are required for cable that runs longer than 100 meters. With fiber optics, repeaters can be tens or even hundreds of kilometers apart.\\nRepeaters work on the physical layer of the OSI model but still require a small amount of time to regenerate the signal. This can cause a propagation delay that affects network performance and may affect proper function. As a result, many network architectures limit the number of repeaters used in a network, e.g., the Ethernet 5-4-3 rule.\\nAn Ethernet repeater with multiple ports is known as an Ethernet hub. In addition to reconditioning and distributing network signals, a repeater hub assists with collision detection and fault isolation for the network. Hubs and repeaters in LANs have been largely obsoleted by modern network switches.\\n\\n\\n=== Bridges and switches ===\\nNetwork bridges and network switches are distinct from a hub in that they only forward frames to the ports involved in the communication whereas a hub forwards to all ports. Bridges only have two ports but a switch can be thought of as a multi-port bridge. Switches normally have numerous ports, facilitating a star topology for devices, and for cascading additional switches.\\nBridges and switches operate at the data link layer (layer 2) of the OSI model and bridge traffic between two or more network segments to form a single local network. Both are devices that forward frames of data between ports based on the destination MAC address in each frame.\\nThey learn the association of physical ports to MAC addresses by examining the source addresses of received frames and only forward the frame when necessary. If an unknown destination MAC is targeted, the device broadcasts the request to all ports except the source, and discovers the location from the reply.\\nBridges and switches divide the network\\'s collision domain but maintain a single broadcast domain. Network segmentation through bridging and switching helps break down a large, congested network into an aggregation of smaller, more efficient networks.\\n\\n\\n=== Routers ===\\n\\nA router is an internetworking device that forwards packets between networks by processing the addressing or routing information included in the packet.  The routing information is often processed in conjunction with the routing table.  A router uses its routing table to determine where to forward packets and does not require broadcasting packets which is inefficient for very big networks.\\n\\n\\n=== Modems ===\\nModems (modulator-demodulator) are used to connect network nodes via wire not originally designed for digital network traffic, or for wireless. To do this one or more carrier signals are modulated by the digital signal to produce an analog signal that can be tailored to give the required properties for transmission. Early modems modulated audio signals sent over a standard voice telephone line. Modems are still commonly used for telephone lines, using a digital subscriber line technology and cable television systems using DOCSIS technology.\\n\\n\\n=== Firewalls ===\\nA firewall is a network device or software for controlling network security and access rules. Firewalls are inserted in connections between secure internal networks and potentially insecure external networks such as the Internet. Firewalls are typically configured to reject access requests from unrecognized sources while allowing actions from recognized ones. The vital role firewalls play in network security grows in parallel with the constant increase in cyber attacks.\\n\\n\\n== Communication protocols ==\\n\\nA communication protocol is a set of rules for exchanging information over a network. Communication protocols have various characteristics.  They may be connection-oriented or connectionless, they may use circuit mode or packet switching, and they may use hierarchical addressing or flat addressing.\\nIn a protocol stack, often constructed per the OSI model, communications functions are divided up into protocol layers, where each layer leverages the services of the layer below it until the lowest layer controls the hardware that sends information across the media. The use of protocol layering is ubiquitous across the field of computer networking. An important example of a protocol stack is HTTP (the World Wide Web protocol) running over TCP over IP (the Internet protocols) over IEEE 802.11 (the Wi-Fi protocol). This stack is used between the wireless router and the home user\\'s personal computer when the user is surfing the web.\\nThere are many communication protocols, a few of which are described below.\\n\\n\\n=== Common protocols ===\\n\\n\\n==== Internet Protocol Suite ====\\nThe Internet Protocol Suite, also called TCP/IP, is the foundation of all modern networking. It offers connection-less and connection-oriented services over an inherently unreliable network traversed by datagram transmission using Internet protocol (IP). At its core, the protocol suite defines the addressing, identification, and routing specifications for Internet Protocol Version 4 (IPv4) and for IPv6, the next generation of the protocol with a much enlarged addressing capability. The Internet Protocol Suite is the defining set of protocols for the Internet.\\n\\n\\n==== IEEE 802 ====\\nIEEE 802 is a family of IEEE standards dealing with local area networks and metropolitan area networks. The complete IEEE 802 protocol suite provides a diverse set of networking capabilities. The protocols have a flat addressing scheme. They operate mostly at layers 1 and 2 of the OSI model.\\nFor example, MAC bridging (IEEE 802.1D) deals with the routing of Ethernet packets using a Spanning Tree Protocol. IEEE 802.1Q describes VLANs, and IEEE 802.1X defines a port-based Network Access Control protocol, which forms the basis for the authentication mechanisms used in VLANs (but it is also found in WLANs) – it is what the home user sees when the user has to enter a \"wireless access key\".\\n\\n\\n===== Ethernet =====\\nEthernet is a family of technologies used in wired LANs. It is described by a set of standards together called IEEE 802.3 published by the Institute of Electrical and Electronics Engineers.\\n\\n\\n===== Wireless LAN =====\\nWireless LAN based on the IEEE 802.11 standards, also widely known as WLAN or WiFi, is probably the most well-known member of the IEEE 802 protocol family for home users today. IEEE 802.11 shares many properties with wired Ethernet.\\n\\n\\n==== SONET/SDH ====\\nSynchronous optical networking (SONET) and Synchronous Digital Hierarchy (SDH) are standardized multiplexing protocols that transfer multiple digital bit streams over optical fiber using lasers. They were originally designed to transport circuit mode communications from a variety of different sources, primarily to support real-time, uncompressed, circuit-switched voice encoded in PCM (Pulse-Code Modulation) format. However, due to its protocol neutrality and transport-oriented features, SONET/SDH also was the obvious choice for transporting Asynchronous Transfer Mode (ATM) frames.\\n\\n\\n==== Asynchronous Transfer Mode ====\\nAsynchronous Transfer Mode (ATM) is a switching technique for telecommunication networks.  It uses asynchronous time-division multiplexing and encodes data into small, fixed-sized cells. This differs from other protocols such as the Internet Protocol Suite or Ethernet that use variable sized packets or frames. ATM has similarities with both circuit and packet switched networking.  This makes it a good choice for a network that must handle both traditional high-throughput data traffic, and real-time, low-latency content such as voice and video. ATM uses a connection-oriented model in which a virtual circuit must be established between two endpoints before the actual data exchange begins.\\nWhile the role of ATM is diminishing in favor of next-generation networks, it still plays a role in the last mile, which is the connection between an Internet service provider and the home user.\\n\\n\\n==== Cellular standards ====\\nThere are a number of different digital cellular standards, including: Global System for Mobile Communications (GSM), General Packet Radio Service (GPRS), cdmaOne, CDMA2000, Evolution-Data Optimized (EV-DO), Enhanced Data Rates for GSM Evolution (EDGE), Universal Mobile Telecommunications System (UMTS), Digital Enhanced Cordless Telecommunications (DECT), Digital AMPS (IS-136/TDMA), and Integrated Digital Enhanced Network (iDEN).\\n\\n\\n=== Routing ===\\n\\nRouting is the process of selecting network paths to carry network traffic. Routing is performed for many kinds of networks, including circuit switching networks and packet switched networks.\\nIn packet-switched networks, routing protocols direct packet forwarding (the transit of logically addressed network packets from their source toward their ultimate destination)  through intermediate nodes. Intermediate nodes are typically network hardware devices such as routers, bridges, gateways, firewalls, or switches. General-purpose computers can also forward packets and perform routing, though they are not specialized hardware and may suffer from the limited performance. The routing process usually directs forwarding on the basis of routing tables, which maintain a record of the routes to various network destinations. Thus, constructing routing tables, which are held in the router\\'s memory, is very important for efficient routing.\\nThere are usually multiple routes that can be taken, and to choose between them, different elements can be considered to decide which routes get installed into the routing table, such as (sorted by priority):\\n\\nPrefix-Length: where longer subnet masks are preferred (independent if it is within a routing protocol or over a different routing protocol)\\nMetric: where a lower metric/cost is preferred (only valid within one and the same routing protocol)\\nAdministrative distance: where a lower distance is preferred (only valid between different routing protocols)Most routing algorithms use only one network path at a time. Multipath routing techniques enable the use of multiple alternative paths.\\nRouting, in a more narrow sense of the term, is often contrasted with bridging in its assumption that network addresses are structured and that similar addresses imply proximity within the network. Structured addresses allow a single routing table entry to represent the route to a group of devices.  In large networks, structured addressing (routing, in the narrow sense) outperforms unstructured addressing (bridging). Routing has become the dominant form of addressing on the Internet. Bridging is still widely used within localized environments.\\n\\n\\n== Geographic scale ==\\nNetworks may be characterized by many properties or features, such as physical capacity, organizational purpose, user authorization, access rights, and others. Another distinct classification method is that of the physical extent or geographic scale.\\n\\nNanoscale networkA nanoscale communication network has key components implemented at the nanoscale including message carriers and leverages physical principles that differ from macroscale communication mechanisms. Nanoscale communication extends communication to very small sensors and actuators such as those found in biological systems and also tends to operate in environments that would be too harsh for classical communication.\\nPersonal area networkA personal area network (PAN) is a computer network used for communication among computers and different information technological devices close to one person. Some examples of devices that are used in a PAN are personal computers, printers, fax machines, telephones, PDAs, scanners, and even video game consoles. A PAN may include wired and wireless devices. The reach of a PAN typically extends to 10 meters. A wired PAN is usually constructed with USB and FireWire connections while technologies such as Bluetooth and infrared communication typically form a wireless PAN.\\n\\nLocal area networkA local area network (LAN) is a network that connects computers and devices in a limited geographical area such as a home, school, office building, or closely positioned group of buildings. Each computer or device on the network is a node.  Wired LANs are most likely based on Ethernet technology.  Newer standards such as ITU-T G.hn also provide a way to create a wired LAN using existing wiring, such as coaxial cables, telephone lines, and power lines.The defining characteristics of a LAN, in contrast to a wide area network (WAN), include higher data transfer rates, limited geographic range, and lack of reliance on leased lines to provide connectivity. Current Ethernet or other IEEE 802.3 LAN technologies operate at data transfer rates up to 100 Gbit/s, standardized by IEEE in 2010. Currently, 400 Gbit/s Ethernet is being developed.\\nA LAN can be connected to a WAN using a router.\\n\\nHome area networkA home area network (HAN) is a residential LAN used for communication between digital devices typically deployed in the home, usually a small number of personal computers and accessories, such as printers and mobile computing devices. An important function is the sharing of Internet access, often a broadband service through a cable TV or digital subscriber line (DSL) provider.\\n\\nStorage area networkA storage area network (SAN) is a dedicated network that provides access to consolidated, block-level data storage. SANs are primarily used to make storage devices, such as disk arrays, tape libraries, and optical jukeboxes, accessible to servers so that the devices appear like locally attached devices to the operating system. A SAN typically has its own network of storage devices that are generally not accessible through the local area network by other devices. The cost and complexity of SANs dropped in the early 2000s to levels allowing wider adoption across both enterprise and small to medium-sized business environments.\\n\\nCampus area networkA campus area network (CAN) is made up of an interconnection of LANs within a limited geographical area. The networking equipment (switches, routers) and transmission media (optical fiber, copper plant, Cat5 cabling, etc.) are almost entirely owned by the campus tenant/owner (an enterprise, university, government, etc.).\\nFor example, a university campus network is likely to link a variety of campus buildings to connect academic colleges or departments, the library, and student residence halls.\\n\\nBackbone networkA backbone network is part of a computer network infrastructure that provides a path for the exchange of information between different LANs or subnetworks.  A backbone can tie together diverse networks within the same building, across different buildings, or over a wide area.\\nFor example, a large company might implement a backbone network to connect departments that are located around the world. The equipment that ties together the departmental networks constitutes the network backbone.  When designing a network backbone, network performance and network congestion are critical factors to take into account.  Normally, the backbone network\\'s capacity is greater than that of the individual networks connected to it.\\nAnother example of a backbone network is the Internet backbone, which is a massive, global system of fiber-optic cable and optical networking that carry the bulk of data between wide area networks (WANs), metro, regional, national and transoceanic networks.\\n\\nMetropolitan area networkA metropolitan area network (MAN) is a large computer network that usually spans a city or a large campus.\\n\\nWide area networkA wide area network (WAN) is a computer network that covers a large geographic area such as a city, country, or spans even intercontinental distances.  A WAN uses a communications channel that combines many types of media such as telephone lines, cables, and airwaves. A WAN often makes use of transmission facilities provided by common carriers, such as telephone companies. WAN technologies generally function at the lower three layers of the OSI reference model: the physical layer, the data link layer, and the network layer.\\n\\nEnterprise private networkAn enterprise private network is a network that a single organization builds to interconnect its office locations (e.g., production sites, head offices, remote offices, shops) so they can share computer resources.\\n\\nVirtual private networkA virtual private network (VPN) is an overlay network in which some of the links between nodes are carried by open connections or virtual circuits in some larger network (e.g., the Internet) instead of by physical wires. The data link layer protocols of the virtual network are said to be tunneled through the larger network when this is the case. One common application is secure communications through the public Internet, but a VPN need not have explicit security features, such as authentication or content encryption. VPNs, for example, can be used to separate the traffic of different user communities over an underlying network with strong security features.\\nVPN may have best-effort performance or may have a defined service level agreement (SLA) between the VPN customer and the VPN service provider. Generally, a VPN has a topology more complex than point-to-point.\\n\\nGlobal area networkA global area network (GAN) is a network used for supporting mobile across an arbitrary number of wireless LANs, satellite coverage areas, etc. The key challenge in mobile communications is handing off user communications from one local coverage area to the next. In IEEE Project 802, this involves a succession of terrestrial wireless LANs.\\n\\n\\n== Organizational scope ==\\nNetworks are typically managed by the organizations that own them. Private enterprise networks may use a combination of intranets and extranets. They may also provide network access to the Internet, which has no single owner and permits virtually unlimited global connectivity.\\n\\n\\n=== Intranet ===\\nAn intranet is a set of networks that are under the control of a single administrative entity.  The intranet uses the IP protocol and IP-based tools such as web browsers and file transfer applications. The administrative entity limits the use of the intranet to its authorized users. Most commonly, an intranet is the internal LAN of an organization. A large intranet typically has at least one web server to provide users with organizational information. An intranet is also anything behind the router on a local area network.\\n\\n\\n=== Extranet ===\\nAn extranet is a network that is also under the administrative control of a single organization but supports a limited connection to a specific external network.  For example, an organization may provide access to some aspects of its intranet to share data with its business partners or customers.  These other entities are not necessarily trusted from a security standpoint.  Network connection to an extranet is often, but not always, implemented via WAN technology.\\n\\n\\n=== Internet ===\\nAn internetwork is the connection of multiple different types of computer networks to form a single computer network by layering on top of the different networking software and connecting them together using routers.\\n\\nThe Internet is the largest example of internetwork. It is a global system of interconnected governmental, academic, corporate, public, and private computer networks. It is based on the networking technologies of the Internet Protocol Suite. It is the successor of the Advanced Research Projects Agency Network (ARPANET) developed by DARPA of the United States Department of Defense. The Internet utilizes copper communications and the optical networking backbone to enable the World Wide Web (WWW), the Internet of Things, video transfer, and a broad range of information services.\\nParticipants on the Internet use a diverse array of methods of several hundred documented, and often standardized, protocols compatible with the Internet Protocol Suite and an addressing system (IP addresses) administered by the Internet Assigned Numbers Authority and address registries. Service providers and large enterprises exchange information about the reachability of their address spaces through the Border Gateway Protocol (BGP), forming a redundant worldwide mesh of transmission paths.\\n\\n\\n=== Darknet ===\\nA darknet is an overlay network, typically running on the Internet, that is only accessible through specialized software. A darknet is an anonymizing network where connections are made only between trusted peers — sometimes called \"friends\" (F2F) — using non-standard protocols and ports.\\nDarknets are distinct from other distributed peer-to-peer networks as sharing is anonymous (that is, IP addresses are not publicly shared), and therefore users can communicate with little fear of governmental or corporate interference.\\n\\n\\n== Network service ==\\nNetwork services are applications hosted by servers on a computer network, to provide some functionality for members or users of the network, or to help the network itself to operate.\\nThe World Wide Web, E-mail, printing and network file sharing are examples of well-known network services. Network services such as DNS (Domain Name System) give names for IP and MAC addresses (people remember names like “nm.lan” better than numbers like “210.121.67.18”), and DHCP to ensure that the equipment on the network has a valid IP address.Services are usually based on a service protocol that defines the format and sequencing of messages between clients and servers of that network service.\\n\\n\\n== Network performance ==\\n\\n\\n=== Bandwidth ===\\n\\nBandwidth in bit/s may refer to consumed bandwidth, corresponding to achieved throughput or goodput, i.e., the average rate of successful data transfer through a communication path. The throughput is affected by technologies such as bandwidth shaping, bandwidth management, bandwidth throttling, bandwidth cap, bandwidth allocation (for example bandwidth allocation protocol and dynamic bandwidth allocation), etc. A bit stream\\'s bandwidth  is proportional to the average consumed signal bandwidth in hertz (the average spectral bandwidth of the analog signal representing the bit stream) during a studied time interval.\\n\\n\\n=== Network delay ===\\n\\nNetwork delay is a design and performance characteristic of a telecommunications network. It specifies the latency for a bit of data to travel across the network from one communication endpoint to another. It is typically measured in multiples or fractions of a second. Delay may differ slightly, depending on the location of the specific pair of communicating endpoints. Engineers usually report both the maximum and average delay, and they divide the delay into several parts:\\n\\nProcessing delay –  time it takes a router to process the packet header\\nQueuing delay –  time the packet spends in routing queues\\nTransmission delay –  time it takes to push the packet\\'s bits onto the link\\nPropagation delay –  time for a signal to propagate through the mediaA certain minimum level of delay is experienced by signals due to the time it takes to transmit a packet serially through a link. This delay is extended by more variable levels of delay due to network congestion. IP network delays can range from a few milliseconds to several hundred milliseconds.\\n\\n\\n=== Quality of service ===\\nDepending on the installation requirements, network performance is usually measured by the quality of service of a telecommunications product. The parameters that affect this typically can include throughput, jitter, bit error rate and latency.\\nThe following list gives examples of network performance measures for a circuit-switched network and one type of packet-switched network, viz. ATM:\\n\\nCircuit-switched networks: In circuit switched networks, network performance is synonymous with the grade of service. The number of rejected calls is a measure of how well the network is performing under heavy traffic loads. Other types of performance measures can include the level of noise and echo.\\nATM: In an Asynchronous Transfer Mode (ATM) network, performance can be measured by line rate, quality of service (QoS), data throughput, connect time, stability, technology, modulation technique, and modem enhancements.There are many ways to measure the performance of a network, as each network is different in nature and design. Performance can also be modeled instead of measured. For example, state transition diagrams are often used to model queuing performance in a circuit-switched network. The network planner uses these diagrams to analyze how the network performs in each state, ensuring that the network is optimally designed.\\n\\n\\n=== Network congestion ===\\nNetwork congestion occurs when a link or node is subjected to a greater data load than it is rated for, resulting in a deterioration of its quality of service. When networks are congested and queues become too full, packets have to be discarded, and so networks rely on re-transmission. Typical effects of congestion include queueing delay, packet loss or the blocking of new connections.  A consequence of these latter two is that incremental increases in offered load lead either to only a small increase in the network throughput or to a reduction in network throughput.\\nNetwork protocols that use aggressive retransmissions to compensate for packet loss tend to keep systems in a state of network congestion—even after the initial load is reduced to a level that would not normally induce network congestion. Thus, networks using these protocols can exhibit two stable states under the same level of load. The stable state with low throughput is known as congestive collapse.\\nModern networks use congestion control, congestion avoidance and traffic control techniques to try to avoid congestion collapse (i.e. endpoints typically slow down or sometimes even stop transmission entirely when the network is congested). These techniques include: exponential backoff in protocols such as 802.11\\'s CSMA/CA and the original Ethernet, window reduction in TCP, and fair queueing in devices such as routers. Another method to avoid the negative effects of network congestion is implementing priority schemes so that some packets are transmitted with higher priority than others. Priority schemes do not solve network congestion by themselves, but they help to alleviate the effects of congestion for some services. An example of this is 802.1p. A third method to avoid network congestion is the explicit allocation of network resources to specific flows. One example of this is the use of Contention-Free Transmission Opportunities (CFTXOPs) in the ITU-T G.hn standard, which provides high-speed (up to 1 Gbit/s) Local area networking over existing home wires (power lines, phone lines and coaxial cables).\\nFor the Internet, RFC 2914 addresses the subject of congestion control in detail.\\n\\n\\n=== Network resilience ===\\nNetwork resilience is \"the ability to provide and maintain an acceptable level of service in the face of faults and challenges to normal operation.”\\n\\n\\n== Security ==\\n\\nComputer networks are also used by security hackers to deploy computer viruses or computer worms on devices connected to the network, or to prevent these devices from accessing the network via a denial-of-service attack.\\n\\n\\n=== Network security ===\\nNetwork Security consists of provisions and policies adopted by the network administrator to prevent and monitor unauthorized access, misuse, modification, or denial of the computer network and its network-accessible resources. Network security is the authorization of access to data in a network, which is controlled by the network administrator. Users are assigned an ID and password that allows them access to information and programs within their authority.  Network security is used on a variety of computer networks, both public and private, to secure daily transactions and communications among businesses, government agencies, and individuals.\\n\\n\\n=== Network surveillance ===\\nNetwork surveillance is the monitoring of data being transferred over computer networks such as the Internet. The monitoring is often done surreptitiously and may be done by or at the behest of governments, by corporations, criminal organizations, or individuals. It may or may not be legal and may or may not require authorization from a court or other independent agency.\\nComputer and network surveillance programs are widespread today, and almost all Internet traffic is or could potentially be monitored for clues to illegal activity.\\nSurveillance is very useful to governments and law enforcement to maintain social control, recognize and monitor threats, and prevent/investigate criminal activity. With the advent of programs such as the Total Information Awareness program, technologies such as high-speed surveillance computers and biometrics software, and laws such as the Communications Assistance For Law Enforcement Act, governments now possess an unprecedented ability to monitor the activities of citizens.However, many civil rights and privacy groups—such as Reporters Without Borders, the Electronic Frontier Foundation, and the American Civil Liberties Union—have expressed concern that increasing surveillance of citizens may lead to a mass surveillance society, with limited political and personal freedoms. Fears such as this have led to numerous lawsuits such as Hepting v. AT&T. The hacktivist group Anonymous has hacked into government websites in protest of what it considers \"draconian surveillance\".\\n\\n\\n=== End to end encryption ===\\nEnd-to-end encryption (E2EE) is a digital communications paradigm of uninterrupted protection of data traveling between two communicating parties. It involves the originating party encrypting data so only the intended recipient can decrypt it, with no dependency on third parties. End-to-end encryption prevents intermediaries, such as Internet providers or application service providers, from discovering or tampering with communications. End-to-end encryption generally protects both confidentiality and integrity.\\nExamples of end-to-end encryption include HTTPS for web traffic, PGP for email, OTR for instant messaging, ZRTP for telephony, and TETRA for radio.\\nTypical server-based communications systems do not include end-to-end encryption. These systems can only guarantee the protection of communications between clients and servers, not between the communicating parties themselves. Examples of non-E2EE systems are Google Talk, Yahoo Messenger, Facebook, and Dropbox. Some such systems, for example, LavaBit and SecretInk, have even described themselves as offering \"end-to-end\" encryption when they do not. Some systems that normally offer end-to-end encryption have turned out to contain a back door that subverts negotiation of the encryption key between the communicating parties, for example Skype or Hushmail.\\nThe end-to-end encryption paradigm does not directly address risks at the endpoints of the communication themselves, such as the technical exploitation of clients, poor quality random number generators, or key escrow. E2EE also does not address traffic analysis, which relates to things such as the identities of the endpoints and the times and quantities of messages that are sent.\\n\\n\\n=== SSL/TLS ===\\nThe introduction and rapid growth of e-commerce on the World Wide Web in the mid-1990s made it obvious that some form of authentication and encryption was needed. Netscape took the first shot at a new standard. At the time, the dominant web browser was Netscape Navigator. Netscape created a standard called secure socket layer (SSL). SSL requires a server with a certificate. When a client requests access to an SSL-secured server, the server sends a copy of the certificate to the client. The SSL client checks this certificate (all web browsers come with an exhaustive list of CA root certificates preloaded), and if the certificate checks out, the server is authenticated and the client negotiates a symmetric-key cipher for use in the session. The session is now in a very secure encrypted tunnel between the SSL server and the SSL client.\\n\\n\\n== Views of networks ==\\nUsers and network administrators typically have different views of their networks. Users can share printers and some servers from a workgroup, which usually means they are in the same geographic location and are on the same LAN, whereas a Network Administrator is responsible to keep that network up and running.  A community of interest has less of a connection of being in a local area and should be thought of as a set of arbitrarily located users who share a set of servers, and possibly also communicate via peer-to-peer technologies.\\nNetwork administrators can see networks from both physical and logical perspectives. The physical perspective involves geographic locations, physical cabling, and the network elements (e.g., routers, bridges and application layer gateways) that interconnect via the transmission media. Logical networks, called, in the TCP/IP architecture, subnets, map onto one or more transmission media. For example, a common practice in a campus of buildings is to make a set of LAN cables in each building appear to be a common subnet, using  virtual LAN (VLAN) technology.\\nBoth users and administrators are aware, to varying extents, of the trust and scope characteristics of a network. Again using TCP/IP architectural terminology, an intranet is a community of interest under private administration usually by an enterprise, and is only accessible by authorized users (e.g. employees).  Intranets do not have to be connected to the Internet, but generally have a limited connection.  An extranet is an extension of an intranet that allows secure communications to users outside of the intranet (e.g. business partners, customers).Unofficially, the Internet is the set of users, enterprises, and content providers that are interconnected by Internet Service Providers (ISP). From an engineering viewpoint, the Internet is the set of subnets, and aggregates of subnets, that share the registered IP address space and exchange information about the reachability of those IP addresses using the Border Gateway Protocol. Typically, the human-readable names of servers are translated to IP addresses, transparently to users, via the directory function of the Domain Name System (DNS).\\nOver the Internet, there can be  business-to-business (B2B),  business-to-consumer (B2C) and consumer-to-consumer (C2C) communications. When money or sensitive information is exchanged, the communications are apt to be protected by some form of communications security mechanism.  Intranets and extranets can be securely superimposed onto the Internet, without any access by general Internet users and administrators, using secure Virtual Private Network (VPN) technology.\\n\\n\\n== Journals and newsletters ==\\nOpen Computer Science (open access journal)\\n\\n\\n== See also ==\\nComparison of network diagram software\\nCyberspace\\nHistory of the Internet\\nInformation Age\\nInformation revolution\\nISO/IEC 11801 – International standard for electrical and optical cables\\nMinimum-Pairs Protocol\\nNetwork simulation\\nNetwork planning and design\\nNetwork traffic control\\n\\n\\n== References ==\\n\\n This article incorporates public domain material from the General Services Administration document: \"Federal Standard 1037C\".\\n\\n\\n== Further reading ==\\nShelly, Gary, et al. \"Discovering Computers\" 2003 Edition.\\nWendell Odom, Rus Healy, Denise Donohue. (2010) CCIE Routing and Switching. Indianapolis, IN: Cisco Press\\nKurose James F and Keith W. Ross: Computer Networking: A Top-Down Approach Featuring the Internet, Pearson Education 2005.\\nWilliam Stallings, Computer Networking with Internet Protocols and Technology, Pearson Education 2004.\\nImportant publications in computer networks\\nNetwork Communication Architecture and Protocols: OSI Network Architecture 7 Layers Model\\nDimitri Bertsekas, and Robert Gallager, \"Data Networks,\" Prentice Hall, 1992.\\n\\n\\n== External links ==\\nNetworking at Curlie\\nIEEE Ethernet manufacturer information\\nA computer networking acronym guide', 'In information theory and coding theory with applications in computer science and telecommunication, error detection and correction or error control are techniques that enable reliable delivery of digital data over unreliable communication channels. Many communication channels are subject to channel noise, and thus errors may be introduced during transmission from the source to a receiver. Error detection techniques allow detecting such errors, while error correction enables reconstruction of the original data in many cases.\\n\\n\\n== Definitions ==\\nError detection  is the detection of errors caused by noise or other impairments during transmission from the transmitter to the receiver. \\nError correction is the detection of errors and reconstruction of the original, error-free data.\\n\\n\\n== History ==\\nIn classical antiquity, copyists of the Hebrew Bible were paid for their work according to the number of stichs (lines of verse). As the prose books of the Bible were hardly ever written in stichs, the copyists, in order to estimate the amount of work, had to count the letters. This also helped ensure accuracy in the transmission of the text with the production of subsequent copies. Between the 7th and 10th centuries CE a group of Jewish scribes formalized and expanded this to create the Numerical Masorah to ensure accurate reproduction of the sacred text. It included counts of the number of words in a line, section, book and groups of books, noting the middle stich of a book, word use statistics, and commentary. Standards became such that a deviation in even a single letter in a Torah scroll was considered unacceptable. The effectiveness of their error correction method was verified by the accuracy of copying through the centuries demonstrated by discovery of the Dead Sea Scrolls in 1947-1956, dating from c.150 BCE-75 CE.The modern development of error correction codes is credited to Richard Hamming in 1947. A description of Hamming\\'s code appeared in Claude Shannon\\'s A Mathematical Theory of Communication and was quickly generalized by Marcel J. E. Golay.\\n\\n\\n== Introduction ==\\nAll error-detection and correction schemes add some redundancy (i.e., some extra data) to a message, which receivers can use to check consistency of the delivered message, and to recover data that has been determined to be corrupted. Error-detection and correction schemes can be either systematic or non-systematic. In a systematic scheme, the transmitter sends the original data, and attaches a fixed number of check bits (or parity data), which are derived from the data bits by some deterministic algorithm. If only error detection is required, a receiver can simply apply the same algorithm to the received data bits and compare its output with the received check bits; if the values do not match, an error has occurred at some point during the transmission. In a system that uses a non-systematic code, the original message is transformed into an encoded message carrying the same information and that has at least as many bits as the original message.\\nGood error control performance requires the scheme to be selected based on the characteristics of the communication channel. Common channel models include memoryless models where errors occur randomly and with a certain probability, and dynamic models where errors occur primarily in bursts. Consequently, error-detecting and correcting codes can be generally distinguished between random-error-detecting/correcting and burst-error-detecting/correcting. Some codes can also be suitable for a mixture of random errors and burst errors.\\nIf the channel characteristics cannot be determined, or are highly variable, an error-detection scheme may be combined with a system for retransmissions of erroneous data. This is known as automatic repeat request (ARQ), and is most notably used in the Internet. An alternate approach for error control is hybrid automatic repeat request (HARQ), which is a combination of ARQ and error-correction coding.\\n\\n\\n== Types of error correction ==\\nThere are three major types of error correction.\\n\\n\\n=== Automatic repeat request (ARQ) ===\\n\\nAutomatic Repeat reQuest (ARQ) is an error control method for data transmission that makes use of error-detection codes, acknowledgment and/or negative acknowledgment messages, and timeouts to achieve reliable data transmission. An acknowledgment is a message sent by the receiver to indicate that it has correctly received a data frame.\\nUsually, when the transmitter does not receive the acknowledgment before the timeout occurs (i.e., within a reasonable amount of time after sending the data frame), it retransmits the frame until it is either correctly received or the error persists beyond a predetermined number of retransmissions.\\nThree types of ARQ protocols are Stop-and-wait ARQ, Go-Back-N ARQ, and Selective Repeat ARQ.\\nARQ is appropriate if the communication channel has varying or unknown capacity, such as is the case on the Internet. However, ARQ requires the availability of a back channel, results in possibly increased latency due to retransmissions, and requires the maintenance of buffers and timers for retransmissions, which in the case of network congestion can put a strain on the server and overall network capacity.For example, ARQ is used on shortwave radio data links in the form of ARQ-E, or combined with multiplexing as ARQ-M.\\n\\n\\n=== Forward  error correction ===\\nForward error correction (FEC) is a process of adding redundant data such as an error-correcting code (ECC) to a message so that it can be recovered by a receiver even when a number of errors (up to the capability of the code being used) were introduced, either during the process of transmission, or on storage. Since the receiver does not have to ask the sender for retransmission of the data, a backchannel is not required in forward error correction, and it is therefore suitable for simplex communication such as broadcasting. Error-correcting codes are frequently used in lower-layer communication, as well as for reliable storage in media such as CDs, DVDs, hard disks, and RAM.\\nError-correcting codes are usually distinguished between convolutional codes and block codes:\\n\\nConvolutional codes are processed on a bit-by-bit basis. They are particularly suitable for implementation in hardware, and the Viterbi decoder allows optimal decoding.\\nBlock codes are processed on a block-by-block basis. Early examples of block codes are repetition codes, Hamming codes and multidimensional parity-check codes. They were followed by a number of efficient codes, Reed–Solomon codes being the most notable due to their current widespread use. Turbo codes and low-density parity-check codes (LDPC) are relatively new constructions that can provide almost optimal efficiency.Shannon\\'s theorem is an important theorem in forward error correction, and describes the maximum information rate at which reliable communication is possible over a channel that has a certain error probability or signal-to-noise ratio (SNR). This strict upper limit is expressed in terms of the channel capacity. More specifically, the theorem says that there exist codes such that with increasing encoding length the probability of error on a discrete memoryless channel can be made arbitrarily small, provided that the code rate is smaller than the channel capacity. The code rate is defined as the fraction k/n of k source symbols and n encoded symbols.\\nThe actual maximum code rate allowed depends on the error-correcting code used, and may be lower. This is because Shannon\\'s proof was only of existential nature, and did not show how to construct codes which are both optimal and have efficient encoding and decoding algorithms.\\n\\n\\n=== Hybrid schemes ===\\n\\nHybrid ARQ is a combination of ARQ and forward error correction. There are two basic approaches:\\nMessages are always transmitted with FEC parity data (and error-detection redundancy). A receiver decodes a message using the parity information, and requests retransmission using ARQ only if the parity data was not sufficient for successful decoding (identified through a failed integrity check).\\nMessages are transmitted without parity data (only with error-detection information). If a receiver detects an error, it requests FEC information from the transmitter using ARQ, and uses it to reconstruct the original message.The latter approach is particularly attractive on an erasure channel when using a rateless erasure code.\\n\\n\\n== Error detection schemes ==\\nError detection is most commonly realized using a suitable hash function (or specifically, a checksum, cyclic redundancy check or other algorithm). A hash function adds a fixed-length tag to a message, which enables receivers to verify the delivered message by recomputing the tag and comparing it with the one provided.\\nThere exists a vast variety of different hash function designs. However, some are of particularly widespread use because of either their simplicity or their suitability for detecting certain kinds of errors (e.g., the cyclic redundancy check\\'s performance in detecting burst errors).\\n\\n\\n=== Minimum distance coding ===\\nA random-error-correcting code based on minimum distance coding can provide a strict guarantee on the number of detectable errors, but it may not protect against a preimage attack.\\n\\n\\n=== Repetition codes ===\\nA repetition code is a coding scheme that repeats the bits across a channel to achieve error-free communication. Given a stream of data to be transmitted, the data are divided into blocks of bits. Each block is transmitted some predetermined number of times. For example, to send the bit pattern \"1011\", the four-bit block can be repeated three times, thus producing \"1011 1011 1011\". If this twelve-bit pattern was received as \"1010 1011 1011\" – where the first block is unlike the other two – an error has occurred.\\nA repetition code is very inefficient, and can be susceptible to problems if the error occurs in exactly the same place for each group (e.g., \"1010 1010 1010\" in the previous example would be detected as correct). The advantage of repetition codes is that they are extremely simple, and are in fact used in some transmissions of numbers stations.\\n\\n\\n=== Parity bit ===\\n\\nA parity bit is a bit that is added to a group of source bits to ensure that the number of set bits (i.e., bits with value 1) in the outcome is even or odd. It is a very simple scheme that can be used to detect single or any other odd number (i.e., three, five, etc.) of errors in the output. An even number of flipped bits will make the parity bit appear correct even though the data is erroneous.\\nParity bits added to each \"word\" sent are called transverse redundancy checks, while those added at the end of a stream of \"words\" are called longitudinal redundancy checks.  For example, if each of a series of m-bit \"words\" has a parity bit added, showing whether there were an odd or even number of ones in that word, any word with a single error in it will be detected. It will not be known where in the word the error is, however. If, in addition, after each stream of n words a parity sum is sent, each bit of which shows whether there were an odd or even number of ones at that bit-position sent in the most recent group, the exact position of the error can be determined and the error corrected. This method is only guaranteed to be effective, however, if there are no more than 1 error in every group of n words.  With more error correction bits, more errors can be detected and in some cases corrected.\\nThere are also other bit-grouping techniques.\\n\\n\\n=== Checksum ===\\n\\nA checksum of a message is a modular arithmetic sum of message code words of a fixed word length (e.g., byte values). The sum may be negated by means of a ones\\'-complement operation prior to transmission to detect unintentional all-zero messages.\\nChecksum schemes include parity bits, check digits, and longitudinal redundancy checks. Some checksum schemes, such as the Damm algorithm, the Luhn algorithm, and the Verhoeff algorithm, are specifically designed to detect errors commonly introduced by humans in writing down or remembering identification numbers.\\n\\n\\n=== Cyclic redundancy check ===\\n\\nA cyclic redundancy check (CRC) is a non-secure hash function designed to detect accidental changes to digital data in computer networks. It is not suitable for detecting maliciously introduced errors. It is characterized by specification of a generator polynomial, which is used as the divisor in a polynomial long division over a finite field, taking the input data as the dividend. The remainder becomes the result.\\nA CRC has properties that make it well suited for detecting burst errors. CRCs are particularly easy to implement in hardware and are therefore commonly used in computer networks and storage devices such as hard disk drives.\\nThe parity bit can be seen as a special-case 1-bit CRC.\\n\\n\\n=== Cryptographic hash function ===\\n\\nThe output of a cryptographic hash function, also known as a message digest, can provide strong assurances about data integrity, whether changes of the data are accidental (e.g., due to transmission errors) or maliciously introduced. Any modification to the data will likely be detected through a mismatching hash value. Furthermore, given some hash value, it is typically infeasible to find some input data (other than the one given) that will yield the same hash value. If an attacker can change not only the message but also the hash value, then a keyed hash or message authentication code (MAC) can be used for additional security. Without knowing the key, it is not possible for the attacker to easily or conveniently calculate the correct keyed hash value for a modified message.\\n\\n\\n=== Error correction code ===\\n\\nAny error-correcting code can be used for error detection. A code with minimum Hamming distance, d, can detect up to d − 1 errors in a code word. Using minimum-distance-based error-correcting codes for error detection can be suitable if a strict limit on the minimum number of errors to be detected is desired.\\nCodes with minimum Hamming distance d = 2 are degenerate cases of error-correcting codes, and can be used to detect single errors. The parity bit is an example of a single-error-detecting code.\\n\\n\\n== Applications ==\\nApplications that require low latency (such as telephone conversations) cannot use automatic repeat request (ARQ); they must use forward error correction (FEC). By the time an ARQ system discovers an error and re-transmits it, the re-sent data will arrive too late to be usable.\\nApplications where the transmitter immediately forgets the information as soon as it is sent (such as most television cameras) cannot use ARQ; they must use FEC because when an error occurs, the original data is no longer available.\\nApplications that use ARQ must have a return channel; applications having no return channel cannot use ARQ.\\nApplications that require extremely low error rates (such as digital money transfers) must use ARQ due to the possibility of uncorrectable errors with FEC.\\nReliability and inspection engineering also make use of the theory of error-correcting codes.\\n\\n\\n=== Internet ===\\nIn a typical TCP/IP stack, error control is performed at multiple levels:\\n\\nEach Ethernet frame uses CRC-32 error detection. Frames with detected errors are discarded by the receiver hardware.\\nThe IPv4 header contains a checksum protecting the contents of the header. Packets with incorrect checksums are dropped within the network or at the receiver.\\nThe checksum was omitted from the IPv6 header in order to minimize processing costs in network routing and because current link layer technology is assumed to provide sufficient error detection (see also RFC 3819).\\nUDP has an optional checksum covering the payload and addressing information in the UDP and IP headers. Packets with incorrect checksums are discarded by the network stack. The checksum is optional under IPv4, and required under IPv6. When omitted, it is assumed the data-link layer provides the desired level of error protection.\\nTCP provides a checksum for protecting the payload and addressing information in the TCP and IP headers. Packets with incorrect checksums are discarded by the network stack, and eventually get retransmitted using ARQ, either explicitly (such as through three-way handshake) or implicitly due to a timeout.\\n\\n\\n=== Deep-space telecommunications ===\\nThe development of error-correction codes was tightly coupled with the history of deep-space missions due to the extreme dilution of signal power over interplanetary distances, and the limited power availability aboard space probes. Whereas early missions sent their data uncoded, starting in 1968, digital error correction was implemented in the form of (sub-optimally decoded) convolutional codes and Reed–Muller codes. The Reed–Muller code was well suited to the noise the spacecraft was subject to (approximately matching a bell curve), and was implemented for the Mariner spacecraft and used on missions between 1969 and 1977.\\nThe Voyager 1 and Voyager 2 missions, which started in 1977, were designed to deliver color imaging and scientific information from Jupiter and Saturn. This resulted in increased coding requirements, and thus, the spacecraft were supported by (optimally Viterbi-decoded) convolutional codes that could be concatenated with an outer Golay (24,12,8) code. The Voyager 2 craft additionally supported an implementation of a Reed–Solomon code. The concatenated Reed–Solomon–Viterbi (RSV) code allowed for very powerful error correction, and enabled the spacecraft\\'s extended journey to Uranus and Neptune. After ECC system upgrades in 1989, both crafts used V2 RSV coding.\\nThe Consultative Committee for Space Data Systems currently recommends usage of error correction codes with performance similar to the Voyager 2 RSV code as a minimum. Concatenated codes are increasingly falling out of favor with space missions, and are replaced by more powerful codes such as Turbo codes or LDPC codes.\\nThe different kinds of deep space and orbital missions that are conducted suggest that trying to find a one-size-fits-all error correction system will be an ongoing problem. For missions close to Earth, the nature of the noise in the communication channel is different from that which a spacecraft on an interplanetary mission experiences. Additionally, as a spacecraft increases its distance from Earth, the problem of correcting for noise becomes more difficult.\\n\\n\\n=== Satellite broadcasting ===\\nThe demand for satellite transponder bandwidth continues to grow, fueled by the desire to deliver television (including new channels and high-definition television) and IP data. Transponder availability and bandwidth constraints have limited this growth. Transponder capacity is determined by the selected modulation scheme and the proportion of capacity consumed by FEC.\\n\\n\\n=== Data storage ===\\nError detection and correction codes are often used to improve the reliability of data storage media.  A parity track capable of detecting single-bit errors was present on the first magnetic tape data storage in 1951. The optimal rectangular code used in group coded recording tapes not only detects but also corrects single-bit errors. Some file formats, particularly archive formats, include a checksum (most often CRC32) to detect corruption and truncation and can employ redundancy or parity files to recover portions of corrupted data. Reed-Solomon codes are used in compact discs to correct errors caused by scratches.\\nModern hard drives use Reed–Solomon codes to detect correct minor errors in sector reads, and to recover corrupted data from failing sectors and store that data in the spare sectors. RAID systems use a variety of error correction techniques to recover data when a hard drive completely fails.  Filesystems such as ZFS or Btrfs, as well as some RAID implementations, support data scrubbing and resilvering, which allows bad blocks to be detected and (hopefully) recovered before they are used. The recovered data may be re-written to exactly the same physical location, to spare blocks elsewhere on the same piece of hardware, or the data may be rewritten onto replacement hardware.\\n\\n\\n=== Error-correcting memory ===\\n\\nDynamic random-access memory (DRAM) may provide stronger protection against soft errors by relying on error-correcting codes. Such error-correcting memory, known as ECC or EDAC-protected memory, is particularly desirable for mission-critical applications, such as scientific computing, financial, medical, etc. as well as extraterrestrial applications due to the increased radiation in space.\\nError-correcting memory controllers traditionally use Hamming codes, although some use triple modular redundancy. Interleaving allows distributing the effect of a single cosmic ray potentially upsetting multiple physically neighboring bits across multiple words by associating neighboring bits to different words. As long as a single-event upset (SEU) does not exceed the error threshold (e.g., a single error) in any particular word between accesses, it can be corrected (e.g., by a single-bit error-correcting code), and the illusion of an error-free memory system may be maintained.In addition to hardware providing features required for ECC memory to operate, operating systems usually contain related reporting facilities that are used to provide notifications when soft errors are transparently recovered. One example is the Linux kernel\\'s EDAC subsystem (previously known as Bluesmoke), which collects the data from error-checking-enabled components inside a computer system; besides collecting and reporting back the events related to ECC memory, it also supports other checksumming errors, including those detected on the PCI bus. A few systems also support memory scrubbing to catch and correct errors early before the become unrecoverable.\\n\\n\\n== See also ==\\nBerger code\\nBurst error-correcting code\\nECC memory, a type of computer data storage\\nLink adaptation\\nList of algorithms § Error detection and correction\\nList of hash functions\\n\\n\\n== References ==\\n\\n\\n== Further reading ==\\nShu Lin; Daniel J. Costello, Jr. (1983). Error Control Coding: Fundamentals and Applications. Prentice Hall. ISBN 0-13-283796-X.\\nSoftECC: A System for Software Memory Integrity Checking\\nA Tunable, Software-based DRAM Error Detection and Correction Library for HPC\\nDetection and Correction of Silent Data Corruption for Large-Scale High-Performance Computing\\n\\n\\n== External links ==\\nThe on-line textbook: Information Theory, Inference, and Learning Algorithms, by David J.C. MacKay, contains chapters on elementary error-correcting codes; on the theoretical limits of error-correction; and on the latest state-of-the-art error-correcting codes, including low-density parity-check codes, turbo codes, and fountain codes.\\nECC Page - implementations of popular ECC encoding and decoding routines', 'Computer security, cybersecurity, or information technology security (IT security) is the protection of computer systems and networks from information disclosure, theft of or damage to their hardware, software, or electronic data, as well as from the disruption or misdirection of the services they provide.The field is becoming increasingly significant due to the continuously expanding reliance on computer systems, the Internet and wireless network standards such as Bluetooth and Wi-Fi, and due to the growth of \"smart\" devices, including smartphones, televisions, and the various devices that constitute the \"Internet of things\". Cybersecurity is also one of the significant challenges in the contemporary world, due to its complexity, both in terms of political usage and technology. \\n\\n\\n== History ==\\nSince the Internet\\'s arrival and with the digital transformation initiated in recent years, the notion of cybersecurity has become a familiar subject both in our professional and personal lives. Cybersecurity and cyber threats have been constant for the last 50 years of technological change. In the 1970s and 1980s, computer security was mainly limited to academia until the conception of the Internet, where, with increased connectivity, computer viruses and network intrusions began to take off. After the spread of viruses in the 1990s, the 2000s marked the institutionalization of cyber threats and cybersecurity.\\nFinally, from the 2010s, large-scale attacks and government regulations started emerging.\\nThe April 1967 session organized by Willis Ware at the Spring Joint Computer Conference, and the later publication of the Ware Report, were foundational moments in the history of the field of computer security. Ware\\'s work straddled the intersection of material, cultural, political, and social concerns.A 1977 NIST publication introduced the \"CIA triad\" of Confidentiality, Integrity, and Availability as a clear and simple way to describe key security goals. While still relevant, many more elaborate frameworks have since been proposed.However, the 1970s and 1980s didn\\'t have any grave computer threats because computers and the internet were still developing, and security threats were easily identifiable. Most often, threats came from malicious insiders who gained unauthorized access to sensitive documents and files. Although malware and network breaches existed during the early years, they did not use them for financial gain. However, by the second half of the 1970s, established computer firms like IBM started offering commercial access control systems and computer security software products.It started with Creeper in 1971. Creeper was an experimental computer program written by Bob Thomas at BBN. It is considered the first computer worm. \\nIn 1972, the first anti-virus software was created, called Reaper. It was created by Ray Tomlinson to move across the ARPANET and delete the Creeper worm. \\nBetween September 1986 and June 1987, a group of German hackers performed the first documented case of cyber espionage. The group hacked into American defense contractors, universities, and military bases\\' networks and sold gathered information to the Soviet KGB. The group was led by Markus Hess, who was arrested on 29 June 1987. He was convicted of espionage (along with two co-conspirators) on  15 Feb 1990.\\nIn 1988, one of the first computer worms, called Morris worm was distributed via the Internet. It gained significant mainstream media attention. \\nIn 1993, Netscape started developing the protocol SSL, shortly after the National Center for Supercomputing Applications (NCSA) launched Mosaic 1.0, the first web browser, in 1993. Netscape had SSL version 1.0 ready in 1994, but it was never released to the public due to many serious security vulnerabilities. These weaknesses included replay attacks and a vulnerability that allowed hackers to alter unencrypted communications sent by users. However, in February 1995, Netscape launched the Version 2.0.\\n\\n\\n=== Failed offensive strategy ===\\nThe National Security Agency (NSA) is responsible for both the protection of U.S. information systems and also for collecting foreign intelligence.  These two duties are in conflict with each other.  Protecting information systems includes evaluating software, identifying security flaws, and taking steps to correct the flaws, which is a defensive action.  Collecting intelligence includes exploiting security flaws to extract information, which is an offensive action.  Correcting security flaws makes the flaws unavailable for NSA exploitation.\\nThe agency analyzes commonly used software in order to find security flaws, which it reserves for offensive purposes against competitors of the United States.  The agency seldom takes defensive action by reporting the flaws to software producers so they can eliminate the security flaws.The offensive strategy worked for a while, but eventually other nations, including Russia, Iran, North Korea, and China have acquired their own offensive capability, and tend to use it against the United States.  NSA contractors created and sold \"click-and-shoot\" attack tools to U.S. agencies and close allies, but eventually the tools made their way to foreign adversaries.  In 2016, NSAs own hacking tools were hacked and have been used by Russia and North Korea.  NSAs employees and contractors have been recruited at high salaries by adversaries, anxious to compete in cyberwarfare.For example, in 2007, the United States and Israel began exploiting security flaws in the Microsoft Windows operating system to attack and damage equipment used in Iran to refine nuclear materials.  Iran responded by heavily investing in their own cyberwarfare capability, which they began using against the United States.\\n\\n\\n== Vulnerabilities and attacks ==\\n\\nA vulnerability is a weakness in design, implementation, operation, or internal control. Most of the vulnerabilities that have been discovered are documented in the Common Vulnerabilities and Exposures (CVE) database. An exploitable vulnerability is one for which at least one working attack or \"exploit\" exists. Vulnerabilities can be researched, reverse-engineered, hunted, or exploited using automated tools or customized scripts. To secure a computer system, it is important to understand the attacks that can be made against it, and these threats can typically be classified into one of these categories below:\\n\\n\\n=== Backdoor ===\\nA backdoor in a computer system, a cryptosystem or an algorithm, is any secret method of bypassing normal authentication or security controls. They may exist for many reasons, including by original design or from poor configuration. They may have been added by an authorized party to allow some legitimate access, or by an attacker for malicious reasons; but regardless of the motives for their existence, they create a vulnerability.  Backdoors can be very hard to detect, and detection of backdoors are usually discovered by someone who has access to application source code or intimate knowledge of Operating System of the computer.\\n\\n\\n=== Denial-of-service attack ===\\nDenial of service attacks (DoS) are designed to make a machine or network resource unavailable to its intended users. Attackers can deny service to individual victims, such as by deliberately entering a wrong password enough consecutive times to cause the victim\\'s account to be locked, or they may overload the capabilities of a machine or network and block all users at once. While a network attack from a single IP address can be blocked by adding a new firewall rule, many forms of Distributed denial of service (DDoS) attacks are possible, where the attack comes from a large number of points – and defending is much more difficult. Such attacks can originate from the zombie computers of a botnet or from a range of other possible techniques, including reflection and amplification attacks, where innocent systems are fooled into sending traffic to the victim.\\n\\n\\n=== Direct-access attacks ===\\nAn unauthorized user gaining physical access to a computer is most likely able to directly copy data from it. They may also compromise security by making operating system modifications, installing software worms, keyloggers, covert listening devices or using wireless microphone. Even when the system is protected by standard security measures, these may be bypassed by booting another operating system or tool from a CD-ROM or other bootable media. Disk encryption and Trusted Platform Module are designed to prevent these attacks.\\n\\n\\n=== Eavesdropping ===\\nEavesdropping is the act of surreptitiously listening to a private computer \"conversation\" (communication), typically between hosts on a network. For instance, programs such as Carnivore and NarusInSight have been used by the FBI and NSA to eavesdrop on the systems of internet service providers. Even machines that operate as a closed system (i.e., with no contact to the outside world) can be eavesdropped upon via monitoring the faint electromagnetic transmissions generated by the hardware; TEMPEST is a specification by the NSA referring to these attacks.\\n\\n\\n=== Multi-vector, polymorphic attacks ===\\nSurfacing in 2017, a new class of multi-vector, polymorphic cyber threats combined several types of attacks and changed form to avoid cybersecurity controls as they spread.\\n\\n\\n=== Phishing ===\\n\\nPhishing is the attempt of acquiring sensitive information such as usernames, passwords, and credit card details directly from users by deceiving the users. Phishing is typically carried out by email spoofing or instant messaging, and it often directs users to enter details at a fake website whose \"look\" and \"feel\" are almost identical to the legitimate one. The fake website often asks for personal information, such as log-in details and passwords. This information can then be used to gain access to the individual\\'s real account on the real website. Preying on a victim\\'s trust, phishing can be classified as a form of social engineering.  Attackers are using creative ways to gain access to real accounts.  A common scam is for attackers to send fake electronic invoices to individuals showing that they recently purchased music, apps, or other, and instructing them to click on a link if the purchases were not authorized.\\n\\n\\n=== Privilege escalation ===\\nPrivilege escalation describes a situation where an attacker with some level of restricted access is able to, without authorization, elevate their privileges or access level. For example, a standard computer user may be able to exploit a vulnerability in the system to gain access to restricted data; or even become \"root\" and have full unrestricted access to a system.\\n\\n\\n=== Reverse engineering ===\\nReverse engineering is the process by which a man-made object is deconstructed to reveal its designs, code, architecture, or to extract knowledge from the object; similar to scientific research, the only difference being that scientific research is about a natural phenomenon.:\\u200a3\\u200a\\n\\n\\n=== Side-channel attack ===\\n\\nAny computational system affects its environment in some form. This effect it has on its environment, includes a wide range of criteria, which can range from electromagnetic radiation, to residual effect on RAM cells which as a consequent make a Cold boot attack possible, to hardware implementation faults which allow for access and or guessing of other values that normally should be inaccessible. In Side-channel attack scenarios the attacker would gather such information about a system or network to guess its internal state, and as a result access the information which is assumed by the victim to be secure.\\n\\n\\n=== Social engineering ===\\nSocial engineering, in the context of computer security, aims to convince a user to disclose secrets such as passwords, card numbers, etc. or grant physical access by, for example, impersonating a senior executive, bank, a contractor, or a customer. This generally involves exploiting peoples trust, and relying on their cognitive biases. A common scam involves emails sent to accounting and finance department personnel, impersonating their CEO and urgently requesting some action. In early 2016, the FBI reported that such \"business email compromise\" (BEC) scams had cost US businesses more than $2 billion in about two years.In May 2016, the Milwaukee Bucks NBA team was the victim of this type of cyber scam with a perpetrator impersonating the team\\'s president Peter Feigin, resulting in the handover of all the team\\'s employees\\' 2015 W-2 tax forms.\\n\\n\\n=== Spoofing ===\\n\\nSpoofing is the act of masquerading as a valid entity through falsification of data (such as an IP address or username), in order to gain access to information or resources that one is otherwise unauthorized to obtain. There are several types of spoofing, including:\\n\\nEmail spoofing, where an attacker forges the sending (From, or source) address of an email.\\nIP address spoofing, where an attacker alters the source IP address in a network packet to hide their identity or impersonate another computing system.\\nMAC spoofing, where an attacker modifies the Media Access Control (MAC) address of their network interface controller to obscure their identity, or to pose as another.\\nBiometric spoofing, where an attacker produces a fake biometric sample to pose as another user.\\n\\n\\n=== Tampering ===\\nTampering describes a malicious modification or alteration of data. So-called Evil Maid attacks and security services planting of surveillance capability into routers are examples.\\n\\n\\n=== Malware ===\\nMalicious software (malware) installed on a computer can leak personal information, can give control of the system to the attacker and can delete data permanently.\\n\\n\\n== Information security culture ==\\nEmployee behavior can have a big impact on information security in organizations. Cultural concepts can help different segments of the organization work effectively or work against effectiveness towards information security within an organization. Information security culture is the \"...totality of patterns of behavior in an organization that contributes to the protection of information of all kinds.\"Andersson and Reimers (2014) found that employees often do not see themselves as part of their organization\\'s information security effort and often take actions that impede organizational changes. Indeed, the Verizon Data Breach Investigations Report 2020, which examined 3,950 security breaches, discovered 30% of cyber security incidents involved internal actors within a company. Research shows information security culture needs to be improved continuously. In ″Information Security Culture from Analysis to Change″, authors commented, ″It\\'s a never-ending process, a cycle of evaluation and change or maintenance.″ To manage the information security culture, five steps should be taken: pre-evaluation, strategic planning, operative planning, implementation, and post-evaluation.\\nPre-evaluation: To identify the awareness of information security within employees and to analyze the current security policies.\\nStrategic planning: To come up with a better awareness program, clear targets need to be set. Assembling a team of skilled professionals is helpful to achieve it.\\nOperative planning: A good security culture can be established based on internal communication, management-buy-in, security awareness and a training program.\\nImplementation: Four stages should be used to implement the information security culture. They are:Commitment of the management\\nCommunication with organizational members\\nCourses for all organizational members\\nCommitment of the employeesPost-evaluation: To assess the success of the planning and implementation, and to identify unresolved areas of concern.\\n\\n\\n== Systems at risk ==\\nThe growth in the number of computer systems and the increasing reliance upon them by individuals, businesses, industries, and governments means that there is an increasing number of systems at risk.\\n\\n\\n=== Financial systems ===\\nThe computer systems of financial regulators and financial institutions like the U.S. Securities and Exchange Commission, SWIFT, investment banks, and commercial banks are prominent hacking targets for cybercriminals interested in manipulating markets and making illicit gains. Websites and apps that accept or store credit card numbers, brokerage accounts, and bank account information are also prominent hacking targets, because of the potential for immediate financial gain from transferring money, making purchases, or selling the information on the black market. In-store payment systems and ATMs have also been tampered with in order to gather customer account data and PINs.\\n\\n\\n=== Utilities and industrial equipment ===\\nComputers control functions at many utilities, including coordination of telecommunications, the power grid, nuclear power plants, and valve opening and closing in water and gas networks. The Internet is a potential attack vector for such machines if connected, but the Stuxnet worm demonstrated that even equipment controlled by computers not connected to the Internet can be vulnerable. In 2014, the Computer Emergency Readiness Team, a division of the Department of Homeland Security, investigated 79 hacking incidents at energy companies.\\n\\n\\n=== Aviation ===\\nThe aviation industry is very reliant on a series of complex systems which could be attacked. A simple power outage at one airport can cause repercussions worldwide, much of the system relies on radio transmissions which could be disrupted, and controlling aircraft over oceans is especially dangerous because radar surveillance only extends 175 to 225 miles offshore. There is also potential for attack from within an aircraft.In Europe, with the (Pan-European Network Service) and NewPENS, and in the US with the NextGen program, air navigation service providers are moving to create their own dedicated networks.\\nThe consequences of a successful attack range from loss of confidentiality to loss of system integrity, air traffic control outages, loss of aircraft, and even loss of life.\\n\\n\\n=== Consumer devices ===\\nDesktop computers and laptops are commonly targeted to gather passwords or financial account information, or to construct a botnet to attack another target. Smartphones, tablet computers, smart watches, and other mobile devices such as quantified self devices like activity trackers have sensors such as cameras, microphones, GPS receivers, compasses, and accelerometers which could be exploited, and may collect personal information, including sensitive health information. WiFi, Bluetooth, and cell phone networks on any of these devices could be used as attack vectors, and sensors might be remotely activated after a successful breach.The increasing number of home automation devices such as the Nest thermostat are also potential targets.\\n\\n\\n=== Large corporations ===\\nLarge corporations are common targets. In many cases attacks are aimed at financial gain through identity theft and involve data breaches. Examples include loss of millions of clients\\' credit card details by Home Depot, Staples, Target Corporation, and the most recent breach of Equifax.Medical records have been targeted in general identify theft, health insurance fraud, and impersonating patients to obtain prescription drugs for recreational purposes or resale. Although cyber threats continue to increase, 62% of all organizations did not increase security training for their business in 2015.Not all attacks are financially motivated, however: security firm HBGary Federal suffered a serious series of attacks in 2011 from hacktivist group Anonymous in retaliation for the firm\\'s CEO claiming to have infiltrated their group, and Sony Pictures was hacked in 2014 with the apparent dual motive of embarrassing the company through data leaks and crippling the company by wiping workstations and servers.\\n\\n\\n=== Automobiles ===\\n\\nVehicles are increasingly computerized, with engine timing, cruise control, anti-lock brakes, seat belt tensioners, door locks, airbags and advanced driver-assistance systems on many models. Additionally, connected cars may use WiFi and Bluetooth to communicate with onboard consumer devices and the cell phone network. Self-driving cars are expected to be even more complex. All of these systems carry some security risk, and such issues have gained wide attention.Simple examples of risk include a malicious compact disc being used as an attack vector, and the car\\'s onboard microphones being used for eavesdropping. However, if access is gained to a car\\'s internal controller area network, the danger is much greater – and in a widely publicized 2015 test, hackers remotely carjacked a vehicle from 10 miles away and drove it into a ditch.Manufacturers are reacting in a number of ways, with Tesla in 2016 pushing out some security fixes \"over the air\" into its cars\\' computer systems. In the area of autonomous vehicles, in September 2016 the United States Department of Transportation announced some initial safety standards, and called for states to come up with uniform policies.\\n\\n\\n=== Government ===\\nGovernment and military computer systems are commonly attacked by activists and foreign powers. Local and regional government infrastructure such as traffic light controls, police and intelligence agency communications, personnel records, student records, and financial systems are also potential targets as they are now all largely computerized. Passports and government ID cards that control access to facilities which use RFID can be vulnerable to cloning.\\n\\n\\n=== Internet of things and physical vulnerabilities ===\\nThe Internet of things (IoT) is the network of physical objects such as devices, vehicles, and buildings that are embedded with electronics, software, sensors, and network connectivity that enables them to collect and exchange data. Concerns have been raised that this is being developed without appropriate consideration of the security challenges involved.While the IoT creates opportunities for more direct integration of the physical world into computer-based systems,\\nit also provides opportunities for misuse. In particular, as the Internet of Things spreads widely, cyberattacks are likely to become an increasingly physical (rather than simply virtual) threat. If a front door\\'s lock is connected to the Internet, and can be locked/unlocked from a phone, then a criminal could enter the home at the press of a button from a stolen or hacked phone. People could stand to lose much more than their credit card numbers in a world controlled by IoT-enabled devices. Thieves have also used electronic means to circumvent non-Internet-connected hotel door locks.An attack that targets physical infrastructure and/or human lives is sometimes referred to as a cyber-kinetic attack. As IoT devices and appliances gain currency, cyber-kinetic attacks can become pervasive and significantly damaging.\\n\\n\\n==== Medical systems ====\\n\\nMedical devices have either been successfully attacked or had potentially deadly vulnerabilities demonstrated, including both in-hospital diagnostic equipment and implanted devices including pacemakers and insulin pumps. There are many reports of hospitals and hospital organizations getting hacked, including ransomware attacks, Windows XP exploits, viruses, and data breaches of sensitive data stored on hospital servers. On 28 December 2016 the US Food and Drug Administration released its recommendations for how medical device manufacturers should maintain the security of Internet-connected devices – but no structure for enforcement.\\n\\n\\n=== Energy sector ===\\nIn distributed generation systems, the risk of a cyber attack is real, according to Daily Energy Insider. An attack could cause a loss of power in a large area for a long period of time, and such an attack could have just as severe consequences as a natural disaster. The District of Columbia is considering creating a Distributed Energy Resources (DER) Authority within the city, with the goal being for customers to have more insight into their own energy use and giving the local electric utility, Pepco, the chance to better estimate energy demand. The D.C. proposal, however, would \"allow third-party vendors to create numerous points of energy distribution, which could potentially create more opportunities for cyber attackers to threaten the electric grid.\"\\n\\n\\n== Impact of security breaches ==\\nSerious financial damage has been caused by security breaches, but because there is no standard model for estimating the cost of an incident, the only data available is that which is made public by the organizations involved. \"Several computer security consulting firms produce estimates of total worldwide losses attributable to virus and worm attacks and to hostile digital acts in general. The 2003 loss estimates by these firms range from $13 billion (worms and viruses only) to $226 billion (for all forms of covert attacks). The reliability of these estimates is often challenged; the underlying methodology is basically anecdotal.\"However, reasonable estimates of the financial cost of security breaches can actually help organizations make rational investment decisions. According to the classic Gordon-Loeb Model analyzing the optimal investment level in information security, one can conclude that the amount a firm spends to protect information should generally be only a small fraction of the expected loss (i.e., the expected value of the loss resulting from a cyber/information security breach).\\n\\n\\n== Attacker motivation ==\\nAs with physical security, the motivations for breaches of computer security vary between attackers. Some are thrill-seekers or vandals, some are activists, others are criminals looking for financial gain. State-sponsored attackers are now common and well resourced but started with amateurs such as Markus Hess who hacked for the KGB, as recounted by Clifford Stoll in The Cuckoo\\'s Egg.\\nAdditionally, recent attacker motivations can be traced back to extremist organizations seeking to gain political advantage or disrupt social agendas. The growth of the internet, mobile technologies, and inexpensive computing devices have led to a rise in capabilities but also to the risk to environments that are deemed as vital to operations. All critical targeted environments are susceptible to compromise and this has led to a series of proactive studies on how to migrate the risk by taking into consideration motivations by these types of actors. Several stark differences exist between the hacker motivation and that of nation state actors seeking to attack based an ideological preference.A standard part of threat modeling for any particular system is to identify what might motivate an attack on that system, and who might be motivated to breach it. The level and detail of precautions will vary depending on the system to be secured. A home personal computer, bank, and classified military network face very different threats, even when the underlying technologies in use are similar.\\n\\n\\n== Computer protection (countermeasures) ==\\nIn computer security, a countermeasure is an action, device, procedure or technique that reduces a threat, a vulnerability, or an attack by eliminating or preventing it, by minimizing the harm it can cause, or by discovering and reporting it so that corrective action can be taken.Some common countermeasures are listed in the following sections:\\n\\n\\n=== Security by design ===\\n\\nSecurity by design, or alternately secure by design, means that the software has been designed from the ground up to be secure. In this case, security is considered as a main feature.\\nSome of the techniques in this approach include:\\n\\nThe principle of least privilege, where each part of the system has only the privileges that are needed for its function. That way, even if an attacker gains access to that part, they only have limited access to the whole system.\\nAutomated theorem proving to prove the correctness of crucial software subsystems.\\nCode reviews and unit testing, approaches to make modules more secure where formal correctness proofs are not possible.\\nDefense in depth, where the design is such that more than one subsystem needs to be violated to compromise the integrity of the system and the information it holds.\\nDefault secure settings, and design to \"fail secure\" rather than \"fail insecure\" (see fail-safe for the equivalent in safety engineering). Ideally, a secure system should require a deliberate, conscious, knowledgeable and free decision on the part of legitimate authorities in order to make it insecure.\\nAudit trails tracking system activity, so that when a security breach occurs, the mechanism and extent of the breach can be determined. Storing audit trails remotely, where they can only be appended to, can keep intruders from covering their tracks.\\nFull disclosure of all vulnerabilities, to ensure that the \"window of vulnerability\" is kept as short as possible when bugs are discovered.\\n\\n\\n=== Security architecture ===\\nThe Open Security Architecture organization defines IT security architecture as \"the design artifacts that describe how the security controls (security countermeasures) are positioned, and how they relate to the overall information technology architecture. These controls serve the purpose to maintain the system\\'s quality attributes: confidentiality, integrity, availability, accountability and assurance services\".Techopedia defines security architecture as \"a unified security design that addresses the necessities and potential risks involved in a certain scenario or environment. It also specifies when and where to apply security controls. The design process is generally reproducible.\" The key attributes of security architecture are:\\nthe relationship of different components and how they depend on each other.\\ndetermination of controls based on risk assessment, good practices, finances, and legal matters.\\nthe standardization of controls.Practicing security architecture provides the right foundation to systematically address business, IT and security concerns in an organization.\\n\\n\\n=== Security measures ===\\nA state of computer \"security\" is the conceptual ideal, attained by the use of the three processes: threat prevention, detection, and response. These processes are based on various policies and system components, which include the following:\\n\\nUser account access controls and cryptography can protect systems files and data, respectively.\\nFirewalls are by far the most common prevention systems from a network security perspective as they can (if properly configured) shield access to internal network services, and block certain kinds of attacks through packet filtering. Firewalls can be both hardware- or software-based.\\nIntrusion Detection System (IDS) products are designed to detect network attacks in-progress and assist in post-attack forensics, while audit trails and logs serve a similar function for individual systems.\\n\"Response\" is necessarily defined by the assessed security requirements of an individual system and may cover the range from simple upgrade of protections to notification of legal authorities, counter-attacks, and the like. In some special cases, the complete destruction of the compromised system is favored, as it may happen that not all the compromised resources are detected.Today, computer security consists mainly of \"preventive\" measures, like firewalls or an exit procedure. A firewall can be defined as a way of filtering network data between a host or a network and another network, such as the Internet, and can be implemented as software running on the machine, hooking into the network stack (or, in the case of most UNIX-based operating systems such as Linux, built into the operating system kernel) to provide real-time filtering and blocking. Another implementation is a so-called \"physical firewall\", which consists of a separate machine filtering network traffic. Firewalls are common amongst machines that are permanently connected to the Internet.\\nSome organizations are turning to big data platforms, such as Apache Hadoop, to extend data accessibility and machine learning to detect advanced persistent threats.However, relatively few organizations maintain computer systems with effective detection systems, and fewer still have organized response mechanisms in place. As a result, as Reuters points out: \"Companies for the first time report they are losing more through electronic theft of data than physical stealing of assets\". The primary obstacle to effective eradication of cybercrime could be traced to excessive reliance on firewalls and other automated \"detection\" systems. Yet it is basic evidence gathering by using packet capture appliances that puts criminals behind bars.In order to ensure adequate security, the confidentiality, integrity and availability of a network, better known as the CIA triad, must be protected and is considered the foundation to information security. To achieve those objectives, administrative, physical and technical security measures should be employed. The amount of security afforded to an asset can only be determined when its value is known.\\n\\n\\n=== Vulnerability management ===\\n\\nVulnerability management is the cycle of identifying, and remediating or mitigating vulnerabilities, especially in software and firmware. Vulnerability management is integral to computer security and network security.\\nVulnerabilities can be discovered with a vulnerability scanner, which analyzes a computer system in search of known vulnerabilities, such as open ports, insecure software configuration, and susceptibility to malware.  In order for these tools to be effective, they must be kept up to date with every new update the vendors release.  Typically, these updates will scan for the new vulnerabilities that were introduced recently.\\nBeyond vulnerability scanning, many organizations contract outside security auditors to run regular penetration tests against their systems to identify vulnerabilities. In some sectors, this is a contractual requirement.\\n\\n\\n=== Reducing vulnerabilities ===\\nWhile formal verification of the correctness of computer systems is possible, it is not yet common. Operating systems formally verified include seL4, and SYSGO\\'s PikeOS – but these make up a very small percentage of the market.\\nTwo factor authentication is a method for mitigating unauthorized access to a system or sensitive information. It requires \"something you know\"; a password or PIN, and \"something you have\"; a card, dongle, cellphone, or another piece of hardware. This increases security as an unauthorized person needs both of these to gain access.\\nSocial engineering and direct computer access (physical) attacks can only be prevented by non-computer means, which can be difficult to enforce, relative to the sensitivity of the information. Training is often involved to help mitigate this risk, but even in highly disciplined environments (e.g. military organizations), social engineering attacks can still be difficult to foresee and prevent.\\nInoculation, derived from inoculation theory, seeks to prevent social engineering and other fraudulent tricks or traps by instilling a resistance to persuasion attempts through exposure to similar or related attempts.It is possible to reduce an attacker\\'s chances by keeping systems up to date with security patches and updates, using a security scanner and/or hiring people with expertise in security, though none of these guarantee the prevention of an attack. The effects of data loss/damage can be reduced by careful backing up and insurance.\\n\\n\\n=== Hardware protection mechanisms ===\\n\\nWhile hardware may be a source of insecurity, such as with microchip vulnerabilities maliciously introduced during the manufacturing process, hardware-based or assisted computer security also offers an alternative to software-only computer security. Using devices and methods such as dongles, trusted platform modules, intrusion-aware cases, drive locks, disabling USB ports, and mobile-enabled access may be considered more secure due to the physical access (or sophisticated backdoor access) required in order to be compromised. Each of these is covered in more detail below.\\n\\nUSB dongles are typically used in software licensing schemes to unlock software capabilities, but they can also be seen as a way to prevent unauthorized access to a computer or other device\\'s software. The dongle, or key, essentially creates a secure encrypted tunnel between the software application and the key. The principle is that an encryption scheme on the dongle, such as Advanced Encryption Standard (AES) provides a stronger measure of security since it is harder to hack and replicate the dongle than to simply copy the native software to another machine and use it. Another security application for dongles is to use them for accessing web-based content such as cloud software or Virtual Private Networks (VPNs). In addition, a USB dongle can be configured to lock or unlock a computer.\\nTrusted platform modules (TPMs) secure devices by integrating cryptographic capabilities onto access devices, through the use of microprocessors, or so-called computers-on-a-chip. TPMs used in conjunction with server-side software offer a way to detect and authenticate hardware devices, preventing unauthorized network and data access.\\nComputer case intrusion detection refers to a device, typically a push-button switch, which detects when a computer case is opened. The firmware or BIOS is programmed to show an alert to the operator when the computer is booted up the next time.\\nDrive locks are essentially software tools to encrypt hard drives, making them inaccessible to thieves. Tools exist specifically for encrypting external drives as well.\\nDisabling USB ports is a security option for preventing unauthorized and malicious access to an otherwise secure computer. Infected USB dongles connected to a network from a computer inside the firewall are considered by the magazine Network World as the most common hardware threat facing computer networks.\\nDisconnecting or disabling peripheral devices ( like camera, GPS, removable storage etc.), that are not in use.\\nMobile-enabled access devices are growing in popularity due to the ubiquitous nature of cell phones. Built-in capabilities such as Bluetooth, the newer Bluetooth low energy (LE), Near field communication (NFC) on non-iOS devices and biometric validation such as thumb print readers, as well as QR code reader software designed for mobile devices, offer new, secure ways for mobile phones to connect to access control systems. These control systems provide computer security and can also be used for controlling access to secure buildings.\\n\\n\\n=== Secure operating systems ===\\n\\nOne use of the term \"computer security\" refers to technology that is used to implement secure operating systems. In the 1980s, the United States Department of Defense (DoD) used the \"Orange Book\" standards, but the current international standard ISO/IEC 15408, \"Common Criteria\" defines a number of progressively more stringent Evaluation Assurance Levels. Many common operating systems meet the EAL4 standard of being \"Methodically Designed, Tested and Reviewed\", but the formal verification required for the highest levels means that they are uncommon. An example of an EAL6 (\"Semiformally Verified Design and Tested\") system is INTEGRITY-178B, which is used in the Airbus A380\\nand several military jets.\\n\\n\\n=== Secure coding ===\\n\\nIn software engineering, secure coding aims to guard against the accidental introduction of security vulnerabilities. It is also possible to create software designed from the ground up to be secure. Such systems are \"secure by design\". Beyond this, formal verification aims to prove the correctness of the algorithms underlying a system;\\nimportant for cryptographic protocols for example.\\n\\n\\n=== Capabilities and access control lists ===\\n\\nWithin computer systems, two of main security models capable of enforcing privilege separation are access control lists (ACLs) and role-based access control (RBAC).\\nAn access-control list (ACL), with respect to a computer file system, is a list of permissions associated with an object. An ACL specifies which users or system processes are granted access to objects, as well as what operations are allowed on given objects.\\nRole-based access control is an approach to restricting system access to authorized users,  used by the majority of enterprises with more than 500 employees, and can implement mandatory access control (MAC) or discretionary access control (DAC).\\nA further approach, capability-based security has been mostly restricted to research operating systems. Capabilities can, however, also be implemented at the language level, leading to a style of programming that is essentially a refinement of standard object-oriented design. An open-source project in the area is the E language.\\n\\n\\n=== End user security training ===\\nThe end-user is widely recognized as the weakest link in the security chain and it is estimated that more than 90% of security incidents and breaches involve some kind of human error. Among the most commonly recorded forms of errors and misjudgment are poor password management, sending emails containing sensitive data and attachments to the wrong recipient, the inability to recognize misleading URLs and to identify fake websites and dangerous email attachments.  A common mistake that users make is saving their user id/password in their browsers to make it easier to log in to banking sites.  This is a gift to attackers who have obtained access to a machine by some means.  The risk may be mitigated by the use of two-factor authentication.As the human component of cyber risk is particularly relevant in determining the global cyber risk an organization is facing, security awareness training, at all levels, not only provides formal compliance with regulatory and industry mandates but is considered essential in reducing cyber risk and protecting individuals and companies from the great majority of cyber threats.\\nThe focus on the end-user represents a profound cultural change for many security practitioners, who have traditionally approached cybersecurity exclusively from a technical perspective, and moves along the lines suggested by major security centers to develop a culture of cyber awareness within the organization, recognizing that a security-aware user provides an important line of defense against cyber attacks.\\n\\n\\n=== Digital hygiene ===\\nRelated to end-user training, digital hygiene or cyber hygiene is a fundamental principle relating to information security and, as the analogy with personal hygiene shows, is the equivalent of establishing simple routine measures to minimize the risks from cyber threats. The assumption is that good cyber hygiene practices can give networked users another layer of protection, reducing the risk that one vulnerable node will be used to either mount attacks or compromise another node or network, especially from common cyberattacks. Cyber hygiene should also not be mistaken for proactive cyber defence, a military term.As opposed to a purely technology-based defense against threats, cyber hygiene mostly regards routine measures that are technically simple to implement and mostly dependent on discipline or education. It can be thought of as an abstract list of tips or measures that have been demonstrated as having a positive effect on personal and/or collective digital security. As such, these measures can be performed by laypeople, not just security experts.\\nCyber hygiene relates to personal hygiene as computer viruses relate to biological viruses (or pathogens). However, while the term computer virus was coined almost simultaneously with the creation of the first working computer viruses, the term cyber hygiene is a much later invention, perhaps as late as 2000 by Internet pioneer Vint Cerf. It has since been adopted by the Congress and Senate of the United States, the FBI, EU institutions and heads of state.\\n\\n\\n=== Response to breaches ===\\nResponding to attempted security breaches is often very difficult for a variety of reasons, including:\\n\\nIdentifying attackers is difficult, as they may operate through proxies, temporary anonymous dial-up accounts, wireless connections, and other anonymizing procedures which make back-tracing difficult -  and are often located in another jurisdiction. If they successfully breach security, they have also often gained enough administrative access to enable them to delete logs to cover their tracks.\\nThe sheer number of attempted attacks, often by automated vulnerability scanners and computer worms, is so large that organizations cannot spend time pursuing each.\\nLaw enforcement officers often lack the skills, interest or budget to pursue attackers. In addition, the identification of attackers across a network may require logs from various points in the network and in many countries, which may be difficult or time-consuming to obtain.Where an attack succeeds and a breach occurs, many jurisdictions now have in place mandatory security breach notification laws.\\n\\n\\n=== Types of security and privacy ===\\n\\n\\n== Incident response planning ==\\nIncident response is an organized approach to addressing and managing the aftermath of a computer security incident or compromise with the goal of preventing a breach or thwarting a cyberattack. An incident that is not identified and managed at the time of intrusion typically escalates to a more damaging event such as a data breach or system failure. The intended outcome of a computer security incident response plan is to contain the incident, limit damage and assist recovery to business as usual. Responding to compromises quickly can mitigate exploited vulnerabilities, restore services and processes and minimize losses.\\nIncident response planning allows an organization to establish a series of best practices to stop an intrusion before it causes damage. Typical incident response plans contain a set of written instructions that outline the organization\\'s response to a cyberattack. Without a documented plan in place, an organization may not successfully detect an intrusion or compromise and stakeholders may not understand their roles, processes and procedures during an escalation, slowing the organization\\'s response and resolution.\\nThere are four key components of a computer security incident response plan:\\n\\nPreparation: Preparing stakeholders on the procedures for handling computer security incidents or compromises\\nDetection and analysis: Identifying and investigating suspicious activity to confirm a security incident, prioritizing the response based on impact and coordinating notification of the incident\\nContainment, eradication and recovery: Isolating affected systems to prevent escalation and limit impact, pinpointing the genesis of the incident, removing malware, affected systems and bad actors from the environment and restoring systems and data when a threat no longer remains\\nPost incident activity: Post mortem analysis of the incident, its root cause and the organization\\'s response with the intent of improving the incident response plan and future response efforts.\\n\\n\\n== Notable attacks and breaches ==\\n\\nSome illustrative examples of different types of computer security breaches are given below.\\n\\n\\n=== Robert Morris and the first computer worm ===\\n\\nIn 1988, 60,000 computers were connected to the Internet, and most were mainframes, minicomputers and professional workstations. On 2 November 1988, many started to slow down, because they were running a malicious code that demanded processor time and that spread itself to other computers – the first internet \"computer worm\". The software was traced back to 23-year-old Cornell University graduate student Robert Tappan Morris who said \"he wanted to count how many machines were connected to the Internet\".\\n\\n\\n=== Rome Laboratory ===\\nIn 1994, over a hundred intrusions were made by unidentified crackers into the Rome Laboratory, the US Air Force\\'s main command and research facility. Using trojan horses, hackers were able to obtain unrestricted access to Rome\\'s networking systems and remove traces of their activities. The intruders were able to obtain classified files, such as air tasking order systems data and furthermore able to penetrate connected networks of National Aeronautics and Space Administration\\'s Goddard Space Flight Center, Wright-Patterson Air Force Base, some Defense contractors, and other private sector organizations, by posing as a trusted Rome center user.\\n\\n\\n=== TJX customer credit card details ===\\nIn early 2007, American apparel and home goods company TJX announced that it was the victim of an unauthorized computer systems intrusion and that the hackers had accessed a system that stored data on credit card, debit card, check, and merchandise return transactions.\\n\\n\\n=== Stuxnet attack ===\\nIn 2010, the computer worm known as Stuxnet reportedly ruined almost one-fifth of Iran\\'s nuclear centrifuges. It did so by disrupting industrial programmable logic controllers (PLCs) in a targeted attack. This is generally believed to have been launched by Israel and the United States to disrupt Iranian\\'s nuclear program – although neither has publicly admitted this.\\n\\n\\n=== Global surveillance disclosures ===\\n\\nIn early 2013, documents provided by Edward Snowden were published by The Washington Post and The Guardian exposing the massive scale of NSA global surveillance. There were also indications that the NSA may have inserted a backdoor in a NIST standard for encryption. This standard was later withdrawn due to widespread criticism. The NSA additionally were revealed to have tapped the links between Google\\'s data centers.\\n\\n\\n=== Target and Home Depot breaches ===\\nIn 2013 and 2014, a Ukrainian hacker known as Rescator broke into Target Corporation computers in 2013, stealing roughly 40 million credit cards, and then Home Depot computers in 2014, stealing between 53 and 56 million credit card numbers. Warnings were delivered at both corporations, but ignored; physical security breaches using self checkout machines are believed to have played a large role. \"The malware utilized is absolutely unsophisticated and uninteresting,\" says Jim Walter, director of threat intelligence operations at security technology company McAfee – meaning that the heists could have easily been stopped by existing antivirus software had administrators responded to the warnings. The size of the thefts has resulted in major attention from state and Federal United States authorities and the investigation is ongoing.\\n\\n\\n=== Office of Personnel Management data breach ===\\nIn April 2015, the Office of Personnel Management discovered it had been hacked more than a year earlier in a data breach, resulting in the theft of approximately 21.5 million personnel records handled by the office. The Office of Personnel Management hack has been described by federal officials as among the largest breaches of government data in the history of the United States. Data targeted in the breach included personally identifiable information such as Social Security numbers, names, dates and places of birth, addresses, and fingerprints of current and former government employees as well as anyone who had undergone a government background check. It is believed the hack was perpetrated by Chinese hackers.\\n\\n\\n=== Ashley Madison breach ===\\n\\nIn July 2015, a hacker group known as \"The Impact Team\" successfully breached the extramarital relationship website Ashley Madison, created by Avid Life Media. The group claimed that they had taken not only company data but user data as well. After the breach, The Impact Team dumped emails from the company\\'s CEO, to prove their point, and threatened to dump customer data unless the website was taken down permanently.\" When Avid Life Media did not take the site offline the group released two more compressed files, one 9.7GB and the second 20GB. After the second data dump, Avid Life Media CEO Noel Biderman resigned; but the website remained functioning.\\n\\n\\n=== Colonial Pipeline Ransomware Attack ===\\nIn June 2021, the cyber attack took down the largest fuel pipeline in the U.S. and led to shortages across the East Coast. \\n\\n\\n== Legal issues and global regulation ==\\nInternational legal issues of cyber attacks are complicated in nature. There is no global base of common rules to judge, and eventually punish, cybercrimes and cybercriminals - and where security firms or agencies do locate the cybercriminal behind the creation of a particular piece of malware or form of cyber attack, often the local authorities cannot take action due to lack of laws under which to prosecute. Proving attribution for cybercrimes and cyberattacks is also a major problem for all law enforcement agencies. \"Computer viruses switch from one country to another, from one jurisdiction to another – moving around the world, using the fact that we don\\'t have the capability to globally police operations like this. So the Internet is as if someone [had] given free plane tickets to all the online criminals of the world.\" The use of techniques such as dynamic DNS, fast flux and bullet proof servers add to the difficulty of investigation and enforcement.\\n\\n\\n== Role of government ==\\nThe role of the government is to make regulations to force companies and organizations to protect their systems, infrastructure and information from any cyberattacks, but also to protect its own national infrastructure such as the national power-grid.The government\\'s regulatory role in cyberspace is complicated. For some, cyberspace was seen as a virtual space that was to remain free of government intervention, as can be seen in many of today\\'s libertarian blockchain and bitcoin discussions.Many government officials and experts think that the government should do more and that there is a crucial need for improved regulation, mainly due to the failure of the private sector to solve efficiently the cybersecurity problem. R. Clarke said during a panel discussion at the RSA Security Conference in San Francisco, he believes that the \"industry only responds when you threaten regulation. If the industry doesn\\'t respond (to the threat), you have to follow through.\" On the other hand, executives from the private sector agree that improvements are necessary, but think that government intervention would affect their ability to innovate efficiently. Daniel R. McCarthy analyzed this public-private partnership in cybersecurity and reflected on the role of cybersecurity in the broader constitution of political order.On 22 May 2020, the UN Security Council held its second ever informal meeting on cybersecurity to focus on cyber challenges to international peace. According to UN Secretary-General António Guterres, new technologies are too often used to violate rights.\\n\\n\\n== International actions ==\\nMany different teams and organizations exist, including:\\n\\nThe Forum of Incident Response and Security Teams (FIRST) is the global association of CSIRTs. The US-CERT, AT&T, Apple, Cisco, McAfee, Microsoft are all members of this international team.\\nThe Council of Europe helps protect societies worldwide from the threat of cybercrime through the Convention on Cybercrime.\\nThe purpose of the Messaging Anti-Abuse Working Group (MAAWG) is to bring the messaging industry together to work collaboratively and to successfully address the various forms of messaging abuse, such as spam, viruses, denial-of-service attacks and other messaging exploitations. France Telecom, Facebook, AT&T, Apple, Cisco, Sprint are some of the members of the MAAWG.\\nENISA : The European Network and Information Security Agency (ENISA) is an agency of the European Union with the objective to improve network and information security in the European Union.\\n\\n\\n=== Europe ===\\nOn 14 April 2016 the European Parliament and Council of the European Union adopted The General Data Protection Regulation (GDPR) (EU) 2016/679. GDPR, which became enforceable beginning 25 May 2018, provides for data protection and privacy for all individuals within the European Union (EU) and the European Economic Area (EEA). GDPR requires that business processes that handle personal data be built with data protection by design and by default. GDPR also requires that certain organizations appoint a Data Protection Officer (DPO).\\n\\n\\n== National actions ==\\n\\n\\n=== Computer emergency response teams ===\\n\\nMost countries have their own computer emergency response team to protect network security.\\n\\n\\n==== Canada ====\\nSince 2010, Canada has had a cybersecurity strategy. This functions as a counterpart document to the National Strategy and Action Plan for Critical Infrastructure. The strategy has three main pillars: securing government systems, securing vital private cyber systems, and helping Canadians to be secure online. There is also a Cyber Incident Management Framework to provide a coordinated response in the event of a cyber incident.The Canadian Cyber Incident Response Centre (CCIRC) is responsible for mitigating and responding to threats to Canada\\'s critical infrastructure and cyber systems. It provides support to mitigate cyber threats, technical support to respond & recover from targeted cyber attacks, and provides online tools for members of Canada\\'s critical infrastructure sectors. It posts regular cybersecurity bulletins & operates an online reporting tool where individuals and organizations can report a cyber incident.To inform the general public on how to protect themselves online, Public Safety Canada has partnered with STOP.THINK.CONNECT, a coalition of non-profit, private sector, and government organizations, and launched the Cyber Security Cooperation Program. They also run the GetCyberSafe portal for Canadian citizens, and Cyber Security Awareness Month during October.Public Safety Canada aims to begin an evaluation of Canada\\'s cybersecurity strategy in early 2015.\\n\\n\\n==== China ====\\nChina\\'s Central Leading Group for Internet Security and Informatization (Chinese: 中央网络安全和信息化领导小组) was established on 27 February 2014. This Leading Small Group (LSG) of the Chinese Communist Party is headed by General Secretary Xi Jinping himself and is staffed with relevant Party and state decision-makers. The LSG was created to overcome the incoherent policies and overlapping responsibilities that characterized China\\'s former cyberspace decision-making mechanisms. The LSG oversees policy-making in the economic, political, cultural, social and military fields as they relate to network security and IT strategy. This LSG also coordinates major policy initiatives in the international arena that promote norms and standards favored by the Chinese government and that emphasizes the principle of national sovereignty in cyberspace.\\n\\n\\n==== Germany ====\\nBerlin starts National Cyber Defense Initiative: On 16 June 2011, the German Minister for Home Affairs, officially opened the new German NCAZ (National Center for Cyber Defense) Nationales Cyber-Abwehrzentrum located in Bonn. The NCAZ closely cooperates with BSI (Federal Office for Information Security) Bundesamt für Sicherheit in der Informationstechnik, BKA (Federal Police Organisation) Bundeskriminalamt (Deutschland), BND (Federal Intelligence Service) Bundesnachrichtendienst, MAD (Military Intelligence Service) Amt für den Militärischen Abschirmdienst and other national organizations in Germany taking care of national security aspects. According to the Minister, the primary task of the new organization founded on 23 February 2011, is to detect and prevent attacks against the national infrastructure and mentioned incidents like Stuxnet. Germany has also established the largest research institution for IT security in Europe, the Center for Research in Security and Privacy (CRISP) in Darmstadt.\\n\\n\\n==== India ====\\nSome provisions for cybersecurity have been incorporated into rules framed under the Information Technology Act 2000.The National Cyber Security Policy 2013 is a policy framework by Ministry of Electronics and Information Technology (MeitY) which aims to protect the public and private infrastructure from cyberattacks, and safeguard \"information, such as personal information (of web users), financial and banking information and sovereign data\". CERT- In is the nodal agency which monitors the cyber threats in the country. The post of National Cyber Security Coordinator has also been created in the Prime Minister\\'s Office (PMO).\\nThe Indian Companies Act 2013 has also introduced cyber law and cybersecurity obligations on the part of Indian directors. Some provisions for cybersecurity have been incorporated into rules framed under the Information Technology Act 2000 Update in 2013.\\n\\n\\n==== South Korea ====\\nFollowing cyber attacks in the first half of 2013, when the government, news media, television station, and bank websites were compromised, the national government committed to the training of 5,000 new cybersecurity experts by 2017. The South Korean government blamed its northern counterpart for these attacks, as well as incidents that occurred in 2009, 2011, and 2012, but Pyongyang denies the accusations.\\n\\n\\n==== United States ====\\n\\n\\n===== Legislation =====\\nThe 1986 18 U.S.C. § 1030, the Computer Fraud and Abuse Act is the key legislation. It prohibits unauthorized access or damage of \"protected computers\" as defined in 18 U.S.C. § 1030(e)(2). Although various other measures have been proposed – none has succeeded.\\nIn 2013, executive order 13636 Improving Critical Infrastructure Cybersecurity was signed, which prompted the creation of the NIST Cybersecurity Framework.\\nIn response to the Colonial Pipeline ransomware attack President Joe Biden signed Executive Order 14028 on May 12, 2021, to increase software security standards for sales to the government, tighten detection and security on existing systems, improve information sharing and training, establish a Cyber Safety Review Board, and improve incident response.\\n\\n\\n===== Standardized government testing services =====\\nThe General Services Administration (GSA) has standardized the \"penetration test\" service as a pre-vetted support service, to rapidly address potential vulnerabilities, and stop adversaries before they impact US federal, state and local governments. These services are commonly referred to as Highly Adaptive Cybersecurity Services (HACS).\\n\\n\\n===== Agencies =====\\nThe Department of Homeland Security has a dedicated division responsible for the response system, risk management program and requirements for cybersecurity in the United States called the National Cyber Security Division. The division is home to US-CERT operations and the National Cyber Alert System. The National Cybersecurity and Communications Integration Center brings together government organizations responsible for protecting computer networks and networked infrastructure.The third priority of the Federal Bureau of Investigation (FBI) is to: \"Protect the United States against cyber-based attacks and high-technology crimes\", and they, along with the National White Collar Crime Center (NW3C), and the Bureau of Justice Assistance (BJA) are part of the multi-agency task force, The Internet Crime Complaint Center, also known as IC3.In addition to its own specific duties, the FBI participates alongside non-profit organizations such as InfraGard.The Computer Crime and Intellectual Property Section (CCIPS) operates in the United States Department of Justice Criminal Division. The CCIPS is in charge of investigating computer crime and intellectual property crime and is specialized in the search and seizure of digital evidence in computers and networks. In 2017, CCIPS published A Framework for a Vulnerability Disclosure Program for Online Systems to help organizations \"clearly describe authorized vulnerability disclosure and discovery conduct, thereby substantially reducing the likelihood that such described activities will result in a civil or criminal violation of law under the Computer Fraud and Abuse Act (18 U.S.C. § 1030).\"The United States Cyber Command, also known as USCYBERCOM, \"has the mission to direct, synchronize, and coordinate cyberspace planning and operations to defend and advance national interests in collaboration with domestic and international partners.\" It has no role in the protection of civilian networks.The U.S. Federal Communications Commission\\'s role in cybersecurity is to strengthen the protection of critical communications infrastructure, to assist in maintaining the reliability of networks during disasters, to aid in swift recovery after, and to ensure that first responders have access to effective communications services.The Food and Drug Administration has issued guidance for medical devices, and the National Highway Traffic Safety Administration is concerned with automotive cybersecurity. After being criticized by the Government Accountability Office, and following successful attacks on airports and claimed attacks on airplanes, the Federal Aviation Administration has devoted funding to securing systems on board the planes of private manufacturers, and the Aircraft Communications Addressing and Reporting System. Concerns have also been raised about the future Next Generation Air Transportation System.\\n\\n\\n===== Computer emergency readiness team =====\\n\"Computer emergency response team\" is a name given to expert groups that handle computer security incidents. In the US, two distinct organization exist, although they do work closely together.\\n\\nUS-CERT: part of the National Cyber Security Division of the United States Department of Homeland Security.\\nCERT/CC: created by the Defense Advanced Research Projects Agency (DARPA) and run by the Software Engineering Institute (SEI).\\n\\n\\n== Modern warfare ==\\n\\nThere is growing concern that cyberspace will become the next theater of warfare. As Mark Clayton from The Christian Science Monitor wrote in a 2015 article titled \"The New Cyber Arms Race\":\\n\\nIn the future, wars will not just be fought by soldiers with guns or with planes that drop bombs. They will also be fought with the click of a mouse a half a world away that unleashes carefully weaponized computer programs that disrupt or destroy critical industries like utilities, transportation, communications, and energy. Such attacks could also disable military networks that control the movement of troops, the path of jet fighters, the command and control of warships.\\nThis has led to new terms such as cyberwarfare and cyberterrorism. The United States Cyber Command was created in 2009 and many other countries have similar forces.\\nThere are a few critical voices that question whether cybersecurity is as significant a threat as it is made out to be.\\n\\n\\n== Careers ==\\nCybersecurity is a fast-growing field of IT concerned with reducing organizations\\' risk of hack or data breach. According to research from the Enterprise Strategy Group, 46% of organizations say that they have a \"problematic shortage\" of cybersecurity skills in 2016, up from 28% in 2015. Commercial, government and non-governmental organizations all employ cybersecurity professionals. The fastest increases in demand for cybersecurity workers are in industries managing increasing volumes of consumer data such as finance, health care, and retail. However, the use of the term \"cybersecurity\" is more prevalent in government job descriptions.Typical cybersecurity job titles and descriptions include:\\n\\n\\n=== Security analyst ===\\nAnalyzes and assesses vulnerabilities in the infrastructure (software, hardware, networks), investigates using available tools and countermeasures to remedy the detected vulnerabilities and recommends solutions and best practices. Analyzes and assesses damage to the data/infrastructure as a result of security incidents, examines available recovery tools and processes, and recommends solutions. Tests for compliance with security policies and procedures. May assist in the creation, implementation, or management of security solutions.\\n\\n\\n=== Security engineer ===\\nPerforms security monitoring, security and data/logs analysis, and forensic analysis, to detect security incidents, and mounts the incident response. Investigates and utilizes new technologies and processes to enhance security capabilities and implement improvements. May also review code or perform other security engineering methodologies.\\n\\n\\n=== Security architect ===\\nDesigns a security system or major components of a security system, and may head a security design team building a new security system.\\n\\n\\n=== Security administrator ===\\nInstalls and manages organization-wide security systems. This position may also include taking on some of the tasks of a security analyst in smaller organizations.\\n\\n\\n=== Chief Information Security Officer (CISO) ===\\nA high-level management position responsible for the entire information security division/staff. The position may include hands-on technical work.\\n\\n\\n=== Chief Security Officer (CSO) ===\\nA high-level management position responsible for the entire security division/staff. A newer position now deemed needed as security risks grow.\\n\\n\\n=== Data Protection Officer (DPO) ===\\nA DPO is tasked with monitoring compliance with the UK GDPR and other data protection laws, our data protection policies, awareness-raising, training, and audits.\\n\\n\\n=== Security Consultant/Specialist/Intelligence ===\\nBroad titles that encompass any one or all of the other roles or titles tasked with protecting computers, networks, software, data or information systems against viruses, worms, spyware, malware, intrusion detection, unauthorized access, denial-of-service attacks, and an ever-increasing list of attacks by hackers acting as individuals or as part of organized crime or foreign governments.Student programs are also available for people interested in beginning a career in cybersecurity. Meanwhile, a flexible and effective option for information security professionals of all experience levels to keep studying is online security training, including webcasts. A wide range of certified courses are also available.In the United Kingdom, a nationwide set of cybersecurity forums, known as the U.K Cyber Security Forum, were established supported by the Government\\'s cybersecurity strategy in order to encourage start-ups and innovation and to address the skills gap identified by the U.K Government.\\nIn Singapore, the Cyber Security Agency has issued a Singapore Operational Technology (OT) Cybersecurity Competency Framework (OTCCF). The framework defines emerging cybersecurity roles in Operational Technology. The OTCCF was endorsed by the Infocomm Media Development Authority (IMDA). It outlines the different OT cybersecurity job positions as well as the technical skills and core competencies necessary. It also depicts the many career paths available, including vertical and lateral advancement opportunities. \\n\\n\\n== Terminology ==\\nThe following terms used with regards to computer security are explained below:\\n\\nAccess authorization restricts access to a computer to a group of users through the use of authentication systems. These systems can protect either the whole computer, such as through an interactive login screen, or individual services, such as a FTP server. There are many methods for identifying and authenticating users, such as passwords, identification cards, smart cards, and biometric systems.\\nAnti-virus software consists of computer programs that attempt to identify, thwart, and eliminate computer viruses and other malicious software (malware).\\nApplications are executable code, so general practice is to disallow users the power to install them; to install only those which are known to be reputable – and to reduce the attack surface by installing as few as possible. They are typically run with least privilege, with a robust process in place to identify, test and install any released security patches or updates for them.\\nAuthentication techniques can be used to ensure that communication end-points are who they say they are.\\nAutomated theorem proving and other verification tools can be used to enable critical algorithms and code used in secure systems to be mathematically proven to meet their specifications.\\nBackups are one or more copies kept of important computer files. Typically, multiple copies will be kept at different locations so that if a copy is stolen or damaged, other copies will still exist.\\nCapability and access control list techniques can be used to ensure privilege separation and mandatory access control. Capabilities vs. ACLs discusses their use.\\nChain of trust techniques can be used to attempt to ensure that all software loaded has been certified as authentic by the system\\'s designers.\\nConfidentiality is the nondisclosure of information except to another authorized person.\\nCryptographic techniques can be used to defend data in transit between systems, reducing the probability that the data exchange between systems can be intercepted or modified.\\nCyberwarfare is an Internet-based conflict that involves politically motivated attacks on information and information systems. Such attacks can, for example, disable official websites and networks, disrupt or disable essential services, steal or alter classified data, and cripple financial systems.\\nData integrity is the accuracy and consistency of stored data, indicated by an absence of any alteration in data between two updates of a data record.\\nEncryption is used to protect the confidentiality of a message. Cryptographically secure ciphers are designed to make any practical attempt of breaking them infeasible. Symmetric-key ciphers are suitable for bulk encryption using shared keys, and public-key encryption using digital certificates can provide a practical solution for the problem of securely communicating when no key is shared in advance.\\nEndpoint security software aids networks in preventing malware infection and data theft at network entry points made vulnerable by the prevalence of potentially infected devices such as laptops, mobile devices, and USB drives.\\nFirewalls serve as a gatekeeper system between networks, allowing only traffic that matches defined rules. They often include detailed logging, and may include intrusion detection and intrusion prevention features. They are near-universal between company local area networks and the Internet, but can also be used internally to impose traffic rules between networks if network segmentation is configured.\\nA hacker is someone who seeks to breach defenses and exploit weaknesses in a computer system or network.\\nHoney pots are computers that are intentionally left vulnerable to attack by crackers. They can be used to catch crackers and to identify their techniques.\\nIntrusion-detection systems are devices or software applications that monitor networks or systems for malicious activity or policy violations.\\nA microkernel is an approach to operating system design which has only the near-minimum amount of code running at the most privileged level – and runs other elements of the operating system such as device drivers, protocol stacks and file systems, in the safer, less privileged user space.\\nPinging. The standard \"ping\" application can be used to test if an IP address is in use. If it is, attackers may then try a port scan to detect which services are exposed.\\nA port scan is used to probe an IP address for open ports to identify accessible network services and applications.\\nA key logger is spyware which silently captures and stores each keystroke that a user types on the computer\\'s keyboard.\\nSocial engineering is the use of deception to manipulate individuals to breach security.\\nLogic bombs is a type of malware added to a legitimate program that lies dormant until it is triggered by a specific event.\\nZero trust security means that no one is trusted by default from inside or outside the network, and verification is required from everyone trying to gain access to resources on the network.\\n\\n\\n== Notable scholars ==\\n\\n\\n== See also ==\\n\\n\\n== References ==\\n\\n\\n== Further reading ==\\nBranch, J. (2020). \"What\\'s in a Name? Metaphors and Cybersecurity.\" International Organization.\\nCostigan, Sean; Hennessy, Michael (2016). Cybersecurity: A Generic Reference Curriculum (PDF). NATO. ISBN 978-9284501960.\\nFuller, Christopher J. \"The Roots of the United States’ Cyber (In)Security,\" Diplomatic History 43:1 (2019): 157–185. online\\nKim, Peter (2014). The Hacker Playbook: Practical Guide To Penetration Testing. Seattle: CreateSpace Independent Publishing Platform. ISBN 978-1494932633.\\nLee, Newton (2015). Counterterrorism and Cybersecurity: Total Information Awareness (2nd ed.). Springer. ISBN 978-3319172439.\\nMontagnani, Maria Lillà and Cavallo, Mirta Antonella (26 July 2018). \"Cybersecurity and Liability in a Big Data World\". SSRN.\\nSinger, P. W.; Friedman, Allan (2014). Cybersecurity and Cyberwar: What Everyone Needs to Know. Oxford University Press. ISBN 978-0199918119.\\nWu, Chwan-Hwa (John); Irwin, J. David (2013). Introduction to Computer Networks and Cybersecurity. Boca Raton: CRC Press. ISBN 978-1466572133.\\nM. Shariati et al. / Procedia Computer Science 3 (2011) 537–543. Enterprise information security, a review of architectures and frameworks from interoperability perspective\\n\\n\\n== External links ==\\nComputer security at Curlie\\nCybersecurity Websites—Cybersecurity & Information Systems Information Analysis Center (CSIAC)', 'Cryptanalysis (from the Greek kryptós, \"hidden\", and analýein, \"to analyze\") refers to the process of analyzing information systems in order to understand hidden aspects of the systems.  Cryptanalysis is used to breach cryptographic security systems and gain access to the contents of encrypted messages, even if the cryptographic key is unknown.\\nIn addition to mathematical analysis of cryptographic algorithms, cryptanalysis includes the study of side-channel attacks that do not target weaknesses in the cryptographic algorithms themselves, but instead exploit weaknesses in their implementation.Even though the goal has been the same, the methods and techniques of cryptanalysis have changed drastically through the history of cryptography, adapting to increasing cryptographic complexity, ranging from the pen-and-paper methods of the past, through machines like the British Bombes and Colossus computers at Bletchley Park in World War II, to the mathematically advanced computerized schemes of the present. Methods for breaking modern cryptosystems often involve solving carefully constructed problems in pure mathematics, the best-known being integer factorization.\\n\\n\\n== Overview ==\\nGiven some encrypted data (\"ciphertext\"), the goal of the cryptanalyst is to gain as much information as possible about the original, unencrypted data (\"plaintext\"). Cryptographic attacks can be characterized in a number of ways:\\n\\n\\n=== Amount of information available to the attacker ===\\nAttacks can be classified based on what type of information the attacker has available. As a basic starting point it is normally assumed that, for the purposes of analysis, the general algorithm is known; this is Shannon\\'s Maxim \"the enemy knows the system\" — in its turn, equivalent to Kerckhoffs\\' principle. This is a reasonable assumption in practice — throughout history, there are countless examples of secret algorithms falling into wider knowledge, variously through espionage, betrayal and reverse engineering. (And on occasion, ciphers have been broken through pure deduction; for example, the German Lorenz cipher and the Japanese Purple code, and a variety of classical schemes):\\nCiphertext-only: the cryptanalyst has access only to a collection of ciphertexts or codetexts.\\nKnown-plaintext: the attacker has a set of ciphertexts to which they know the corresponding plaintext.\\nChosen-plaintext (chosen-ciphertext): the attacker can obtain the ciphertexts (plaintexts) corresponding to an arbitrary set of plaintexts (ciphertexts) of their own choosing.\\nAdaptive chosen-plaintext: like a chosen-plaintext attack, except the attacker can choose subsequent plaintexts based on information learned from previous encryptions, similarly to the Adaptive chosen ciphertext attack.\\nRelated-key attack: Like a chosen-plaintext attack, except the attacker can obtain ciphertexts encrypted under two different keys. The keys are unknown, but the relationship between them is known; for example, two keys that differ in the one bit.\\n\\n\\n=== Computational resources required ===\\nAttacks can also be characterised by the resources they require. Those resources include:\\nTime — the number of computation steps (e.g., test encryptions) which must be performed.\\nMemory — the amount of storage required to perform the attack.\\nData — the quantity and type of plaintexts and ciphertexts required for a particular approach.It\\'s sometimes difficult to predict these quantities precisely, especially when the attack isn\\'t practical to actually implement for testing. But academic cryptanalysts tend to provide at least the estimated order of magnitude of their attacks\\' difficulty, saying, for example, \"SHA-1 collisions now 252.\"Bruce Schneier notes that even computationally impractical attacks can be considered breaks: \"Breaking a cipher simply means finding a weakness in the cipher that can be exploited with a complexity less than brute force. Never mind that brute-force might require 2128 encryptions; an attack requiring 2110 encryptions would be considered a break...simply put, a break can just be a certificational weakness: evidence that the cipher does not perform as advertised.\"\\n\\n\\n=== Partial breaks ===\\nThe results of cryptanalysis can also vary in usefulness. Cryptographer Lars Knudsen (1998) classified various types of attack on block ciphers according to the amount and quality of secret information that was discovered:\\n\\nTotal break — the attacker deduces the secret key.\\nGlobal deduction — the attacker discovers a functionally equivalent algorithm for encryption and decryption, but without learning the key.\\nInstance (local) deduction — the attacker discovers additional plaintexts (or ciphertexts) not previously known.\\nInformation deduction — the attacker gains some Shannon information about plaintexts (or ciphertexts) not previously known.\\nDistinguishing algorithm — the attacker can distinguish the cipher from a random permutation.Academic attacks are often against weakened versions of a cryptosystem, such as a block cipher or hash function with some rounds removed. Many, but not all, attacks become exponentially more difficult to execute as rounds are added to a cryptosystem, so it\\'s possible for the full cryptosystem to be strong even though reduced-round variants are weak. Nonetheless, partial breaks that come close to breaking the original cryptosystem may mean that a full break will follow; the successful attacks on DES, MD5, and SHA-1 were all preceded by attacks on weakened versions.\\nIn academic cryptography, a weakness or a break in a scheme is usually defined quite conservatively: it might require impractical amounts of time, memory, or known plaintexts. It also might require the attacker be able to do things many real-world attackers can\\'t: for example, the attacker may need to choose particular plaintexts to be encrypted or even to ask for plaintexts to be encrypted using several keys related to the secret key. Furthermore, it might only reveal a small amount of information, enough to prove the cryptosystem imperfect but too little to be useful to real-world attackers. Finally, an attack might only apply to a weakened version of cryptographic tools, like a reduced-round block cipher, as a step towards breaking the full system.\\n\\n\\n== History ==\\n\\nCryptanalysis has coevolved together with cryptography, and the contest can be traced through the history of cryptography—new ciphers being designed to replace old broken designs, and new cryptanalytic techniques invented to crack the improved schemes. In practice, they are viewed as two sides of the same coin: secure cryptography requires design against possible cryptanalysis.\\n\\n\\n=== Classical ciphers ===\\n\\nAlthough the actual word \"cryptanalysis\" is relatively recent (it was coined by William Friedman in 1920), methods for breaking codes and ciphers are much older. David Kahn notes in The Codebreakers that Arab scholars were the first people to systematically document cryptanalytic methods.The first known recorded explanation of cryptanalysis was given by Al-Kindi (c. 801–873, also known as \"Alkindus\" in Europe), a 9th-century Arab polymath, in Risalah fi Istikhraj al-Mu\\'amma (A Manuscript on Deciphering Cryptographic Messages). This treatise contains the first description of the method of frequency analysis. Al-Kindi is thus regarded as the first codebreaker in history. His breakthrough work was influenced by Al-Khalil (717–786), who wrote the Book of Cryptographic Messages, which contains the first use of permutations and combinations to list all possible Arabic words with and without vowels.Frequency analysis is the basic tool for breaking most classical ciphers. In natural languages, certain letters of the alphabet appear more often than others; in English, \"E\" is likely to be the most common letter in any sample of plaintext. Similarly, the digraph \"TH\" is the most likely pair of letters in English, and so on. Frequency analysis relies on a cipher failing to hide these statistics. For example, in a simple substitution cipher (where each letter is simply replaced with another), the most frequent letter in the ciphertext would be a likely candidate for \"E\". Frequency analysis of such a cipher is therefore relatively easy, provided that the ciphertext is long enough to give a reasonably representative count of the letters of the alphabet that it contains.Al-Kindi\\'s invention of the frequency analysis technique for breaking monoalphabetic substitution ciphers was the most significant cryptanalytic advance until World War II. Al-Kindi\\'s Risalah fi Istikhraj al-Mu\\'amma described the first cryptanalytic techniques, including some for polyalphabetic ciphers, cipher classification, Arabic phonetics and syntax, and most importantly, gave the first descriptions on frequency analysis. He also covered methods of encipherments, cryptanalysis of certain encipherments, and statistical analysis of letters and letter combinations in Arabic. An important contribution of Ibn Adlan (1187–1268) was on sample size for use of frequency analysis.In Europe, Italian scholar Giambattista della Porta (1535-1615) was the author of a seminal work on cryptanalysis, De Furtivis Literarum Notis.Successful cryptanalysis has undoubtedly influenced history; the ability to read the presumed-secret thoughts and plans of others can be a decisive advantage. For example, in England in 1587, Mary, Queen of Scots was tried and executed for treason as a result of her involvement in three plots to assassinate Elizabeth I of England. The plans came to light after her coded correspondence with fellow conspirators was deciphered by Thomas Phelippes.\\nIn Europe during the 15th and 16th centuries, the idea of a polyalphabetic substitution cipher was developed, among others by the French diplomat Blaise de Vigenère (1523–96). For some three centuries, the Vigenère cipher, which uses a repeating key to select different encryption alphabets in rotation, was considered to be completely secure (le chiffre indéchiffrable—\"the indecipherable cipher\"). Nevertheless, Charles Babbage (1791–1871) and later, independently, Friedrich Kasiski (1805–81) succeeded in breaking this cipher. During World War I, inventors in several countries developed rotor cipher machines such as Arthur Scherbius\\' Enigma, in an attempt to minimise the repetition that had been exploited to break the Vigenère system.\\n\\n\\n=== Ciphers from World War I and World War II ===\\n\\nIn World War I, the breaking of the Zimmermann Telegram was instrumental in bringing the United States into the war. In World War II, the Allies benefitted enormously from their joint success cryptanalysis of the German ciphers — including the Enigma machine and the Lorenz cipher — and Japanese ciphers, particularly \\'Purple\\' and JN-25. \\'Ultra\\' intelligence has been credited with everything between shortening the end of the European war by up to two years, to determining the eventual result. The war in the Pacific was similarly helped by \\'Magic\\' intelligence.Cryptanalysis of enemy messages played a significant part in the Allied victory in World War II. F. W. Winterbotham, quoted the western Supreme Allied Commander, Dwight D. Eisenhower, at the war\\'s end as describing Ultra intelligence as having been \"decisive\" to Allied victory. Sir Harry Hinsley, official historian of British Intelligence in World War II, made a similar assessment about Ultra, saying that it shortened the war \"by not less than two years and probably by four years\"; moreover, he said that in the absence of Ultra, it is uncertain how the war would have ended.In practice, frequency analysis relies as much on linguistic knowledge as it does on statistics, but as ciphers became more complex, mathematics became more important in cryptanalysis. This change was particularly evident before and during World War II, where efforts to crack Axis ciphers required new levels of mathematical sophistication. Moreover, automation was first applied to cryptanalysis in that era with the Polish Bomba device, the British Bombe, the use of punched card equipment, and in the Colossus computers — the first electronic digital computers to be controlled by a program.\\n\\n\\n==== Indicator ====\\nWith reciprocal machine ciphers such as the Lorenz cipher and the Enigma machine used by Nazi Germany during World War II, each message had its own key. Usually, the transmitting operator informed the receiving operator of this message key by transmitting some plaintext and/or ciphertext before the enciphered message. This is termed the indicator, as it indicates to the receiving operator how to set his machine to decipher the message.Poorly designed and implemented indicator systems allowed first Polish cryptographers and then the British cryptographers at Bletchley Park to break the Enigma cipher system. Similar poor indicator systems allowed the British to identify depths that led to the diagnosis of the Lorenz SZ40/42 cipher system, and the comprehensive breaking of its messages without the cryptanalysts seeing the cipher machine.\\n\\n\\n==== Depth ====\\nSending two or more messages with the same key is an insecure process. To a cryptanalyst the messages are then said to be \"in depth.\" This may be detected by the messages having the same indicator by which the sending operator informs the receiving operator about the key generator initial settings for the message.Generally, the cryptanalyst may benefit from lining up identical enciphering operations among a set of messages. For example, the Vernam cipher enciphers by bit-for-bit combining plaintext with a long key using the \"exclusive or\" operator, which is also known as \"modulo-2 addition\" (symbolized by ⊕ ):\\n\\nPlaintext ⊕ Key = CiphertextDeciphering combines the same key bits with the ciphertext to reconstruct the plaintext:\\n\\nCiphertext ⊕ Key = Plaintext(In modulo-2 arithmetic, addition is the same as subtraction.) When two such ciphertexts are aligned in depth, combining them eliminates the common key, leaving just a combination of the two plaintexts:\\n\\nCiphertext1 ⊕ Ciphertext2 = Plaintext1 ⊕ Plaintext2The individual plaintexts can then be worked out linguistically by trying probable words (or phrases), also known as \"cribs,\" at various locations; a correct guess, when combined with the merged plaintext stream, produces intelligible text from the other plaintext component:\\n\\n(Plaintext1 ⊕ Plaintext2) ⊕ Plaintext1 = Plaintext2The recovered fragment of the second plaintext can often be extended in one or both directions, and the extra characters can be combined with the merged plaintext stream to extend the first plaintext. Working back and forth between the two plaintexts, using the intelligibility criterion to check guesses, the analyst may recover much or all of the original plaintexts. (With only two plaintexts in depth, the analyst may not know which one corresponds to which ciphertext, but in practice this is not a large problem.) When a recovered plaintext is then combined with its ciphertext, the key is revealed:\\n\\nPlaintext1 ⊕ Ciphertext1 = KeyKnowledge of a key then allows the analyst to read other messages encrypted with the same key, and knowledge of a set of related keys may allow cryptanalysts to diagnose the system used for constructing them.\\n\\n\\n=== Development of modern cryptography ===\\nGovernments have long recognized the potential benefits of cryptanalysis for intelligence, both military and diplomatic, and established dedicated organizations devoted to breaking the codes and ciphers of other nations, for example, GCHQ and the NSA, organizations which are still very active today.\\n\\nEven though computation was used to great effect in the cryptanalysis of the Lorenz cipher and other systems during World War II, it also made possible new methods of cryptography orders of magnitude more complex than ever before. Taken as a whole, modern cryptography has become much more impervious to cryptanalysis than the pen-and-paper systems of the past, and now seems to have the upper hand against pure cryptanalysis. The historian David Kahn notes:\\nMany are the cryptosystems offered by the hundreds of commercial vendors today that cannot be broken by any known methods of cryptanalysis. Indeed, in such systems even a chosen plaintext attack, in which a selected plaintext is matched against its ciphertext, cannot yield the key that unlock[s] other messages. In a sense, then, cryptanalysis is dead. But that is not the end of the story. Cryptanalysis may be dead, but there is - to mix my metaphors - more than one way to skin a cat.\\nKahn goes on to mention increased opportunities for interception, bugging, side channel attacks, and quantum computers as replacements for the traditional means of cryptanalysis. In 2010, former NSA technical director Brian Snow said that both academic and government cryptographers are \"moving very slowly forward in a mature field.\"However, any postmortems for cryptanalysis may be premature. While the effectiveness of cryptanalytic methods employed by intelligence agencies remains unknown, many serious attacks against both academic and practical cryptographic primitives have been published in the modern era of computer cryptography:\\nThe block cipher Madryga, proposed in 1984 but not widely used, was found to be susceptible to ciphertext-only attacks in 1998.\\nFEAL-4, proposed as a replacement for the DES standard encryption algorithm but not widely used, was demolished by a spate of attacks from the academic community, many of which are entirely practical.\\nThe A5/1, A5/2, CMEA, and DECT systems used in mobile and wireless phone technology can all be broken in hours, minutes or even in real-time using widely available computing equipment.\\nBrute-force keyspace search has broken some real-world ciphers and applications, including single-DES (see EFF DES cracker), 40-bit \"export-strength\" cryptography, and the DVD Content Scrambling System.\\nIn 2001, Wired Equivalent Privacy (WEP), a protocol used to secure Wi-Fi wireless networks, was shown to be breakable in practice because of a weakness in the RC4 cipher and aspects of the WEP design that made related-key attacks practical. WEP was later replaced by Wi-Fi Protected Access.\\nIn 2008, researchers conducted a proof-of-concept break of SSL using weaknesses in the MD5 hash function and certificate issuer practices that made it possible to exploit collision attacks on hash functions. The certificate issuers involved changed their practices to prevent the attack from being repeated.Thus, while the best modern ciphers may be far more resistant to cryptanalysis than the Enigma, cryptanalysis and the broader field of information security remain quite active.\\n\\n\\n== Symmetric ciphers ==\\nBoomerang attack\\nBrute-force attack\\nDavies\\' attack\\nDifferential cryptanalysis\\nImpossible differential cryptanalysis\\nImprobable differential cryptanalysis\\nIntegral cryptanalysis\\nLinear cryptanalysis\\nMeet-in-the-middle attack\\nMod-n cryptanalysis\\nRelated-key attack\\nSandwich attack\\nSlide attack\\nXSL attack\\n\\n\\n== Asymmetric ciphers ==\\nAsymmetric cryptography (or public-key cryptography) is cryptography that relies on using two (mathematically related) keys; one private, and one public. Such ciphers invariably rely on \"hard\" mathematical problems as the basis of their security, so an obvious point of attack is to develop methods for solving the problem. The security of two-key cryptography depends on mathematical questions in a way that single-key cryptography generally does not, and conversely links cryptanalysis to wider mathematical research in a new way.Asymmetric schemes are designed around the (conjectured) difficulty of solving various mathematical problems. If an improved algorithm can be found to solve the problem, then the system is weakened. For example, the security of the Diffie–Hellman key exchange scheme depends on the difficulty of calculating the discrete logarithm. In 1983, Don Coppersmith found a faster way to find discrete logarithms (in certain groups), and thereby requiring cryptographers to use larger groups (or different types of groups). RSA\\'s security depends (in part) upon the difficulty of integer factorization — a breakthrough in factoring would impact the security of RSA.In 1980, one could factor a difficult 50-digit number at an expense of 1012 elementary computer operations. By 1984 the state of the art in factoring algorithms had advanced to a point where a 75-digit number could be factored in 1012 operations. Advances in computing technology also meant that the operations could be performed much faster, too. Moore\\'s law predicts that computer speeds will continue to increase. Factoring techniques may continue to do so as well, but will most likely depend on mathematical insight and creativity, neither of which has ever been successfully predictable. 150-digit numbers of the kind once used in RSA have been factored. The effort was greater than above, but was not unreasonable on fast modern computers. By the start of the 21st century, 150-digit numbers were no longer considered a large enough key size for RSA. Numbers with several hundred digits were still considered too hard to factor in 2005, though methods will probably continue to improve over time, requiring key size to keep pace or other methods such as elliptic curve cryptography to be used.Another distinguishing feature of asymmetric schemes is that, unlike attacks on symmetric cryptosystems, any cryptanalysis has the opportunity to make use of knowledge gained from the public key.\\n\\n\\n== Attacking cryptographic hash systems ==\\nBirthday attack\\nHash function security summary\\nRainbow table\\n\\n\\n== Side-channel attacks ==\\n\\nBlack-bag cryptanalysis\\nMan-in-the-middle attack\\nPower analysis\\nReplay attack\\nRubber-hose cryptanalysis\\nTiming analysis\\n\\n\\n== Quantum computing applications for cryptanalysis ==\\nQuantum computers, which are still in the early phases of research, have potential use in cryptanalysis. For example, Shor\\'s Algorithm could factor large numbers in polynomial time, in effect breaking some commonly used forms of public-key encryption.By using Grover\\'s algorithm on a quantum computer, brute-force key search can be made quadratically faster. However, this could be countered by doubling the key length.\\n\\n\\n== See also ==\\nEconomics of security\\nGlobal surveillance\\nInformation assurance, a term for information security often used in government\\nInformation security, the overarching goal of most cryptography\\nNational Cipher Challenge\\nSecurity engineering, the design of applications and protocols\\nSecurity vulnerability; vulnerabilities can include cryptographic or other flaws\\nTopics in cryptography\\nZendian Problem\\n\\n\\n=== Historic cryptanalysts ===\\nConel Hugh O\\'Donel Alexander\\nCharles Babbage\\nLambros D. Callimahos\\nJoan Clarke\\nAlastair Denniston\\nAgnes Meyer Driscoll\\nElizebeth Friedman\\nWilliam F. Friedman\\nMeredith Gardner\\nFriedrich Kasiski\\nAl-Kindi\\nDilly Knox\\nSolomon Kullback\\nMarian Rejewski\\nJoseph Rochefort, whose contributions affected the outcome of the Battle of Midway\\nFrank Rowlett\\nAbraham Sinkov\\nGiovanni Soro, the Renaissance\\'s first outstanding cryptanalyst\\nJohn Tiltman\\nAlan Turing\\nWilliam T. Tutte\\nJohn Wallis - 17th-century English mathematician\\nWilliam Stone Weedon - worked with Fredson Bowers in World War II\\nHerbert Yardley\\n\\n\\n== References ==\\n\\n\\n=== Citations ===\\n\\n\\n=== Sources ===\\n\\n\\n== Further reading ==\\nBard, Gregory V. (2009). Algebraic Cryptanalysis. Springer. ISBN 978-1-4419-1019-6.\\nHinek, M. Jason (2009). Cryptanalysis of RSA and Its Variants. CRC Press. ISBN 978-1-4200-7518-2.\\nJoux, Antoine (2009). Algorithmic Cryptanalysis. CRC Press. ISBN 978-1-4200-7002-6.\\nJunod, Pascal; Canteaut, Anne (2011). Advanced Linear Cryptanalysis of Block and Stream Ciphers. IOS Press. ISBN 978-1-60750-844-1.\\nStamp, Mark & Low, Richard (2007). Applied Cryptanalysis: Breaking Ciphers in the Real World. John Wiley & Sons. ISBN 978-0-470-11486-5.CS1 maint: uses authors parameter (link)\\nSwenson, Christopher (2008). Modern cryptanalysis: techniques for advanced code breaking. John Wiley & Sons. ISBN 978-0-470-13593-8.\\nWagstaff, Samuel S. (2003). Cryptanalysis of number-theoretic ciphers. CRC Press. ISBN 978-1-58488-153-7.\\n\\n\\n== External links ==\\nBasic Cryptanalysis (files contain 5 line header, that has to be removed first)\\nDistributed Computing Projects\\nList of tools for cryptanalysis on modern cryptography\\nSimon Singh\\'s crypto corner\\nThe National Museum of Computing\\nUltraAnvil tool for attacking simple substitution ciphers\\nHow Alan Turing Cracked The Enigma Code Imperial War Museums', 'A security protocol (cryptographic protocol or encryption protocol) is an abstract or concrete protocol that performs a security-related function and applies cryptographic methods, often as sequences of cryptographic primitives. A protocol describes how the algorithms should be used and includes details about data structures and representations, at which point it can be used to implement multiple, interoperable versions of a program.Cryptographic protocols are widely used for secure application-level data transport. A cryptographic protocol usually incorporates at least some of these aspects:\\n\\nKey agreement or establishment\\nEntity authentication\\nSymmetric encryption and message authentication   material construction\\nSecured application-level data transport\\nNon-repudiation methods\\nSecret sharing methods\\nSecure multi-party computationFor example, Transport Layer Security (TLS) is a cryptographic protocol that is used to secure web (HTTPS) connections. It has an entity authentication mechanism, based on the X.509 system; a key setup phase, where a symmetric encryption key is formed by employing public-key cryptography; and an application-level data transport function. These three aspects have important interconnections. Standard TLS does not have non-repudiation support.\\nThere are other types of cryptographic protocols as well, and even the term itself has various readings; Cryptographic application protocols often use one or more underlying key agreement methods, which are also sometimes themselves referred to as \"cryptographic protocols\". For instance, TLS employs what is known as the Diffie–Hellman key exchange, which although it is only a part of TLS per se, Diffie–Hellman may be seen as a complete cryptographic protocol in itself for other applications.\\n\\n\\n== Advanced cryptographic protocols ==\\nA wide variety of cryptographic protocols go beyond the traditional goals of data confidentiality, integrity, and authentication to also secure a variety of other desired characteristics of computer-mediated collaboration. Blind signatures can be used for digital cash and digital credentials to prove that a person holds an attribute or right without revealing that person\\'s identity or the identities of parties that person transacted with.  Secure digital timestamping can be used to prove that data (even if confidential) existed at a certain time.  Secure multiparty computation can be used to compute answers (such as determining the highest bid in an auction) based on confidential data (such as private bids), so that when the protocol is complete the participants know only their own input and the answer. End-to-end auditable voting systems provide sets of desirable privacy and auditability properties for conducting e-voting.  Undeniable signatures include interactive protocols that allow the signer to prove a forgery and limit who can verify the signature.  Deniable encryption augments standard encryption by making it impossible for an attacker to mathematically prove the existence of a plain text message. Digital mixes create hard-to-trace communications.\\n\\n\\n== Formal verification ==\\nCryptographic protocols can sometimes be verified formally on an abstract level. When it is done, there is a necessity to formalize the environment in which the protocol operates in order to identify threats. This is frequently done through the Dolev-Yao model.\\nLogics, concepts and calculi used for formal reasoning of security protocols:\\n\\nBurrows–Abadi–Needham logic (BAN logic)\\nDolev–Yao model\\nπ-calculus\\nProtocol composition logic (PCL)\\nStrand spaceResearch projects and tools used for formal verification of security protocols:\\n\\nAutomated Validation of Internet Security Protocols and Applications (AVISPA) and follow-up project AVANTSSARConstraint Logic-based Attack Searcher (CL-AtSe)\\nOpen-Source Fixed-Point Model-Checker (OFMC)\\nSAT-based Model-Checker (SATMC)\\nCasper\\nCryptoVerif\\nCryptographic Protocol Shapes Analyzer (CPSA)\\nKnowledge In Security protocolS (KISS)\\nMaude-NRL Protocol Analyzer (Maude-NPA)\\nProVerif\\nScyther\\nTamarin Prover\\n\\n\\n=== Notion of abstract protocol ===\\n\\nTo formally verify a protocol it is often abstracted and modelled using Alice & Bob notation. A simple example is the following:\\n\\n  \\n    \\n      \\n        A\\n        →\\n        B\\n        :\\n        {\\n        X\\n        \\n          }\\n          \\n            \\n              K\\n              \\n                A\\n                ,\\n                B\\n              \\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle A\\\\rightarrow B:\\\\{X\\\\}_{K_{A,B}}}\\n  This states that Alice \\n  \\n    \\n      \\n        A\\n      \\n    \\n    {\\\\displaystyle A}\\n   intends a message for Bob \\n  \\n    \\n      \\n        B\\n      \\n    \\n    {\\\\displaystyle B}\\n   consisting of a message \\n  \\n    \\n      \\n        X\\n      \\n    \\n    {\\\\displaystyle X}\\n   encrypted under shared key \\n  \\n    \\n      \\n        \\n          K\\n          \\n            A\\n            ,\\n            B\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle K_{A,B}}\\n  .\\n\\n\\n== Examples ==\\nInternet Key Exchange\\nIPsec\\nKerberos\\nOff-the-Record Messaging\\nPoint to Point Protocol\\nSecure Shell (SSH)\\nSignal Protocol\\nTransport Layer Security\\nZRTP\\n\\n\\n== See also ==\\nList of cryptosystems\\nSecure channel\\nSecurity Protocols Open Repository\\n\\n\\n== References ==\\n\\n\\n== Further reading ==\\nErmoshina, Ksenia; Musiani, Francesca; Halpin, Harry (September 2016). \"End-to-End Encrypted Messaging Protocols: An Overview\" (PDF).  In Bagnoli, Franco;  et al. (eds.). Internet Science. INSCI 2016. Florence, Italy: Springer. pp. 244–254. doi:10.1007/978-3-319-45982-0_22. ISBN 978-3-319-45982-0.\\n\\n\\n== External links ==\\nSecure protocols open repository', 'In computer engineering, computer architecture is a set of rules and methods that describe the functionality, organization, and implementation of computer systems. The architecture of a system refers to its structure in terms of separately specified components of that system and their interrelationships.Some definitions of architecture define it as describing the capabilities and programming model of a computer but not a particular implementation. In other definitions computer architecture involves instruction set architecture design, microarchitecture design, logic design, and implementation.\\n\\n\\n== History ==\\nThe first documented computer architecture was in the correspondence between Charles Babbage and Ada Lovelace, describing the analytical engine. When building the computer Z1 in 1936, Konrad Zuse described in two patent applications for his future projects that machine instructions could be stored in the same storage used for data, i.e., the stored-program concept. Two other early and important examples are:\\n\\nJohn von Neumann\\'s 1945 paper, First Draft of a Report on the EDVAC, which described an organization of logical elements; and\\nAlan Turing\\'s more detailed Proposed Electronic Calculator for the Automatic Computing Engine, also 1945 and which cited John von Neumann\\'s paper.The term “architecture” in computer literature can be traced to the work of Lyle R. Johnson and Frederick P. Brooks, Jr., members of the Machine Organization department in IBM\\'s main research center in 1959. Johnson had the opportunity to write a proprietary research communication about the Stretch, an IBM-developed supercomputer for Los Alamos National Laboratory (at the time known as Los Alamos Scientific Laboratory). To describe the level of detail for discussing the luxuriously embellished computer, he noted that his description of formats, instruction types, hardware parameters, and speed enhancements were at the level of “system architecture”, a term that seemed more useful than “machine organization”.Subsequently, Brooks, a Stretch designer, opened Chapter 2 of a book called Planning a Computer System: Project Stretch by stating, “Computer architecture, like other architecture, is the art of determining the needs of the user of a structure and then designing to meet those needs as effectively as possible within economic and technological constraints.”Brooks went on to help develop the IBM System/360 (now called the IBM zSeries) line of computers, in which “architecture” became a noun defining “what the user needs to know”. Later, computer users came to use the term in many less explicit ways.The earliest computer architectures were designed on paper and then directly built into the final hardware form.\\nLater, computer architecture prototypes were physically built in the form of a transistor–transistor logic (TTL) computer—such as the prototypes of the 6800 and the PA-RISC—tested, and tweaked, before committing to the final hardware form.\\nAs of the 1990s, new computer architectures are typically \"built\", tested, and tweaked—inside some other computer architecture in a computer architecture simulator; or inside a FPGA as a soft microprocessor; or both—before committing to the final hardware form.\\n\\n\\n== Subcategories ==\\nThe discipline of computer architecture has three main subcategories:\\nInstruction set architecture (ISA): defines the machine code that a processor reads and acts upon as well as the word size, memory address modes, processor registers, and data type.\\nMicroarchitecture: also known as \"computer organization\", this describes how a particular processor will implement the ISA. The size of a computer\\'s CPU cache for instance, is an issue that generally has nothing to do with the ISA.\\nSystems design: includes all of the other hardware components within a computing system, such as data processing other than the CPU (e.g., direct memory access), virtualization, and multiprocessingThere are other technologies in computer architecture. The following technologies are used in bigger companies like Intel, and were estimated in 2002 to count for 1% of all of computer architecture:\\n\\nMacroarchitecture: architectural layers more abstract than microarchitecture\\nAssembly instruction set architecture: A smart assembler may convert an abstract assembly language common to a group of machines into slightly different machine language for different implementations.\\nProgrammer-visible macroarchitecture: higher-level language tools such as compilers may define a consistent interface or contract to programmers using them, abstracting differences between underlying ISA, UISA, and microarchitectures. For example, the C, C++, or Java standards define different programmer-visible macroarchitectures.Microcode: microcode is software that translates instructions to run on a chip. It acts like a wrapper around the hardware, presenting a preferred version of the hardware\\'s instruction set interface. This instruction translation facility gives chip designers flexible options: E.g. 1. A new improved version of the chip can use microcode to present the exact same instruction set as the old chip version, so all software targeting that instruction set will run on the new chip without needing changes. E.g. 2. Microcode can present a variety of instruction sets for the same underlying chip, allowing it to run a wider variety of software.UISA: User Instruction Set Architecture, refers to one of three subsets of the RISC CPU instructions provided by PowerPC RISC Processors. The UISA subset, are those RISC instructions of interest to application developers. The other two subsets are VEA (Virtual Environment Architecture) instructions used by virtualisation system developers, and OEA (Operating Environment Architecture) used by Operation System developers.Pin architecture: The hardware functions that a microprocessor should provide to a hardware platform, e.g., the x86 pins A20M, FERR/IGNNE or FLUSH. Also, messages that the processor should emit so that external caches can be invalidated (emptied). Pin architecture functions are more flexible than ISA functions because external hardware can adapt to new encodings, or change from a pin to a message. The term \"architecture\" fits, because the functions must be provided for compatible systems, even if the detailed method changes.\\n\\n\\n== Roles ==\\n\\n\\n=== Definition ===\\nComputer architecture is concerned with balancing the performance, efficiency, cost, and reliability of a computer system. The case of instruction set architecture can be used to illustrate the balance of these competing factors. More complex instruction sets enable programmers to write more space efficient programs, since a single instruction can encode some higher-level abstraction (such as the x86 Loop instruction). However, longer and more complex instructions take longer for the processor to decode and can be more costly to implement effectively. The increased complexity from a large instruction set also creates more room for unreliability when instructions interact in unexpected ways.\\nThe implementation involves integrated circuit design, packaging, power, and cooling. Optimization of the design requires familiarity with compilers, operating systems to logic design, and packaging.\\n\\n\\n=== Instruction set architecture ===\\n\\nAn instruction set architecture (ISA) is the interface between the computer\\'s software and hardware and also can be viewed as the programmer\\'s view of the machine. Computers do not understand high-level programming languages such as Java, C++, or most programming languages used. A processor only understands instructions encoded in some numerical fashion, usually as binary numbers. Software tools, such as compilers, translate those high level languages into instructions that the processor can understand.\\nBesides instructions, the ISA defines items in the computer that are available to a program—e.g., data types, registers, addressing modes, and memory.  Instructions locate these available items with register indexes (or names) and memory addressing modes.\\nThe ISA of a computer is usually described in a small instruction manual, which describes how the instructions are encoded. Also, it may define short (vaguely) mnemonic names for the instructions. The names can be recognized by a software development tool called an assembler.  An assembler is a computer program that translates a human-readable form of the ISA into a computer-readable form.  Disassemblers are also widely available, usually in debuggers and software programs to isolate and correct malfunctions in binary computer programs.\\nISAs vary in quality and completeness.  A good ISA compromises between programmer convenience (how easy the code is to understand), size of the code (how much code is required to do a specific action), cost of the computer to interpret the instructions (more complexity means more hardware needed to decode and execute the instructions), and speed of the computer (with more complex decoding hardware comes longer decode time).  Memory organization defines how instructions interact with the memory, and how memory interacts with itself.\\nDuring design emulation, emulators can run programs written in a proposed instruction set. Modern emulators can measure size, cost, and speed to determine whether a particular ISA is meeting its goals.\\n\\n\\n=== Computer organization ===\\n\\nComputer organization helps optimize performance-based products. For example, software engineers need to know the processing power of processors. They may need to optimize software in order to gain the most performance for the lowest price. This can require quite a detailed analysis of the computer\\'s organization.  For example, in an SD card, the designers might need to arrange the card so that the most data can be processed in the fastest possible way.\\nComputer organization also helps plan the selection of a processor for a particular project. Multimedia projects may need very rapid data access, while virtual machines may need fast interrupts. Sometimes certain tasks need additional components as well.  For example, a computer capable of running a virtual machine needs virtual memory hardware so that the memory of different virtual computers can be kept separated. Computer organization and features also affect power consumption and processor cost.\\n\\n\\n=== Implementation ===\\nOnce an instruction set and micro-architecture have been designed, a practical machine must be developed. This design process is called the implementation. Implementation is usually not considered architectural design, but rather hardware design engineering. Implementation can be further broken down into several steps:\\n\\nLogic implementation designs the circuits required at a logic-gate level.\\nCircuit implementation does transistor-level designs of basic elements (e.g., gates, multiplexers, latches) as well as of some larger blocks (ALUs, caches etc.) that may be implemented at the logic-gate level, or even at the physical level if the design calls for it.\\nPhysical implementation draws physical circuits.  The different circuit components are placed in a chip floorplan or on a board and the wires connecting them are created.\\nDesign validation tests the computer as a whole to see if it works in all situations and all timings. Once the design validation process starts, the design at the logic level are tested using logic emulators. However, this is usually too slow to run a realistic test.  So, after making corrections based on the first test, prototypes are constructed using Field-Programmable Gate-Arrays (FPGAs). Most hobby projects stop at this stage.  The final step is to test prototype integrated circuits, which may require several redesigns.For CPUs, the entire implementation process is organized differently and is often referred to as CPU design.\\n\\n\\n== Design goals ==\\nThe exact form of a computer system depends on the constraints and goals. Computer architectures usually trade off standards, power versus performance, cost, memory capacity, latency (latency is the amount of time that it takes for information from one node to travel to the source) and throughput. Sometimes other considerations, such as features, size, weight, reliability, and expandability are also factors.\\nThe most common scheme does an in-depth power analysis and figures out how to keep power consumption low while maintaining adequate performance.\\n\\n\\n=== Performance ===\\nModern computer performance is often described in instructions per cycle (IPC), which measures the efficiency of the architecture at any clock frequency; a faster IPC rate means the computer is faster. Older computers had IPC counts as low as 0.1 while modern processors easily reach near 1. Superscalar processors may reach three to five IPC by executing several instructions per clock cycle.Counting machine-language instructions would be misleading because they can do varying amounts of work in different ISAs. The \"instruction\" in the standard measurements is not a count of the ISA\\'s machine-language instructions, but a unit of measurement, usually based on the speed of the VAX computer architecture.\\nMany people used to measure a computer\\'s speed by the clock rate (usually in MHz or GHz). This refers to the cycles per second of the main clock of the CPU. However, this metric is somewhat misleading, as a machine with a higher clock rate may not necessarily have greater performance. As a result, manufacturers have moved away from clock speed as a measure of performance.\\nOther factors influence speed, such as the mix of functional units, bus speeds, available memory, and the type and order of instructions in the programs.\\nThere are two main types of speed: latency and throughput. Latency is the time between the start of a process and its completion. Throughput is the amount of work done per unit time.  Interrupt latency is the guaranteed maximum response time of the system to an electronic event (like when the disk drive finishes moving some data).\\nPerformance is affected by a very wide range of design choices — for example, pipelining a processor usually makes latency worse, but makes throughput better. Computers that control machinery usually need low interrupt latencies. These computers operate in a real-time environment and fail if an operation is not completed in a specified amount of time. For example, computer-controlled anti-lock brakes must begin braking within a predictable and limited time period after the brake pedal is sensed or else failure of the brake will occur.\\nBenchmarking takes all these factors into account by measuring the time a computer takes to run through a series of test programs. Although benchmarking shows strengths, it shouldn\\'t be how you choose a computer. Often the measured machines split on different measures. For example, one system might handle scientific applications quickly, while another might render video games more smoothly. Furthermore, designers may target and add special features to their products, through hardware or software, that permit a specific benchmark to execute quickly but don\\'t offer similar advantages to general tasks.\\n\\n\\n=== Power efficiency ===\\n\\nPower efficiency is another important measurement in modern computers. A higher power efficiency can often be traded for lower speed or higher cost. The typical measurement when referring to power consumption in computer architecture is MIPS/W (millions of instructions per second per watt).\\nModern circuits have less power required per transistor as the number of transistors per chip grows. This is because each transistor that is put in a new chip requires its own power supply and requires new pathways to be built to power it. However the number of transistors per chip is starting to increase at a slower rate. Therefore, power efficiency is starting to become as important, if not more important than fitting more and more transistors into a single chip. Recent processor designs have shown this emphasis as they put more focus on power efficiency rather than cramming as many transistors into a single chip as possible. In the world of embedded computers, power efficiency has long been an important goal next to throughput and latency.\\n\\n\\n=== Shifts in market demand ===\\nIncreases in clock frequency have grown more slowly over the past few years, compared to power reduction improvements. This has been driven by the end of Moore\\'s Law and demand for longer battery life and reductions in size for mobile technology. This change in focus from higher clock rates to power consumption and miniaturization can be shown by the significant reductions in power consumption, as much as 50%, that were reported by Intel in their release of the Haswell microarchitecture; where they dropped their power consumption benchmark from 30 to 40 watts down to 10-20 watts. Comparing this to the processing speed increase of 3 GHz to 4 GHz (2002 to 2006) it can be seen that the focus in research and development are shifting away from clock frequency and moving towards consuming less power and taking up less space.\\n\\n\\n== See also ==\\n\\nComparison of CPU architectures\\nComputer hardware\\nCPU design\\nFloating point\\nHarvard architecture (Modified)\\nDataflow architecture\\nTransport triggered architecture\\nReconfigurable computing\\nInfluence of the IBM PC on the personal computer market\\nOrthogonal instruction set\\nSoftware architecture\\nvon Neumann architecture\\nFlynn\\'s taxonomy\\n\\n\\n== References ==\\n\\n\\n== Sources ==\\nJohn L. Hennessy and David Patterson (2006). Computer Architecture: A Quantitative Approach (Fourth ed.). Morgan Kaufmann. ISBN 978-0-12-370490-0.\\nBarton, Robert S., \"Functional Design of Computers\", Communications of the ACM 4(9): 405 (1961).\\nBarton, Robert S., \"A New Approach to the Functional Design of a Digital Computer\", Proceedings of the Western Joint Computer Conference, May 1961, pp. 393–396. About the design of the Burroughs B5000 computer.\\nBell, C. Gordon; and Newell, Allen (1971). \"Computer Structures: Readings and Examples\", McGraw-Hill.\\nBlaauw, G.A., and Brooks, F.P., Jr., \"The Structure of System/360, Part I-Outline of the Logical Structure\", IBM Systems Journal, vol. 3, no. 2, pp. 119–135, 1964.\\nTanenbaum, Andrew S. (1979). Structured Computer Organization. Englewood Cliffs, New Jersey: Prentice-Hall. ISBN 0-13-148521-0.\\n\\n\\n== External links ==\\nISCA: Proceedings of the International Symposium on Computer Architecture\\nMicro: IEEE/ACM International Symposium on Microarchitecture\\nHPCA: International Symposium on High Performance Computer Architecture\\nASPLOS: International Conference on Architectural Support for Programming Languages and Operating Systems\\nACM Transactions on Architecture and Code Optimization\\nIEEE Transactions on Computers\\nThe von Neumann Architecture of Computer Systems', 'A central processing unit (CPU), also called a central processor, main processor or just processor, is the electronic circuitry that executes instructions comprising a computer program. The CPU performs basic arithmetic, logic, controlling, and input/output (I/O) operations specified by the instructions in the program. This contrasts with external components such as main memory and I/O circuitry, and specialized processors such as graphics processing units (GPUs).\\nThe form, design, and implementation of CPUs have changed over time, but their fundamental operation remains almost unchanged. Principal components of a CPU include the arithmetic logic unit (ALU) that performs arithmetic and logic operations, processor registers that supply operands to the ALU and store the results of ALU operations, and a control unit that orchestrates the fetching (from memory), decoding and execution of instructions by directing the coordinated operations of the ALU, registers and other components.\\nMost modern CPUs are implemented on integrated circuit (IC) microprocessors, with one or more CPUs on a single metal-oxide-semiconductor (MOS) IC chip. Microprocessors chips with multiple CPUs are multi-core processors. The individual physical CPUs, processor cores, can also be multithreaded to create additional virtual or logical CPUs.An IC that contains a CPU may also contain memory, peripheral interfaces, and other components of a computer; such integrated devices are variously called microcontrollers or systems on a chip (SoC). \\nArray processors or vector processors have multiple processors that operate in parallel, with no unit considered central. Virtual CPUs are an abstraction of dynamical aggregated computational resources.\\n\\n\\n== History ==\\n\\nEarly computers such as the ENIAC had to be physically rewired to perform different tasks, which caused these machines to be called \"fixed-program computers\". The \"central processing unit\" term has been in use since as early as 1955. Since the term \"CPU\" is generally defined as a device for software (computer program) execution, the earliest devices that could rightly be called CPUs came with the advent of the stored-program computer.\\nThe idea of a stored-program computer had been already present in the design of J. Presper Eckert and John William Mauchly\\'s ENIAC, but was initially omitted so that it could be finished sooner. On June 30, 1945, before ENIAC was made, mathematician John von Neumann distributed the paper entitled First Draft of a Report on the EDVAC. It was the outline of a stored-program computer that would eventually be completed in August 1949. EDVAC was designed to perform a certain number of instructions (or operations) of various types. Significantly, the programs written for EDVAC were to be stored in high-speed computer memory rather than specified by the physical wiring of the computer. This overcame a severe limitation of ENIAC, which was the considerable time and effort required to reconfigure the computer to perform a new task. With von Neumann\\'s design, the program that EDVAC ran could be changed simply by changing the contents of the memory. EDVAC, however, was not the first stored-program computer; the Manchester Baby, a small-scale experimental stored-program computer, ran its first program on 21 June 1948 and the Manchester Mark 1 ran its first program during the night of 16–17 June 1949.Early CPUs were custom designs used as part of a larger and sometimes distinctive computer. However, this method of designing custom CPUs for a particular application has largely given way to the development of multi-purpose processors produced in large quantities. This standardization began in the era of discrete transistor mainframes and minicomputers and has rapidly accelerated with the popularization of the integrated circuit (IC). The IC has allowed increasingly complex CPUs to be designed and manufactured to tolerances on the order of nanometers. Both the miniaturization and standardization of CPUs have increased the presence of digital devices in modern life far beyond the limited application of dedicated computing machines. Modern microprocessors appear in electronic devices ranging from automobiles to cellphones, and sometimes even in toys.While von Neumann is most often credited with the design of the stored-program computer because of his design of EDVAC, and the design became known as the von Neumann architecture, others before him, such as Konrad Zuse, had suggested and implemented similar ideas. The so-called Harvard architecture of the Harvard Mark I, which was completed before EDVAC, also used a stored-program design using punched paper tape rather than electronic memory. The key difference between the von Neumann and Harvard architectures is that the latter separates the storage and treatment of CPU instructions and data, while the former uses the same memory space for both. Most modern CPUs are primarily von Neumann in design, but CPUs with the Harvard architecture are seen as well, especially in embedded applications; for instance, the Atmel AVR microcontrollers are Harvard architecture processors.Relays and vacuum tubes (thermionic tubes) were commonly used as switching elements; a useful computer requires thousands or tens of thousands of switching devices. The overall speed of a system is dependent on the speed of the switches. Vacuum tube computers such as EDVAC tended to average eight hours between failures, whereas relay computers like the (slower, but earlier) Harvard Mark I failed very rarely. In the end, tube-based CPUs became dominant because the significant speed advantages afforded generally outweighed the reliability problems. Most of these early synchronous CPUs ran at low clock rates compared to modern microelectronic designs. Clock signal frequencies ranging from 100 kHz to 4 MHz were very common at this time, limited largely by the speed of the switching devices they were built with.\\n\\n\\n=== Transistor CPUs ===\\n\\nThe design complexity of CPUs increased as various technologies facilitated building smaller and more reliable electronic devices. The first such improvement came with the advent of the transistor. Transistorized CPUs during the 1950s and 1960s no longer had to be built out of bulky, unreliable and fragile switching elements like vacuum tubes and relays. With this improvement, more complex and reliable CPUs were built onto one or several printed circuit boards containing discrete (individual) components.\\nIn 1964, IBM introduced its IBM System/360 computer architecture that was used in a series of computers capable of running the same programs with different speed and performance. This was significant at a time when most electronic computers were incompatible with one another, even those made by the same manufacturer. To facilitate this improvement, IBM used the concept of a microprogram (often called \"microcode\"), which still sees widespread usage in modern CPUs. The System/360 architecture was so popular that it dominated the mainframe computer market for decades and left a legacy that is still continued by similar modern computers like the IBM zSeries. In 1965, Digital Equipment Corporation (DEC) introduced another influential computer aimed at the scientific and research markets, the PDP-8.\\n\\nTransistor-based computers had several distinct advantages over their predecessors. Aside from facilitating increased reliability and lower power consumption, transistors also allowed CPUs to operate at much higher speeds because of the short switching time of a transistor in comparison to a tube or relay. The increased reliability and dramatically increased speed of the switching elements (which were almost exclusively transistors by this time); CPU clock rates in the tens of megahertz were easily obtained during this period. Additionally, while discrete transistor and IC CPUs were in heavy usage, new high-performance designs like SIMD (Single Instruction Multiple Data) vector processors began to appear. These early experimental designs later gave rise to the era of specialized supercomputers like those made by Cray Inc and Fujitsu Ltd.\\n\\n\\n=== Small-scale integration CPUs ===\\n\\nDuring this period, a method of manufacturing many interconnected transistors in a compact space was developed. The integrated circuit (IC) allowed a large number of transistors to be manufactured on a single semiconductor-based die, or \"chip\". At first, only very basic non-specialized digital circuits such as NOR gates were miniaturized into ICs. CPUs based on these \"building block\" ICs are generally referred to as \"small-scale integration\" (SSI) devices. SSI ICs, such as the ones used in the Apollo Guidance Computer, usually contained up to a few dozen transistors. To build an entire CPU out of SSI ICs required thousands of individual chips, but still consumed much less space and power than earlier discrete transistor designs.IBM\\'s System/370, follow-on to the System/360, used SSI ICs rather than Solid Logic Technology discrete-transistor modules. DEC\\'s PDP-8/I and KI10 PDP-10 also switched from the individual transistors used by the PDP-8 and PDP-10 to SSI ICs, and their extremely popular PDP-11 line was originally built with SSI ICs but was eventually implemented with LSI components once these became practical.\\n\\n\\n=== Large-scale integration CPUs ===\\nLee Boysel published influential articles, including a 1967 \"manifesto\", which described how to build the equivalent of a 32-bit mainframe computer from a relatively small number of large-scale integration circuits (LSI). The only way to build LSI chips, which are chips with a hundred or more gates, was to build them using a MOS semiconductor manufacturing process (either PMOS logic, NMOS logic, or CMOS logic). However, some companies continued to build processors out of bipolar transistor–transistor logic (TTL) chips because bipolar junction transistors were faster than MOS chips up until the 1970s (a few companies such as Datapoint continued to build processors out of TTL chips until the early 1980s). In the 1960s, MOS ICs were slower and initially considered useful only in applications that required low power. Following the development of silicon-gate MOS technology by Federico Faggin at Fairchild Semiconductor in 1968, MOS ICs largely replaced bipolar TTL as the standard chip technology in the early 1970s.As the microelectronic technology advanced, an increasing number of transistors were placed on ICs, decreasing the number of individual ICs needed for a complete CPU. MSI and LSI ICs increased transistor counts to hundreds, and then thousands. By 1968, the number of ICs required to build a complete CPU had been reduced to 24 ICs of eight different types, with each IC containing roughly 1000 MOSFETs. In stark contrast with its SSI and MSI predecessors, the first LSI implementation of the PDP-11 contained a CPU composed of only four LSI integrated circuits.\\n\\n\\n=== Microprocessors ===\\n\\nAdvances in MOS IC technology led to the invention of the microprocessor in the early 1970s. Since the introduction of the first commercially available microprocessor, the Intel 4004 in 1971, and the first widely used microprocessor, the Intel 8080 in 1974, this class of CPUs has almost completely overtaken all other central processing unit implementation methods. Mainframe and minicomputer manufacturers of the time launched proprietary IC development programs to upgrade their older computer architectures, and eventually produced instruction set compatible microprocessors that were backward-compatible with their older hardware and software. Combined with the advent and eventual success of the ubiquitous personal computer, the term CPU is now applied almost exclusively to microprocessors. Several CPUs (denoted cores) can be combined in a single processing chip.\\nPrevious generations of CPUs were implemented as discrete components and numerous small integrated circuits (ICs) on one or more circuit boards. Microprocessors, on the other hand, are CPUs manufactured on a very small number of ICs; usually just one. The overall smaller CPU size, as a result of being implemented on a single die, means faster switching time because of physical factors like decreased gate parasitic capacitance. This has allowed synchronous microprocessors to have clock rates ranging from tens of megahertz to several gigahertz. Additionally, the ability to construct exceedingly small transistors on an IC has increased the complexity and number of transistors in a single CPU many fold. This widely observed trend is described by Moore\\'s law, which had proven to be a fairly accurate predictor of the growth of CPU (and other IC) complexity until 2016.While the complexity, size, construction and general form of CPUs have changed enormously since 1950, the basic design and function has not changed much at all. Almost all common CPUs today can be very accurately described as von Neumann stored-program machines. As Moore\\'s law no longer holds, concerns have arisen about the limits of integrated circuit transistor technology. Extreme miniaturization of electronic gates is causing the effects of phenomena like electromigration and subthreshold leakage to become much more significant. These newer concerns are among the many factors causing researchers to investigate new methods of computing such as the quantum computer, as well as to expand the usage of parallelism and other methods that extend the usefulness of the classical von Neumann model.\\n\\n\\n== Operation ==\\nThe fundamental operation of most CPUs, regardless of the physical form they take, is to execute a sequence of stored instructions that is called a program. The instructions to be executed are kept in some kind of computer memory. Nearly all CPUs follow the fetch, decode and execute steps in their operation, which are collectively known as the instruction cycle.\\nAfter the execution of an instruction, the entire process repeats, with the next instruction cycle normally fetching the next-in-sequence instruction because of the incremented value in the program counter. If a jump instruction was executed, the program counter will be modified to contain the address of the instruction that was jumped to and program execution continues normally. In more complex CPUs, multiple instructions can be fetched, decoded and executed simultaneously. This section describes what is generally referred to as the \"classic RISC pipeline\", which is quite common among the simple CPUs used in many electronic devices (often called microcontrollers). It largely ignores the important role of CPU cache, and therefore the access stage of the pipeline.\\nSome instructions manipulate the program counter rather than producing result data directly; such instructions are generally called \"jumps\" and facilitate program behavior like loops, conditional program execution (through the use of a conditional jump), and existence of functions. In some processors, some other instructions change the state of bits in a \"flags\" register. These flags can be used to influence how a program behaves, since they often indicate the outcome of various operations. For example, in such processors a \"compare\" instruction evaluates two values and sets or clears bits in the flags register to indicate which one is greater or whether they are equal; one of these flags could then be used by a later jump instruction to determine program flow.\\n\\n\\n=== Fetch ===\\nThe first step, fetch, involves retrieving an instruction (which is represented by a number or sequence of numbers) from program memory. The instruction\\'s location (address) in program memory is determined by the program counter (PC; called the \"instruction pointer\" in Intel x86 microprocessors), which stores a number that identifies the address of the next instruction to be fetched. After an instruction is fetched, the PC is incremented by the length of the instruction so that it will contain the address of the next instruction in the sequence. Often, the instruction to be fetched must be retrieved from relatively slow memory, causing the CPU to stall while waiting for the instruction to be returned. This issue is largely addressed in modern processors by caches and pipeline architectures (see below).\\n\\n\\n=== Decode ===\\nThe instruction that the CPU fetches from memory determines what the CPU will do. In the decode step, performed by binary decoder circuitry known as the instruction decoder, the instruction is converted into signals that control other parts of the CPU.\\nThe way in which the instruction is interpreted is defined by the CPU\\'s instruction set architecture (ISA). Often, one group of bits (that is, a \"field\") within the instruction, called the opcode, indicates which operation is to be performed, while the remaining fields usually provide supplemental information required for the operation, such as the operands. Those operands may be specified as a constant value (called an immediate value), or as the location of a value that may be a processor register or a memory address, as determined by some addressing mode.\\nIn some CPU designs the instruction decoder is implemented as a hardwired, unchangeable binary decoder circuit. In others, a microprogram is used to translate instructions into sets of CPU configuration signals that are applied sequentially over multiple clock pulses. In some cases the memory that stores the microprogram is rewritable, making it possible to change the way in which the CPU decodes instructions.\\n\\n\\n=== Execute ===\\nAfter the fetch and decode steps, the execute step is performed. Depending on the CPU architecture, this may consist of a single action or a sequence of actions. During each action, control signals electrically enable or disable various parts of the CPU so they can perform all or part of the desired operation. The action is then completed, typically in response to a clock pulse. Very often the results are written to an internal CPU register for quick access by subsequent instructions. In other cases results may be written to slower, but less expensive and higher capacity main memory.\\nFor example, if an addition instruction is to be executed, registers containing operands (numbers to be summed) are activated, as are the parts of the arithmetic logic unit (ALU) that perform addition. When the clock pulse occurs, the operands flow from the source registers into the ALU, and the sum appears at its output. On subsequent clock pulses, other components are enabled (and disabled) to move the output (the sum of the operation) to storage (e.g., a register or memory). If the resulting sum is too large (i.e., it is larger than the ALU\\'s output word size), an arithmetic overflow flag will be set, influencing the next operation.\\n\\n\\n== Structure and implementation ==\\n\\nHardwired into a CPU\\'s circuitry is a set of basic operations it can perform, called an instruction set. Such operations may involve, for example, adding or subtracting two numbers, comparing two numbers, or jumping to a different part of a program. Each instruction is represented by a unique combination of bits, known as the machine language opcode. While processing an instruction, the CPU decodes the opcode (via a binary decoder) into control signals, which orchestrate the behavior of the CPU. A complete machine language instruction consists of an opcode and, in many cases, additional bits that specify arguments for the operation (for example, the numbers to be summed in the case of an addition operation). Going up the complexity scale, a machine language program is a collection of machine language instructions that the CPU executes.\\nThe actual mathematical operation for each instruction is performed by a combinational logic circuit within the CPU\\'s processor known as the arithmetic logic unit or ALU. In general, a CPU executes an instruction by fetching it from memory, using its ALU to perform an operation, and then storing the result to memory. Beside the instructions for integer mathematics and logic operations, various other machine instructions exist, such as those for loading data from memory and storing it back, branching operations, and mathematical operations on floating-point numbers performed by the CPU\\'s floating-point unit (FPU).\\n\\n\\n=== Control unit ===\\n\\nThe control unit (CU) is a component of the CPU that directs the operation of the processor. It tells the computer\\'s memory, arithmetic and logic unit and input and output devices how to respond to the instructions that have been sent to the processor.\\nIt directs the operation of the other units by providing timing and control signals. Most computer resources are managed by the CU. It directs the flow of data between the CPU and the other devices. John von Neumann included the control unit as part of the von Neumann architecture. In modern computer designs, the control unit is typically an internal part of the CPU with its overall role and operation unchanged since its introduction.\\n\\n\\n=== Arithmetic logic unit ===\\n\\nThe arithmetic logic unit (ALU) is a digital circuit within the processor that performs integer arithmetic and bitwise logic operations. The inputs to the ALU are the data words to be operated on (called operands), status information from previous operations, and a code from the control unit indicating which operation to perform. Depending on the instruction being executed, the operands may come from internal CPU registers or external memory, or they may be constants generated by the ALU itself.\\nWhen all input signals have settled and propagated through the ALU circuitry, the result of the performed operation appears at the ALU\\'s outputs. The result consists of both a data word, which may be stored in a register or memory, and status information that is typically stored in a special, internal CPU register reserved for this purpose.\\n\\n\\n=== Address generation unit ===\\n\\nAddress generation unit (AGU), sometimes also called address computation unit (ACU), is an execution unit inside the CPU that calculates addresses used by the CPU to access main memory. By having address calculations handled by separate circuitry that operates in parallel with the rest of the CPU, the number of CPU cycles required for executing various machine instructions can be reduced, bringing performance improvements.\\nWhile performing various operations, CPUs need to calculate memory addresses required for fetching data from the memory; for example, in-memory positions of array elements must be calculated before the CPU can fetch the data from actual memory locations. Those address-generation calculations involve different integer arithmetic operations, such as addition, subtraction, modulo operations, or bit shifts. Often, calculating a memory address involves more than one general-purpose machine instruction, which do not necessarily decode and execute quickly. By incorporating an AGU into a CPU design, together with introducing specialized instructions that use the AGU, various address-generation calculations can be offloaded from the rest of the CPU, and can often be executed quickly in a single CPU cycle.\\nCapabilities of an AGU depend on a particular CPU and its architecture. Thus, some AGUs implement and expose more address-calculation operations, while some also include more advanced specialized instructions that can operate on multiple operands at a time. Furthermore, some CPU architectures include multiple AGUs so more than one address-calculation operation can be executed simultaneously, bringing further performance improvements by capitalizing on the superscalar nature of advanced CPU designs. For example, Intel incorporates multiple AGUs into its Sandy Bridge and Haswell microarchitectures, which increase bandwidth of the CPU memory subsystem by allowing multiple memory-access instructions to be executed in parallel.\\n\\n\\n=== Memory management unit (MMU) ===\\n\\nMany microprocessors (in smartphones and desktop, laptop, server computers) have a memory management unit, translating logical addresses into physical RAM addresses, providing memory protection and paging abilities, useful for virtual memory. Simpler processors, especially microcontrollers, usually don\\'t include an MMU.\\n\\n\\n=== Cache ===\\nA CPU cache is a hardware cache used by the central processing unit (CPU) of a computer to reduce the average cost (time or energy) to access data from the main memory. A cache is a smaller, faster memory, closer to a processor core, which stores copies of the data from frequently used main memory locations. Most CPUs have different independent caches, including instruction and data caches, where the data cache is usually organized as a hierarchy of more cache levels (L1, L2, L3, L4, etc.).\\nAll modern (fast) CPUs (with few specialized exceptions) have multiple levels of CPU caches. The first CPUs that used a cache had only one level of cache; unlike later level 1 caches, it was not split into L1d (for data) and L1i (for instructions). Almost all current CPUs with caches have a split L1 cache. They also have L2 caches and, for larger processors, L3 caches as well. The L2 cache is usually not split and acts as a common repository for the already split L1 cache. Every core of a multi-core processor has a dedicated L2 cache and is usually not shared between the cores. The L3 cache, and higher-level caches, are shared between the cores and are not split. An L4 cache is currently uncommon, and is generally on dynamic random-access memory (DRAM), rather than on static random-access memory (SRAM), on a separate die or chip. That was also the case historically with L1, while bigger chips have allowed integration of it and generally all cache levels, with the possible exception of the last level. Each extra level of cache tends to be bigger and be optimized differently.\\nOther types of caches exist (that are not counted towards the \"cache size\" of the most important caches mentioned above), such as the translation lookaside buffer (TLB) that is part of the memory management unit (MMU) that most CPUs have.\\nCaches are generally sized in powers of two: 2, 8, 16 etc. KiB or MiB (for larger non-L1) sizes, although the IBM z13 has a 96 KiB L1 instruction cache.\\n\\n\\n=== Clock rate ===\\n\\nMost CPUs are synchronous circuits, which means they employ a clock signal to pace their sequential operations. The clock signal is produced by an external oscillator circuit that generates a consistent number of pulses each second in the form of a periodic square wave. The frequency of the clock pulses determines the rate at which a CPU executes instructions and, consequently, the faster the clock, the more instructions the CPU will execute each second.\\nTo ensure proper operation of the CPU, the clock period is longer than the maximum time needed for all signals to propagate (move) through the CPU. In setting the clock period to a value well above the worst-case propagation delay, it is possible to design the entire CPU and the way it moves data around the \"edges\" of the rising and falling clock signal. This has the advantage of simplifying the CPU significantly, both from a design perspective and a component-count perspective. However, it also carries the disadvantage that the entire CPU must wait on its slowest elements, even though some portions of it are much faster. This limitation has largely been compensated for by various methods of increasing CPU parallelism (see below).\\nHowever, architectural improvements alone do not solve all of the drawbacks of globally synchronous CPUs. For example, a clock signal is subject to the delays of any other electrical signal. Higher clock rates in increasingly complex CPUs make it more difficult to keep the clock signal in phase (synchronized) throughout the entire unit. This has led many modern CPUs to require multiple identical clock signals to be provided to avoid delaying a single signal significantly enough to cause the CPU to malfunction. Another major issue, as clock rates increase dramatically, is the amount of heat that is dissipated by the CPU. The constantly changing clock causes many components to switch regardless of whether they are being used at that time. In general, a component that is switching uses more energy than an element in a static state. Therefore, as clock rate increases, so does energy consumption, causing the CPU to require more heat dissipation in the form of CPU cooling solutions.\\nOne method of dealing with the switching of unneeded components is called clock gating, which involves turning off the clock signal to unneeded components (effectively disabling them). However, this is often regarded as difficult to implement and therefore does not see common usage outside of very low-power designs. One notable recent CPU design that uses extensive clock gating is the IBM PowerPC-based Xenon used in the Xbox 360; that way, power requirements of the Xbox 360 are greatly reduced. Another method of addressing some of the problems with a global clock signal is the removal of the clock signal altogether. While removing the global clock signal makes the design process considerably more complex in many ways, asynchronous (or clockless) designs carry marked advantages in power consumption and heat dissipation in comparison with similar synchronous designs. While somewhat uncommon, entire asynchronous CPUs have been built without using a global clock signal. Two notable examples of this are the ARM compliant AMULET and the MIPS R3000 compatible MiniMIPS.\\nRather than totally removing the clock signal, some CPU designs allow certain portions of the device to be asynchronous, such as using asynchronous ALUs in conjunction with superscalar pipelining to achieve some arithmetic performance gains. While it is not altogether clear whether totally asynchronous designs can perform at a comparable or better level than their synchronous counterparts, it is evident that they do at least excel in simpler math operations. This, combined with their excellent power consumption and heat dissipation properties, makes them very suitable for embedded computers.\\n\\n\\n=== Voltage regulator module ===\\n\\nMany modern CPUs have a die-integrated power managing module which regulates on-demand voltage supply to the CPU circuitry allowing it to keep balance between performance and power consumption.\\n\\n\\n=== Integer range ===\\nEvery CPU represents numerical values in a specific way. For example, some early digital computers represented numbers as familiar decimal (base 10) numeral system values, and others have employed more unusual representations such as ternary (base three). Nearly all modern CPUs represent numbers in binary form, with each digit being represented by some two-valued physical quantity such as a \"high\" or \"low\" voltage.\\n\\nRelated to numeric representation is the size and precision of integer numbers that a CPU can represent. In the case of a binary CPU, this is measured by the number of bits (significant digits of a binary encoded integer) that the CPU can process in one operation, which is commonly called word size, bit width, data path width, integer precision, or integer size. A CPU\\'s integer size determines the range of integer values it can directly operate on. For example, an 8-bit CPU can directly manipulate integers represented by eight bits, which have a range of 256 (28) discrete integer values.\\nInteger range can also affect the number of memory locations the CPU can directly address (an address is an integer value representing a specific memory location). For example, if a binary CPU uses 32 bits to represent a memory address then it can directly address 232 memory locations. To circumvent this limitation and for various other reasons, some CPUs use mechanisms (such as bank switching) that allow additional memory to be addressed.\\nCPUs with larger word sizes require more circuitry and consequently are physically larger, cost more and consume more power (and therefore generate more heat). As a result, smaller 4- or 8-bit microcontrollers are commonly used in modern applications even though CPUs with much larger word sizes (such as 16, 32, 64, even 128-bit) are available. When higher performance is required, however, the benefits of a larger word size (larger data ranges and address spaces) may outweigh the disadvantages. A CPU can have internal data paths shorter than the word size to reduce size and cost. For example, even though the IBM System/360 instruction set was a 32-bit instruction set, the System/360 Model 30 and Model 40 had 8-bit data paths in the arithmetic logical unit, so that a 32-bit add required four cycles, one for each 8 bits of the operands, and, even though the Motorola 68000 series instruction set was a 32-bit instruction set, the Motorola 68000 and Motorola 68010 had 16-bit data paths in the arithmetic logical unit, so that a 32-bit add required two cycles.\\nTo gain some of the advantages afforded by both lower and higher bit lengths, many instruction sets have different bit widths for integer and floating-point data, allowing CPUs implementing that instruction set to have different bit widths for different portions of the device. For example, the IBM System/360 instruction set was primarily 32 bit, but supported 64-bit floating point values to facilitate greater accuracy and range in floating point numbers. The System/360 Model 65 had an 8-bit adder for decimal and fixed-point binary arithmetic and a 60-bit adder for floating-point arithmetic. Many later CPU designs use similar mixed bit width, especially when the processor is meant for general-purpose usage where a reasonable balance of integer and floating point capability is required.\\n\\n\\n=== Parallelism ===\\n\\nThe description of the basic operation of a CPU offered in the previous section describes the simplest form that a CPU can take. This type of CPU, usually referred to as subscalar, operates on and executes one instruction on one or two pieces of data at a time, that is less than one instruction per clock cycle (IPC < 1).\\nThis process gives rise to an inherent inefficiency in subscalar CPUs. Since only one instruction is executed at a time, the entire CPU must wait for that instruction to complete before proceeding to the next instruction. As a result, the subscalar CPU gets \"hung up\" on instructions which take more than one clock cycle to complete execution. Even adding a second execution unit (see below) does not improve performance much; rather than one pathway being hung up, now two pathways are hung up and the number of unused transistors is increased. This design, wherein the CPU\\'s execution resources can operate on only one instruction at a time, can only possibly reach scalar performance (one instruction per clock cycle, IPC = 1). However, the performance is nearly always subscalar (less than one instruction per clock cycle, IPC < 1).\\nAttempts to achieve scalar and better performance have resulted in a variety of design methodologies that cause the CPU to behave less linearly and more in parallel. When referring to parallelism in CPUs, two terms are generally used to classify these design techniques:\\n\\ninstruction-level parallelism (ILP), which seeks to increase the rate at which instructions are executed within a CPU (that is, to increase the use of on-die execution resources);\\ntask-level parallelism (TLP), which purposes to increase the number of threads or processes that a CPU can execute simultaneously.Each methodology differs both in the ways in which they are implemented, as well as the relative effectiveness they afford in increasing the CPU\\'s performance for an application.\\n\\n\\n==== Instruction-level parallelism ====\\n\\nOne of the simplest methods for increased parallelism is to begin the first steps of instruction fetching and decoding before the prior instruction finishes executing. This is a technique known as instruction pipelining, and is used in almost all modern general-purpose CPUs. Pipelining allows multiple instruction to be executed at a time by breaking the execution pathway into discrete stages. This separation can be compared to an assembly line, in which an instruction is made more complete at each stage until it exits the execution pipeline and is retired.\\nPipelining does, however, introduce the possibility for a situation where the result of the previous operation is needed to complete the next operation; a condition often termed data dependency conflict. Therefore pipelined processors must check for these sorts of conditions and delay a portion of the pipeline if necessary.  A pipelined processor can become very nearly scalar, inhibited only by pipeline stalls (an instruction spending more than one clock cycle in a stage).\\n\\nImprovements in instruction pipelining led to further decreases in the idle time of CPU components. Designs that are said to be superscalar include a long instruction pipeline and multiple identical execution units, such as load-store units, arithmetic-logic units, floating-point units and address generation units. In a superscalar pipeline, instructions are read and passed to a dispatcher, which decides whether or not the instructions can be executed in parallel (simultaneously). If so, they are dispatched to execution units, resulting in their simultaneous execution. In general, the number of instructions that a superscalar CPU will complete in a cycle is dependent on the number of instructions it is able to dispatch simultaneously to execution units.\\nMost of the difficulty in the design of a superscalar CPU architecture lies in creating an effective dispatcher. The dispatcher needs to be able to quickly determine whether instructions can be executed in parallel, as well as dispatch them in such a way as to keep as many execution units busy as possible. This requires that the instruction pipeline is filled as often as possible and requires significant amounts of CPU cache. It also makes hazard-avoiding techniques like branch prediction, speculative execution, register renaming, out-of-order execution and transactional memory crucial to maintaining high levels of performance. By attempting to predict which branch (or path) a conditional instruction will take, the CPU can minimize the number of times that the entire pipeline must wait until a conditional instruction is completed. Speculative execution often provides modest performance increases by executing portions of code that may not be needed after a conditional operation completes. Out-of-order execution somewhat rearranges the order in which instructions are executed to reduce delays due to data dependencies. Also in case of single instruction stream, multiple data stream—a case when a lot of data from the same type has to be processed—, modern processors can disable parts of the pipeline so that when a single instruction is executed many times, the CPU skips the fetch and decode phases and thus greatly increases performance on certain occasions, especially in highly monotonous program engines such as video creation software and photo processing.\\nIn the case where just a portion of the CPU is superscalar, the part which is not suffers a performance penalty due to scheduling stalls. The Intel P5 Pentium had two superscalar ALUs which could accept one instruction per clock cycle each, but its FPU could not. Thus the P5 was integer superscalar but not floating point superscalar. Intel\\'s successor to the P5 architecture, P6, added superscalar abilities to its floating point features.\\nSimple pipelining and superscalar design increase a CPU\\'s ILP by allowing it to execute instructions at rates surpassing one instruction per clock cycle. Most modern CPU designs are at least somewhat superscalar, and nearly all general purpose CPUs designed in the last decade are superscalar. In later years some of the emphasis in designing high-ILP computers has been moved out of the CPU\\'s hardware and into its software interface, or instruction set architecture (ISA). The strategy of the very long instruction word (VLIW) causes some ILP to become implied directly by the software, reducing the CPU’s work in boosting ILP and thereby reducing design complexity.\\n\\n\\n==== Task-level parallelism ====\\n\\nAnother strategy of achieving performance is to execute multiple threads or processes in parallel. This area of research is known as parallel computing. In Flynn\\'s taxonomy, this strategy is known as multiple instruction stream, multiple data stream (MIMD).One technology used for this purpose was multiprocessing (MP). The initial flavor of this technology is known as symmetric multiprocessing (SMP), where a small number of CPUs share a coherent view of their memory system. In this scheme, each CPU has additional hardware to maintain a constantly up-to-date view of memory. By avoiding stale views of memory, the CPUs can cooperate on the same program and programs can migrate from one CPU to another. To increase the number of cooperating CPUs beyond a handful, schemes such as non-uniform memory access (NUMA) and directory-based coherence protocols were introduced in the 1990s. SMP systems are limited to a small number of CPUs while NUMA systems have been built with thousands of processors. Initially, multiprocessing was built using multiple discrete CPUs and boards to implement the interconnect between the processors. When the processors and their interconnect are all implemented on a single chip, the technology is known as chip-level multiprocessing (CMP) and the single chip as a multi-core processor.\\nIt was later recognized that finer-grain parallelism existed with a single program. A single program might have several threads (or functions) that could be executed separately or in parallel. Some of the earliest examples of this technology implemented input/output processing such as direct memory access as a separate thread from the computation thread. A more general approach to this technology was introduced in the 1970s when systems were designed to run multiple computation threads in parallel. This technology is known as multi-threading (MT). This approach is considered more cost-effective than multiprocessing, as only a small number of components within a CPU is replicated to support MT as opposed to the entire CPU in the case of MP. In MT, the execution units and the memory system including the caches are shared among multiple threads. The downside of MT is that the hardware support for multithreading is more visible to software than that of MP and thus supervisor software like operating systems have to undergo larger changes to support MT. One type of MT that was implemented is known as temporal multithreading, where one thread is executed until it is stalled waiting for data to return from external memory. In this scheme, the CPU would then quickly context switch to another thread which is ready to run, the switch often done in one CPU clock cycle, such as the UltraSPARC T1. Another type of MT is simultaneous multithreading, where instructions from multiple threads are executed in parallel within one CPU clock cycle.\\nFor several decades from the 1970s to early 2000s, the focus in designing high performance general purpose CPUs was largely on achieving high ILP through technologies such as pipelining, caches, superscalar execution, out-of-order execution, etc. This trend culminated in large, power-hungry CPUs such as the Intel Pentium 4. By the early 2000s, CPU designers were thwarted from achieving higher performance from ILP techniques due to the growing disparity between CPU operating frequencies and main memory operating frequencies as well as escalating CPU power dissipation owing to more esoteric ILP techniques.\\nCPU designers then borrowed ideas from commercial computing markets such as transaction processing, where the aggregate performance of multiple programs, also known as throughput computing, was more important than the performance of a single thread or process.\\nThis reversal of emphasis is evidenced by the proliferation of dual and more core processor designs and notably, Intel\\'s newer designs resembling its less superscalar P6 architecture. Late designs in several processor families exhibit CMP, including the x86-64 Opteron and Athlon 64 X2, the SPARC UltraSPARC T1, IBM POWER4 and POWER5, as well as several video game console CPUs like the Xbox 360\\'s triple-core PowerPC design, and the PlayStation 3\\'s 7-core Cell microprocessor.\\n\\n\\n==== Data parallelism ====\\n\\nA less common but increasingly important paradigm of processors (and indeed, computing in general) deals with data parallelism. The processors discussed earlier are all referred to as some type of scalar device. As the name implies, vector processors deal with multiple pieces of data in the context of one instruction. This contrasts with scalar processors, which deal with one piece of data for every instruction. Using Flynn\\'s taxonomy, these two schemes of dealing with data are generally referred to as single instruction stream, multiple data stream (SIMD) and single instruction stream, single data stream (SISD), respectively. The great utility in creating processors that deal with vectors of data lies in optimizing tasks that tend to require the same operation (for example, a sum or a dot product) to be performed on a large set of data. Some classic examples of these types of tasks include multimedia applications (images, video and sound), as well as many types of scientific and engineering tasks. Whereas a scalar processor must complete the entire process of fetching, decoding and executing each instruction and value in a set of data, a vector processor can perform a single operation on a comparatively large set of data with one instruction. This is only possible when the application tends to require many steps which apply one operation to a large set of data.\\nMost early vector processors, such as the Cray-1, were associated almost exclusively with scientific research and cryptography applications. However, as multimedia has largely shifted to digital media, the need for some form of SIMD in general-purpose processors has become significant. Shortly after inclusion of floating-point units started to become commonplace in general-purpose processors, specifications for and implementations of SIMD execution units also began to appear for general-purpose processors. Some of these early SIMD specifications - like HP\\'s Multimedia Acceleration eXtensions (MAX) and Intel\\'s MMX - were integer-only. This proved to be a significant impediment for some software developers, since many of the applications that benefit from SIMD primarily deal with floating-point numbers. Progressively, developers refined and remade these early designs into some of the common modern SIMD specifications, which are usually associated with one instruction set architecture (ISA). Some notable modern examples include Intel\\'s Streaming SIMD Extensions (SSE) and the PowerPC-related AltiVec (also known as VMX).\\n\\n\\n== Virtual CPUs ==\\nCloud computing can involve subdividing CPU operation into virtual central processing units (vCPUs).\\nA host is the virtual equivalent of a physical machine, on which a virtual system is operating. When there are several physical machines operating in tandem and managed as a whole, the grouped computing and memory resources form a cluster. In some systems, it is possible to dynamically add and remove from a cluster. Resources available at a host and cluster level can be partitioned out into resources pools with fine granularity.\\n\\n\\n== Performance ==\\n\\nThe performance or speed of a processor depends on, among many other factors, the clock rate (generally given in multiples of hertz) and the instructions per clock (IPC), which together are the factors for the instructions per second (IPS) that the CPU can perform.\\nMany reported IPS values have represented \"peak\" execution rates on artificial instruction sequences with few branches, whereas realistic workloads consist of a mix of instructions and applications, some of which take longer to execute than others. The performance of the memory hierarchy also greatly affects processor performance, an issue barely considered in MIPS calculations. Because of these problems, various standardized tests, often called \"benchmarks\" for this purpose\\u200d—\\u200csuch as SPECint\\u200d—\\u200chave been developed to attempt to measure the real effective performance in commonly used applications.\\nProcessing performance of computers is increased by using multi-core processors, which essentially is plugging two or more individual processors (called cores in this sense) into one integrated circuit. Ideally, a dual core processor would be nearly twice as powerful as a single core processor. In practice, the performance gain is far smaller, only about 50%, due to imperfect software algorithms and implementation. Increasing the number of cores in a processor (i.e. dual-core, quad-core, etc.) increases the workload that can be handled. This means that the processor can now handle numerous asynchronous events, interrupts, etc. which can take a toll on the CPU when overwhelmed. These cores can be thought of as different floors in a processing plant, with each floor handling a different task. Sometimes, these cores will handle the same tasks as cores adjacent to them if a single core is not enough to handle the information.\\nDue to specific capabilities of modern CPUs, such as simultaneous multithreading and uncore, which involve sharing of actual CPU resources while aiming at increased utilization, monitoring performance levels and hardware use gradually became a more complex task. As a response, some CPUs implement additional hardware logic that monitors actual use of various parts of a CPU and provides various counters accessible to software; an example is Intel\\'s Performance Counter Monitor technology.\\n\\n\\n== See also ==\\n\\n\\n== Notes ==\\n\\n\\n== References ==\\n\\n\\n== External links ==\\nHow Microprocessors Work at HowStuffWorks.\\n25 Microchips that shook the world – an article by the Institute of Electrical and Electronics Engineers.', 'In computing, memory is a device or system that is used to store information for immediate use in a computer or related computer hardware and digital electronic devices. The term memory is often synonymous with the term primary storage or main memory. An archaic synonym for memory is store.Computer memory operates at a high speed compared to storage that is slower but offers higher capacities. If needed, contents of the computer memory can be transferred to storage; a common way of doing this is through a memory management technique called virtual memory. \\nModern memory is implemented as semiconductor memory, where data is stored within memory cells built from MOS transistors and other components on an integrated circuit. There are two main kinds of semiconductor memory, volatile and non-volatile. Examples of non-volatile memory are flash memory and ROM, PROM, EPROM and EEPROM memory. Examples of volatile memory are dynamic random-access memory (DRAM) used for primary storage, and static random-access memory (SRAM) used for CPU cache.\\nMost semiconductor memory is organized into memory cells each storing one bit (0 or 1). Flash memory organization includes both one bit per memory cell and multi-level cell capable of storing multiple bits per cell.  The memory cells are grouped into words of fixed word length, for example, 1, 2, 4, 8, 16, 32, 64 or 128 bits. Each word can be accessed by a binary address of N bits, making it possible to store 2N words in the memory.\\n\\n\\n== History ==\\n\\nIn the early 1940s, memory technology often permitted a capacity of a few bytes.  The first electronic programmable digital computer, the ENIAC, using thousands of vacuum tubes, could perform simple calculations involving 20 numbers of ten decimal digits stored in the vacuum tubes.\\nThe next significant advance in computer memory came with acoustic delay line memory, developed by J. Presper Eckert in the early 1940s. Through the construction of a glass tube filled with mercury and plugged at each end with a quartz crystal, delay lines could store bits of information in the form of sound waves propagating through the mercury, with the quartz crystals acting as transducers to read and write bits. Delay line memory was limited to a capacity of up to a few thousand bits.\\nTwo alternatives to the delay line, the Williams tube and Selectron tube, originated in 1946, both using electron beams in glass tubes as means of storage.  Using cathode ray tubes, Fred Williams invented the Williams tube, which was the first random-access computer memory.  The Williams tube was able to store more information than the Selectron tube (the Selectron was limited to 256 bits, while the Williams tube could store thousands) and less expensive.  The Williams tube was nevertheless frustratingly sensitive to environmental disturbances.\\nEfforts began in the late 1940s to find non-volatile memory. Magnetic-core memory allowed for recall of memory after power loss. It was developed by Frederick W. Viehe and An Wang in the late 1940s, and improved by Jay Forrester and Jan A. Rajchman in the early 1950s, before being commercialised with the Whirlwind computer in 1953. Magnetic-core memory was the dominant form of memory until the development of MOS semiconductor memory in the 1960s.The first semiconductor memory was implemented as a flip-flop circuit in the early 1960s using bipolar transistors. Semiconductor memory made from discrete devices was first shipped by Texas Instruments to the United States Air Force in 1961. The same year, the concept of solid-state memory on an integrated circuit (IC) chip was proposed by applications engineer Bob Norman at Fairchild Semiconductor. The first bipolar semiconductor memory IC chip was the SP95 introduced by IBM in 1965. While semiconductor memory offered improved performance over magnetic-core memory, it remain larger and more expensive and did not displace magnetic-core memory until the late 1960s.\\n\\n\\n=== MOS memory ===\\n\\nThe invention of the MOSFET (metal–oxide–semiconductor field-effect transistor, or MOS transistor), by Mohamed M. Atalla and Dawon Kahng at Bell Labs in 1959, enabled the practical use of metal–oxide–semiconductor (MOS) transistors as memory cell storage elements. MOS memory was developed by John Schmidt at Fairchild Semiconductor in 1964. In addition to higher performance, MOS semiconductor memory was cheaper and consumed less power than magnetic core memory. In 1965, J. Wood and R. Ball of the Royal Radar Establishment proposed digital storage systems that use CMOS (complementary MOS) memory cells, in addition to MOSFET power devices for the power supply, switched cross-coupling, switches and delay line storage. The development of silicon-gate MOS integrated circuit (MOS IC) technology by Federico Faggin at Fairchild in 1968 enabled the production of MOS memory chips. NMOS memory was commercialized by IBM in the early 1970s. MOS memory overtook magnetic core memory as the dominant memory technology in the early 1970s.The two main types of volatile random-access memory (RAM) are static random-access memory (SRAM) and dynamic random-access memory (DRAM). Bipolar SRAM was invented by Robert Norman at Fairchild Semiconductor in 1963, followed by the development of MOS SRAM by John Schmidt at Fairchild in 1964. SRAM became an alternative to magnetic-core memory, but required six MOS transistors for each bit of data. Commercial use of SRAM began in 1965, when IBM introduced their SP95 SRAM chip for the System/360 Model 95.Toshiba introduced bipolar DRAM memory cells for its Toscal BC-1411 electronic calculator in 1965. While it offered improved performance over magnetic-core memory, bipolar DRAM could not compete with the lower price of the then dominant magnetic-core memory. MOS technology is the basis for modern DRAM. In 1966, Dr. Robert H. Dennard at the IBM Thomas J. Watson Research Center was working on MOS memory. While examining the characteristics of MOS technology, he found it was capable of building capacitors, and that storing a charge or no charge on the MOS capacitor could represent the 1 and 0 of a bit, while the MOS transistor could control writing the charge to the capacitor. This led to his development of a single-transistor DRAM memory cell. In 1967, Dennard filed a patent under IBM for a single-transistor DRAM memory cell, based on MOS technology. This led to the first commercial DRAM IC chip, the Intel 1103, in October 1970. Synchronous dynamic random-access memory (SDRAM) later debuted with the Samsung KM48SL2000 chip in 1992.The term memory is also often used to refer to non-volatile memory, specifically flash memory. It has origins in read-only memory (ROM). Programmable read-only memory (PROM) was invented by Wen Tsing Chow in 1956, while working for the Arma Division of the American Bosch Arma Corporation. In 1967, Dawon Kahng and Simon Sze of Bell Labs proposed that the floating gate of a MOS semiconductor device could be used for the cell of a reprogrammable read-only memory (ROM), which led to Dov Frohman of Intel inventing EPROM (erasable PROM) in 1971. EEPROM (electrically erasable PROM) was developed by Yasuo Tarui, Yutaka Hayashi and Kiyoko Naga at the Electrotechnical Laboratory in 1972. Flash memory was invented by Fujio Masuoka at Toshiba in the early 1980s. Masuoka and colleagues presented the invention of NOR flash in 1984, and then NAND flash in 1987. Toshiba commercialized NAND flash memory in 1987.Developments in technology and economies of scale have made possible so-called Very Large Memory (VLM) computers.\\n\\n\\n== Volatile memory ==\\n\\nVolatile memory is computer memory that requires power to maintain the stored information.  Most modern semiconductor volatile memory is either static RAM (SRAM) or dynamic RAM (DRAM). SRAM retains its contents as long as the power is connected and is simpler for interfacing, but uses six transistors per bit. Dynamic RAM is more complicated for interfacing and control, needing regular refresh cycles to prevent losing its contents, but uses only one transistor and one capacitor per bit, allowing it to reach much higher densities and much cheaper per-bit costs.SRAM is not worthwhile for desktop system memory, where DRAM dominates, but is used for their cache memories.  SRAM is commonplace in small embedded systems, which might only need tens of kilobytes or less. Volatile memory technologies that have attempted to compete or replace SRAM and DRAM include Z-RAM and A-RAM.\\n\\n\\n== Non-volatile memory ==\\n\\nNon-volatile memory is computer memory that can retain the stored information even when not powered. Examples of non-volatile memory include read-only memory (see ROM), flash memory, most types of magnetic computer storage devices (e.g. hard disk drives, floppy disks and magnetic tape), optical discs, and early computer storage methods such as paper tape and punched cards.Forthcoming non-volatile memory technologies include FERAM, CBRAM, PRAM, STT-RAM, SONOS, RRAM, racetrack memory, NRAM, 3D XPoint, and millipede memory.\\n\\n\\n== Semi-volatile memory ==\\nA third category of memory is \"semi-volatile\".  The term is used to describe a memory which has some limited non-volatile duration after power is removed, but then data is ultimately lost.  A typical goal when using a semi-volatile memory is to provide high performance/durability/etc. associated with volatile memories, while providing some benefits of a true non-volatile memory.\\nFor example, some non-volatile memory types can wear out, where a \"worn\" cell has increased volatility but otherwise continues to work.  Data locations which are written frequently can thus be directed to use worn circuits.  As long as the location is updated within some known retention time, the data stays valid.  If the retention time \"expires\" without an update, then the value is copied to a less-worn circuit with longer retention.  Writing first to the worn area allows a high write rate while avoiding wear on the not-worn circuits.As a second example, an STT-RAM can be made non-volatile by building large cells, but the cost per bit and write power go up, while the write speed goes down.  Using small cells improves cost, power, and speed, but leads to semi-volatile behavior.  In some applications the increased volatility can be managed to provide many benefits of a non-volatile memory, for example by removing power but forcing a wake-up before data is lost; or by caching read-only data and discarding the cached data if the power-off time exceeds the non-volatile threshold.The term semi-volatile is also used to describe semi-volatile behavior constructed from other memory types.  For example, a volatile and a non-volatile memory may be combined, where an external signal copies data from the volatile memory to the non-volatile memory, but if power is removed without copying, the data is lost.  Or, a battery-backed volatile memory, and if external power is lost there is some known period where the battery can continue to power the volatile memory, but if power is off for an extended time, the battery runs down and data is lost.\\n\\n\\n== Management ==\\n\\nProper management of memory is vital for a computer system to operate properly. Modern operating systems have complex systems to properly manage memory. Failure to do so can lead to bugs, slow performance, and at worst case, takeover by viruses and malicious software.\\n\\n\\n=== Bugs ===\\nImproper management of memory is a common cause of bugs, including the following types:\\n\\nIn an arithmetic overflow, a calculation results in a number larger than the allocated memory permits. For example, a signed 8-bit integer allows the numbers −128 to +127. If its value is 127 and it is instructed to add one, the computer can not store the number 128 in that space. Such a case will result in undesired operation, such as changing the number\\'s value to −128 instead of +128.\\nA memory leak occurs when a program requests memory from the operating system and never returns the memory when it\\'s done with it. A program with this bug will gradually require more and more memory until the program fails as it runs out.\\nA segmentation fault results when a program tries to access memory that it does not have permission to access. Generally a program doing so will be terminated by the operating system.\\nA buffer overflow means that a program writes data to the end of its allocated space and then continues to write data to memory that has been allocated for other purposes. This may result in erratic program behavior, including memory access errors, incorrect results, a crash, or a breach of system security. They are thus the basis of many software vulnerabilities and can be maliciously exploited.\\n\\n\\n=== Early computer systems ===\\nIn early computer systems, programs typically specified the location to write memory and what data to put there. This location was a physical location on the actual memory hardware. The slow processing of such computers did not allow for the complex memory management systems used today. Also, as most such systems were single-task, sophisticated systems were not required as much.\\nThis approach has its pitfalls. If the location specified is incorrect, this will cause the computer to write the data to some other part of the program. The results of an error like this are unpredictable. In some cases, the incorrect data might overwrite memory used by the operating system. Computer crackers can take advantage of this to create viruses and malware.\\n\\n\\n=== Virtual memory ===\\n\\nVirtual memory is a system where all physical memory is controlled by the operating system. When a program needs memory, it requests it from the operating system. The operating system then decides in what physical location to place the program\\'s code and data.\\nThis offers several advantages. Computer programmers no longer need to worry about where their data is physically stored or whether the user\\'s computer will have enough memory. It also allows multiple types of memory to be used. For example, some data can be stored in physical RAM chips while other data is stored on a hard drive (e.g. in a swapfile), functioning as an extension of the cache hierarchy. This drastically increases the amount of memory available to programs. The operating system will place actively used data in physical RAM, which is much faster than hard disks. When the amount of RAM is not sufficient to run all the current programs, it can result in a situation where the computer spends more time moving data from RAM to disk and back than it does accomplishing tasks; this is known as thrashing.\\n\\n\\n=== Protected memory ===\\n\\nProtected memory is a system where each program is given an area of memory to use and is not permitted to go outside that range. Use of protected memory greatly enhances both the reliability and security of a computer system.\\nWithout protected memory, it is possible that a bug in one program will alter the memory used by another program. This will cause that other program to run off of corrupted memory with unpredictable results. If the operating system\\'s memory is corrupted, the entire computer system may crash and need to be rebooted. At times programs intentionally alter the memory used by other programs. This is done by viruses and malware to take over computers. It may also be used benignly by desirable programs which are intended to modify other programs; in the modern age, this is generally considered bad programming practice for application programs, but it may be used by system development tools such as debuggers, for example to insert breakpoints or hooks.\\nProtected memory assigns programs their own areas of memory. If the operating system detects that a program has tried to alter memory that does not belong to it, the program is terminated (or otherwise restricted or redirected). This way, only the offending program crashes, and other programs are not affected by the misbehavior (whether accidental or intentional).\\nProtected memory systems almost always include virtual memory as well.\\n\\n\\n== See also ==\\nMemory geometry\\nMemory hierarchy\\nMemory organization\\nProcessor registers store data but normally are not considered as memory, since they only store one word and do not include an addressing mechanism.\\nSemiconductor memory\\nUnits of information\\n\\n\\n== Notes ==\\n\\n\\n== References ==\\n\\n\\n== Further reading ==\\nMiller, Stephen W. (1977), Memory and Storage Technology, Montvale.: AFIPS PressMemory and Storage Technology, Alexandria, Virginia.: Time Life Books, 1988', 'An operating system (OS) is system software that manages computer hardware, software resources, and provides common services for computer programs.\\nTime-sharing operating systems schedule tasks for efficient use of the system and may also include accounting software for cost allocation of processor time, mass storage, printing, and other resources.\\nFor hardware functions such as input and output and memory allocation, the operating system acts as an intermediary between programs and the computer hardware, although the application code is usually executed directly by the hardware and frequently makes system calls to an OS function or is interrupted by it. Operating systems are found on many devices that contain a computer –  from cellular phones and video game consoles to web servers and supercomputers.\\nThe dominant general-purpose personal computer operating system is Microsoft Windows with a market share of around 76.45%. macOS by Apple Inc. is in second place (17.72%), and the varieties of Linux are collectively in third place (1.73%). In the mobile sector (including smartphones and tablets), Android\\'s share is up to 72% in the year 2020. According to third quarter 2016 data, Android\\'s share on smartphones is dominant with 87.5 percent with also a growth rate of 10.3 percent per year, followed by Apple\\'s iOS with 12.1 percent with per year decrease in market share of 5.2 percent, while other operating systems amount to just 0.3 percent. Linux distributions are dominant in the server and supercomputing sectors. Other specialized classes of operating systems (special-purpose operating systems), such as embedded and real-time systems, exist for many applications. Security-focused operating systems also exist. Some operating systems have low system requirements (e.g. light-weight Linux distribution). Others may have higher system requirements.\\nSome operating systems require installation or may come pre-installed with purchased computers (OEM-installation), whereas others may run directly from media (i.e. live CD) or flash memory (i.e. USB stick).\\n\\n\\n== Types of operating systems ==\\n\\n\\n=== Single-tasking and multi-tasking ===\\nA single-tasking system can only run one program at a time, while a multi-tasking operating system allows more than one program to be running in concurrency. This is achieved by time-sharing, where the available processor time is divided between multiple processes. These processes are each interrupted repeatedly in time slices by a task-scheduling subsystem of the operating system. Multi-tasking may be characterized in preemptive and co-operative types. In preemptive multitasking, the operating system slices the CPU time and dedicates a slot to each of the programs. Unix-like operating systems, such as Linux—as well as non-Unix-like, such as AmigaOS—support preemptive multitasking. Cooperative multitasking is achieved by relying on each process to provide time to the other processes in a defined manner. 16-bit versions of Microsoft Windows used cooperative multi-tasking; 32-bit versions of both Windows NT and Win9x used preemptive multi-tasking.\\n\\n\\n=== Single- and multi-user ===\\nSingle-user operating systems have no facilities to distinguish users, but may allow multiple programs to run in tandem. A multi-user operating system extends the basic concept of multi-tasking with facilities that identify processes and resources, such as disk space, belonging to multiple users, and the system permits multiple users to interact with the system at the same time. Time-sharing operating systems schedule tasks for efficient use of the system and may also include accounting software for cost allocation of processor time, mass storage, printing, and other resources to multiple users.\\n\\n\\n=== Distributed ===\\nA distributed operating system manages a group of distinct, networked computers and makes them appear to be a single computer, as all computations are distributed (divided amongst the constituent computers).\\n\\n\\n=== Templated ===\\nIn the distributed and cloud computing context of an OS, templating refers to creating a single virtual machine image as a guest operating system, then saving it as a tool for multiple running virtual machines. The technique is used both in virtualization and cloud computing management, and is common in large server warehouses.\\n\\n\\n=== Embedded ===\\nEmbedded operating systems are designed to be used in embedded computer systems. They are designed to operate on small machines with less autonomy (e.g. PDAs). They are very compact and extremely efficient by design, and are able to operate with a limited amount of resources. Windows CE and Minix 3 are some examples of embedded operating systems.\\n\\n\\n=== Real-time ===\\nA real-time operating system is an operating system that guarantees to process events or data by a specific moment in time. A real-time operating system may be single- or multi-tasking, but when multitasking, it uses specialized scheduling algorithms so that a deterministic nature of behavior is achieved. Such an event-driven system switches between tasks based on their priorities or external events, whereas time-sharing operating systems switch tasks based on clock interrupts.\\n\\n\\n=== Library ===\\nA library operating system is one in which the services that a typical operating system provides, such as networking, are provided in the form of libraries and composed with the application and configuration code to construct a unikernel: a specialized, single address space, machine image that can be deployed to cloud or embedded environments.\\n\\n\\n== History ==\\n\\nEarly computers were built to perform a series of single tasks, like a calculator. Basic operating system features were developed in the 1950s, such as resident monitor functions that could automatically run different programs in succession to speed up processing. Operating systems did not exist in their modern and more complex forms until the early 1960s.   Hardware features were added, that enabled use of runtime libraries, interrupts, and parallel processing. When personal computers became popular in the 1980s, operating systems were made for them similar in concept to those used on larger computers.\\nIn the 1940s, the earliest electronic digital systems had no operating systems.  Electronic systems of this time were programmed on rows of mechanical switches or by jumper wires on plugboards.  These were special-purpose systems that, for example, generated ballistics tables for the military or controlled the printing of payroll checks from data on punched paper cards.  After programmable general-purpose computers were invented, machine languages(consisting of strings of the binary digits 0 and 1 on punched paper tape) were introduced that sped up the programming process (Stern, 1981).\\n\\nIn the early 1950s, a computer could execute only one program at a time.  Each user had sole use of the computer for a limited period and would arrive at a scheduled time with their program and data on punched paper cards or punched tape. The program would be loaded into the machine, and the machine would be set to work until the program completed or crashed. Programs could generally be debugged via a front panel using toggle switches and panel lights. It is said that Alan Turing was a master of this on the early Manchester Mark 1 machine, and he was already deriving the primitive conception of an operating system from the principles of the universal Turing machine.Later machines came with libraries of programs, which would be linked to a user\\'s program to assist in operations such as input and output and compiling (generating machine code from human-readable symbolic code). This was the genesis of the modern-day operating system. However, machines still ran a single job at a time. At Cambridge University in England, the job queue was at one time a washing line (clothesline) from which tapes were hung with different colored clothes-pegs to indicate job priority.An improvement was the Atlas Supervisor. Introduced with the Manchester Atlas in 1962, it is considered by many to be the first recognisable modern operating system. Brinch Hansen described it as \"the most significant breakthrough in the history of operating systems.\"\\n\\n\\n=== Mainframes ===\\n\\nThrough the 1950s, many major features were pioneered in the field of operating systems on mainframe computers, including batch processing, input/output interrupting, buffering, multitasking, spooling, runtime libraries, link-loading, and programs for sorting records in files. These features were included or not included in application software at the option of application programmers, rather than in a separate operating system used by all applications.  In 1959, the SHARE Operating System was released as an integrated utility for the IBM 704, and later in the 709 and 7090 mainframes, although it was quickly supplanted by IBSYS/IBJOB on the 709, 7090 and 7094.\\nDuring the 1960s, IBM\\'s OS/360 introduced the concept of a single OS spanning an entire product line, which was crucial for the success of the System/360 machines. IBM\\'s current mainframe operating systems are distant descendants of this original system and modern machines are backwards-compatible with applications written for OS/360.OS/360 also pioneered the concept that the operating system keeps track of all of the system resources that are used, including program and data space allocation in main memory and file space in secondary storage, and file locking during updates. When a process is terminated for any reason, all of these resources are re-claimed by the operating system.\\nThe alternative CP-67 system for the S/360-67 started a whole line of IBM operating systems focused on the concept of virtual machines. Other operating systems used on IBM S/360 series mainframes included systems developed by IBM: DOS/360 (Disk Operating System), TSS/360 (Time Sharing System), TOS/360 (Tape Operating System), BOS/360 (Basic Operating System), and ACP (Airline Control Program), as well as a few non-IBM systems: MTS (Michigan Terminal System), MUSIC (Multi-User System for Interactive Computing), and ORVYL (Stanford Timesharing System).\\nControl Data Corporation developed the SCOPE operating system in the 1960s, for batch processing. In cooperation with the University of Minnesota, the Kronos and later the NOS operating systems were developed during the 1970s, which supported simultaneous batch and timesharing use. Like many commercial timesharing systems, its interface was an extension of the Dartmouth BASIC operating systems, one of the pioneering efforts in timesharing and programming languages. In the late 1970s, Control Data and the University of Illinois developed the PLATO operating system, which used plasma panel displays and long-distance time sharing networks. Plato was remarkably innovative for its time, featuring real-time chat, and multi-user graphical games.\\nIn 1961, Burroughs Corporation introduced the B5000 with the MCP (Master Control Program) operating system. The B5000 was a stack machine designed to exclusively support high-level languages with no assembler; indeed, the MCP was the first OS to be written exclusively in a high-level language (ESPOL, a dialect of ALGOL). MCP also introduced many other ground-breaking innovations, such as being the first commercial implementation of virtual memory. During development of the AS/400, IBM made an approach to Burroughs to license MCP to run on the AS/400 hardware. This proposal was declined by Burroughs management to protect its existing hardware production. MCP is still in use today in the Unisys company\\'s MCP/ClearPath line of computers.\\nUNIVAC, the first commercial computer manufacturer, produced a series of EXEC operating systems. Like all early main-frame systems, this batch-oriented system managed magnetic drums, disks, card readers and line printers. In the 1970s, UNIVAC produced the Real-Time Basic (RTB) system to support large-scale time sharing, also patterned after the Dartmouth BC system.\\nGeneral Electric and MIT developed General Electric Comprehensive Operating Supervisor (GECOS), which introduced the concept of ringed security privilege levels. After acquisition by Honeywell it was renamed General Comprehensive Operating System (GCOS).\\nDigital Equipment Corporation developed many operating systems for its various computer lines, including TOPS-10 and TOPS-20 time sharing systems for the 36-bit PDP-10 class systems. Before the widespread use of UNIX, TOPS-10 was a particularly popular system in universities, and in the early ARPANET community. RT-11 was a single-user real-time OS for the PDP-11 class minicomputer, and RSX-11 was the corresponding multi-user OS.\\nFrom the late 1960s through the late 1970s, several hardware capabilities evolved that allowed similar or ported software to run on more than one system. Early systems had utilized microprogramming to implement features on their systems in order to permit different underlying computer architectures to appear to be the same as others in a series. In fact, most 360s after the 360/40 (except the 360/44, 360/75, 360/91, 360/95 and 360/195) were microprogrammed implementations.\\nThe enormous investment in software for these systems made since the 1960s caused most of the original computer manufacturers to continue to develop compatible operating systems along with the hardware. Notable supported mainframe operating systems include:\\n\\nBurroughs MCP –  B5000, 1961 to Unisys Clearpath/MCP, present\\nIBM OS/360 –  IBM System/360, 1966 to IBM z/OS, present\\nIBM CP-67 –  IBM System/360, 1967 to IBM z/VM\\nUNIVAC EXEC 8 –  UNIVAC 1108, 1967, to OS 2200 Unisys Clearpath Dorado, present\\n\\n\\n=== Microcomputers ===\\n\\nThe first microcomputers did not have the capacity or need for the elaborate operating systems that had been developed for mainframes and minis; minimalistic operating systems were developed, often loaded from ROM and known as monitors. One notable early disk operating system was CP/M, which was supported on many early microcomputers and was closely imitated by Microsoft\\'s MS-DOS, which became widely popular as the operating system chosen for the IBM PC (IBM\\'s version of it was called IBM DOS or PC DOS). In the 1980s, Apple Computer Inc. (now Apple Inc.) abandoned its popular Apple II series of microcomputers to introduce the Apple Macintosh computer with an innovative graphical user interface (GUI) to the Mac OS.\\nThe introduction of the Intel 80386 CPU chip in October 1985, with 32-bit architecture and paging capabilities, provided personal computers with the ability to run multitasking operating systems like those of earlier minicomputers and mainframes. Microsoft responded to this progress by hiring Dave Cutler, who had developed the VMS operating system for Digital Equipment Corporation. He would lead the development of the Windows NT operating system, which continues to serve as the basis for Microsoft\\'s operating systems line. Steve Jobs, a co-founder of Apple Inc., started NeXT Computer Inc., which developed the NEXTSTEP operating system. NEXTSTEP would later be acquired by Apple Inc. and used, along with code from FreeBSD as the core of Mac OS X (macOS after latest name change).\\nThe GNU Project was started by activist and programmer Richard Stallman with the goal of creating a complete free software replacement to the proprietary UNIX operating system. While the project was highly successful in duplicating the functionality of various parts of UNIX, development of the GNU Hurd kernel proved to be unproductive. In 1991, Finnish computer science student Linus Torvalds, with cooperation from volunteers collaborating over the Internet, released the first version of the Linux kernel. It was soon merged with the GNU user space components and system software to form a complete operating system. Since then, the combination of the two major components has usually been referred to as simply \"Linux\" by the software industry, a naming convention that Stallman and the Free Software Foundation remain opposed to, preferring the name GNU/Linux. The Berkeley Software Distribution, known as BSD, is the UNIX derivative distributed by the University of California, Berkeley, starting in the 1970s. Freely distributed and ported to many minicomputers, it eventually also gained a following for use on PCs, mainly as FreeBSD, NetBSD and OpenBSD.\\n\\n\\n== Examples ==\\n\\n\\n=== Unix and Unix-like operating systems ===\\n\\nUnix was originally written in assembly language. Ken Thompson wrote B, mainly based on BCPL, based on his experience in the MULTICS project. B was replaced by C, and Unix, rewritten in C, developed into a large, complex family of inter-related operating systems which have been influential in every modern operating system (see History).\\nThe Unix-like family is a diverse group of operating systems, with several major sub-categories including System V, BSD, and Linux. The name \"UNIX\" is a trademark of The Open Group which licenses it for use with any operating system that has been shown to conform to their definitions. \"UNIX-like\" is commonly used to refer to the large set of operating systems which resemble the original UNIX.\\nUnix-like systems run on a wide variety of computer architectures. They are used heavily for servers in business, as well as workstations in academic and engineering environments. Free UNIX variants, such as Linux and BSD, are popular in these areas.\\nFour operating systems are certified by The Open Group (holder of the Unix trademark) as Unix. HP\\'s HP-UX and IBM\\'s AIX are both descendants of the original System V Unix and are designed to run only on their respective vendor\\'s hardware. In contrast, Sun Microsystems\\'s Solaris can run on multiple types of hardware, including x86 and Sparc servers, and PCs. Apple\\'s macOS, a replacement for Apple\\'s earlier (non-Unix) Mac OS, is a hybrid kernel-based BSD variant derived from NeXTSTEP, Mach, and FreeBSD.\\nUnix interoperability was sought by establishing the POSIX standard. The POSIX standard can be applied to any operating system, although it was originally created for various Unix variants.\\n\\n\\n==== BSD and its descendants ====\\n\\nA subgroup of the Unix family is the Berkeley Software Distribution family, which includes FreeBSD, NetBSD, and OpenBSD. These operating systems are most commonly found on webservers, although they can also function as a personal computer OS. The Internet owes much of its existence to BSD, as many of the protocols now commonly used by computers to connect, send and receive data over a network were widely implemented and refined in BSD. The World Wide Web was also first demonstrated on a number of computers running an OS based on BSD called NeXTSTEP.\\nIn 1974, University of California, Berkeley installed its first Unix system. Over time, students and staff in the computer science department there began adding new programs to make things easier, such as text editors. When Berkeley received new VAX computers in 1978 with Unix installed, the school\\'s undergraduates modified Unix even more in order to take advantage of the computer\\'s hardware possibilities. The Defense Advanced Research Projects Agency of the US Department of Defense took interest, and decided to fund the project. Many schools, corporations, and government organizations took notice and started to use Berkeley\\'s version of Unix instead of the official one distributed by AT&T.\\nSteve Jobs, upon leaving Apple Inc. in 1985, formed NeXT Inc., a company that manufactured high-end computers running on a variation of BSD called NeXTSTEP. One of these computers was used by Tim Berners-Lee as the first webserver to create the World Wide Web.\\nDevelopers like Keith Bostic encouraged the project to replace any non-free code that originated with Bell Labs. Once this was done, however, AT&T sued. After two years of legal disputes, the BSD project spawned a number of free derivatives, such as NetBSD and FreeBSD (both in 1993), and OpenBSD (from NetBSD in 1995).\\n\\n\\n==== macOS ====\\n\\nmacOS (formerly \"Mac OS X\" and later \"OS X\")  is a line of open core graphical operating systems developed, marketed, and sold by Apple Inc., the latest of which is pre-loaded on all currently shipping Macintosh computers. macOS is the successor to the original classic Mac OS, which had been Apple\\'s primary operating system since 1984. Unlike its predecessor, macOS is a UNIX operating system built on technology that had been developed at NeXT through the second half of the 1980s and up until Apple purchased the company in early 1997.\\nThe operating system was first released in 1999 as Mac OS X Server 1.0, followed in March 2001 by a client version (Mac OS X v10.0 \"Cheetah\"). Since then, six more distinct \"client\" and \"server\" editions of macOS have been released, until the two were merged in OS X 10.7 \"Lion\".\\nPrior to its merging with macOS, the server edition –  macOS Server –  was architecturally identical to its desktop counterpart and usually ran on Apple\\'s line of Macintosh server hardware. macOS Server included work group management and administration software tools that provide simplified access to key network services, including a mail transfer agent, a Samba server, an LDAP server, a domain name server, and others. With Mac OS X v10.7 Lion, all server aspects of Mac OS X Server have been integrated into the client version and the product re-branded as \"OS X\" (dropping \"Mac\" from the name). The server tools are now offered as an application.\\n\\n\\n==== Linux ====\\n\\nThe Linux kernel originated in 1991, as a project of Linus Torvalds, while a university student in Finland. He posted information about his project on a newsgroup for computer students and programmers, and received support and assistance from volunteers who succeeded in creating a complete and functional kernel.\\nLinux is Unix-like, but was developed without any Unix code, unlike BSD and its variants. Because of its open license model, the Linux kernel code is available for study and modification, which resulted in its use on a wide range of computing machinery from supercomputers to smart-watches. Although estimates suggest that Linux is used on only 1.82% of all \"desktop\" (or laptop) PCs, it has been widely adopted for use in servers and embedded systems such as cell phones. Linux has superseded Unix on many platforms and is used on most supercomputers including the top 385. Many of the same computers are also on Green500 (but in different order), and Linux runs on the top 10. Linux is also commonly used on other small energy-efficient computers, such as smartphones and smartwatches. The Linux kernel is used in some popular distributions, such as Red Hat, Debian, Ubuntu, Linux Mint and Google\\'s Android, Chrome OS, and Chromium OS.\\n\\n\\n=== Microsoft Windows ===\\n\\nMicrosoft Windows is a family of proprietary operating systems designed by Microsoft Corporation and primarily targeted to Intel architecture based computers, with an estimated 88.9 percent total usage share on Web connected computers. The latest version is Windows 11.\\nIn 2011, Windows 7 overtook Windows XP as most common version in use.Microsoft Windows was first released in 1985, as an operating environment running on top of MS-DOS, which was the standard operating system shipped on most Intel architecture personal computers at the time. In 1995, Windows 95 was released which only used MS-DOS as a bootstrap. For backwards compatibility, Win9x could run real-mode MS-DOS and 16-bit Windows 3.x drivers. Windows ME, released in 2000, was the last version in the Win9x family. Later versions have all been based on the Windows NT kernel. Current client versions of Windows run on IA-32, x86-64 and ARM microprocessors. In addition Itanium is still supported in older server version Windows Server 2008 R2. In the past, Windows NT supported additional architectures.\\nServer editions of Windows are widely used. In recent years, Microsoft has expended significant capital in an effort to promote the use of Windows as a server operating system. However, Windows\\' usage on servers is not as widespread as on personal computers as Windows competes against Linux and BSD for server market share.ReactOS is a Windows-alternative operating system, which is being developed on the principles of Windows –  without using any of Microsoft\\'s code.\\n\\n\\n=== Other ===\\nThere have been many operating systems that were significant in their day but are no longer so, such as AmigaOS; OS/2 from IBM and Microsoft; classic Mac OS, the non-Unix precursor to Apple\\'s macOS; BeOS; XTS-300; RISC OS; MorphOS; Haiku; BareMetal and FreeMint. Some are still used in niche markets and continue to be developed as minority platforms for enthusiast communities and specialist applications. OpenVMS, formerly from DEC, is still under active development by VMS Software Inc. Yet other operating systems are used almost exclusively in academia, for operating systems education or to do research on operating system concepts. A typical example of a system that fulfills both roles is MINIX, while for example Singularity is used purely for research. Another example is the Oberon System designed at ETH Zürich by Niklaus Wirth, Jürg Gutknecht and a group of students at the former Computer Systems Institute in the 1980s. It was used mainly for research, teaching, and daily work in Wirth\\'s group.\\nOther operating systems have failed to win significant market share, but have introduced innovations that have influenced mainstream operating systems, not least Bell Labs\\' Plan 9.\\n\\n\\n== Components ==\\nThe components of an operating system all exist in order to make the different parts of a computer work together. All user software needs to go through the operating system in order to use any of the hardware, whether it be as simple as a mouse or keyboard or as complex as an Internet component.\\n\\n\\n=== Kernel ===\\n\\nWith the aid of the firmware and device drivers, the kernel provides the most basic level of control over all of the computer\\'s hardware devices. It manages memory access for programs in the RAM, it determines which programs get access to which hardware resources, it sets up or resets the CPU\\'s operating states for optimal operation at all times, and it organizes the data for long-term non-volatile storage with file systems on such media as disks, tapes, flash memory, etc.\\n\\n\\n==== Program execution ====\\n\\nThe operating system provides an interface between an application program and the computer hardware, so that an application program can interact with the hardware only by obeying rules and procedures programmed into the operating system.  The operating system is also a set of services which simplify development and execution of application programs. Executing an application program involves the creation of a process by the operating system kernel which assigns memory space and other resources, establishes a priority for the process in multi-tasking systems, loads program binary code into memory, and initiates execution of the application program which then interacts with the user and with hardware devices.\\n\\n\\n==== Interrupts ====\\n\\nInterrupts are central to operating systems, as they provide an efficient way for the operating system to interact with and react to its environment.  The alternative –  having the operating system \"watch\" the various sources of input for events (polling) that require action –  can be found in older systems with very small stacks (50 or 60 bytes) but is unusual in modern systems with large stacks. Interrupt-based programming is directly supported by most modern CPUs. Interrupts provide a computer with a way of automatically saving local register contexts, and running specific code in response to events. Even very basic computers support hardware interrupts, and allow the programmer to specify code which may be run when that event takes place.\\nWhen an interrupt is received, the computer\\'s hardware automatically suspends whatever program is currently running, saves its status, and runs computer code previously associated with the interrupt; this is analogous to placing a bookmark in a book in response to a phone call.  In modern operating systems, interrupts are handled by the operating system\\'s kernel. Interrupts may come from either the computer\\'s hardware or the running program.\\nWhen a hardware device triggers an interrupt, the operating system\\'s kernel decides how to deal with this event, generally by running some processing code. The amount of code being run depends on the priority of the interrupt (for example: a person usually responds to a smoke detector alarm before answering the phone). The processing of hardware interrupts is a task that is usually delegated to software called a device driver, which may be part of the operating system\\'s kernel, part of another program, or both. Device drivers may then relay information to a running program by various means.\\nA program may also trigger an interrupt to the operating system. If a program wishes to access hardware, for example, it may interrupt the operating system\\'s kernel, which causes control to be passed back to the kernel. The kernel then processes the request.  If a program wishes additional resources (or wishes to shed resources) such as memory, it triggers an interrupt to get the kernel\\'s attention.\\n\\n\\n==== Modes ====\\n\\nModern computers support multiple modes of operation. CPUs with this capability offer at least two modes: user mode and supervisor mode. In general terms, supervisor mode operation allows unrestricted access to all machine resources, including all MPU instructions.  User mode operation sets limits on instruction use and typically disallows direct access to machine resources. CPUs might have other modes similar to user mode as well, such as the virtual modes in order to emulate older processor types, such as 16-bit processors on a 32-bit one, or 32-bit processors on a 64-bit one.\\nAt power-on or reset, the system begins in supervisor mode. Once an operating system kernel has been loaded and started, the boundary between user mode and supervisor mode (also known as kernel mode) can be established.\\nSupervisor mode is used by the kernel for low level tasks that need unrestricted access to hardware, such as controlling how memory is accessed, and communicating with devices such as disk drives and video display devices. User mode, in contrast, is used for almost everything else. Application programs, such as word processors and database managers, operate within user mode, and can only access machine resources by turning control over to the kernel, a process which causes a switch to supervisor mode.  Typically, the transfer of control to the kernel is achieved by executing a software interrupt instruction, such as the Motorola 68000 TRAP instruction.  The software interrupt causes the processor to switch from user mode to supervisor mode and begin executing code that allows the kernel to take control.\\nIn user mode, programs usually have access to a restricted set of processor instructions, and generally cannot execute any instructions that could potentially cause disruption to the system\\'s operation.  In supervisor mode, instruction execution restrictions are typically removed, allowing the kernel unrestricted access to all machine resources.\\nThe term \"user mode resource\" generally refers to one or more CPU registers, which contain information that the running program isn\\'t allowed to alter. Attempts to alter these resources generally causes a switch to supervisor mode, where the operating system can deal with the illegal operation the program was attempting, for example, by forcibly terminating (\"killing\") the program.\\n\\n\\n==== Memory management ====\\n\\nAmong other things, a multiprogramming operating system kernel must be responsible for managing all system memory which is currently in use by programs. This ensures that a program does not interfere with memory already in use by another program. Since programs time share, each program must have independent access to memory.\\nCooperative memory management, used by many early operating systems, assumes that all programs make voluntary use of the kernel\\'s memory manager, and do not exceed their allocated memory. This system of memory management is almost never seen any more, since programs often contain bugs which can cause them to exceed their allocated memory. If a program fails, it may cause memory used by one or more other programs to be affected or overwritten. Malicious programs or viruses may purposefully alter another program\\'s memory, or may affect the operation of the operating system itself. With cooperative memory management, it takes only one misbehaved program to crash the system.\\nMemory protection enables the kernel to limit a process\\' access to the computer\\'s memory. Various methods of memory protection exist, including memory segmentation and paging. All methods require some level of hardware support (such as the 80286 MMU), which doesn\\'t exist in all computers.\\nIn both segmentation and paging, certain protected mode registers specify to the CPU what memory address it should allow a running program to access. Attempts to access other addresses trigger an interrupt which cause the CPU to re-enter supervisor mode, placing the kernel in charge. This is called a segmentation violation or Seg-V for short, and since it is both difficult to assign a meaningful result to such an operation, and because it is usually a sign of a misbehaving program, the kernel generally resorts to terminating the offending program, and reports the error.\\nWindows versions 3.1 through ME had some level of memory protection, but programs could easily circumvent the need to use it. A general protection fault would be produced, indicating a segmentation violation had occurred; however, the system would often crash anyway.\\n\\n\\n==== Virtual memory ====\\n\\nThe use of virtual memory addressing (such as paging or segmentation) means that the kernel can choose what memory each program may use at any given time, allowing the operating system to use the same memory locations for multiple tasks.\\nIf a program tries to access memory that isn\\'t in its current range of accessible memory, but nonetheless has been allocated to it, the kernel is interrupted in the same way as it would if the program were to exceed its allocated memory. (See section on memory management.) Under UNIX this kind of interrupt is referred to as a page fault.\\nWhen the kernel detects a page fault it generally adjusts the virtual memory range of the program which triggered it, granting it access to the memory requested. This gives the kernel discretionary power over where a particular application\\'s memory is stored, or even whether or not it has actually been allocated yet.\\nIn modern operating systems, memory which is accessed less frequently can be temporarily stored on disk or other media to make that space available for use by other programs. This is called swapping, as an area of memory can be used by multiple programs, and what that memory area contains can be swapped or exchanged on demand.\\n\"Virtual memory\" provides the programmer or the user with the perception that there is a much larger amount of RAM in the computer than is really there.\\n\\n\\n==== Multitasking ====\\n\\nMultitasking refers to the running of multiple independent computer programs on the same computer; giving the appearance that it is performing the tasks at the same time. Since most computers can do at most one or two things at one time, this is generally done via time-sharing, which means that each program uses a share of the computer\\'s time to execute.\\nAn operating system kernel contains a scheduling program which determines how much time each process spends executing, and in which order execution control should be passed to programs. Control is passed to a process by the kernel, which allows the program access to the CPU and memory. Later, control is returned to the kernel through some mechanism, so that another program may be allowed to use the CPU. This so-called passing of control between the kernel and applications is called a context switch.\\nAn early model which governed the allocation of time to programs was called cooperative multitasking. In this model, when control is passed to a program by the kernel, it may execute for as long as it wants before explicitly returning control to the kernel. This means that a malicious or malfunctioning program may not only prevent any other programs from using the CPU, but it can hang the entire system if it enters an infinite loop.\\nModern operating systems extend the concepts of application preemption to device drivers and kernel code, so that the operating system has preemptive control over internal run-times as well.\\nThe philosophy governing preemptive multitasking is that of ensuring that all programs are given regular time on the CPU. This implies that all programs must be limited in how much time they are allowed to spend on the CPU without being interrupted. To accomplish this, modern operating system kernels make use of a timed interrupt. A protected mode timer is set by the kernel which triggers a return to supervisor mode after the specified time has elapsed. (See above sections on Interrupts and Dual Mode Operation.)\\nOn many single user operating systems cooperative multitasking is perfectly adequate, as home computers generally run a small number of well tested programs. The AmigaOS is an exception, having preemptive multitasking from its first version. Windows NT was the first version of Microsoft Windows which enforced preemptive multitasking, but it didn\\'t reach the home user market until Windows XP (since Windows NT was targeted at professionals).\\n\\n\\n==== Disk access and file systems ====\\n\\nAccess to data stored on disks is a central feature of all operating systems. Computers store data on disks using files, which are structured in specific ways in order to allow for faster access, higher reliability, and to make better use of the drive\\'s available space. The specific way in which files are stored on a disk is called a file system, and enables files to have names and attributes. It also allows them to be stored in a hierarchy of directories or folders arranged in a directory tree.\\nEarly operating systems generally supported a single type of disk drive and only one kind of file system. Early file systems were limited in their capacity, speed, and in the kinds of file names and directory structures they could use. These limitations often reflected limitations in the operating systems they were designed for, making it very difficult for an operating system to support more than one file system.\\nWhile many simpler operating systems support a limited range of options for accessing storage systems, operating systems like UNIX and Linux support a technology known as a virtual file system or VFS. An operating system such as UNIX supports a wide array of storage devices, regardless of their design or file systems, allowing them to be accessed through a common application programming interface (API). This makes it unnecessary for programs to have any knowledge about the device they are accessing. A VFS allows the operating system to provide programs with access to an unlimited number of devices with an infinite variety of file systems installed on them, through the use of specific device drivers and file system drivers.\\nA connected storage device, such as a hard drive, is accessed through a device driver. The device driver understands the specific language of the drive and is able to translate that language into a standard language used by the operating system to access all disk drives. On UNIX, this is the language of block devices.\\nWhen the kernel has an appropriate device driver in place, it can then access the contents of the disk drive in raw format, which may contain one or more file systems. A file system driver is used to translate the commands used to access each specific file system into a standard set of commands that the operating system can use to talk to all file systems. Programs can then deal with these file systems on the basis of filenames, and directories/folders, contained within a hierarchical structure. They can create, delete, open, and close files, as well as gather various information about them, including access permissions, size, free space, and creation and modification dates.\\nVarious differences between file systems make supporting all file systems difficult. Allowed characters in file names, case sensitivity, and the presence of various kinds of file attributes makes the implementation of a single interface for every file system a daunting task. Operating systems tend to recommend using (and so support natively) file systems specifically designed for them; for example, NTFS in Windows and ext3 and ReiserFS in Linux. However, in practice, third party drivers are usually available to give support for the most widely used file systems in most general-purpose operating systems (for example, NTFS is available in Linux through NTFS-3g, and ext2/3 and ReiserFS are available in Windows through third-party software).\\nSupport for file systems is highly varied among modern operating systems, although there are several common file systems which almost all operating systems include support and drivers for. Operating systems vary on file system support and on the disk formats they may be installed on. Under Windows, each file system is usually limited in application to certain media; for example, CDs must use ISO 9660 or UDF, and as of Windows Vista, NTFS is the only file system which the operating system can be installed on.  It is possible to install Linux onto many types of file systems. Unlike other operating systems, Linux and UNIX allow any file system to be used regardless of the media it is stored in, whether it is a hard drive, a disc (CD, DVD...), a USB flash drive, or even contained within a file located on another file system.\\n\\n\\n==== Device drivers ====\\n\\nA device driver is a specific type of computer software developed to allow interaction with hardware devices. Typically this constitutes an interface for communicating with the device, through the specific computer bus or communications subsystem that the hardware is connected to, providing commands to and/or receiving data from the device, and on the other end, the requisite interfaces to the operating system and software applications. It is a specialized hardware-dependent computer program which is also operating system specific that enables another program, typically an operating system or applications software package or computer program running under the operating system kernel, to interact transparently with a hardware device, and usually provides the requisite interrupt handling necessary for any necessary asynchronous time-dependent hardware interfacing needs.\\nThe key design goal of device drivers is abstraction. Every model of hardware (even within the same class of device) is different. Newer models also are released by manufacturers that provide more reliable or better performance and these newer models are often controlled differently. Computers and their operating systems cannot be expected to know how to control every device, both now and in the future. To solve this problem, operating systems essentially dictate how every type of device should be controlled. The function of the device driver is then to translate these operating system mandated function calls into device specific calls. In theory a new device, which is controlled in a new manner, should function correctly if a suitable driver is available. This new driver ensures that the device appears to operate as usual from the operating system\\'s point of view.\\nUnder versions of Windows before Vista and versions of Linux before 2.6, all driver execution was co-operative, meaning that if a driver entered an infinite loop it would freeze the system. More recent revisions of these operating systems incorporate kernel preemption, where the kernel interrupts the driver to give it tasks, and then separates itself from the process until it receives a response from the device driver, or gives it more tasks to do.\\n\\n\\n=== Networking ===\\n\\nCurrently most operating systems support a variety of networking protocols, hardware, and applications for using them. This means that computers running dissimilar operating systems can participate in a common network for sharing resources such as computing, files, printers, and scanners using either wired or wireless connections. Networks can essentially allow a computer\\'s operating system to access the resources of a remote computer to support the same functions as it could if those resources were connected directly to the local computer. This includes everything from simple communication, to using networked file systems or even sharing another computer\\'s graphics or sound hardware. Some network services allow the resources of a computer to be accessed transparently, such as SSH which allows networked users direct access to a computer\\'s command line interface.\\nClient/server networking allows a program on a computer, called a client, to connect via a network to another computer, called a server. Servers offer (or host) various services to other network computers and users. These services are usually provided through ports or numbered access points beyond the server\\'s IP address. Each port number is usually associated with a maximum of one running program, which is responsible for handling requests to that port. A daemon, being a user program, can in turn access the local hardware resources of that computer by passing requests to the operating system kernel.\\nMany operating systems support one or more vendor-specific or open networking protocols as well, for example, SNA on IBM systems, DECnet on systems from Digital Equipment Corporation, and Microsoft-specific protocols (SMB) on Windows. Specific protocols for specific tasks may also be supported such as NFS for file access. Protocols like ESound, or esd can be easily extended over the network to provide sound from local applications, on a remote system\\'s sound hardware.\\n\\n\\n=== Security ===\\n\\nA computer being secure depends on a number of technologies working properly. A modern operating system provides access to a number of resources, which are available to software running on the system, and to external devices like networks via the kernel.The operating system must be capable of distinguishing between requests which should be allowed to be processed, and others which should not be processed. While some systems may simply distinguish between \"privileged\" and \"non-privileged\", systems commonly have a form of requester identity, such as a user name. To establish identity there may be a process of authentication. Often a username must be quoted, and each username may have a password. Other methods of authentication, such as magnetic cards or biometric data, might be used instead. In some cases, especially connections from the network, resources may be accessed with no authentication at all (such as reading files over a network share). Also covered by the concept of requester identity is authorization; the particular services and resources accessible by the requester once logged into a system are tied to either the requester\\'s user account or to the variously configured groups of users to which the requester belongs.In addition to the allow or disallow model of security, a system with a high level of security also offers auditing options. These would allow tracking of requests for access to resources (such as, \"who has been reading this file?\"). Internal security, or security from an already running program is only possible if all possibly harmful requests must be carried out through interrupts to the operating system kernel. If programs can directly access hardware and resources, they cannot be secured.External security involves a request from outside the computer, such as a login at a connected console or some kind of network connection. External requests are often passed through device drivers to the operating system\\'s kernel, where they can be passed onto applications, or carried out directly. Security of operating systems has long been a concern because of highly sensitive data held on computers, both of a commercial and military nature. The United States Government Department of Defense (DoD) created the Trusted Computer System Evaluation Criteria (TCSEC) which is a standard that sets basic requirements for assessing the effectiveness of security. This became of vital importance to operating system makers, because the TCSEC was used to evaluate, classify and select trusted operating systems being considered for the processing, storage and retrieval of sensitive or classified information.\\nNetwork services include offerings such as file sharing, print services, email, web sites, and file transfer protocols (FTP), most of which can have compromised security. At the front line of security are hardware devices known as firewalls or intrusion detection/prevention systems. At the operating system level, there are a number of software firewalls available, as well as intrusion detection/prevention systems. Most modern operating systems include a software firewall, which is enabled by default. A software firewall can be configured to allow or deny network traffic to or from a service or application running on the operating system. Therefore, one can install and be running an insecure service, such as Telnet or FTP, and not have to be threatened by a security breach because the firewall would deny all traffic trying to connect to the service on that port.\\nAn alternative strategy, and the only sandbox strategy available in systems that do not meet the Popek and Goldberg virtualization requirements, is where the operating system is not running user programs as native code, but instead either emulates a processor or provides a host for a p-code based system such as Java.\\nInternal security is especially relevant for multi-user systems; it allows each user of the system to have private files that the other users cannot tamper with or read. Internal security is also vital if auditing is to be of any use, since a program can potentially bypass the operating system, inclusive of bypassing auditing.\\n\\n\\n=== User interface ===\\n\\nEvery computer that is to be operated by an individual requires a user interface. The user interface is usually referred to as a shell and is essential if human interaction is to be supported.  The user interface views the directory structure and requests services from the operating system that will acquire data from input hardware devices, such as a keyboard, mouse or credit card reader, and requests operating system services to display prompts, status messages and such on output hardware devices, such as a video monitor or printer. The two most common forms of a user interface have historically been the command-line interface, where computer commands are typed out line-by-line, and the graphical user interface, where a visual environment (most commonly a WIMP) is present.\\n\\n\\n==== Graphical user interfaces ====\\n\\nMost of the modern computer systems support graphical user interfaces (GUI), and often include them. In some computer systems, such as the original implementation of the classic Mac OS, the GUI is integrated into the kernel.\\nWhile technically a graphical user interface is not an operating system service, incorporating support for one into the operating system kernel can allow the GUI to be more responsive by reducing the number of context switches required for the GUI to perform its output functions. Other operating systems are modular, separating the graphics subsystem from the kernel and the Operating System. In the 1980s UNIX, VMS and many others had operating systems that were built this way. Linux and macOS are also built this way. Modern releases of Microsoft Windows such as Windows Vista implement a graphics subsystem that is mostly in user-space; however the graphics drawing routines of versions between Windows NT 4.0 and Windows Server 2003 exist mostly in kernel space. Windows 9x had very little distinction between the interface and the kernel.\\nMany computer operating systems allow the user to install or create any user interface they desire. The X Window System in conjunction with GNOME or KDE Plasma 5 is a commonly found setup on most Unix and Unix-like (BSD, Linux, Solaris) systems. A number of Windows shell replacements have been released for Microsoft Windows, which offer alternatives to the included Windows shell, but the shell itself cannot be separated from Windows.\\nNumerous Unix-based GUIs have existed over time, most derived from X11. Competition among the various vendors of Unix (HP, IBM, Sun) led to much fragmentation, though an effort to standardize in the 1990s to COSE and CDE failed for various reasons, and were eventually eclipsed by the widespread adoption of GNOME and K Desktop Environment. Prior to free software-based toolkits and desktop environments, Motif was the prevalent toolkit/desktop combination (and was the basis upon which CDE was developed).\\nGraphical user interfaces evolve over time. For example, Windows has modified its user interface almost every time a new major version of Windows is released, and the Mac OS GUI changed dramatically with the introduction of Mac OS X in 1999.\\n\\n\\n== Real-time operating systems ==\\n\\nA real-time operating system (RTOS) is an operating system intended for applications with fixed deadlines (real-time computing). Such applications include some small embedded systems, automobile engine controllers, industrial robots, spacecraft, industrial control, and some large-scale computing systems.\\nAn early example of a large-scale real-time operating system was Transaction Processing Facility developed by American Airlines and IBM for the Sabre Airline Reservations System.\\nEmbedded systems that have fixed deadlines use a real-time operating system such as VxWorks, PikeOS, eCos, QNX, MontaVista Linux and RTLinux. Windows CE is a real-time operating system that shares similar APIs to desktop Windows but shares none of desktop Windows\\' codebase. Symbian OS also has an RTOS kernel (EKA2) starting with version 8.0b.\\nSome embedded systems use operating systems such as Palm OS, BSD, and Linux, although such operating systems do not support real-time computing.\\n\\n\\n== Operating system development as a hobby ==\\n\\nA hobby operating system may be classified as one whose code has not been directly derived from an existing operating system, and has few users and active developers.In some cases, hobby development is in support of a \"homebrew\" computing device, for example, a simple single-board computer powered by a 6502 microprocessor.  Or, development may be for an architecture already in widespread use.  Operating system development may come from entirely new concepts, or may commence by modeling an existing operating system.  In either case, the hobbyist is his/her own developer, or may interact with a small and sometimes unstructured group of individuals who have like interests.\\nExamples of a hobby operating system include Syllable and TempleOS.\\n\\n\\n== Diversity of operating systems and portability ==\\nApplication software is generally written for use on a specific operating system, and sometimes even for specific hardware. When porting the application to run on another OS, the functionality required by that application may be implemented differently by that OS (the names of functions, meaning of arguments, etc.) requiring the application to be adapted, changed, or otherwise maintained.\\nUnix was the first operating system not written in assembly language, making it very portable to systems different from its native PDP-11.This cost in supporting operating systems diversity can be avoided by instead writing applications against software platforms such as Java or Qt. These abstractions have already borne the cost of adaptation to specific operating systems and their system libraries.\\nAnother approach is for operating system vendors to adopt standards. For example, POSIX and OS abstraction layers provide commonalities that reduce porting costs.\\n\\n\\n== Market share ==\\n\\n\\n== See also ==\\n\\n\\n== Notes ==\\n\\n\\n== References ==\\n\\n\\n== Further reading ==\\n\\n\\n== External links ==\\nOperating Systems at Curlie\\nMultics History and the history of operating systems', 'Computer graphics deals with generating images with the aid of computers. Today, computer graphics is a core technology in digital photography, film, video games, cell phone and computer displays, and many specialized applications. A great deal of specialized hardware and software has been developed, with the displays of most devices being driven by computer graphics hardware. It is a vast and recently developed area of computer science. The phrase was coined in 1960 by computer graphics researchers Verne Hudson and William Fetter of Boeing. It is often abbreviated as CG, or typically in the context of film as computer generated imagery (CGI). The non-artistic aspects of computer graphics are the subject of computer science research.Some topics in computer graphics include user interface design, sprite graphics, rendering, ray tracing, geometry processing, computer animation, vector graphics, 3D modeling, shaders, GPU design, implicit surfaces, visualization, scientific computing, image processing, computational photography, scientific visualization, computational geometry and computer vision, among others. The overall methodology depends heavily on the underlying sciences of geometry, optics, physics, and perception.\\nComputer graphics is responsible for displaying art and image data effectively and meaningfully to the consumer. It is also used for processing image data received from the physical world, such as photo and video content. Computer graphics development has had a significant impact on many types of media and has revolutionized animation, movies, advertising, video games, in general.\\n\\n\\n== Overview ==\\nThe term computer graphics has been used in a broad sense to describe \"almost everything on computers that is not text or sound\". Typically, the term computer graphics refers to several different things:\\n\\nthe representation and manipulation of image data by a computer\\nthe various technologies used to create and manipulate images\\nmethods for digitally synthesizing and manipulating visual content, see study of computer graphicsToday, computer graphics is widespread. Such imagery is found in and on television, newspapers, weather reports, and in a variety of medical investigations and surgical procedures. A well-constructed graph can present complex statistics in a form that is easier to understand and interpret. In the media \"such graphs are used to illustrate papers, reports, theses\", and other presentation material.Many tools have been developed to visualize data. Computer-generated imagery can be categorized into several different types: two dimensional (2D), three dimensional (3D), and animated graphics. As technology has improved, 3D computer graphics have become more common, but 2D computer graphics are still widely used. Computer graphics has emerged as a sub-field of computer science which studies methods for digitally synthesizing and manipulating visual content. Over the past decade, other specialized fields have been developed like information visualization, and scientific visualization more concerned with \"the visualization of three dimensional phenomena (architectural, meteorological, medical, biological, etc.), where the emphasis is on realistic renderings of volumes, surfaces, illumination sources, and so forth, perhaps with a dynamic (time) component\".\\n\\n\\n== History ==\\n\\nThe precursor sciences to the development of modern computer graphics were the advances in electrical engineering, electronics, and television that took place during the first half of the twentieth century. Screens could display art since the Lumiere brothers\\' use of mattes to create special effects for the earliest films dating from 1895, but such displays were limited and not interactive. The first cathode ray tube, the Braun tube, was invented in 1897 – it in turn would permit the oscilloscope and the military control panel – the more direct precursors of the field, as they provided the first two-dimensional electronic displays that responded to programmatic or user input. Nevertheless, computer graphics remained relatively unknown as a discipline until the 1950s and the post-World War II period – during which time the discipline emerged from a combination of both pure university and laboratory academic research into more advanced computers and the United States military\\'s further development of technologies like radar, advanced aviation, and rocketry developed during the war. New kinds of displays were needed to process the wealth of information resulting from such projects, leading to the development of computer graphics as a discipline.\\n\\n\\n=== 1950s ===\\n\\nEarly projects like the Whirlwind and SAGE Projects introduced the CRT as a viable display and interaction interface and introduced the light pen as an input device. Douglas T. Ross of the Whirlwind SAGE system performed a personal experiment in  which  he wrote a small program that captured the movement of his finger and displayed its vector (his traced name) on a display scope. One of the first interactive video games to feature recognizable, interactive graphics – Tennis for Two – was created for an oscilloscope by William Higinbotham to entertain visitors in 1958 at Brookhaven National Laboratory and simulated a tennis match. In 1959, Douglas T. Ross innovated again while working at MIT on transforming mathematic statements into computer generated 3D machine tool vectors by taking the opportunity to create a display scope image of a Disney cartoon character.Electronics pioneer Hewlett-Packard went public in 1957 after incorporating the decade prior, and established strong ties with Stanford University through its founders, who were alumni. This began the decades-long transformation of the southern San Francisco Bay Area into the world\\'s leading computer technology hub – now known as Silicon Valley. The field of computer graphics developed with the emergence of computer graphics hardware.\\nFurther advances in computing led to greater advancements in interactive computer graphics. In 1959, the TX-2 computer was developed at MIT\\'s Lincoln Laboratory. The TX-2 integrated a number of new man-machine interfaces. A light pen could be used to draw sketches on the computer using Ivan Sutherland\\'s revolutionary Sketchpad software. Using a light pen, Sketchpad allowed one to draw simple shapes on the computer screen, save them and even recall them later. The light pen itself had a small photoelectric cell in its tip. This cell emitted an electronic pulse whenever it was placed in front of a computer screen and the screen\\'s electron gun fired directly at it. By simply timing the electronic pulse with the current location of the electron gun, it was easy to pinpoint exactly where the pen was on the screen at any given moment. Once that was determined, the computer could then draw a cursor at that location. Sutherland seemed to find the perfect solution for many of the graphics problems he faced. Even today, many standards of computer graphics interfaces got their start with this early Sketchpad program. One example of this is in drawing constraints. If one wants to draw a square for example, they do not have to worry about drawing four lines perfectly to form the edges of the box. One can simply specify that they want to draw a box, and then specify the location and size of the box. The software will then construct a perfect box, with the right dimensions and at the right location. Another example is that Sutherland\\'s software modeled objects – not just a picture of objects. In other words, with a model of a car, one could change the size of the tires without affecting the rest of the car. It could stretch the body of car without deforming the tires.\\n\\n\\n=== 1960s ===\\n\\nThe phrase \"computer graphics\" itself was coined in 1960 by William Fetter, a graphic designer for Boeing. This old quote in many secondary sources comes complete with the following sentence:\\n\\nFetter has said that the terms were actually given to him by Verne Hudson of the Wichita Division of Boeing.In 1961 another student at MIT, Steve Russell, created another important title in the history of video games, Spacewar! Written for the DEC PDP-1, Spacewar was an instant success and copies started flowing to other PDP-1 owners and eventually DEC got a copy. The engineers at DEC used it as a diagnostic program on every new PDP-1 before shipping it. The sales force picked up on this quickly enough and when installing new units, would run the \"world\\'s first video game\" for their new customers. (Higginbotham\\'s Tennis For Two had beaten Spacewar by almost three years; but it was almost unknown outside of a research or academic setting.)\\nAt around the same time (1961-1962) in the University of Cambridge, Elizabeth Waldram wrote code to display radio-astronomy maps on a cathode ray tube.E. E. Zajac, a scientist at Bell Telephone Laboratory (BTL), created a film called \"Simulation of a two-giro gravity attitude control system\" in 1963. In this computer-generated film, Zajac showed how the attitude of a satellite could be altered as it orbits the Earth. He created the animation on an IBM 7090 mainframe computer. Also at BTL, Ken Knowlton, Frank Sinden, Ruth A. Weiss and Michael Noll started working in the computer graphics field. Sinden created a film called Force, Mass and Motion illustrating Newton\\'s laws of motion in operation. Around the same time, other scientists were creating computer graphics to illustrate their research. At Lawrence Radiation Laboratory, Nelson Max created the films Flow of a Viscous Fluid and Propagation of Shock Waves in a Solid Form. Boeing Aircraft created a film called Vibration of an Aircraft.\\nAlso sometime in the early 1960s, automobiles would also provide a boost through the early work of Pierre Bézier at Renault, who used Paul de Casteljau\\'s curves – now called Bézier curves after Bézier\\'s work in the field – to develop 3d modeling techniques for Renault car bodies. These curves would form the foundation for much curve-modeling work in the field, as curves – unlike polygons – are mathematically complex entities to draw and model well.\\n\\nIt was not long before major corporations started taking an interest in computer graphics. TRW, Lockheed-Georgia, General Electric and Sperry Rand are among the many companies that were getting started in computer graphics by the mid-1960s. IBM was quick to respond to this interest by releasing the IBM 2250 graphics terminal, the first commercially available graphics computer. Ralph Baer, a supervising engineer at Sanders Associates, came up with a home video game in 1966 that was later licensed to Magnavox and called the Odyssey. While very simplistic, and requiring fairly inexpensive electronic parts, it allowed the player to move points of light around on a screen. It was the first consumer computer graphics product. David C. Evans was director of engineering at Bendix Corporation\\'s computer division from 1953 to 1962, after which he worked for the next five years as a visiting professor at Berkeley. There he continued his interest in computers and how they interfaced with people. In 1966, the University of Utah recruited Evans to form a computer science program, and computer graphics quickly became his primary interest. This new department would become the world\\'s primary research center for computer graphics through the 1970s.\\nAlso, in 1966, Ivan Sutherland continued to innovate at MIT when he invented the first computer-controlled head-mounted display (HMD). It displayed two separate wireframe images, one for each eye. This allowed the viewer to see the computer scene in stereoscopic 3D. The heavy hardware required for supporting the display and tracker was called the Sword of Damocles because of the potential danger if it were to fall upon the wearer. After receiving his Ph.D. from MIT, Sutherland became Director of Information Processing at ARPA (Advanced Research Projects Agency), and later became a professor at Harvard. In 1967 Sutherland was recruited by Evans to join the computer science program at the University of Utah – a development which would turn that department into one of the most important research centers in graphics for nearly a decade thereafter, eventually producing some of the most important pioneers in the field. There Sutherland perfected his HMD; twenty years later, NASA would re-discover his techniques in their virtual reality research. At Utah, Sutherland and Evans were highly sought after consultants by large companies, but they were frustrated at the lack of graphics hardware available at the time, so they started formulating a plan to start their own company.\\nIn 1968, Dave Evans and Ivan Sutherland founded the first computer graphics hardware company, Evans & Sutherland. While Sutherland originally wanted the company to be located in Cambridge, Massachusetts, Salt Lake City was instead chosen due to its proximity to the professors\\' research group at the University of Utah.\\nAlso in 1968 Arthur Appel described the first ray casting algorithm, the first of a class of ray tracing-based rendering algorithms that have since become fundamental in achieving photorealism in graphics by modeling the paths that rays of light take from a light source, to surfaces in a scene, and into the camera.\\nIn 1969, the ACM initiated A Special Interest Group on Graphics (SIGGRAPH) which organizes conferences, graphics standards, and publications within the field of computer graphics. By 1973, the first annual SIGGRAPH conference was held, which has become one of the focuses of the organization. SIGGRAPH has grown in size and importance as the field of computer graphics has expanded over time.\\n\\n\\n=== 1970s ===\\nAn important technological advance that enabled practical computer graphics technology was the emergence of metal–oxide–semiconductor (MOS) large-scale integration (LSI) technology in the early 1970s. MOS LSI technology made possible large amounts of computational capability in small MOS integrated circuit chips, which led to the development of the Tektronix 4010 computer graphics terminal in 1972, as well as the microprocessor in 1971. MOS memory, particularly the dynamic random-access memory (DRAM) chip introduced in 1970, was also capable of holding kilobits of data on a single high-density memory chip, making it possible to hold an entire standard-definition (SD) raster graphics image in a digital frame buffer, which was used by Xerox PARC to develop SuperPaint, the first video-compatible, raster-based computer graphics system, in 1972.\\n\\nSubsequently, a number of breakthroughs in the field – particularly important early breakthroughs in the transformation of graphics from utilitarian to realistic – occurred at the University of Utah in the 1970s, which had hired Ivan Sutherland. He was paired with David C. Evans to teach an advanced computer graphics class, which contributed a great deal of founding research to the field and taught several students who would grow to found several of the industry\\'s most important companies – namely Pixar, Silicon Graphics, and Adobe Systems.  Tom Stockham led the image processing group at UU which worked closely with the computer graphics lab.\\nOne of these students was Edwin Catmull. Catmull had just come from The Boeing Company and had been working on his degree in physics. Growing up on Disney, Catmull loved animation yet quickly discovered that he did not have the talent for drawing. Now Catmull (along with many others) saw computers as the natural progression of animation and they wanted to be part of the revolution. The first computer animation that Catmull saw was his own. He created an animation of his hand opening and closing. He also pioneered texture mapping to paint textures on three-dimensional models in 1974, now considered one of the fundamental techniques in 3D modeling. It became one of his goals to produce a feature-length motion picture using computer graphics – a goal he would achieve two decades later after his founding role in Pixar. In the same class, Fred Parke created an animation of his wife\\'s face. The two animations were included in the 1976 feature film Futureworld.\\nAs the UU computer graphics laboratory was attracting people from all over, John Warnock was another of those early pioneers; he later founded Adobe Systems and create a revolution in the publishing world with his PostScript page description language, and Adobe would go on later to create the industry standard photo editing software in Adobe Photoshop and a prominent movie industry special effects program in Adobe After Effects.\\nJames Clark was also there; he later founded Silicon Graphics, a maker of advanced rendering systems that would dominate the field of high-end graphics until the early 1990s.\\nA major advance in 3D computer graphics was created at UU by these early pioneers – hidden surface determination. In order to draw a representation of a 3D object on the screen, the computer must determine which surfaces are \"behind\" the object from the viewer\\'s perspective, and thus should be \"hidden\" when the computer creates (or renders) the image. The 3D Core Graphics System (or Core) was the first graphical standard to be developed. A group of 25 experts of the ACM Special Interest Group SIGGRAPH developed this \"conceptual framework\". The specifications were published in 1977, and it became a foundation for many future developments in the field.\\nAlso in the 1970s, Henri Gouraud, Jim Blinn and Bui Tuong Phong contributed to the foundations of shading in CGI via the development of the Gouraud shading and Blinn–Phong shading models, allowing graphics to move beyond a \"flat\" look to a look more accurately portraying depth. Jim Blinn also innovated further in 1978 by introducing bump mapping, a technique for simulating uneven surfaces, and the predecessor to many more advanced kinds of mapping used today.\\nThe modern videogame arcade as is known today was birthed in the 1970s, with the first arcade games using real-time 2D sprite graphics. Pong in 1972 was one of the first hit arcade cabinet games. Speed Race in 1974 featured sprites moving along a vertically scrolling road. Gun Fight in 1975 featured human-looking animated characters, while Space Invaders in 1978 featured a large number of animated figures on screen; both used a specialized barrel shifter circuit made from discrete chips to help their Intel 8080 microprocessor animate their framebuffer graphics.\\n\\n\\n=== 1980s ===\\n\\nThe 1980s began to see the modernization and commercialization of computer graphics. As the home computer proliferated, a subject which had previously been an academics-only discipline was adopted by a much larger audience, and the number of computer graphics developers increased significantly.\\nIn the early 1980s, metal–oxide–semiconductor (MOS) very-large-scale integration (VLSI) technology led to the availability of 16-bit central processing unit (CPU) microprocessors and the first graphics processing unit (GPU) chips, which began to revolutionize computer graphics, enabling high-resolution graphics for computer graphics terminals as well as personal computer (PC) systems. NEC\\'s µPD7220 was the first GPU, fabricated on a fully integrated NMOS VLSI chip. It supported up to 1024x1024 resolution, and laid the foundations for the emerging PC graphics market. It was used in a number of graphics cards, and was licensed for clones such as the Intel 82720, the first of Intel\\'s graphics processing units. MOS memory also became cheaper in the early 1980s, enabling the development of affordable framebuffer memory, notably video RAM (VRAM) introduced by Texas Instruments (TI) in the mid-1980s. In 1984, Hitachi released the ARTC HD63484, the first complementary MOS (CMOS) GPU. It was capable of displaying high-resolution in color mode and up to 4K resolution in monochrome mode, and it was used in a number of graphics cards and terminals during the late 1980s. In 1986, TI introduced the TMS34010, the first fully programmable MOS graphics processor.Computer graphics terminals during this decade became increasingly intelligent, semi-standalone and standalone workstations. Graphics and application processing were increasingly migrated to the intelligence in the workstation, rather than continuing to rely on central mainframe and mini-computers. Typical of the early move to high-resolution computer graphics intelligent workstations for the computer-aided engineering market were the Orca 1000, 2000 and 3000 workstations, developed by Orcatech of Ottawa, a spin-off from Bell-Northern Research, and led by David Pearson, an early workstation pioneer. The Orca 3000 was based on the 16-bit Motorola 68000 microprocessor and AMD bit-slice processors, and had Unix as its operating system. It was targeted squarely at the sophisticated end of the design engineering sector. Artists and graphic designers began to see the personal computer, particularly the Commodore Amiga and Macintosh, as a serious design tool, one that could save time and draw more accurately than other methods.  The Macintosh remains a highly popular tool for computer graphics among graphic design studios and businesses. Modern computers, dating from the 1980s, often use graphical user interfaces (GUI) to present data and information with symbols, icons and pictures, rather than text. Graphics are one of the five key elements of multimedia technology.\\nIn the field of realistic rendering, Japan\\'s Osaka University developed the LINKS-1 Computer Graphics System, a supercomputer that used up to 257 Zilog Z8001 microprocessors, in 1982, for the purpose of rendering realistic 3D computer graphics. According to the Information Processing Society of Japan: \"The core of 3D image rendering is calculating the luminance of each pixel making up a rendered surface from the given viewpoint, light source, and object position. The LINKS-1 system was developed to realize an image rendering methodology in which each pixel could be parallel processed independently using ray tracing. By developing a new software methodology specifically for high-speed image rendering, LINKS-1 was able to rapidly render highly realistic images. It was used to create the world\\'s first 3D planetarium-like video of the entire heavens that was made completely with computer graphics. The video was presented at the Fujitsu pavilion at the 1985 International Exposition in Tsukuba.\" The LINKS-1 was the world\\'s most powerful computer, as of 1984. Also in the field of realistic rendering, the general rendering equation of David Immel and James Kajiya was developed in 1986 – an important step towards implementing global illumination, which is necessary to pursue photorealism in computer graphics.\\nThe continuing popularity of Star Wars and other science fiction franchises were relevant in cinematic CGI at this time, as Lucasfilm and Industrial Light & Magic became known as the \"go-to\" house by many other studios for topnotch computer graphics in film. Important advances in chroma keying (\"bluescreening\", etc.) were made for the later films of the original trilogy. Two other pieces of video would also outlast the era as historically relevant: Dire Straits\\' iconic, near-fully-CGI video for their song \"Money for Nothing\" in 1985, which popularized CGI among music fans of that era, and a scene from Young Sherlock Holmes the same year featuring the first fully CGI character in a feature movie (an animated stained-glass knight). In 1988, the first shaders – small programs designed specifically to do shading as a separate algorithm – were developed by Pixar, which had already spun off from Industrial Light & Magic as a separate entity – though the public would not see the results of such technological progress until the next decade. In the late 1980s, Silicon Graphics (SGI) computers were used to create some of the first fully computer-generated short films at Pixar, and Silicon Graphics machines were considered a high-water mark for the field during the decade.\\nThe 1980s is also called the golden era of videogames; millions-selling systems from Atari, Nintendo and Sega, among other companies, exposed computer graphics for the first time to a new, young, and impressionable audience – as did MS-DOS-based personal computers, Apple IIs, Macs, and Amigas, all of which also allowed users to program their own games if skilled enough. For the arcades, advances were made in commercial, real-time 3D graphics. In 1988, the first dedicated real-time 3D graphics boards were introduced for arcades, with the Namco System 21 and Taito Air System. On the professional side, Evans & Sutherland and SGI developed 3D raster graphics hardware that directly influenced the later single-chip graphics processing unit (GPU), a technology where a separate and very powerful chip is used in parallel processing with a CPU to optimize graphics.\\nThe decade also saw computer graphics applied to many additional professional markets, including location-based entertainment and education with the E&S Digistar, vehicle design, vehicle simulation, and chemistry.\\n\\n\\n=== 1990s ===\\n\\nThe 1990s\\' overwhelming note was the emergence of 3D modeling on a mass scale and an impressive rise in the quality of CGI generally. Home computers became able to take on rendering tasks that previously had been limited to workstations costing thousands of dollars; as 3D modelers became available for home systems, the popularity of Silicon Graphics workstations declined and powerful Microsoft Windows and Apple Macintosh machines running Autodesk products like 3D Studio or other home rendering software ascended in importance. By the end of the decade, the GPU would begin its rise to the prominence it still enjoys today.\\nThe field began to see the first rendered graphics that could truly pass as photorealistic to the untrained eye (though they could not yet do so with a trained CGI artist) and 3D graphics became far more popular in gaming, multimedia, and animation. At the end of the 1980s and the beginning of the nineties were created, in France, the very first computer graphics TV series: La Vie des bêtes by studio Mac Guff Ligne (1988), Les Fables Géométriques (1989–1991) by studio Fantôme, and Quarxs, the first HDTV computer graphics series by Maurice Benayoun and François Schuiten (studio Z-A production, 1990–1993).\\nIn film, Pixar began its serious commercial rise in this era under Edwin Catmull, with its first major film release, in 1995 – Toy Story – a critical and commercial success of nine-figure magnitude. The studio to invent the programmable shader would go on to have many animated hits, and its work on prerendered video animation is still considered an industry leader and research trail breaker.\\nIn video games, in 1992, Virtua Racing, running on the Sega Model 1 arcade system board, laid the foundations for fully 3D racing games and popularized real-time 3D polygonal graphics among a wider audience in the video game industry. The Sega Model 2 in 1993 and Sega Model 3 in 1996 subsequently pushed the boundaries of commercial, real-time 3D graphics. Back on the PC, Wolfenstein 3D, Doom and Quake, three of the first massively popular 3D first-person shooter games, were released by id Software to critical and popular acclaim during this decade using a rendering engine innovated primarily by John Carmack. The Sony PlayStation, Sega Saturn, and Nintendo 64, among other consoles, sold in the millions and popularized 3D graphics for home gamers. Certain late-1990s first-generation 3D titles became seen as influential in popularizing 3D graphics among console users, such as platform games Super Mario 64 and The Legend Of Zelda: Ocarina Of Time, and early 3D fighting games like Virtua Fighter, Battle Arena Toshinden, and Tekken.\\nTechnology and algorithms for rendering continued to improve greatly. In 1996, Krishnamurty and Levoy invented normal mapping – an improvement on Jim Blinn\\'s bump mapping. 1999 saw Nvidia release the seminal GeForce 256, the first home video card billed as a graphics processing unit or GPU, which in its own words contained \"integrated transform, lighting, triangle setup/clipping, and rendering engines\". By the end of the decade, computers adopted common frameworks for graphics processing such as DirectX and OpenGL. Since then, computer graphics have only become more detailed and realistic, due to more powerful graphics hardware and 3D modeling software.   AMD also became a leading developer of graphics boards in this decade, creating a \"duopoly\" in the field which exists this day.\\n\\n\\n=== 2000s ===\\n\\nCGI became ubiquitous in earnest during this era. Video games and CGI cinema had spread the reach of computer graphics to the mainstream by the late 1990s and continued to do so at an accelerated pace in the 2000s. CGI was also adopted en masse for television advertisements widely in the late 1990s and 2000s, and so became familiar to a massive audience.\\nThe continued rise and increasing sophistication of the graphics processing unit were crucial to this decade, and 3D rendering capabilities became a standard feature as 3D-graphics GPUs became considered a necessity for desktop computer makers to offer. The Nvidia GeForce line of graphics cards dominated the market in the early decade with occasional significant competing presence from ATI. As the decade progressed, even low-end machines usually contained a 3D-capable GPU of some kind as Nvidia and AMD both introduced low-priced chipsets and continued to dominate the market. Shaders which had been introduced in the 1980s to perform specialized processing on the GPU would by the end of the decade become supported on most consumer hardware, speeding up graphics considerably and allowing for greatly improved texture and shading in computer graphics via the widespread adoption of normal mapping, bump mapping, and a variety of other techniques allowing the simulation of a great amount of detail.\\nComputer graphics used in films and video games gradually began to be realistic to the point of entering the uncanny valley. CGI movies proliferated, with traditional animated cartoon films like Ice Age and Madagascar as well as numerous Pixar offerings like Finding Nemo dominating the box office in this field. The Final Fantasy: The Spirits Within, released in 2001, was the first fully computer-generated feature film to use photorealistic CGI characters and be fully made with motion capture. The film was not a box-office success, however. Some commentators have suggested this may be partly because the lead CGI characters had facial features which fell into the \"uncanny valley\". Other animated films like The Polar Express drew attention at this time as well. Star Wars also resurfaced with its prequel trilogy and the effects continued to set a bar for CGI in film.\\nIn videogames, the Sony PlayStation 2 and 3, the Microsoft Xbox line of consoles, and offerings from Nintendo such as the GameCube maintained a large following, as did the Windows PC. Marquee CGI-heavy titles like the series of Grand Theft Auto, Assassin\\'s Creed, Final Fantasy, BioShock, Kingdom Hearts, Mirror\\'s Edge and dozens of others continued to approach photorealism, grow the video game industry and impress, until that industry\\'s revenues became comparable to those of movies. Microsoft made a decision to expose DirectX more easily to the independent developer world with the XNA program, but it was not a success. DirectX itself remained a commercial success, however. OpenGL continued to mature as well, and it and DirectX improved greatly; the second-generation shader languages HLSL and GLSL began to be popular in this decade.\\nIn scientific computing, the GPGPU technique to pass large amounts of data bidirectionally between a GPU and CPU was invented; speeding up analysis on many kinds of bioinformatics and molecular biology experiments. The technique has also been used for Bitcoin mining and has applications in computer vision.\\n\\n\\n=== 2010s ===\\n\\nIn the 2010s, CGI has been nearly ubiquitous in video, pre-rendered graphics are nearly scientifically photorealistic, and real-time graphics on a suitably high-end system may simulate photorealism to the untrained eye.\\nTexture mapping has matured into a multistage process with many layers; generally, it is not uncommon to implement texture mapping, bump mapping or isosurfaces or normal mapping, lighting maps including specular highlights and reflection techniques, and shadow volumes into one rendering engine using shaders, which are maturing considerably. Shaders are now very nearly a necessity for advanced work in the field, providing considerable complexity in manipulating pixels, vertices, and textures on a per-element basis, and countless possible effects. Their shader languages HLSL and GLSL are active fields of research and development. Physically based rendering or PBR, which implements many maps and performs advanced calculation to simulate real optic light flow, is an active research area as well, along with advanced areas like  ambient occlusion, subsurface scattering, Rayleigh scattering, photon mapping, and many others. Experiments into the processing power required to provide graphics in real time at ultra-high-resolution modes like 4K Ultra HD are beginning, though beyond reach of all but the highest-end hardware.\\nIn cinema, most animated movies are CGI now; a great many animated CGI films are made per year, but few, if any, attempt photorealism due to continuing fears of the uncanny valley. Most are 3D cartoons.\\nIn videogames, the Microsoft Xbox One, Sony PlayStation 4, and Nintendo Switch currently dominate the home space and are all capable of highly advanced 3D graphics; the Windows PC is still one of the most active gaming platforms as well.\\n\\n\\n== Image types ==\\n\\n\\n=== Two-dimensional ===\\n\\n2D computer graphics are the computer-based generation of digital images—mostly from models, such as digital image, and by techniques specific to them.\\n2D computer graphics are mainly used in applications that were originally developed upon traditional printing and drawing technologies such as typography. In those applications, the two-dimensional image is not just a representation of a real-world object, but an independent artifact with added semantic value; two-dimensional models are therefore preferred because they give more direct control of the image than 3D computer graphics, whose approach is more akin to photography than to typography.\\n\\n\\n==== Pixel art ====\\n\\nA large form of digital art, pixel art is created through the use of raster graphics software, where images are edited on the pixel level. Graphics in most old (or relatively limited) computer and video games, graphing calculator games, and many mobile phone games are mostly pixel art.\\n\\n\\n==== Sprite graphics ====\\n\\nA sprite is a two-dimensional image or animation that is integrated into a larger scene. Initially including just graphical objects handled separately from the memory bitmap of a video display, this now includes various manners of graphical overlays.\\nOriginally, sprites were a method of integrating unrelated bitmaps so that they appeared to be part of the normal bitmap on a screen, such as creating an animated character that can be moved on a screen without altering the data defining the overall screen. Such sprites can be created by either electronic circuitry or software. In circuitry, a hardware sprite is a hardware construct that employs custom DMA channels to integrate visual elements with the main screen in that it super-imposes two discrete video sources. Software can simulate this through specialized rendering methods.\\n\\n\\n==== Vector graphics ====\\n\\nVector graphics formats are complementary to raster graphics. Raster graphics is the representation of images as an array of pixels and is typically used for the representation of photographic images. Vector graphics consists in encoding information about shapes and colors that comprise the image, which can allow for more flexibility in rendering. There are instances when working with vector tools and formats is best practice, and instances when working with raster tools and formats is best practice. There are times when both formats come together. An understanding of the advantages and limitations of each technology and the relationship between them is most likely to result in efficient and effective use of tools.\\n\\n\\n=== Three-dimensional ===\\n\\n3D graphics, compared to 2D graphics, are graphics that use a three-dimensional representation of geometric data. For the purpose of performance, this is stored in the computer. This includes images that may be for later display or for real-time viewing.\\nDespite these differences, 3D computer graphics rely on similar algorithms as 2D computer graphics do in the frame and raster graphics (like in 2D) in the final rendered display. In computer graphics software, the distinction between 2D and 3D is occasionally blurred; 2D applications may use 3D techniques to achieve effects such as lighting, and primarily 3D may use 2D rendering techniques.\\n3D computer graphics are the same as 3D models. The model is contained within the graphical data file, apart from the rendering. However, there are differences that include the 3D model is the representation of any 3D object. Until visually displayed a model is not graphic. Due to printing, 3D models are not only confined to virtual space. 3D rendering is how a model can be displayed. Also can be used in non-graphical computer simulations and calculations.\\n\\n\\n=== Computer animation ===\\n\\nComputer animation is the art of creating moving images via the use of computers. It is a subfield of computer graphics and animation. Increasingly it is created by means of 3D computer graphics, though 2D computer graphics are still widely used for stylistic, low bandwidth, and faster real-time rendering needs. Sometimes the target of the animation is the computer itself, but sometimes the target is another medium, such as film. It is also referred to as CGI (Computer-generated imagery or computer-generated imaging), especially when used in films.\\nVirtual entities may contain and be controlled by assorted attributes, such as transform values (location, orientation, and scale) stored in an object\\'s transformation matrix. Animation is the change of an attribute over time. Multiple methods of achieving animation exist; the rudimentary form is based on the creation and editing of keyframes, each storing a value at a given time, per attribute to be animated. The 2D/3D graphics software will change with each keyframe, creating an editable curve of a value mapped over time, in which results in animation. Other methods of animation include procedural and expression-based techniques: the former consolidates related elements of animated entities into sets of attributes, useful for creating particle effects and crowd simulations; the latter allows an evaluated result returned from a user-defined logical expression, coupled with mathematics, to automate animation in a predictable way (convenient for controlling bone behavior beyond what a hierarchy offers in skeletal system set up).\\nTo create the illusion of movement, an image is displayed on the computer screen then quickly replaced by a new image that is similar to the previous image, but shifted slightly. This technique is identical to the illusion of movement in television and motion pictures.\\n\\n\\n== Concepts and principles ==\\nImages are typically created by devices such as cameras, mirrors, lenses, telescopes, microscopes, etc.\\nDigital images include both vector images and raster images, but raster images are more commonly used.\\n\\n\\n=== Pixel ===\\n\\nIn digital imaging, a pixel (or picture element) is a single point in a raster image. Pixels are placed on a regular 2-dimensional grid, and are often represented using dots or squares. Each pixel is a sample of an original image, where more samples typically provide a more accurate representation of the original. The intensity of each pixel is variable; in color systems, each pixel has typically three\\ncomponents such as red, green, and blue.\\nGraphics are visual presentations on a surface, such as a computer screen. Examples are photographs, drawing, graphics designs, maps, engineering drawings, or other images. Graphics often combine text and illustration. Graphic design may consist of the deliberate selection, creation, or arrangement of typography alone, as in a brochure, flier, poster, web site, or book without any other element. Clarity or effective communication may be the objective, association with other cultural elements may be sought, or merely, the creation of a distinctive style.\\n\\n\\n=== Primitives ===\\nPrimitives are basic units which a graphics system may combine to create more complex images or models. Examples would be sprites and character maps in 2D video games, geometric primitives in CAD, or polygons or triangles in 3D rendering. Primitives may be supported in hardware for efficient rendering, or the building blocks provided by a graphics application.\\n\\n\\n=== Rendering ===\\nRendering is the generation of a 2D image from a 3D model by means of computer programs. A scene file contains objects in a strictly defined language or data structure; it would contain geometry, viewpoint, texture, lighting, and shading information as a description of the virtual scene. The data contained in the scene file is then passed to a rendering program to be processed and output to a digital image or raster graphics image file. The rendering program is usually built into the computer graphics software, though others are available as plug-ins or entirely separate programs. The term \"rendering\" may be by analogy with an \"artist\\'s rendering\" of a scene. Although the technical details of rendering methods vary, the general challenges to overcome in producing a 2D image from a 3D representation stored in a scene file are outlined as the graphics pipeline along a rendering device, such as a GPU. A GPU is a device able to assist the CPU in calculations. If a scene is to look relatively realistic and predictable under virtual lighting, the rendering software should solve the rendering equation. The rendering equation does not account for all lighting phenomena, but is a general lighting model for computer-generated imagery. \\'Rendering\\' is also used to describe the process of calculating effects in a video editing file to produce final video output.\\n\\n3D projection\\n3D projection is a method of mapping three dimensional points to a two dimensional plane. As most current methods for displaying graphical data are based on planar two dimensional media, the use of this type of projection is widespread. This method is used in most real-time 3D applications and typically uses rasterization to produce the final image.Ray tracing\\nRay tracing is a technique from the family of image order algorithms for generating an image by tracing the path of light through pixels in an image plane. The technique is capable of producing a high degree of photorealism; usually higher than that of typical scanline rendering methods, but at a greater computational cost.Shading\\nShading refers to depicting depth in 3D models or illustrations by varying levels of darkness. It is a process used in drawing for depicting levels of darkness on paper by applying media more densely or with a darker shade for darker areas, and less densely or with a lighter shade for lighter areas. There are various techniques of shading including cross hatching where perpendicular lines of varying closeness are drawn in a grid pattern to shade an area. The closer the lines are together, the darker the area appears. Likewise, the farther apart the lines are, the lighter the area appears. The term has been recently generalized to mean that shaders are applied.Texture mapping\\nTexture mapping is a method for adding detail, surface texture, or colour to a computer-generated graphic or 3D model. Its application to 3D graphics was pioneered by Dr Edwin Catmull in 1974. A texture map is applied (mapped) to the surface of a shape, or polygon. This process is akin to applying patterned paper to a plain white box. Multitexturing is the use of more than one texture at a time on a polygon. Procedural textures (created from adjusting parameters of an underlying algorithm that produces an output texture), and bitmap textures (created in an image editing application or imported from a digital camera) are, generally speaking, common methods of implementing texture definition on 3D models in computer graphics software, while intended placement of textures onto a model\\'s surface often requires a technique known as UV mapping (arbitrary, manual layout of texture coordinates) for polygon surfaces, while non-uniform rational B-spline (NURB) surfaces have their own intrinsic parameterization used as texture coordinates. Texture mapping as a discipline also encompasses techniques for creating normal maps and bump maps that correspond to a texture to simulate height and specular maps to help simulate shine and light reflections, as well as environment mapping to simulate mirror-like reflectivity, also called gloss.Anti-aliasing\\nRendering resolution-independent entities (such as 3D models) for viewing on a raster (pixel-based) device such as a liquid-crystal display or CRT television inevitably causes aliasing artifacts mostly along geometric edges and the boundaries of texture details; these artifacts are informally called \"jaggies\". Anti-aliasing methods rectify such problems, resulting in imagery more pleasing to the viewer, but can be somewhat computationally expensive. Various anti-aliasing algorithms (such as supersampling) are able to be employed, then customized for the most efficient rendering performance versus quality of the resultant imagery; a graphics artist should consider this trade-off if anti-aliasing methods are to be used. A pre-anti-aliased bitmap texture being displayed on a screen (or screen location) at a resolution different than the resolution of the texture itself (such as a textured model in the distance from the virtual camera) will exhibit aliasing artifacts, while any procedurally defined texture will always show aliasing artifacts as they are resolution-independent; techniques such as mipmapping and texture filtering help to solve texture-related aliasing problems.\\n\\n\\n=== Volume rendering ===\\n\\nVolume rendering is a technique used to display a 2D projection of a 3D discretely sampled data set. A typical 3D data set is a group of 2D slice images acquired by a CT or MRI scanner.\\nUsually these are acquired in a regular pattern (e.g., one slice every millimeter) and usually have a regular number of image pixels in a regular pattern. This is an example of a regular volumetric grid, with each volume element, or voxel represented by a single value that is obtained by sampling the immediate area surrounding the voxel.\\n\\n\\n=== 3D modeling ===\\n\\n3D modeling is the process of developing a mathematical, wireframe representation of any three-dimensional object, called a \"3D model\", via specialized software. Models may be created automatically or manually; the manual modeling process of preparing geometric data for 3D computer graphics is similar to plastic arts such as sculpting. 3D models may be created using multiple approaches: use of NURBs to generate accurate and smooth surface patches, polygonal mesh modeling (manipulation of faceted geometry), or polygonal mesh subdivision (advanced tessellation of polygons, resulting in smooth surfaces similar to NURB models). A 3D model can be displayed as a two-dimensional image through a process called 3D rendering, used in a computer simulation of physical phenomena, or animated directly for other purposes. The model can also be physically created using 3D Printing devices.\\n\\n\\n== Pioneers in computer graphics ==\\nCharles Csuri\\nCharles Csuri is a pioneer in computer animation and digital fine art and created the first computer art in 1964. Csuri was recognized by Smithsonian as the father of digital art and computer animation, and as a pioneer of computer animation by the Museum of Modern Art (MoMA) and Association for Computing Machinery-SIGGRAPH.Donald P. Greenberg\\nDonald P. Greenberg is a leading innovator in computer graphics. Greenberg has authored hundreds of articles and served as a teacher and mentor to many prominent computer graphic artists, animators, and researchers such as Robert L. Cook, Marc Levoy, Brian A. Barsky, and Wayne Lytle. Many of his former students have won Academy Awards for technical achievements and several have won the SIGGRAPH Achievement Award. Greenberg was the founding director of the NSF Center for Computer Graphics and Scientific Visualization.A. Michael Noll\\nNoll was one of the first researchers to use a digital computer to create artistic patterns and to formalize the use of random processes in the creation of visual arts. He began creating digital art in 1962, making him one of the earliest digital artists. In 1965, Noll along with Frieder Nake and Georg Nees were the first to publicly exhibit their computer art. During April 1965, the Howard Wise Gallery exhibited Noll\\'s computer art along with random-dot patterns by Bela Julesz.\\n\\n\\n=== Other pioneers ===\\n\\nPierre Bézier\\nJim Blinn\\nJack Bresenham\\nJohn Carmack\\nPaul de Casteljau\\nEd Catmull\\nFrank Crow\\nJames D. Foley\\nWilliam Fetter\\nHenry Fuchs\\nHenri Gouraud\\nCharles Loop\\nNadia Magnenat Thalmann\\nBenoit Mandelbrot\\nMartin Newell\\nFred Parke\\nBui Tuong Phong\\nDavid Pearson\\nSteve Russell\\nDaniel J. Sandin\\nAlvy Ray Smith\\nBob Sproull\\nIvan Sutherland\\nDaniel Thalmann\\nAndries van Dam\\nJohn Warnock\\nJ. Turner Whitted\\nLance Williams\\nJim Kajiya\\n\\n\\n=== Organizations ===\\nSIGGRAPH\\nGDC\\nBell Telephone Laboratories\\nUnited States Armed Forces, particularly the Whirlwind computer and SAGE Project\\nBoeing\\nIBM\\nRenault\\nThe computer science department of the University of Utah\\nLucasfilm and Industrial Light & Magic\\nAutodesk\\nAdobe Systems\\nPixar\\nSilicon Graphics, Khronos Group & OpenGL\\nThe DirectX division at Microsoft\\nNvidia\\nAMD\\n\\n\\n== Study of computer graphics ==\\n\\nThe study of computer graphics is a sub-field of computer science which studies methods for digitally synthesizing and manipulating visual content. Although the term often refers to three-dimensional computer graphics, it also encompasses two-dimensional graphics and image processing.\\nAs an academic discipline, computer graphics studies the manipulation of visual and geometric information using computational techniques. It focuses on the mathematical and computational foundations of image generation and processing rather than purely aesthetic issues. Computer graphics is often differentiated from the field of visualization, although the two fields have many similarities.\\n\\n\\n== Applications ==\\nComputer graphics may be used in the following areas:\\n\\nComputational biology\\nComputational photography\\nComputational physics\\nComputer-aided design\\nComputer simulation\\nDesign\\nDigital art\\nEducation\\nGraphic design\\nInfographics\\nInformation visualization\\nRational drug design\\nScientific visualization\\nSpecial Effects for cinema\\nVideo Games\\nVirtual reality\\nWeb design\\n\\n\\n== See also ==\\nComputer representation of surfaces\\nGlossary of computer graphics\\n\\n\\n== Notes ==\\n\\n\\n== References ==\\n\\n\\n== Further reading ==\\nL. Ammeraal and K. Zhang (2007). Computer Graphics for Java Programmers, Second Edition, John-Wiley & Sons, ISBN 978-0-470-03160-5.\\nDavid Rogers (1998). Procedural Elements for Computer Graphics. McGraw-Hill.\\nJames D. Foley, Andries Van Dam, Steven K. Feiner and John F. Hughes (1995). Computer Graphics: Principles and Practice. Addison-Wesley.\\nDonald Hearn and M. Pauline Baker (1994). Computer Graphics. Prentice-Hall.\\nFrancis S. Hill (2001). Computer Graphics. Prentice Hall.\\nJohn Lewell (1985). Computer Graphics: A Survey of Current Techniques and Applications. Van Nostrand Reinhold.\\nJeffrey J. McConnell (2006). Computer Graphics: Theory Into Practice. Jones & Bartlett Publishers.\\nR. D. Parslow, R. W. Prowse, Richard Elliot Green (1969). Computer Graphics: Techniques and Applications.\\nPeter Shirley and others. (2005). Fundamentals of computer graphics. A.K. Peters, Ltd.\\nM. Slater, A. Steed, Y. Chrysantho (2002). Computer graphics and virtual environments: from realism to real-time. Addison-Wesley.\\nWolfgang Höhl (2008): Interactive environments with open-source software, Springer Wien New York, ISBN 3-211-79169-8\\n\\n\\n== External links ==\\nA Critical History of Computer Graphics and Animation\\nHistory of Computer Graphics series of articles\\nComputer Graphics research at UC Berkeley\\nThomas Dreher: History of Computer Art, chap. IV.2 Computer Animation\\nHistory of Computer Graphics on RUS', \"Digital image processing is the use of a digital computer to process digital images through an algorithm. As a subcategory or field of digital signal processing, digital image processing has many advantages over analog image processing. It allows a much wider range of algorithms to be applied to the input data and can avoid problems such as the build-up of noise and distortion during processing. Since images are defined over two dimensions (perhaps more) digital image processing may be modeled in the form of multidimensional systems. The generation and development of digital image processing are mainly affected by three factors: first, the development of computers; second, the development of mathematics (especially the creation and improvement of discrete mathematics theory); third, the demand for a wide range of applications in environment, agriculture, military, industry and medical science has increased.\\n\\n\\n== History ==\\n\\nMany of the techniques of digital image processing, or digital picture processing as it often was called, were developed in the 1960s, at Bell Laboratories, the Jet Propulsion Laboratory, Massachusetts Institute of Technology, University of Maryland, and a few other research facilities, with application to satellite imagery, wire-photo standards conversion, medical imaging, videophone, character recognition, and photograph enhancement. The purpose of early image processing was to improve the quality of the image. It was aimed for human beings to improve the visual effect of people. In image processing, the input is a low-quality image, and the output is an image with improved quality. Common image processing include image enhancement, restoration, encoding, and compression. The first successful application was the American Jet Propulsion Laboratory (JPL). They used image processing techniques such as geometric correction, gradation transformation, noise removal, etc. on the thousands of lunar photos sent back by the Space Detector Ranger 7 in 1964, taking into account the position of the sun and the environment of the moon. The impact of the successful mapping of the moon's surface map by the computer has been a huge success. Later, more complex image processing was performed on the nearly 100,000 photos sent back by the spacecraft, so that the topographic map, color map and panoramic mosaic of the moon were obtained, which achieved extraordinary results and laid a solid foundation for human landing on the moon.The cost of processing was fairly high, however, with the computing equipment of that era. That changed in the 1970s, when digital image processing proliferated as cheaper computers and dedicated hardware became available. This led to images being processed in real-time, for some dedicated problems such as television standards conversion. As general-purpose computers became faster, they started to take over the role of dedicated hardware for all but the most specialized and computer-intensive operations. With the fast computers and signal processors available in the 2000s, digital image processing has become the most common form of image processing, and is generally used because it is not only the most versatile method, but also the cheapest.\\n\\n\\n=== Image sensors ===\\n\\nThe basis for modern image sensors is metal-oxide-semiconductor (MOS) technology, which originates from the invention of the MOSFET (MOS field-effect transistor) by Mohamed M. Atalla and Dawon Kahng at Bell Labs in 1959. This led to the development of digital semiconductor image sensors, including the charge-coupled device (CCD) and later the CMOS sensor.The charge-coupled device was invented by Willard S. Boyle and George E. Smith at Bell Labs in 1969. While researching MOS technology, they realized that an electric charge was the analogy of the magnetic bubble and that it could be stored on a tiny MOS capacitor. As it was fairly straightforward to fabricate a series of MOS capacitors in a row, they connected a suitable voltage to them so that the charge could be stepped along from one to the next. The CCD is a semiconductor circuit that was later used in the first digital video cameras for television broadcasting.The NMOS active-pixel sensor (APS) was invented by Olympus in Japan during the mid-1980s. This was enabled by advances in MOS semiconductor device fabrication, with MOSFET scaling reaching smaller micron and then sub-micron levels. The NMOS APS was fabricated by Tsutomu Nakamura's team at Olympus in 1985. The CMOS active-pixel sensor (CMOS sensor) was later developed by Eric Fossum's team at the NASA Jet Propulsion Laboratory in 1993. By 2007, sales of CMOS sensors had surpassed CCD sensors.\\n\\n\\n=== Image compression ===\\n\\nAn important development in digital image compression technology was the discrete cosine transform (DCT), a lossy compression technique first proposed by Nasir Ahmed in 1972. DCT compression became the basis for JPEG, which was introduced by the Joint Photographic Experts Group in 1992. JPEG compresses images down to much smaller file sizes, and has become the most widely used image file format on the Internet. Its highly efficient DCT compression algorithm was largely responsible for the wide proliferation of digital images and digital photos, with several billion JPEG images produced every day as of 2015.\\n\\n\\n=== Digital signal processor (DSP) ===\\n\\nElectronic signal processing was revolutionized by the wide adoption of MOS technology in the 1970s. MOS integrated circuit technology was the basis for the first single-chip microprocessors and microcontrollers in the early 1970s, and then the first single-chip digital signal processor (DSP) chips in the late 1970s. DSP chips have since been widely used in digital image processing.The discrete cosine transform (DCT) image compression algorithm has been widely implemented in DSP chips, with many companies developing DSP chips based on DCT technology. DCTs are widely used for encoding, decoding, video coding, audio coding, multiplexing, control signals, signaling, analog-to-digital conversion, formatting luminance and color differences, and color formats such as YUV444 and YUV411. DCTs are also used for encoding operations such as motion estimation, motion compensation, inter-frame prediction, quantization, perceptual weighting, entropy encoding, variable encoding, and motion vectors, and decoding operations such as the inverse operation between different color formats (YIQ, YUV and RGB) for display purposes. DCTs are also commonly used for high-definition television (HDTV) encoder/decoder chips.\\n\\n\\n=== Medical imaging ===\\n\\nIn 1972, the engineer from British company EMI Housfield invented the X-ray computed tomography device for head diagnosis, which is what is usually called CT (computer tomography). The CT nucleus method is based on the projection of the human head section and is processed by computer to reconstruct the cross-sectional image, which is called image reconstruction. In 1975, EMI successfully developed a CT device for the whole body, which obtained a clear tomographic image of various parts of the human body. In 1979, this diagnostic technique won the Nobel Prize. Digital image processing technology for medical applications was inducted into the Space Foundation Space Technology Hall of Fame in 1994.\\n\\n\\n== Tasks ==\\nDigital image processing allows the use of much more complex algorithms, and hence, can offer both more sophisticated performance at simple tasks, and the implementation of methods which would be impossible by analogue means.\\nIn particular, digital image processing is a concrete application of, and a practical technology based on:\\n\\nClassification\\nFeature extraction\\nMulti-scale signal analysis\\nPattern recognition\\nProjectionSome techniques which are used in digital image processing include:\\n\\nAnisotropic diffusion\\nHidden Markov models\\nImage editing\\nImage restoration\\nIndependent component analysis\\nLinear filtering\\nNeural networks\\nPartial differential equations\\nPixelation\\nPoint feature matching\\nPrincipal components analysis\\nSelf-organizing maps\\nWavelets\\n\\n\\n== Digital image transformations ==\\n\\n\\n=== Filtering ===\\nDigital filters are used to blur and sharpen digital images. Filtering can be performed by:\\n\\nconvolution with specifically designed kernels (filter array) in the spatial domain\\nmasking specific frequency regions in the frequency (Fourier) domainThe following examples show both methods:\\n\\n\\n==== Image padding in Fourier domain filtering ====\\nImages are typically padded before being transformed to the Fourier space, the highpass filtered images below illustrate the consequences of different padding techniques:\\n\\nNotice that the highpass filter shows extra edges when zero padded compared to the repeated edge padding.\\n\\n\\n==== Filtering code examples ====\\nMATLAB example for spatial domain highpass filtering.\\n\\n\\n=== Affine transformations ===\\nAffine transformations enable basic image transformations including scale, rotate, translate, mirror and shear as is shown in the following examples:\\nTo apply the affine matrix to an image, the image is converted to matrix in which each entry corresponds to the pixel intensity at that location. Then each pixel's location can be represented as a vector indicating the coordinates of that pixel in the image, [x, y], where x and y are the row and column of a pixel in the image matrix. This allows the coordinate to be multiplied by an affine-transformation matrix, which gives the position that the pixel value will be copied to in the output image.\\nHowever, to allow transformations that require translation transformations, 3 dimensional homogeneous coordinates are needed. The third dimension is usually set to a non-zero constant, usually 1, so that the new coordinate is [x, y, 1]. This allows the coordinate vector to be multiplied by a 3 by 3 matrix, enabling translation shifts. So the third dimension, which is the constant 1, allows translation.\\nBecause matrix multiplication is associative, multiple affine transformations can be combined into a single affine transformation by multiplying the matrix of each individual transformation in the order that the transformations are done. This results in a single matrix that, when applied to a point vector, gives the same result as all the individual transformations performed on the vector [x, y, 1] in sequence. Thus a sequence of affine transformation matrices can be reduced to a single affine transformation matrix.\\nFor example, 2 dimensional coordinates only allow rotation about the origin (0, 0). But 3 dimensional homogeneous coordinates can be used to first translate any point to (0, 0), then perform the rotation, and lastly translate the origin (0, 0) back to the original point (the opposite of the first translation). These 3 affine transformations can be combined into a single matrix, thus allowing rotation around any point in the image.\\n\\n\\n=== Image denoising with Morphology ===\\nMathematical morphology is suitable for denoising images. Structuring element are important in Mathematical morphology. \\nThe following examples are about Structuring elements.  The denoise function, image as I, and structuring element as B are shown as below and table.\\neg. \\n  \\n    \\n      \\n        (\\n        \\n          I\\n          ′\\n        \\n        )\\n        =\\n        \\n          \\n            [\\n            \\n              \\n                \\n                  45\\n                \\n                \\n                  50\\n                \\n                \\n                  65\\n                \\n              \\n              \\n                \\n                  40\\n                \\n                \\n                  60\\n                \\n                \\n                  55\\n                \\n              \\n              \\n                \\n                  25\\n                \\n                \\n                  15\\n                \\n                \\n                  5\\n                \\n              \\n            \\n            ]\\n          \\n        \\n        B\\n        =\\n        \\n          \\n            [\\n            \\n              \\n                \\n                  1\\n                \\n                \\n                  2\\n                \\n                \\n                  1\\n                \\n              \\n              \\n                \\n                  2\\n                \\n                \\n                  1\\n                \\n                \\n                  1\\n                \\n              \\n              \\n                \\n                  1\\n                \\n                \\n                  0\\n                \\n                \\n                  3\\n                \\n              \\n            \\n            ]\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle (I')={\\\\begin{bmatrix}45&50&65\\\\\\\\40&60&55\\\\\\\\25&15&5\\\\end{bmatrix}}B={\\\\begin{bmatrix}1&2&1\\\\\\\\2&1&1\\\\\\\\1&0&3\\\\end{bmatrix}}}\\n  \\nDefine Dilation(I, B)(i,j) = \\n  \\n    \\n      \\n        m\\n        a\\n        x\\n        {\\n        I\\n        (\\n        i\\n        +\\n        m\\n        ,\\n        j\\n        +\\n        n\\n        )\\n        +\\n        B\\n        (\\n        m\\n        ,\\n        n\\n        )\\n        }\\n      \\n    \\n    {\\\\displaystyle max\\\\{I(i+m,j+n)+B(m,n)\\\\}}\\n  . Let Dilation(I,B) = D(I,B)\\nD(I', B)(1,1) = \\n  \\n    \\n      \\n        m\\n        a\\n        x\\n        (\\n        45\\n        +\\n        1\\n        ,\\n        50\\n        +\\n        2\\n        ,\\n        65\\n        +\\n        1\\n        ,\\n        40\\n        +\\n        2\\n        ,\\n        60\\n        +\\n        1\\n        ,\\n        55\\n        +\\n        1\\n        ,\\n        25\\n        +\\n        1\\n        ,\\n        15\\n        +\\n        0\\n        ,\\n        5\\n        +\\n        3\\n        )\\n        =\\n        66\\n      \\n    \\n    {\\\\displaystyle max(45+1,50+2,65+1,40+2,60+1,55+1,25+1,15+0,5+3)=66}\\n  \\nDefine Erosion(I, B)(i,j) = \\n  \\n    \\n      \\n        m\\n        i\\n        n\\n        {\\n        I\\n        (\\n        i\\n        +\\n        m\\n        ,\\n        j\\n        +\\n        n\\n        )\\n        −\\n        B\\n        (\\n        m\\n        ,\\n        n\\n        )\\n        }\\n      \\n    \\n    {\\\\displaystyle min\\\\{I(i+m,j+n)-B(m,n)\\\\}}\\n  . Let Erosion(I,B) = E(I,B)\\nE(I', B)(1,1) = \\n  \\n    \\n      \\n        m\\n        i\\n        n\\n        (\\n        45\\n        −\\n        1\\n        ,\\n        50\\n        −\\n        2\\n        ,\\n        65\\n        −\\n        1\\n        ,\\n        40\\n        −\\n        2\\n        ,\\n        60\\n        −\\n        1\\n        ,\\n        55\\n        −\\n        1\\n        ,\\n        25\\n        −\\n        1\\n        ,\\n        15\\n        −\\n        0\\n        ,\\n        5\\n        −\\n        3\\n        )\\n        =\\n        2\\n      \\n    \\n    {\\\\displaystyle min(45-1,50-2,65-1,40-2,60-1,55-1,25-1,15-0,5-3)=2}\\n  \\nAfter dilation\\n\\n  \\n    \\n      \\n        (\\n        \\n          I\\n          ′\\n        \\n        )\\n        =\\n        \\n          \\n            [\\n            \\n              \\n                \\n                  45\\n                \\n                \\n                  50\\n                \\n                \\n                  65\\n                \\n              \\n              \\n                \\n                  40\\n                \\n                \\n                  66\\n                \\n                \\n                  55\\n                \\n              \\n              \\n                \\n                  25\\n                \\n                \\n                  15\\n                \\n                \\n                  5\\n                \\n              \\n            \\n            ]\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle (I')={\\\\begin{bmatrix}45&50&65\\\\\\\\40&66&55\\\\\\\\25&15&5\\\\end{bmatrix}}}\\n  \\nAfter erosion\\n\\n  \\n    \\n      \\n        (\\n        \\n          I\\n          ′\\n        \\n        )\\n        =\\n        \\n          \\n            [\\n            \\n              \\n                \\n                  45\\n                \\n                \\n                  50\\n                \\n                \\n                  65\\n                \\n              \\n              \\n                \\n                  40\\n                \\n                \\n                  2\\n                \\n                \\n                  55\\n                \\n              \\n              \\n                \\n                  25\\n                \\n                \\n                  15\\n                \\n                \\n                  5\\n                \\n              \\n            \\n            ]\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle (I')={\\\\begin{bmatrix}45&50&65\\\\\\\\40&2&55\\\\\\\\25&15&5\\\\end{bmatrix}}}\\n  \\nAn opening method is just simply erosion first, and then dilation while the closing method is vice versa. In reality, the D(I,B) and E(I,B) can implemented by Convolution\\n\\nIn order to apply the denoising method to an image, the image is converted into grayscale. A mask with denoising method is logical matrix with \\n  \\n    \\n      \\n        [\\n        111\\n        ;\\n        111\\n        ;\\n        111\\n        ]\\n      \\n    \\n    {\\\\displaystyle [111;111;111]}\\n  . The denoising methods start from the center of the picture with half of height, half of width, and end with the image boundary of row number, column number. Neighbor is a block in the original image with the boundary [the point below center: the point above, the point on left of center: the point on the right of center]. Convolution Neighbor and structuring element and then replace the center with a minimum of neighbor.\\nTake the Closing method for example.\\nDilation first\\n\\nRead the image and convert it into grayscale with Matlab.\\nGet the size of an image. The return value row numbers and column numbers are the boundaries we are going to use later.\\nstructuring elements depend on your dilation or erosion function. The minimum of the neighbor of a pixel leads to an erosion method and the maximum of neighbor leads to a dilation method.\\nSet the time for dilation, erosion, and closing.\\nCreate a zero matrix of the same size as the original image.\\nDilation first with structuring window.\\nstructuring window is 3*3 matrix and convolution\\nFor loop extract the minimum with window from row range [2 ~ image height - 1] with column range [2 ~ image width - 1]\\nFill the minimum value to the zero matrix and save a new image\\nFor the boundary, it can still be improved. Since in the method, a boundary is ignored. Padding elements can be applied to deal with boundaries.Then Erosion (Take the dilation image as input)\\n\\nCreate a zero matrix of the same size as the original image.\\nErosion with structuring window.\\nstructuring window is 3*3 matrix and convolution\\nFor loop extract the maximum with window from row range [2 ~ image height - 1] with column range [2 ~ image width - 1]\\nFill the maximum value to the zero matrix and save a new image\\nFor the boundary, it can still be improved. Since in the method, boundary is ignored. Padding elements can be applied to deal with boundaries.\\nResults are as above table shown\\n\\n\\n== Applications ==\\n\\n\\n=== Digital camera images ===\\nDigital cameras generally include specialized digital image processing hardware – either dedicated chips or added circuitry on other chips – to convert the raw data from their image sensor into a color-corrected image in a standard image file format.\\n\\n\\n=== Film ===\\nWestworld (1973) was the first feature film to use the digital image processing to pixellate photography to simulate an android's point of view.\\n\\n\\n=== Face detection ===\\n\\nFace detection can be implemented with Mathematical morphology, Discrete cosine transform which is usually called DCT, and horizontal Projection (mathematics).\\nGeneral method with feature-based method\\nThe feature-based method of face detection is using skin tone, edge detection, face shape, and feature of a face (like eyes, mouth, etc) to achieve face detection. The skin tone, face shape, and all the unique elements that only the human face have can be described as features.\\nProcess explanation\\n\\nGiven a batch of face images, first, extract the skin tone range by sampling face images. The skin tone range is just a skin filter.\\nStructural similarity index measure (SSIM) can be applied to compare images in terms of extracting the skin tone.\\nNormally, HSV or RGB color spaces are suitable for the skin filter. Eg. HSV mode, the skin tone range is [0,48,50] ~ [20,255,255]\\nAfter filtering images with skin tone, to get the face edge, morphology and DCT are used to remove noise and fill up missing skin areas.\\nOpening method or closing method can be used to achieve filling up missing skin.\\nDCT is to avoid the object with tone-like skin. Since human faces always have higher texture.\\nSobel operator or other operators can be applied to detect face edge.\\nTo position human features like eyes, using the projection and find the peak of the histogram of projection help to get the detail feature like mouse, hair, and lip.\\nProjection is just projecting the image to see the high frequency which is usually the feature position.\\n\\n\\n=== Improvement of image quality method ===\\nImage quality can be influenced by camera vibration, over-exposure, gray level distribution too centralized, and noise, etc. For example, noise problem can be solved by Smoothing method while gray level distribution problem can be improved by Histogram Equalization.\\nSmoothing method\\nIn drawing, if there is some dissatisfied color, taking some color around dissatisfied color and averaging them. This is an easy way to think of Smoothing method.\\nSmoothing method can be implemented with mask and Convolution. Take the small image and mask for instance as below.\\nimage is\\n\\n  \\n    \\n      \\n        \\n          \\n            [\\n            \\n              \\n                \\n                  2\\n                \\n                \\n                  5\\n                \\n                \\n                  6\\n                \\n                \\n                  5\\n                \\n              \\n              \\n                \\n                  3\\n                \\n                \\n                  1\\n                \\n                \\n                  4\\n                \\n                \\n                  6\\n                \\n              \\n              \\n                \\n                  1\\n                \\n                \\n                  28\\n                \\n                \\n                  30\\n                \\n                \\n                  2\\n                \\n              \\n              \\n                \\n                  7\\n                \\n                \\n                  3\\n                \\n                \\n                  2\\n                \\n                \\n                  2\\n                \\n              \\n            \\n            ]\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\begin{bmatrix}2&5&6&5\\\\\\\\3&1&4&6\\\\\\\\1&28&30&2\\\\\\\\7&3&2&2\\\\end{bmatrix}}}\\n  \\nmask is \\n  \\n    \\n      \\n        \\n          \\n            [\\n            \\n              \\n                \\n                  1\\n                  \\n                    /\\n                  \\n                  9\\n                \\n                \\n                  1\\n                  \\n                    /\\n                  \\n                  9\\n                \\n                \\n                  1\\n                  \\n                    /\\n                  \\n                  9\\n                \\n              \\n              \\n                \\n                  1\\n                  \\n                    /\\n                  \\n                  9\\n                \\n                \\n                  1\\n                  \\n                    /\\n                  \\n                  9\\n                \\n                \\n                  1\\n                  \\n                    /\\n                  \\n                  9\\n                \\n              \\n              \\n                \\n                  1\\n                  \\n                    /\\n                  \\n                  9\\n                \\n                \\n                  1\\n                  \\n                    /\\n                  \\n                  9\\n                \\n                \\n                  1\\n                  \\n                    /\\n                  \\n                  9\\n                \\n              \\n            \\n            ]\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\begin{bmatrix}1/9&1/9&1/9\\\\\\\\1/9&1/9&1/9\\\\\\\\1/9&1/9&1/9\\\\end{bmatrix}}}\\n  \\nAfter Convolution and smoothing, image is\\n\\n  \\n    \\n      \\n        \\n          \\n            [\\n            \\n              \\n                \\n                  2\\n                \\n                \\n                  5\\n                \\n                \\n                  6\\n                \\n                \\n                  5\\n                \\n              \\n              \\n                \\n                  3\\n                \\n                \\n                  9\\n                \\n                \\n                  10\\n                \\n                \\n                  6\\n                \\n              \\n              \\n                \\n                  1\\n                \\n                \\n                  9\\n                \\n                \\n                  9\\n                \\n                \\n                  2\\n                \\n              \\n              \\n                \\n                  7\\n                \\n                \\n                  3\\n                \\n                \\n                  2\\n                \\n                \\n                  2\\n                \\n              \\n            \\n            ]\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\begin{bmatrix}2&5&6&5\\\\\\\\3&9&10&6\\\\\\\\1&9&9&2\\\\\\\\7&3&2&2\\\\end{bmatrix}}}\\n  \\nOberseving image[1, 1], image[1, 2], image[2, 1], and image[2, 2].\\nThe original image pixel is 1, 4, 28, 30. After smoothing mask, the pixel becomes 9, 10, 9, 9 respectively.\\nnew image[1, 1] = \\n  \\n    \\n      \\n        \\n          \\n            \\n              1\\n              9\\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\tfrac {1}{9}}}\\n   * (image[0,0]+image[0,1]+image[0,2]+image[1,0]+image[1,1]+image[1,2]+image[2,0]+image[2,1]+image[2,2])\\nnew image[1, 1] = floor(\\n  \\n    \\n      \\n        \\n          \\n            \\n              1\\n              9\\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\tfrac {1}{9}}}\\n   * (2+5+6+3+1+4+1+28+30)) = 9\\nnew image[1, 2] = floor({\\n  \\n    \\n      \\n        \\n          \\n            \\n              1\\n              9\\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\tfrac {1}{9}}}\\n   * (5+6+5+1+4+6+28+30+2)) = 10\\nnew image[2, 1] = floor(\\n  \\n    \\n      \\n        \\n          \\n            \\n              1\\n              9\\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\tfrac {1}{9}}}\\n   * (3+1+4+1+28+30+73+3+2)) = 9\\nnew image[2, 2] = floor(\\n  \\n    \\n      \\n        \\n          \\n            \\n              1\\n              9\\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\tfrac {1}{9}}}\\n   * (1+4+6+28+30+2+3+2+2)) = 9\\nGray Level Histogram method\\nGenerally, given a gray level histogram from an image as below. Changing the histogram to uniform distribution from an image is usually what we called Histogram equalization. \\n\\nIn discrete time, the area of gray level histogram is \\n  \\n    \\n      \\n        \\n          ∑\\n          \\n            i\\n            =\\n            0\\n          \\n          \\n            k\\n          \\n        \\n        H\\n        (\\n        \\n          p\\n          \\n            i\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle \\\\sum _{i=0}^{k}H(p_{i})}\\n  (see figure 1) while the area of uniform distribution is \\n  \\n    \\n      \\n        \\n          ∑\\n          \\n            i\\n            =\\n            0\\n          \\n          \\n            k\\n          \\n        \\n        G\\n        (\\n        \\n          q\\n          \\n            i\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle \\\\sum _{i=0}^{k}G(q_{i})}\\n  (see figure 2). It's clear that the area won't change, so \\n  \\n    \\n      \\n        \\n          ∑\\n          \\n            i\\n            =\\n            0\\n          \\n          \\n            k\\n          \\n        \\n        H\\n        (\\n        \\n          p\\n          \\n            i\\n          \\n        \\n        )\\n        =\\n        \\n          ∑\\n          \\n            i\\n            =\\n            0\\n          \\n          \\n            k\\n          \\n        \\n        G\\n        (\\n        \\n          q\\n          \\n            i\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle \\\\sum _{i=0}^{k}H(p_{i})=\\\\sum _{i=0}^{k}G(q_{i})}\\n  .\\nFrom the uniform distribution, the probability of \\n  \\n    \\n      \\n        \\n          q\\n          \\n            i\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle q_{i}}\\n   is \\n  \\n    \\n      \\n        \\n          \\n            \\n              \\n                N\\n                \\n                  2\\n                \\n              \\n              \\n                \\n                  q\\n                  \\n                    k\\n                  \\n                \\n                −\\n                \\n                  q\\n                  \\n                    0\\n                  \\n                \\n              \\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\tfrac {N^{2}}{q_{k}-q_{0}}}}\\n   while the \\n  \\n    \\n      \\n        0\\n        <\\n        i\\n        <\\n        k\\n      \\n    \\n    {\\\\displaystyle 0<i<k}\\n  \\nIn continuous time, the equation is \\n  \\n    \\n      \\n        \\n          \\n            ∫\\n            \\n              \\n                q\\n                \\n                  0\\n                \\n              \\n            \\n            \\n              q\\n            \\n          \\n          \\n            \\n              \\n                \\n                  N\\n                  \\n                    2\\n                  \\n                \\n                \\n                  \\n                    q\\n                    \\n                      k\\n                    \\n                  \\n                  −\\n                  \\n                    q\\n                    \\n                      0\\n                    \\n                  \\n                \\n              \\n            \\n          \\n          d\\n          s\\n          =\\n          \\n            \\n              ∫\\n              \\n                \\n                  p\\n                  \\n                    0\\n                  \\n                \\n              \\n              \\n                p\\n              \\n            \\n            H\\n            (\\n            s\\n            )\\n            d\\n            s\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle \\\\displaystyle \\\\int _{q_{0}}^{q}{\\\\tfrac {N^{2}}{q_{k}-q_{0}}}ds=\\\\displaystyle \\\\int _{p_{0}}^{p}H(s)ds}\\n  .\\nMoreover, based on the definition of a function, the Gray level histogram method is like finding a function \\n  \\n    \\n      \\n        f\\n      \\n    \\n    {\\\\displaystyle f}\\n   that satisfies f(p)=q.\\n\\n\\n== Fatigue detection and monitoring technologies ==\\nThere were significant advancements in fatigue monitoring technology the past decade. These innovative technology solutions are now commercially available and offer real safety benefits to drivers, operators and other shift workers across all industries.Software developers, engineers and scientists develop fatigue detection software using various physiological cues to determine the state of fatigue or drowsiness. The measurement of brain activity (electroencephalogram) is widely accepted as the standard in fatigue monitoring. Other technology used to determine fatigue related impairment include behavioural symptom measurements such as; eye behaviour, gaze direction, micro-corrections in steering and throttle use as well as heart rate variability.\\n\\n\\n== See also ==\\nDigital imaging\\nComputer graphics\\nComputer vision\\nCVIPtools\\nDigitizing\\nFree boundary condition\\nGPGPU\\nHomomorphic filtering\\nImage analysis\\nIEEE Intelligent Transportation Systems Society\\nMultidimensional systems\\nRemote sensing software\\nStandard test image\\nSuperresolution\\nTotal variation denoising\\nMachine Vision\\nBounded variation\\nRadiomics\\n\\n\\n== References ==\\n\\n\\n== Further reading ==\\nSolomon, C.J.; Breckon, T.P. (2010). Fundamentals of Digital Image Processing: A Practical Approach with Examples in Matlab. Wiley-Blackwell. doi:10.1002/9780470689776. ISBN 978-0470844731.\\nWilhelm Burger; Mark J. Burge (2007). Digital Image Processing: An Algorithmic Approach Using Java. Springer. ISBN 978-1-84628-379-6.\\nR. Fisher; K Dawson-Howe; A. Fitzgibbon; C. Robertson; E. Trucco (2005). Dictionary of Computer Vision and Image Processing. John Wiley. ISBN 978-0-470-01526-1.\\nRafael C. Gonzalez; Richard E. Woods; Steven L. Eddins (2004). Digital Image Processing using MATLAB. Pearson Education. ISBN 978-81-7758-898-9.\\nTim Morris (2004). Computer Vision and Image Processing. Palgrave Macmillan. ISBN 978-0-333-99451-1.\\nTyagi Vipin (2018). Understanding Digital Image Processing. Taylor and Francis CRC Press. ISBN 978-11-3856-6842.\\nMilan Sonka; Vaclav Hlavac; Roger Boyle (1999). Image Processing, Analysis, and Machine Vision. PWS Publishing. ISBN 978-0-534-95393-5.\\nRafael C. Gonzalez (2008). Digital Image Processing. Prentice Hall. ISBN 9780131687288\\n\\n\\n== External links ==\\nLectures on Image Processing, by Alan Peters. Vanderbilt University. Updated 7 January 2016.\\nProcessing digital images with computer algorithms\", 'Information visualization (or visualisation) is the study of visual representations of abstract data to reinforce human cognition. The abstract data include both numerical and non-numerical data, such as text and geographic information. It is related to data visualization, infographics, and scientific visualization. One definition is that it\\'s information visualization when the spatial representation (e.g., the page layout of a graphic design) is chosen, whereas it\\'s scientific visualization when the spatial representation is given.\\n\\n\\n== Overview ==\\n\\nThe field of information visualization has emerged \"from research in human–computer interaction, computer science, graphics, visual design, psychology, and business methods. It is increasingly applied as a critical component in scientific research, digital libraries, data mining, financial data analysis, market studies, manufacturing production control, and drug discovery\".Information visualization presumes that \"visual representations and interaction techniques take advantage of the human eye’s broad bandwidth pathway into the mind to allow users to see, explore, and understand large amounts of information at once. Information visualization focused on the creation of approaches for conveying abstract information in intuitive ways.\"Data analysis is an indispensable part of all applied research and problem solving in industry. The most fundamental data analysis approaches are visualization (histograms, scatter plots, surface plots, tree maps, parallel coordinate plots, etc.), statistics (hypothesis test, regression, PCA, etc.), data mining (association mining, etc.), and machine learning methods (clustering, classification, decision trees, etc.). Among these approaches,  information visualization, or visual data analysis, is the most reliant on the cognitive skills of human analysts, and allows the discovery of unstructured actionable insights that are limited only by human imagination and creativity. The analyst does not have to learn any sophisticated methods to be able to interpret the visualizations of the data. Information visualization is also a hypothesis generation scheme, which can be, and is typically followed by more analytical or formal analysis, such as statistical hypothesis testing.\\n\\n\\n== History ==\\n\\nThe modern study of visualization started with computer graphics, which \"has from its beginning been used to study scientific problems. However, in its early days the lack of graphics power often limited its usefulness. The recent emphasis on visualization started in 1987 with the special issue of Computer Graphics on Visualization in Scientific Computing. Since then there have been several conferences and workshops, co-sponsored by the IEEE Computer Society and ACM SIGGRAPH\". They have been devoted to the general topics of data visualization, information visualization and scientific visualization, and more specific areas such as volume visualization.\\n\\nIn 1786, William Playfair published the first presentation graphics.\\n\\n\\n== Techniques ==\\n\\nCartogram\\nCladogram (phylogeny)\\nConcept Mapping\\nDendrogram (classification)\\nInformation visualization reference model\\nGraph drawing\\nHeatmap\\nHyperbolicTree\\nMultidimensional scaling\\nParallel coordinates\\nProblem solving environment\\nTreemapping\\n\\n\\n== Applications ==\\nInformation visualization insights are being applied in areas such as:\\nScientific research\\nDigital libraries\\nData mining\\nInformation graphics\\nFinancial data analysis\\nHealth care\\nMarket studies\\nManufacturing production control\\nCrime mapping\\neGovernance and Policy Modeling\\n\\n\\n== Organization ==\\nNotable academic and industry laboratories in the field are:\\n\\nAdobe Research\\nIBM Research\\nGoogle Research\\nMicrosoft Research\\nPanopticon Software\\nScientific Computing and Imaging Institute\\nTableau Software\\nUniversity of Maryland Human-Computer Interaction Lab\\nVviConferences in this field, ranked by significance in data visualization research, are:\\n\\nIEEE Visualization: An annual international conference on scientific visualization, information visualization, and visual analytics. Conference is held in October.\\nACM SIGGRAPH: An annual international conference on computer graphics, convened by the ACM SIGGRAPH organization. Conference dates vary.\\nEuroVis: An annual Europe-wide conference on data visualization, organized by the Eurographics Working Group on Data Visualization and supported by the IEEE Visualization and Graphics Technical Committee (IEEE VGTC). Conference is usually held in June.\\nConference on Human Factors in Computing Systems (CHI): An annual international conference on human–computer interaction, hosted by ACM SIGCHI. Conference is usually held in April or May.\\nEurographics: An annual Europe-wide computer graphics conference, held by the European Association for Computer Graphics. Conference is usually held in April or May.\\nPacificVis: An annual visualization symposium held in the Asia-Pacific region, sponsored by the IEEE Visualization and Graphics Technical Committee (IEEE VGTC). Conference is usually held in March or April.For further examples, see: Category:Computer graphics organizations\\n\\n\\n== See also ==\\nColor coding technology for visualization\\nComputational visualistics\\nData art\\nData Presentation Architecture\\nData visualization\\nGeovisualization\\nInfographics\\nPatent visualisation\\nSoftware visualization\\nVisual analytics\\nList of information graphics software\\nList of countries by economic complexity, example of Treemapping\\n\\n\\n== References ==\\n\\n\\n== Further reading ==\\nBen Bederson and Ben Shneiderman (2003). The Craft of Information Visualization: Readings and Reflections. Morgan Kaufmann.\\nStuart K. Card, Jock D. Mackinlay and Ben Shneiderman (1999). Readings in Information Visualization: Using Vision to Think, Morgan Kaufmann Publishers.\\nJeffrey Heer, Stuart K. Card, James Landay (2005). \"Prefuse: a toolkit for interactive information visualization\". In: ACM Human Factors in Computing Systems CHI 2005.\\nAndreas Kerren, John T. Stasko, Jean-Daniel Fekete, and Chris North (2008). Information Visualization – Human-Centered Issues and Perspectives. Volume 4950 of LNCS State-of-the-Art Survey, Springer.\\nRiccardo Mazza (2009). Introduction to Information Visualization, Springer.\\nSpence, Robert Information Visualization: Design for Interaction (2nd Edition), Prentice Hall, 2007, ISBN 0-13-206550-9.\\nColin Ware (2000). Information Visualization: Perception for design. San Francisco, CA: Morgan Kaufmann.\\nKawa Nazemi (2014). Adaptive Semantics Visualization Eurographics Association.\\n\\n\\n== External links ==\\n Media related to Information visualization at Wikimedia Commons\\nInformation Visualization at Curlie', 'Parallel computing is a type of computation in which many calculations or processes are carried out simultaneously. Large problems can often be divided into smaller ones, which can then be solved at the same time. There are several different forms of parallel computing: bit-level, instruction-level, data, and task parallelism. Parallelism has long been employed in high-performance computing, but has gained broader interest due to the physical constraints preventing frequency scaling. As power consumption (and consequently heat generation) by computers has become a concern in recent years, parallel computing has become the dominant paradigm in computer architecture, mainly in the form of multi-core processors.Parallel computing is closely related to concurrent computing—they are frequently used together, and often conflated, though the two are distinct: it is possible to have parallelism without concurrency (such as bit-level parallelism), and concurrency without parallelism (such as multitasking by time-sharing on a single-core CPU). In parallel computing, a computational task is typically broken down into several, often many, very similar sub-tasks that can be processed independently and whose results are combined afterwards, upon completion. In contrast, in concurrent computing, the various processes often do not address related tasks; when they do, as is typical in distributed computing, the separate tasks may have a varied nature and often require some inter-process communication during execution.\\nParallel computers can be roughly classified according to the level at which the hardware supports parallelism, with multi-core and multi-processor computers having multiple processing elements within a single machine, while clusters, MPPs, and grids use multiple computers to work on the same task. Specialized parallel computer architectures are sometimes used alongside traditional processors, for accelerating specific tasks.\\nIn some cases parallelism is transparent to the programmer, such as in bit-level or instruction-level parallelism, but explicitly parallel algorithms, particularly those that use concurrency, are more difficult to write than sequential ones, because concurrency introduces several new classes of potential software bugs, of which race conditions are the most common. Communication and synchronization between the different subtasks are typically some of the greatest obstacles to getting optimal parallel program performance.\\nA theoretical upper bound on the speed-up of a single program as a result of parallelization is given by Amdahl\\'s law.\\n\\n\\n== Background ==\\nTraditionally, computer software has been written for serial computation. To solve a problem, an algorithm is constructed and implemented as a serial stream of instructions. These instructions are executed on a central processing unit on one computer. Only one instruction may execute at a time—after that instruction is finished, the next one is executed.Parallel computing, on the other hand, uses multiple processing elements simultaneously to solve a problem. This is accomplished by breaking the problem into independent parts so that each processing element can execute its part of the algorithm simultaneously with the others. The processing elements can be diverse and include resources such as a single computer with multiple processors, several networked computers, specialized hardware, or any combination of the above. Historically parallel computing was used for scientific computing and the simulation of scientific problems, particularly in the natural and engineering sciences, such as meteorology. This led to the design of parallel hardware and software, as well as high performance computing.Frequency scaling was the dominant reason for improvements in computer performance from the mid-1980s until 2004. The runtime of a program is equal to the number of instructions multiplied by the average time per instruction. Maintaining everything else constant, increasing the clock frequency decreases the average time it takes to execute an instruction. An increase in frequency thus decreases runtime for all compute-bound programs. However, power consumption P by a chip is given by the equation P = C × V 2 × F, where C is the capacitance being switched per clock cycle (proportional to the number of transistors whose inputs change), V is voltage, and F is the processor frequency (cycles per second). Increases in frequency increase the amount of power used in a processor. Increasing processor power consumption led ultimately to Intel\\'s May 8, 2004 cancellation of its Tejas and Jayhawk processors, which is generally cited as the end of frequency scaling as the dominant computer architecture paradigm.To deal with the problem of power consumption and overheating the major central processing unit (CPU or processor) manufacturers started to produce power efficient processors with multiple cores. The core is the computing unit of the processor and in multi-core processors each core is independent and can access the same memory concurrently. Multi-core processors have brought parallel computing to desktop computers. Thus parallelisation of serial programmes has become a mainstream programming task. In 2012 quad-core processors became standard for desktop computers, while servers have 10 and 12 core processors. From Moore\\'s law it can be predicted that the number of cores per processor will double every 18–24 months. This could mean that after 2020 a typical processor will have dozens or hundreds of cores.An operating system can ensure that different tasks and user programmes are run in parallel on the available cores. However, for a serial software programme to take full advantage of the multi-core architecture the programmer needs to restructure and parallelise the code. A speed-up of application software runtime will no longer be achieved through frequency scaling, instead programmers will need to parallelise their software code to take advantage of the increasing computing power of multicore architectures.\\n\\n\\n=== Amdahl\\'s law and Gustafson\\'s law ===\\n\\nOptimally, the speedup from parallelization would be linear—doubling the number of processing elements should halve the runtime, and doubling it a second time should again halve the runtime. However, very few parallel algorithms achieve optimal speedup. Most of them have a near-linear speedup for small numbers of processing elements, which flattens out into a constant value for large numbers of processing elements.\\nThe potential speedup of an algorithm on a parallel computing platform is given by Amdahl\\'s law\\n\\n  \\n    \\n      \\n        \\n          S\\n          \\n            latency\\n          \\n        \\n        (\\n        s\\n        )\\n        =\\n        \\n          \\n            1\\n            \\n              1\\n              −\\n              p\\n              +\\n              \\n                \\n                  p\\n                  s\\n                \\n              \\n            \\n          \\n        \\n        ,\\n      \\n    \\n    {\\\\displaystyle S_{\\\\text{latency}}(s)={\\\\frac {1}{1-p+{\\\\frac {p}{s}}}},}\\n  where\\n\\nSlatency is the potential speedup in latency of the execution of the whole task;\\ns is the speedup in latency of the execution of the parallelizable part of the task;\\np is the percentage of the execution time of the whole task concerning the parallelizable part of the task before parallelization.Since Slatency < 1/(1 - p), it shows that a small part of the program which cannot be parallelized will limit the overall speedup available from parallelization. A program solving a large mathematical or engineering problem will typically consist of several parallelizable parts and several non-parallelizable (serial) parts. If the non-parallelizable part of a program accounts for 10% of the runtime (p = 0.9), we can get no more than a 10 times speedup, regardless of how many processors are added. This puts an upper limit on the usefulness of adding more parallel execution units. \"When a task cannot be partitioned because of sequential constraints, the application of more effort has no effect on the schedule. The bearing of a child takes nine months, no matter how many women are assigned.\"\\n\\nAmdahl\\'s law only applies to cases where the problem size is fixed. In practice, as more computing resources become available, they tend to get used on larger problems (larger datasets), and the time spent in the parallelizable part often grows much faster than the inherently serial work. In this case, Gustafson\\'s law gives a less pessimistic and more realistic assessment of parallel performance:\\n\\n  \\n    \\n      \\n        \\n          S\\n          \\n            latency\\n          \\n        \\n        (\\n        s\\n        )\\n        =\\n        1\\n        −\\n        p\\n        +\\n        s\\n        p\\n        .\\n      \\n    \\n    {\\\\displaystyle S_{\\\\text{latency}}(s)=1-p+sp.}\\n  Both Amdahl\\'s law and Gustafson\\'s law assume that the running time of the serial part of the program is independent of the number of processors. Amdahl\\'s law assumes that the entire problem is of fixed size so that the total amount of work to be done in parallel is also independent of the number of processors, whereas Gustafson\\'s law assumes that the total amount of work to be done in parallel varies linearly with the number of processors.\\n\\n\\n=== Dependencies ===\\nUnderstanding data dependencies is fundamental in implementing parallel algorithms. No program can run more quickly than the longest chain of dependent calculations (known as the critical path), since calculations that depend upon prior calculations in the chain must be executed in order. However, most algorithms do not consist of just a long chain of dependent calculations; there are usually opportunities to execute independent calculations in parallel.\\nLet Pi and Pj be two program segments. Bernstein\\'s conditions describe when the two are independent and can be executed in parallel. For Pi, let Ii be all of the input variables and Oi the output variables, and likewise for Pj. Pi and Pj are independent if they satisfy\\n\\n  \\n    \\n      \\n        \\n          I\\n          \\n            j\\n          \\n        \\n        ∩\\n        \\n          O\\n          \\n            i\\n          \\n        \\n        =\\n        ∅\\n        ,\\n      \\n    \\n    {\\\\displaystyle I_{j}\\\\cap O_{i}=\\\\varnothing ,}\\n  \\n\\n  \\n    \\n      \\n        \\n          I\\n          \\n            i\\n          \\n        \\n        ∩\\n        \\n          O\\n          \\n            j\\n          \\n        \\n        =\\n        ∅\\n        ,\\n      \\n    \\n    {\\\\displaystyle I_{i}\\\\cap O_{j}=\\\\varnothing ,}\\n  \\n\\n  \\n    \\n      \\n        \\n          O\\n          \\n            i\\n          \\n        \\n        ∩\\n        \\n          O\\n          \\n            j\\n          \\n        \\n        =\\n        ∅\\n        .\\n      \\n    \\n    {\\\\displaystyle O_{i}\\\\cap O_{j}=\\\\varnothing .}\\n  Violation of the first condition introduces a flow dependency, corresponding to the first segment producing a result used by the second segment. The second condition represents an anti-dependency, when the second segment produces a variable needed by the first segment. The third and final condition represents an output dependency: when two segments write to the same location, the result comes from the logically last executed segment.Consider the following functions, which demonstrate several kinds of dependencies:\\n\\n1: function Dep(a, b)\\n2: c := a * b\\n3: d := 3 * c\\n4: end function\\n\\nIn this example, instruction 3 cannot be executed before (or even in parallel with) instruction 2, because instruction 3 uses a result from instruction 2. It violates condition 1, and thus introduces a flow dependency.\\n\\n1: function NoDep(a, b)\\n2: c := a * b\\n3: d := 3 * b\\n4: e := a + b\\n5: end function\\n\\nIn this example, there are no dependencies between the instructions, so they can all be run in parallel.\\nBernstein\\'s conditions do not allow memory to be shared between different processes. For that, some means of enforcing an ordering between accesses is necessary, such as semaphores, barriers or some other synchronization method.\\n\\n\\n=== Race conditions, mutual exclusion, synchronization, and parallel slowdown ===\\nSubtasks in a parallel program are often called threads. Some parallel computer architectures use smaller, lightweight versions of threads known as fibers, while others use bigger versions known as processes. However, \"threads\" is generally accepted as a generic term for subtasks. Threads will often need synchronized access to an object or other resource, for example when they must update a variable that is shared between them. Without synchronization, the instructions between the two threads may be interleaved in any order. For example, consider the following program:\\n\\nIf instruction 1B is executed between 1A and 3A, or if instruction 1A is executed between 1B and 3B, the program will produce incorrect data. This is known as a race condition. The programmer must use a lock to provide mutual exclusion. A lock is a programming language construct that allows one thread to take control of a variable and prevent other threads from reading or writing it, until that variable is unlocked. The thread holding the lock is free to execute its critical section (the section of a program that requires exclusive access to some variable), and to unlock the data when it is finished. Therefore, to guarantee correct program execution, the above program can be rewritten to use locks:\\n\\nOne thread will successfully lock variable V, while the other thread will be locked out—unable to proceed until V is unlocked again. This guarantees correct execution of the program. Locks may be necessary to ensure correct program execution when threads must serialize access to resources, but their use can greatly slow a program and may affect its reliability.Locking multiple variables using non-atomic locks introduces the possibility of program deadlock. An atomic lock locks multiple variables all at once. If it cannot lock all of them, it does not lock any of them. If two threads each need to lock the same two variables using non-atomic locks, it is possible that one thread will lock one of them and the second thread will lock the second variable. In such a case, neither thread can complete, and deadlock results.Many parallel programs require that their subtasks act in synchrony. This requires the use of a barrier. Barriers are typically implemented using a lock or a semaphore. One class of algorithms, known as lock-free and wait-free algorithms, altogether avoids the use of locks and barriers. However, this approach is generally difficult to implement and requires correctly designed data structures.Not all parallelization results in speed-up. Generally, as a task is split up into more and more threads, those threads spend an ever-increasing portion of their time communicating with each other or waiting on each other for access to resources. Once the overhead from resource contention or communication dominates the time spent on other computation, further parallelization (that is, splitting the workload over even more threads) increases rather than decreases the amount of time required to finish. This problem, known as parallel slowdown, can be improved in some cases by software analysis and redesign.\\n\\n\\n=== Fine-grained, coarse-grained, and embarrassing parallelism ===\\nApplications are often classified according to how often their subtasks need to synchronize or communicate with each other. An application exhibits fine-grained parallelism if its subtasks must communicate many times per second; it exhibits coarse-grained parallelism if they do not communicate many times per second, and it exhibits embarrassing parallelism if they rarely or never have to communicate. Embarrassingly parallel applications are considered the easiest to parallelize.\\n\\n\\n=== Flynn\\'s taxonomy ===\\nMichael J. Flynn created one of the earliest classification systems for parallel (and sequential) computers and programs, now known as Flynn\\'s taxonomy. Flynn classified programs and computers by whether they were operating using a single set or multiple sets of instructions, and whether or not those instructions were using a single set or multiple sets of data.\\n\\nThe single-instruction-single-data (SISD) classification is equivalent to an entirely sequential program. The single-instruction-multiple-data (SIMD) classification is analogous to doing the same operation repeatedly over a large data set. This is commonly done in signal processing applications. Multiple-instruction-single-data (MISD) is a rarely used classification. While computer architectures to deal with this were devised (such as systolic arrays), few applications that fit this class materialized. Multiple-instruction-multiple-data (MIMD) programs are by far the most common type of parallel programs.\\nAccording to David A. Patterson and John L. Hennessy, \"Some machines are hybrids of these categories, of course, but this classic model has survived because it is simple, easy to understand, and gives a good first approximation. It is also—perhaps because of its understandability—the most widely used scheme.\"\\n\\n\\n== Types of parallelism ==\\n\\n\\n=== Bit-level parallelism ===\\n\\nFrom the advent of very-large-scale integration (VLSI) computer-chip fabrication technology in the 1970s until about 1986, speed-up in computer architecture was driven by doubling computer word size—the amount of information the processor can manipulate per cycle. Increasing the word size reduces the number of instructions the processor must execute to perform an operation on variables whose sizes are greater than the length of the word. For example, where an 8-bit processor must add two 16-bit integers, the processor must first add the 8 lower-order bits from each integer using the standard addition instruction, then add the 8 higher-order bits using an add-with-carry instruction and the carry bit from the lower order addition; thus, an 8-bit processor requires two instructions to complete a single operation, where a 16-bit processor would be able to complete the operation with a single instruction.\\nHistorically, 4-bit microprocessors were replaced with 8-bit, then 16-bit, then 32-bit microprocessors. This trend generally came to an end with the introduction of 32-bit processors, which has been a standard in general-purpose computing for two decades. Not until the early 2000s, with the advent of x86-64 architectures, did 64-bit processors become commonplace.\\n\\n\\n=== Instruction-level parallelism ===\\n\\nA computer program is, in essence, a stream of instructions executed by a processor. Without instruction-level parallelism, a processor can only issue less than one instruction per clock cycle (IPC < 1). These processors are known as subscalar processors. These instructions can be re-ordered and combined into groups which are then executed in parallel without changing the result of the program. This is known as instruction-level parallelism. Advances in instruction-level parallelism dominated computer architecture from the mid-1980s until the mid-1990s.\\n\\nAll modern processors have multi-stage instruction pipelines. Each stage in the pipeline corresponds to a different action the processor performs on that instruction in that stage; a processor with an N-stage pipeline can have up to N different instructions at different stages of completion and thus can issue one instruction per clock cycle (IPC = 1). These processors are known as scalar processors. The canonical example of a pipelined processor is a RISC processor, with five stages: instruction fetch (IF), instruction decode (ID), execute (EX), memory access (MEM), and register write back (WB). The Pentium 4 processor had a 35-stage pipeline.\\n\\nMost modern processors also have multiple execution units. They usually combine this feature with pipelining and thus can issue more than one instruction per clock cycle (IPC > 1). These processors are known as superscalar processors. Superscalar processors differ from multi-core processors in that the several execution units are not entire processors (i.e. processing units). Instructions can be grouped together only if there is no data dependency between them. Scoreboarding and the Tomasulo algorithm (which is similar to scoreboarding but makes use of register renaming) are two of the most common techniques for implementing out-of-order execution and instruction-level parallelism.\\n\\n\\n=== Task parallelism ===\\n\\nTask parallelisms is the characteristic of a parallel program that \"entirely different calculations can be performed on either the same or different sets of data\". This contrasts with data parallelism, where the same calculation is performed on the same or different sets of data. Task parallelism involves the decomposition of a task into sub-tasks and then allocating each sub-task to a processor for execution. The processors would then execute these sub-tasks concurrently and often cooperatively. Task parallelism does not usually scale with the size of a problem.\\n\\n\\n=== Superword level parallelism ===\\nSuperword level parallelism is a vectorization technique based on loop unrolling and basic block vectorization. It is distinct from loop vectorization algorithms in that it can exploit parallelism of inline code, such as manipulating coordinates, color channels or in loops unrolled by hand.\\n\\n\\n== Hardware ==\\n\\n\\n=== Memory and communication ===\\nMain memory in a parallel computer is either shared memory (shared between all processing elements in a single address space), or distributed memory (in which each processing element has its own local address space). Distributed memory refers to the fact that the memory is logically distributed, but often implies that it is physically distributed as well. Distributed shared memory and memory virtualization combine the two approaches, where the processing element has its own local memory and access to the memory on non-local processors. Accesses to local memory are typically faster than accesses to non-local memory. On the supercomputers, distributed shared memory space can be implemented using the programming model such as PGAS.  This model allows processes on one compute node to transparently access the remote memory of another compute node. All compute nodes are also connected to an external shared memory system via high-speed interconnect, such as Infiniband, this external shared memory system is known as burst buffer, which is typically built from arrays of non-volatile memory physically distributed across multiple I/O nodes.\\n\\nComputer architectures in which each element of main memory can be accessed with equal latency and bandwidth are known as uniform memory access (UMA) systems. Typically, that can be achieved only by a shared memory system, in which the memory is not physically distributed. A system that does not have this property is known as a non-uniform memory access (NUMA) architecture. Distributed memory systems have non-uniform memory access.\\nComputer systems make use of caches—small and fast memories located close to the processor which store temporary copies of memory values (nearby in both the physical and logical sense). Parallel computer systems have difficulties with caches that may store the same value in more than one location, with the possibility of incorrect program execution. These computers require a cache coherency system, which keeps track of cached values and strategically purges them, thus ensuring correct program execution. Bus snooping is one of the most common methods for keeping track of which values are being accessed (and thus should be purged). Designing large, high-performance cache coherence systems is a very difficult problem in computer architecture. As a result, shared memory computer architectures do not scale as well as distributed memory systems do.Processor–processor and processor–memory communication can be implemented in hardware in several ways, including via shared (either multiported or multiplexed) memory, a crossbar switch, a shared bus or an interconnect network of a myriad of topologies including star, ring, tree, hypercube, fat hypercube (a hypercube with more than one processor at a node), or n-dimensional mesh.\\nParallel computers based on interconnected networks need to have some kind of routing to enable the passing of messages between nodes that are not directly connected. The medium used for communication between the processors is likely to be hierarchical in large multiprocessor machines.\\n\\n\\n=== Classes of parallel computers ===\\nParallel computers can be roughly classified according to the level at which the hardware supports parallelism. This classification is broadly analogous to the distance between basic computing nodes. These are not mutually exclusive; for example, clusters of symmetric multiprocessors are relatively common.\\n\\n\\n==== Multi-core computing ====\\n\\nA multi-core processor is a processor that includes multiple processing units (called \"cores\") on the same chip. This processor differs from a superscalar processor, which includes multiple execution units and can issue multiple instructions per clock cycle from one instruction stream (thread); in contrast, a multi-core processor can issue multiple instructions per clock cycle from multiple instruction streams. IBM\\'s Cell microprocessor, designed for use in the Sony PlayStation 3, is a prominent multi-core processor. Each core in a multi-core processor can potentially be superscalar as well—that is, on every clock cycle, each core can issue multiple instructions from one thread.\\nSimultaneous multithreading  (of which Intel\\'s Hyper-Threading is the best known) was an early form of pseudo-multi-coreism. A processor capable of concurrent multithreading includes multiple execution units in the same processing unit—that is it has a superscalar architecture—and can issue multiple instructions per clock cycle from multiple threads. Temporal multithreading on the other hand includes a single execution unit in the same processing unit and can issue one instruction at a time from multiple threads.\\n\\n\\n==== Symmetric multiprocessing ====\\n\\nA symmetric multiprocessor (SMP) is a computer system with multiple identical processors that share memory and connect via a bus. Bus contention prevents bus architectures from scaling. As a result, SMPs generally do not comprise more than 32 processors. Because of the small size of the processors and the significant reduction in the requirements for bus bandwidth achieved by large caches, such symmetric multiprocessors are extremely cost-effective, provided that a sufficient amount of memory bandwidth exists.\\n\\n\\n==== Distributed computing ====\\n\\nA distributed computer (also known as a distributed memory multiprocessor) is a distributed memory computer system in which the processing elements are connected by a network. Distributed computers are highly scalable. The terms \"concurrent computing\", \"parallel computing\", and \"distributed computing\" have a lot of overlap, and no clear distinction exists between them. The same system may be characterized both as \"parallel\" and \"distributed\"; the processors in a typical distributed system run concurrently in parallel.\\n\\n\\n===== Cluster computing =====\\n\\nA cluster is a group of loosely coupled computers that work together closely, so that in some respects they can be regarded as a single computer. Clusters are composed of multiple standalone machines connected by a network. While machines in a cluster do not have to be symmetric, load balancing is more difficult if they are not. The most common type of cluster is the Beowulf cluster, which is a cluster implemented on multiple identical commercial off-the-shelf computers connected with a TCP/IP Ethernet local area network. Beowulf technology was originally developed by Thomas Sterling and Donald Becker. 87% of all Top500 supercomputers are clusters. The remaining are Massively Parallel Processors, explained below.\\nBecause grid computing systems (described below) can easily handle embarrassingly parallel problems, modern clusters are typically designed to handle more difficult problems—problems that require nodes to share intermediate results with each other more often. This requires a high bandwidth and, more importantly, a low-latency interconnection network. Many historic and current supercomputers use customized high-performance network hardware specifically designed for cluster computing, such as the Cray Gemini network. As of 2014, most current supercomputers use some off-the-shelf standard network hardware, often Myrinet, InfiniBand, or Gigabit Ethernet.\\n\\n\\n===== Massively parallel computing =====\\n\\nA massively parallel processor (MPP) is a single computer with many networked processors. MPPs have many of the same characteristics as clusters, but MPPs have specialized interconnect networks (whereas clusters use commodity hardware for networking). MPPs also tend to be larger than clusters, typically having \"far more\" than 100 processors. In an MPP, \"each CPU contains its own memory and copy of the operating system and application. Each subsystem communicates with the others via a high-speed interconnect.\"IBM\\'s Blue Gene/L, the fifth fastest supercomputer in the world according to the June 2009 TOP500 ranking, is an MPP.\\n\\n\\n===== Grid computing =====\\n\\nGrid computing is the most distributed form of parallel computing. It makes use of computers communicating over the Internet to work on a given problem. Because of the low bandwidth and extremely high latency available on the Internet, distributed computing typically deals only with embarrassingly parallel problems. Many distributed computing applications have been created, of which SETI@home and Folding@home are the best-known examples.Most grid computing applications use middleware (software that sits between the operating system and the application to manage network resources and standardize the software interface). The most common distributed computing middleware is the Berkeley Open Infrastructure for Network Computing (BOINC). Often, distributed computing software makes use of \"spare cycles\", performing computations at times when a computer is idling.\\n\\n\\n==== Specialized parallel computers ====\\nWithin parallel computing, there are specialized parallel devices that remain niche areas of interest. While not domain-specific, they tend to be applicable to only a few classes of parallel problems.\\n\\n\\n===== Reconfigurable computing with field-programmable gate arrays =====\\nReconfigurable computing is the use of a field-programmable gate array (FPGA) as a co-processor to a general-purpose computer. An FPGA is, in essence, a computer chip that can rewire itself for a given task.\\nFPGAs can be programmed with hardware description languages such as VHDL or Verilog. However, programming in these languages can be tedious. Several vendors have created C to HDL languages that attempt to emulate the syntax and semantics of the C programming language, with which most programmers are familiar. The best known C to HDL languages are Mitrion-C, Impulse C, DIME-C, and Handel-C. Specific subsets of SystemC based on C++ can also be used for this purpose.\\nAMD\\'s decision to open its HyperTransport technology to third-party vendors has become the enabling technology for high-performance reconfigurable computing. According to Michael R. D\\'Amour, Chief Operating Officer of DRC Computer Corporation, \"when we first walked into AMD, they called us \\'the socket stealers.\\' Now they call us their partners.\"\\n\\n\\n===== General-purpose computing on graphics processing units (GPGPU) =====\\n\\nGeneral-purpose computing on graphics processing units (GPGPU) is a fairly recent trend in computer engineering research. GPUs are co-processors that have been heavily optimized for computer graphics processing. Computer graphics processing is a field dominated by data parallel operations—particularly linear algebra matrix operations.\\nIn the early days, GPGPU programs used the normal graphics APIs for executing programs. However, several new programming languages and platforms have been built to do general purpose computation on GPUs with both Nvidia and AMD releasing programming environments with CUDA and Stream SDK respectively. Other GPU programming languages include BrookGPU, PeakStream, and RapidMind. Nvidia has also released specific products for computation in their Tesla series. The technology consortium Khronos Group has released the OpenCL specification, which is a framework for writing programs that execute across platforms consisting of CPUs and GPUs. AMD, Apple, Intel, Nvidia and others are supporting OpenCL.\\n\\n\\n===== Application-specific integrated circuits =====\\n\\nSeveral application-specific integrated circuit (ASIC) approaches have been devised for dealing with parallel applications.Because an ASIC is (by definition) specific to a given application, it can be fully optimized for that application. As a result, for a given application, an ASIC tends to outperform a general-purpose computer. However, ASICs are created by UV photolithography. This process requires a mask set, which can be extremely expensive. A mask set can cost over a million US dollars. (The smaller the transistors required for the chip, the more expensive the mask will be.) Meanwhile, performance increases in general-purpose computing over time (as described by Moore\\'s law) tend to wipe out these gains in only one or two chip generations. High initial cost, and the tendency to be overtaken by Moore\\'s-law-driven general-purpose computing, has rendered ASICs unfeasible for most parallel computing applications. However, some have been built. One example is the PFLOPS RIKEN MDGRAPE-3 machine which uses custom ASICs for molecular dynamics simulation.\\n\\n\\n===== Vector processors =====\\n\\nA vector processor is a CPU or computer system that can execute the same instruction on large sets of data. Vector processors have high-level operations that work on linear arrays of numbers or vectors. An example vector operation is A = B × C, where A, B, and C are each 64-element vectors of 64-bit floating-point numbers. They are closely related to Flynn\\'s SIMD classification.Cray computers became famous for their vector-processing computers in the 1970s and 1980s. However, vector processors—both as CPUs and as full computer systems—have generally disappeared. Modern processor instruction sets do include some vector processing instructions, such as with Freescale Semiconductor\\'s AltiVec and Intel\\'s Streaming SIMD Extensions (SSE).\\n\\n\\n== Software ==\\n\\n\\n=== Parallel programming languages ===\\n\\nConcurrent programming languages, libraries, APIs, and parallel programming models (such as algorithmic skeletons) have been created for programming parallel computers. These can generally be divided into classes based on the assumptions they make about the underlying memory architecture—shared memory, distributed memory, or shared distributed memory. Shared memory programming languages communicate by manipulating shared memory variables. Distributed memory uses message passing. POSIX Threads and OpenMP are two of the most widely used shared memory APIs, whereas Message Passing Interface (MPI) is the most widely used message-passing system API. One concept used in programming parallel programs is the future concept, where one part of a program promises to deliver a required datum to another part of a program at some future time.\\nCAPS entreprise and Pathscale are also coordinating their effort to make hybrid multi-core parallel programming (HMPP) directives an open standard called OpenHMPP. The OpenHMPP directive-based programming model offers a syntax to efficiently offload computations on hardware accelerators and to optimize data movement to/from the hardware memory. OpenHMPP directives describe remote procedure call (RPC) on an accelerator device (e.g. GPU) or more generally a set of cores. The directives annotate C or Fortran codes to describe two sets of functionalities: the offloading of procedures (denoted codelets) onto a remote device and the optimization of data transfers between the CPU main memory and the accelerator memory.\\nThe rise of consumer GPUs has led to support for compute kernels, either in graphics APIs (referred to as compute shaders), in dedicated APIs (such as OpenCL), or in other language extensions.\\n\\n\\n=== Automatic parallelization ===\\n\\nAutomatic parallelization of a sequential program by a compiler is the \"holy grail\" of parallel computing, especially with the aforementioned limit of processor frequency. Despite decades of work by compiler researchers, automatic parallelization has had only limited success.Mainstream parallel programming languages remain either explicitly parallel or (at best) partially implicit, in which a programmer gives the compiler directives for parallelization. A few fully implicit parallel programming languages exist—SISAL, Parallel Haskell, SequenceL, System C (for FPGAs), Mitrion-C, VHDL, and Verilog.\\n\\n\\n=== Application checkpointing ===\\n\\nAs a computer system grows in complexity, the mean time between failures usually decreases. Application checkpointing is a technique whereby the computer system takes a \"snapshot\" of the application—a record of all current resource allocations and variable states, akin to a core dump—; this information can be used to restore the program if the computer should fail. Application checkpointing means that the program has to restart from only its last checkpoint rather than the beginning. While checkpointing provides benefits in a variety of situations, it is especially useful in highly parallel systems with a large number of processors used in high performance computing.\\n\\n\\n== Algorithmic methods ==\\nAs parallel computers become larger and faster, we are now able to solve problems that had previously taken too long to run. Fields as varied as bioinformatics (for protein folding and sequence analysis) and economics (for mathematical finance) have taken advantage of parallel computing. Common types of problems in parallel computing applications include:\\nDense linear algebra\\nSparse linear algebra\\nSpectral methods (such as Cooley–Tukey fast Fourier transform)\\nN-body problems (such as Barnes–Hut simulation)\\nstructured grid problems (such as Lattice Boltzmann methods)\\nUnstructured grid problems (such as found in finite element analysis)\\nMonte Carlo method\\nCombinational logic (such as brute-force cryptographic techniques)\\nGraph traversal (such as sorting algorithms)\\nDynamic programming\\nBranch and bound methods\\nGraphical models (such as detecting hidden Markov models and constructing Bayesian networks)\\nFinite-state machine simulation\\n\\n\\n== Fault tolerance ==\\n\\nParallel computing can also be applied to the design of fault-tolerant computer systems, particularly via lockstep systems performing the same operation in parallel. This provides redundancy in case one component fails, and also allows automatic error detection and error correction if the results differ. These methods can be used to help prevent single-event upsets caused by transient errors. Although additional measures may be required in embedded or specialized systems, this method can provide a cost-effective approach to achieve n-modular redundancy in commercial off-the-shelf systems.\\n\\n\\n== History ==\\n\\nThe origins of true (MIMD) parallelism go back to Luigi Federico Menabrea and his Sketch of the Analytic Engine Invented by Charles Babbage.In April 1958, Stanley Gill (Ferranti) discussed parallel programming and the need for branching and waiting. Also in 1958, IBM researchers John Cocke and Daniel Slotnick discussed the use of parallelism in numerical calculations for the first time. Burroughs Corporation introduced the D825 in 1962, a four-processor computer that accessed up to 16 memory modules through a crossbar switch. In 1967, Amdahl and Slotnick published a debate about the feasibility of parallel processing at American Federation of Information Processing Societies Conference. It was during this debate that Amdahl\\'s law was coined to define the limit of speed-up due to parallelism.\\nIn 1969, Honeywell introduced its first Multics system, a symmetric multiprocessor system capable of running up to eight processors in parallel. C.mmp, a multi-processor project at Carnegie Mellon University in the 1970s, was among the first multiprocessors with more than a few processors. The first bus-connected multiprocessor with snooping caches was the Synapse N+1 in 1984.SIMD parallel computers can be traced back to the 1970s. The motivation behind early SIMD computers was to amortize the gate delay of the processor\\'s control unit over multiple instructions. In 1964, Slotnick had proposed building a massively parallel computer for the Lawrence Livermore National Laboratory. His design was funded by the US Air Force, which was the earliest SIMD parallel-computing effort, ILLIAC IV. The key to its design was a fairly high parallelism, with up to 256 processors, which allowed the machine to work on large datasets in what would later be known as vector processing. However, ILLIAC IV was called \"the most infamous of supercomputers\", because the project was only one-fourth completed, but took 11 years and cost almost four times the original estimate. When it was finally ready to run its first real application in 1976, it was outperformed by existing commercial supercomputers such as the Cray-1.\\n\\n\\n== Biological brain as massively parallel computer ==\\nIn the early 1970s, at the MIT Computer Science and Artificial Intelligence Laboratory, Marvin Minsky and Seymour Papert started developing the Society of Mind theory, which views the biological brain as massively parallel computer. In 1986, Minsky published The Society of Mind, which claims that “mind is formed from many little agents, each mindless by itself”. The theory attempts to explain how what we call intelligence could be a product of the interaction of non-intelligent parts. Minsky says that the biggest source of ideas about the theory came from his work in trying to create a machine that uses a robotic arm, a video camera, and a computer to build with children\\'s blocks.Similar models (which also view the biological brain as a massively parallel computer, i.e., the brain is made up of a constellation of independent or semi-independent agents) were also described by:\\n\\nThomas R. Blakeslee,\\nMichael S. Gazzaniga,\\nRobert E. Ornstein,\\nErnest Hilgard,\\nMichio Kaku,\\nGeorge Ivanovich Gurdjieff,\\nNeurocluster Brain Model.\\n\\n\\n== See also ==\\n\\n\\n== References ==\\n\\n\\n== Further reading ==\\nRodriguez, C.; Villagra, M.; Baran, B. (29 August 2008). \"Asynchronous team algorithms for Boolean Satisfiability\". Bio-Inspired Models of Network, Information and Computing Systems, 2007. Bionetics 2007. 2nd: 66–69. doi:10.1109/BIMNICS.2007.4610083. S2CID 15185219.\\nSechin, A.; Parallel Computing in Photogrammetry. GIM International. #1, 2016, pp. 21–23.\\n\\n\\n== External links ==\\n\\nInstructional videos on CAF in the Fortran Standard by John Reid (see Appendix B)\\nParallel computing at Curlie\\nLawrence Livermore National Laboratory: Introduction to Parallel Computing\\nDesigning and Building Parallel Programs, by Ian Foster\\nInternet Parallel Computing Archive\\nParallel processing topic area at IEEE Distributed Computing Online\\nParallel Computing Works Free On-line Book\\nFrontiers of Supercomputing Free On-line Book Covering topics like algorithms and industrial applications\\nUniversal Parallel Computing Research Center\\nCourse in Parallel Programming at Columbia University (in collaboration with IBM T.J. Watson X10 project)\\nParallel and distributed Gröbner bases computation in JAS, see also Gröbner basis\\nCourse in Parallel Computing at University of Wisconsin-Madison\\nBerkeley Par Lab: progress in the parallel computing landscape, Editors: David Patterson, Dennis Gannon, and Michael Wrinn, August 23, 2013\\nThe trouble with multicore, by David Patterson, posted 30 Jun 2010\\nParallel Computing : A View From Techsevi\\nIntroduction to Parallel Computing\\nCoursera: Parallel Programming\\nParallel Computing : A View From Gyan Grih', 'In computer science, concurrency is the ability of different parts or units of a program, algorithm, or problem to be executed out-of-order or at the same time simultaneously partial order, without affecting the final outcome.  This allows for parallel execution of the concurrent units, which can significantly improve overall speed of the execution in multi-processor and multi-core systems. In more technical terms, concurrency refers to the decomposability of a program, algorithm, or problem into order-independent or partially-ordered components or units of computation.According to Rob Pike, concurrency is the composition of independently executing computations, and concurrency is not parallelism: concurrency is about dealing with lots of things at once but parallelism is about doing lots of things at once. Concurrency is about structure, parallelism is about execution, concurrency provides a way to structure a solution to solve a problem that may (but not necessarily) be parallelizable.A number of mathematical models have been developed for general concurrent computation including Petri nets, process calculi, the parallel random-access machine model, the actor model and the Reo Coordination Language.\\n\\n\\n== History ==\\nAs Leslie Lamport (2015) notes, \"While concurrent program execution had been considered for years, the computer science of concurrency began with Edsger Dijkstra\\'s seminal 1965 paper that introduced the mutual exclusion problem. ... The ensuing decades have seen a huge growth of interest in concurrency—particularly in distributed systems. Looking back at the origins of the field, what stands out is the fundamental role played by Edsger Dijkstra\".\\n\\n\\n== Issues ==\\nBecause computations in a concurrent system can interact with each other while being executed, the number of possible execution paths in the system can be extremely large, and the resulting outcome can be indeterminate. Concurrent use of shared resources can be a source of indeterminacy leading to issues such as deadlocks, and resource starvation.Design of concurrent systems often entails finding reliable techniques for coordinating their execution, data exchange, memory allocation, and execution scheduling to minimize response time and maximise throughput.\\n\\n\\n== Theory ==\\nConcurrency theory has been an active field of research in theoretical computer science.  One of the first proposals was  Carl Adam Petri\\'s seminal work on Petri nets in the early 1960s. In the years since, a wide variety of formalisms have been developed for modeling and reasoning about concurrency.\\n\\n\\n=== Models ===\\nA number of formalisms for modeling and understanding concurrent systems have been developed, including:\\nThe parallel random-access machine\\nThe actor model\\nComputational bridging models such as the bulk synchronous parallel (BSP) model\\nPetri nets\\nProcess calculi\\nCalculus of communicating systems (CCS)\\nCommunicating sequential processes (CSP) model\\nπ-calculus\\nTuple spaces, e.g., Linda\\nSimple Concurrent Object-Oriented Programming (SCOOP)\\nReo Coordination LanguageSome of these models of concurrency are primarily intended to support reasoning and specification, while others can be used through the entire development cycle, including design, implementation, proof, testing and simulation of concurrent systems. Some of these are based on message passing, while others have different mechanisms for concurrency.\\nThe proliferation of different models of concurrency has motivated some researchers to develop ways to unify these different theoretical models. For example, Lee and Sangiovanni-Vincentelli have demonstrated that a so-called \"tagged-signal\" model can be used to provide a common framework for defining the denotational semantics of a variety of different models of concurrency, while Nielsen, Sassone, and Winskel have demonstrated that category theory can be used to provide a similar unified understanding of different models.The Concurrency Representation Theorem in the actor model provides a fairly general way to represent concurrent systems that are closed in the sense that they do not receive communications from outside. (Other concurrency systems, e.g., process calculi can be modeled in the actor model using a two-phase commit protocol.) The mathematical denotation denoted by a closed system S is constructed increasingly better approximations from an initial behavior called ⊥S using a behavior approximating function progressionS to construct a denotation (meaning ) for S as follows:\\nDenoteS ≡ ⊔i∈ω progressionSi(⊥S)In this way, S can be mathematically characterized in terms of all its possible behaviors.\\n\\n\\n=== Logics ===\\nVarious types of temporal logic can be used to help reason about concurrent systems. Some of these logics, such as linear temporal logic and computation tree logic, allow assertions to be made about the sequences of states that a concurrent system can pass through. Others, such as action computational tree logic, Hennessy–Milner logic, and Lamport\\'s temporal logic of actions, build their assertions from sequences of actions (changes in state). The principal application of these logics is in writing specifications for concurrent systems.\\n\\n\\n== Practice ==\\nConcurrent programming encompasses programming languages and algorithms used to implement concurrent systems.  Concurrent programming is usually considered to be more general than parallel programming because it can involve arbitrary and dynamic patterns of communication and interaction, whereas parallel systems generally have a predefined and well-structured communications pattern. The base goals of concurrent programming include correctness, performance and robustness. Concurrent systems such as Operating systems and Database management systems are generally designed to operate indefinitely, including automatic recovery from failure, and not terminate unexpectedly (see Concurrency control). Some concurrent systems implement a form of transparent concurrency, in which concurrent computational entities may compete for and share a single resource, but the complexities of this competition and sharing are shielded from the programmer.\\nBecause they use shared resources, concurrent systems in general require the inclusion of some kind of arbiter somewhere in their implementation (often in the underlying hardware), to control access to those resources. The use of arbiters introduces the possibility of indeterminacy in concurrent computation which has major implications for practice including correctness and performance.  For example, arbitration introduces unbounded nondeterminism which raises issues with model checking because it causes explosion in the state space and can even cause models to have an infinite number of states.\\nSome concurrent programming models include coprocesses and deterministic concurrency. In these models, threads of control explicitly yield their timeslices, either to the system or to another process.\\n\\n\\n== See also ==\\nChu space\\nClient–server network nodes\\nClojure\\nCluster nodes\\nConcurrency control\\nConcurrent computing\\nConcurrent object-oriented programming\\nConcurrency pattern\\nConstruction and Analysis of Distributed Processes (CADP)\\nD (programming language)\\nDistributed systemnodes\\nElixir (programming language)\\nErlang (programming language)\\nGo (programming language)\\nGordon Pask\\nInternational Conference on Concurrency Theory (CONCUR)\\nOpenMP\\nParallel computing\\nPartitioned global address space\\nProcesses\\nPtolemy Project\\nRust (programming language)\\nSheaf (mathematics)\\nThreads\\nX10 (programming language)\\n\\n\\n== References ==\\n\\n\\n== Further reading ==\\nLynch, Nancy A. (1996). Distributed Algorithms. Morgan Kaufmann. ISBN 978-1-55860-348-6.\\nTanenbaum, Andrew S.; Van Steen, Maarten (2002). Distributed Systems: Principles and Paradigms. Prentice Hall. ISBN 978-0-13-088893-8.\\nKurki-Suonio, Reino (2005). A Practical Theory of Reactive Systems. Springer. ISBN 978-3-540-23342-8.\\nGarg, Vijay K. (2002). Elements of Distributed Computing. Wiley-IEEE Press. ISBN 978-0-471-03600-5.\\nMagee, Jeff; Kramer, Jeff (2006). Concurrency: State Models and Java Programming. Wiley. ISBN 978-0-470-09355-9.\\nDistefano, S., & Bruneo, D. (2015). Quantitative assessments of distributed systems: Methodologies and techniques (1st ed.). Somerset: John Wiley & Sons Inc.ISBN 9781119131144\\nBhattacharyya, S. S. (2013;2014;). Handbook of signal processing systems (Second;2;2nd 2013; ed.). New York, NY: Springer.10.1007/978-1-4614-6859-2 ISBN 9781461468592\\nWolter, K. (2012;2014;). Resilience assessment and evaluation of computing systems (1. Aufl.;1; ed.). London;Berlin;: Springer. ISBN 9783642290329\\n\\n\\n== External links ==\\nConcurrent Systems at The WWW Virtual Library\\nConcurrency patterns presentation given at scaleconf', 'Distributed computing is a field of computer science that studies distributed systems. A distributed system is a system whose components are located on different networked computers, which communicate and coordinate their actions by passing messages to one another from any system. The components interact with one another in order to achieve a common goal. Three significant characteristics of distributed systems are: concurrency of components, lack of a global clock, and independent failure of components. It deals with a central challenge that, when components of a system fails, it doesn\\'t imply the entire system fails. Examples of distributed systems vary from SOA-based systems to massively multiplayer online games to peer-to-peer applications.\\nA computer program that runs within a distributed system is called  a distributed program (and distributed programming is the process of writing such programs). There are many different types of implementations for the message passing mechanism, including pure HTTP, RPC-like connectors and message queues.Distributed computing also refers to the use of distributed systems to solve computational problems. In distributed computing, a problem is divided into many tasks, each of which is solved by one or more computers, which communicate with each other via message passing.\\n\\n\\n== Introduction ==\\nThe word distributed in terms such as \"distributed system\", \"distributed programming\", and \"distributed algorithm\" originally referred to computer networks where individual computers were physically distributed within some geographical area. The terms are nowadays used in a much wider sense, even referring to autonomous processes that run on the same physical computer and interact with each other by message passing.While there is no single definition of a distributed system, the following defining properties are commonly used as:\\n\\nThere are several autonomous computational entities (computers or nodes), each of which has its own local memory.\\nThe entities communicate with each other by message passing.A distributed system may have a common goal, such as solving a large computational problem; the user then perceives the collection of autonomous processors as a unit. Alternatively, each computer may have its own user with individual needs, and the purpose of the distributed system is to coordinate the use of shared resources or provide communication services to the users.Other typical properties of distributed systems include the following:\\n\\nThe system has to tolerate failures in individual computers.\\nThe structure of the system (network topology, network latency, number of computers) is not known in advance, the system may consist of different kinds of computers and network links, and the system may change during the execution of a distributed program.\\nEach computer has only a limited, incomplete view of the system. Each computer may know only one part of the input.\\n\\n\\n== Parallel and distributed computing ==\\n\\nDistributed systems are groups of networked computers which share a common goal for their work.\\nThe terms \"concurrent computing\", \"parallel computing\", and \"distributed computing\" have much overlap, and no clear distinction exists between them. The same system may be characterized both as \"parallel\" and \"distributed\"; the processors in a typical distributed system run concurrently in parallel. Parallel computing may be seen as a particular tightly coupled form of distributed computing, and distributed computing may be seen as a loosely coupled form of parallel computing. Nevertheless, it is possible to roughly classify concurrent systems as \"parallel\" or \"distributed\" using the following criteria:\\n\\nIn parallel computing, all processors may have access to a shared memory to exchange information between processors.\\nIn distributed computing, each processor has its own private memory (distributed memory). Information is exchanged by passing messages between the processors.The figure on the right illustrates the difference between distributed and parallel systems. Figure (a) is a schematic view of a typical distributed system; the system is represented as a network topology in which each node is a computer and each line connecting the nodes is a communication link. Figure (b) shows the same distributed system in more detail: each computer has its own local memory, and information can be exchanged only by passing messages from one node to another by using the available communication links. Figure (c) shows a parallel system in which each processor has a direct access to a shared memory.\\nThe situation is further complicated by the traditional uses of the terms parallel and distributed algorithm that do not quite match the above definitions of parallel and distributed systems (see below for more detailed discussion). Nevertheless, as a rule of thumb, high-performance parallel computation in a shared-memory multiprocessor uses parallel algorithms while the coordination of a large-scale distributed system uses distributed algorithms.\\n\\n\\n== History ==\\nThe use of concurrent processes which communicate through message-passing has its roots in operating system architectures studied in the 1960s. The first widespread distributed systems were local-area networks such as Ethernet, which was invented in the 1970s.ARPANET, one of the predecessors of the Internet, was introduced in the late 1960s, and ARPANET e-mail was invented in the early 1970s. E-mail became the most successful application of ARPANET, and it is probably the earliest example of a large-scale distributed application. In addition to ARPANET (and its successor, the global Internet), other early worldwide computer networks included Usenet and FidoNet from the 1980s, both of which were used to support distributed discussion systems.The study of distributed computing became its own branch of computer science in the late 1970s and early 1980s. The first conference in the field, Symposium on Principles of Distributed Computing (PODC), dates back to 1982, and its counterpart International Symposium on Distributed Computing (DISC) was first held in Ottawa in 1985 as the International Workshop on Distributed Algorithms on Graphs.\\n\\n\\n== Architectures ==\\nVarious hardware and software architectures are used for distributed computing. At a lower level, it is necessary to interconnect multiple CPUs with some sort of network, regardless of whether that network is printed onto a circuit board or made up of loosely coupled devices and cables. At a higher level, it is necessary to interconnect processes running on those CPUs with some sort of communication system.Distributed programming typically falls into one of several basic architectures: client–server, three-tier, n-tier, or peer-to-peer; or categories: loose coupling, or tight coupling.\\nClient–server: architectures where smart clients contact the server for data then format and display it to the users. Input at the client is committed back to the server when it represents a permanent change.\\nThree-tier: architectures that move the client intelligence to a middle tier so that stateless clients can be used. This simplifies application deployment. Most web applications are three-tier.\\nn-tier: architectures that refer typically to web applications which further forward their requests to other enterprise services. This type of application is the one most responsible for the success of application servers.\\nPeer-to-peer: architectures where there are no special machines that provide a service or manage the network resources.:\\u200a227\\u200a Instead all responsibilities are uniformly divided among all machines, known as peers. Peers can serve both as clients and as servers. Examples of this architecture include BitTorrent and the bitcoin network.Another basic aspect of distributed computing architecture is the method of communicating and coordinating work among concurrent processes. Through various message passing protocols, processes may communicate directly with one another, typically in a master/slave relationship. Alternatively, a \"database-centric\" architecture can enable distributed computing to be done without any form of direct inter-process communication, by utilizing a shared database. Database-centric architecture in particular provides relational processing analytics in a schematic architecture allowing for live environment relay. This enables distributed computing functions both within and beyond the parameters of a networked database.\\n\\n\\n== Applications ==\\nReasons for using distributed systems and distributed computing may include:\\n\\nThe very nature of an application may require the use of a communication network that connects several computers: for example, data produced in one physical location and required in another location.\\nThere are many cases in which the use of a single computer would be possible in principle, but the use of a distributed system is beneficial for practical reasons. For example, it may be more cost-efficient to obtain the desired level of performance by using a cluster of several low-end computers, in comparison with a single high-end computer. A distributed system can provide more reliability than a non-distributed system, as there is no single point of failure. Moreover, a distributed system may be easier to expand and manage than a monolithic uniprocessor system.\\n\\n\\n== Examples ==\\nExamples of distributed systems and applications of distributed computing include the following:\\ntelecommunication networks:\\ntelephone networks and cellular networks,\\ncomputer networks such as the Internet,\\nwireless sensor networks,\\nrouting algorithms;\\nnetwork applications:\\nWorld Wide Web and peer-to-peer networks,\\nmassively multiplayer online games and virtual reality communities,\\ndistributed databases and distributed database management systems,\\nnetwork file systems,\\ndistributed cache such as burst buffers,\\ndistributed information processing systems such as banking systems and airline reservation systems;\\nreal-time process control:\\naircraft control systems,\\nindustrial control systems;\\nparallel computation:\\nscientific computing, including cluster computing, grid computing, cloud computing, and various volunteer computing projects (see the list of distributed computing projects),\\ndistributed rendering in computer graphics.\\n\\n\\n== Theoretical foundations ==\\n\\n\\n=== Models ===\\nMany tasks that we would like to automate by using a computer are of question–answer type: we would like to ask a question and the computer should produce an answer. In theoretical computer science, such tasks are called computational problems. Formally, a computational problem consists of instances together with a solution for each instance. Instances are questions that we can ask, and solutions are desired answers to these questions.\\nTheoretical computer science seeks to understand which computational problems can be solved by using a computer (computability theory) and how efficiently (computational complexity theory). Traditionally, it is said that a problem can be solved by using a computer if we can design an algorithm that produces a correct solution for any given instance. Such an algorithm can be implemented as a computer program that runs on a general-purpose computer: the program reads a problem instance from input, performs some computation, and produces the solution as output. Formalisms such as random-access machines or universal Turing machines can be used as abstract models of a sequential general-purpose computer executing such an algorithm.The field of concurrent and distributed computing studies similar questions in the case of either multiple computers, or a computer that executes a network of interacting processes: which computational problems can be solved in such a network and how efficiently? However, it is not at all obvious what is meant by \"solving a problem\" in the case of a concurrent or distributed system: for example, what is the task of the algorithm designer, and what is the concurrent or distributed equivalent of a sequential general-purpose computer?The discussion below focuses on the case of multiple computers, although many of the issues are the same for concurrent processes running on a single computer.\\nThree viewpoints are commonly used:\\n\\nParallel algorithms in shared-memory modelAll processors have access to a shared memory. The algorithm designer chooses the program executed by each processor.\\nOne theoretical model is the parallel random-access machines (PRAM) that are used. However, the classical PRAM model assumes synchronous access to the shared memory.\\nShared-memory programs can be extended to distributed systems if the underlying operating system encapsulates the communication between nodes and virtually unifies the memory across all individual systems.\\nA model that is closer to the behavior of real-world multiprocessor machines and takes into account the use of machine instructions, such as Compare-and-swap (CAS), is that of asynchronous shared memory. There is a wide body of work on this model, a summary of which can be found in the literature.Parallel algorithms in message-passing modelThe algorithm designer chooses the structure of the network, as well as the program executed by each computer.\\nModels such as Boolean circuits and sorting networks are used. A Boolean circuit can be seen as a computer network: each gate is a computer that runs an extremely simple computer program. Similarly, a sorting network can be seen as a computer network: each comparator is a computer.Distributed algorithms in message-passing modelThe algorithm designer only chooses the computer program. All computers run the same program. The system must work correctly regardless of the structure of the network.\\nA commonly used model is a graph with one finite-state machine per node.In the case of distributed algorithms, computational problems are typically related to graphs. Often the graph that describes the structure of the computer network is the problem instance. This is illustrated in the following example.\\n\\n\\n=== An example ===\\nConsider the computational problem of finding a coloring of a given graph G. Different fields might take the following approaches:\\n\\nCentralized algorithmsThe graph G is encoded as a string, and the string is given as input to a computer. The computer program finds a coloring of the graph, encodes the coloring as a string, and outputs the result.Parallel algorithmsAgain, the graph G is encoded as a string. However, multiple computers can access the same string in parallel. Each computer might focus on one part of the graph and produce a coloring for that part.\\nThe main focus is on high-performance computation that exploits the processing power of multiple computers in parallel.Distributed algorithmsThe graph G is the structure of the computer network. There is one computer for each node of G and one communication link for each edge of G. Initially, each computer only knows about its immediate neighbors in the graph G; the computers must exchange messages with each other to discover more about the structure of G. Each computer must produce its own color as output.\\nThe main focus is on coordinating the operation of an arbitrary distributed system.While the field of parallel algorithms has a different focus than the field of distributed algorithms, there is much interaction between the two fields. For example, the Cole–Vishkin algorithm for graph coloring was originally presented as a parallel algorithm, but the same technique can also be used directly as a distributed algorithm.\\nMoreover, a parallel algorithm can be implemented either in a parallel system (using shared memory) or in a distributed system (using message passing). The traditional boundary between parallel and distributed algorithms (choose a suitable network vs. run in any given network) does not lie in the same place as the boundary between parallel and distributed systems (shared memory vs. message passing).\\n\\n\\n=== Complexity measures ===\\nIn parallel algorithms, yet another resource in addition to time and space is the number of computers. Indeed, often there is a trade-off between the running time and the number of computers: the problem can be solved faster if there are more computers running in parallel (see speedup). If a decision problem can be solved in polylogarithmic time by using a polynomial number of processors, then the problem is said to be in the class NC. The class NC can be defined equally well by using the PRAM formalism or Boolean circuits—PRAM machines can simulate Boolean circuits efficiently and vice versa.In the analysis of distributed algorithms, more attention is usually paid on communication operations than computational steps. Perhaps the simplest model of distributed computing is a synchronous system where all nodes operate in a lockstep fashion. This model is commonly known as the LOCAL model. During each communication round, all nodes in parallel (1) receive the latest messages from their neighbours, (2) perform arbitrary local computation, and (3) send new messages to their neighbors. In such systems, a central complexity measure is the number of synchronous communication rounds required to complete the task.This complexity measure is closely related to the diameter of the network. Let D be the diameter of the network. On the one hand, any computable problem can be solved trivially in a synchronous distributed system in approximately 2D communication rounds: simply gather all information in one location (D rounds), solve the problem, and inform each node about the solution (D rounds).\\nOn the other hand, if the running time of the algorithm is much smaller than D communication rounds, then the nodes in the network must produce their output without having the possibility to obtain information about distant parts of the network. In other words, the nodes must make globally consistent decisions based on information that is available in their local D-neighbourhood. Many distributed algorithms are known with the running time much smaller than D rounds, and understanding which problems can be solved by such algorithms is one of the central research questions of the field. Typically an algorithm which solves a problem in polylogarithmic time in the network size is considered efficient in this model.\\nAnother commonly used measure is the total number of bits transmitted in the network (cf. communication complexity). The features of this concept are typically captured with the CONGEST(B) model, which similarly defined as the LOCAL model but where single messages can only contain B bits.\\n\\n\\n=== Other problems ===\\nTraditional computational problems take the perspective that the user asks a question, a computer (or a distributed system) processes the question, then produces an answer and stops. However, there are also problems where the system is required not to stop, including the dining philosophers problem and other similar mutual exclusion problems. In these problems, the distributed system is supposed to continuously coordinate the use of shared resources so that no conflicts or deadlocks occur.\\nThere are also fundamental challenges that are unique to distributed computing, for example those related to fault-tolerance. Examples of related problems include consensus problems, Byzantine fault tolerance, and self-stabilisation.Much research is also focused on understanding the asynchronous nature of distributed systems:\\n\\nSynchronizers can be used to run synchronous algorithms in asynchronous systems.\\nLogical clocks provide a causal happened-before ordering of events.\\nClock synchronization algorithms provide globally consistent physical time stamps.\\n\\n\\n=== Election ===\\nCoordinator election (or leader election) is the process of designating a single process as the organizer of some task distributed among several computers (nodes). Before the task is begun, all network nodes are either unaware which node will serve as the \"coordinator\" (or leader) of the task, or unable to communicate with the current coordinator. After a coordinator election algorithm has been run, however, each node throughout the network recognizes a particular, unique node as the task coordinator.The network nodes communicate among themselves in order to decide which of them will get into the \"coordinator\" state. For that, they need some method in order to break the symmetry among them. For example, if each node has unique and comparable identities, then the nodes can compare their identities, and decide that the node with the highest identity is the coordinator.The definition of this problem is often attributed to LeLann, who formalized it as a method to create a new token in a token ring network in which the token has been lost.Coordinator election algorithms are designed to be economical in terms of total bytes transmitted, and time. The algorithm suggested by Gallager, Humblet, and Spira  for general undirected graphs has had a strong impact on the design of distributed algorithms in general, and won the Dijkstra Prize for an influential paper in distributed computing.\\nMany other algorithms were suggested for different kind of network graphs, such as undirected rings, unidirectional rings, complete graphs, grids, directed Euler graphs, and others. A general method that decouples the issue of the graph family from the design of the coordinator election algorithm was suggested by Korach, Kutten, and Moran.In order to perform coordination, distributed systems employ the concept of coordinators. The coordinator election problem is to choose a process from among a group of processes on different processors in a distributed system to act as the central coordinator. Several central coordinator election algorithms exist.\\n\\n\\n=== Properties of distributed systems ===\\nSo far the focus has been on designing a distributed system that solves a given problem. A complementary research problem is studying the properties of a given distributed system.The halting problem is an analogous example from the field of centralised computation: we are given a computer program and the task is to decide whether it halts or runs forever. The halting problem is undecidable in the general case, and naturally understanding the behaviour of a computer network is at least as hard as understanding the behaviour of one computer.However, there are many interesting special cases that are decidable. In particular, it is possible to reason about the behaviour of a network of finite-state machines. One example is telling whether a given network of interacting (asynchronous and non-deterministic) finite-state machines can reach a deadlock. This problem is PSPACE-complete, i.e., it is decidable, but not likely that there is an efficient (centralised, parallel or distributed) algorithm that solves the problem in the case of large networks.\\n\\n\\n== See also ==\\n\\n\\n== Notes ==\\n\\n\\n== References ==\\nBooksAndrews, Gregory R. (2000), Foundations of Multithreaded, Parallel, and Distributed Programming, Addison–Wesley, ISBN 978-0-201-35752-3.\\nArora, Sanjeev; Barak, Boaz (2009), Computational Complexity – A Modern Approach, Cambridge, ISBN 978-0-521-42426-4.\\nCormen, Thomas H.; Leiserson, Charles E.; Rivest, Ronald L. (1990), Introduction to Algorithms (1st ed.), MIT Press, ISBN 978-0-262-03141-7.\\nDolev, Shlomi (2000), Self-Stabilization, MIT Press, ISBN 978-0-262-04178-2.\\nElmasri, Ramez; Navathe, Shamkant B. (2000), Fundamentals of Database Systems (3rd ed.), Addison–Wesley, ISBN 978-0-201-54263-9.\\nGhosh, Sukumar (2007), Distributed Systems – An Algorithmic Approach, Chapman & Hall/CRC, ISBN 978-1-58488-564-1.\\nLynch, Nancy A. (1996), Distributed Algorithms, Morgan Kaufmann, ISBN 978-1-55860-348-6.\\nHerlihy, Maurice P.; Shavit, Nir N. (2008), The Art of Multiprocessor Programming, Morgan Kaufmann, ISBN 978-0-12-370591-4.\\nPapadimitriou, Christos H. (1994), Computational Complexity, Addison–Wesley, ISBN 978-0-201-53082-7.\\nPeleg, David (2000), Distributed Computing: A Locality-Sensitive Approach, SIAM, ISBN 978-0-89871-464-7, archived from the original on 2009-08-06, retrieved 2009-07-16.ArticlesCole, Richard; Vishkin, Uzi (1986), \"Deterministic coin tossing with applications to optimal parallel list ranking\", Information and Control, 70 (1): 32–53, doi:10.1016/S0019-9958(86)80023-7.\\nKeidar, Idit (2008), \"Distributed computing column 32 – The year in review\", ACM SIGACT News, 39 (4): 53–54, CiteSeerX 10.1.1.116.1285, doi:10.1145/1466390.1466402.\\nLinial, Nathan (1992), \"Locality in distributed graph algorithms\", SIAM Journal on Computing, 21 (1): 193–201, CiteSeerX 10.1.1.471.6378, doi:10.1137/0221015.\\nNaor, Moni; Stockmeyer, Larry (1995), \"What can be computed locally?\" (PDF), SIAM Journal on Computing, 24 (6): 1259–1277, CiteSeerX 10.1.1.29.669, doi:10.1137/S0097539793254571.Web sitesGodfrey, Bill (2002). \"A primer on distributed computing\".\\nPeter, Ian (2004). \"Ian Peter\\'s History of the Internet\". Retrieved 2009-08-04.\\n\\n\\n== Further reading ==\\nBooksAttiya, Hagit and Jennifer Welch (2004), Distributed Computing: Fundamentals, Simulations, and Advanced Topics, Wiley-Interscience ISBN 0-471-45324-2.\\nChristian Cachin; Rachid Guerraoui; Luís Rodrigues (2011), Introduction to Reliable and Secure Distributed Programming (2. ed.), Springer, Bibcode:2011itra.book.....C, ISBN 978-3-642-15259-7\\nCoulouris, George;  et al. (2011), Distributed Systems: Concepts and Design (5th Edition), Addison-Wesley ISBN 0-132-14301-1.\\nFaber, Jim (1998), Java Distributed Computing, O\\'Reilly: Java Distributed Computing by Jim Faber, 1998\\nGarg, Vijay K. (2002), Elements of Distributed Computing, Wiley-IEEE Press ISBN 0-471-03600-5.\\nTel, Gerard (1994), Introduction to Distributed Algorithms, Cambridge University Press\\nChandy, Mani;  et al., Parallel Program Design\\nDusseau, Remzi H.; Dusseau, Andrea (2016). Operating Systems: Three Easy Pieces, Chapter 48 Distributed Systems (PDF). Archived from the original (PDF) on 31 August 2021. Retrieved 8 October 2021.ArticlesKeidar, Idit; Rajsbaum, Sergio, eds. (2000–2009), \"Distributed computing column\", ACM SIGACT News.\\nBirrell, A. D.; Levin, R.; Schroeder, M. D.; Needham, R. M. (April 1982). \"Grapevine: An exercise in distributed computing\" (PDF). Communications of the ACM. 25 (4): 260–274. doi:10.1145/358468.358487. S2CID 16066616.Conference PapersRodriguez, Carlos; Villagra, Marcos; Baran, Benjamin (2007). \"Asynchronous team algorithms for Boolean Satisfiability\". 2007 2nd Bio-Inspired Models of Network, Information and Computing Systems. pp. 66–69. doi:10.1109/BIMNICS.2007.4610083. S2CID 15185219.\\n\\n\\n== External links ==\\nDistributed computing at Curlie\\nDistributed computing journals at Curlie', 'A relational database is a digital database based on the relational model of data, as proposed by E. F. Codd in 1970. A system used to maintain relational databases is a relational database management system (RDBMS). Many relational database systems have an option of using the SQL (Structured Query Language) for querying and maintaining the database.\\n\\n\\n== History ==\\nThe term \"relational database\" was invented by E. F. Codd at IBM in 1970. Codd introduced the term in his research paper \"A Relational Model of Data for Large Shared Data Banks\".\\nIn this paper and later papers, he defined what he meant by \"relational\". One well-known definition of what constitutes a relational database system is composed of Codd\\'s 12 rules. However, no commercial implementations of the relational model conform to all of Codd\\'s rules, so the term has gradually come to describe a broader class of database systems, which at a minimum:\\n\\nPresent the data to the user as relations (a presentation in tabular form, i.e. as a collection of tables with each table consisting of a set of rows and columns);\\nProvide relational operators to manipulate the data in tabular form.In 1974, IBM began developing System R, a research project to develop a prototype RDBMS.\\nThe first system sold as an RDBMS was Multics Relational Data Store (June 1976). Oracle was released in 1979 by Relational Software, now Oracle Corporation. Ingres and IBM BS12 followed. Other examples of an RDBMS include DB2, SAP Sybase ASE, and Informix. In 1984, the first RDBMS for Macintosh began being developed, code-named Silver Surfer, and was released in 1987 as 4th Dimension and known today as 4D.The first systems that were relatively faithful implementations of the relational model were from:\\n\\nUniversity of Michigan – Micro DBMS (1969)\\nMassachusetts Institute of Technology (1971)\\nIBM UK Scientific Centre at Peterlee – IS1 (1970–72) and its successor, PRTV (1973–79)The most common definition of an RDBMS is a product that presents a view of data as a collection of rows and columns, even if it is not based strictly upon relational theory. By this definition, RDBMS products typically implement some but not all of Codd\\'s 12 rules.\\nA second school of thought argues that if a database does not implement all of Codd\\'s rules (or the current understanding on the relational model, as expressed by Christopher J. Date, Hugh Darwen and others), it is not relational. This view, shared by many theorists and other strict adherents to Codd\\'s principles, would disqualify most DBMSs as not relational. For clarification, they often refer to some RDBMSs as truly-relational database management systems (TRDBMS), naming others pseudo-relational database management systems (PRDBMS). \\nAs of 2009, most commercial relational DBMSs employ SQL as their query language.Alternative query languages have been proposed and implemented, notably the pre-1996 implementation of Ingres QUEL.\\n\\n\\n== Relational model ==\\n\\nThis model organizes data into one or more tables (or \"relations\") of columns and rows, with a unique key identifying each row. Rows are also called records or tuples. Columns are also called attributes. Generally, each table/relation represents one \"entity type\" (such as customer or product). The rows represent instances of that type of entity (such as \"Lee\" or \"chair\") and the columns representing values attributed to that instance (such as address or price).\\nFor example, each row of a class table corresponds to a class, and a class corresponds to multiple students, so the relationship between the class table and the student table is \"one to many\"\\n\\n\\n== Keys ==\\nEach row in a table has its own unique key. Rows in a table can be linked to rows in other tables by adding a column for the unique key of the linked row (such columns are known as foreign keys). Codd showed that data relationships of arbitrary complexity can be represented by a simple set of concepts.Part of this processing involves consistently being able to select or modify one and only one row in a table. Therefore, most physical implementations have a unique primary key (PK) for each row in a table. When a new row is written to the table, a new unique value for the primary key is generated; this is the key that the system uses primarily for accessing the table. System performance is optimized for PKs. Other, more natural keys may also be identified and defined as alternate keys (AK). Often several columns are needed to form an AK (this is one reason why a single integer column is usually made the PK). Both PKs and AKs have the ability to uniquely identify a row within a table. Additional technology may be applied to ensure a unique ID across the world, a globally unique identifier, when there are broader system requirements.\\nThe primary keys within a database are used to define the relationships among the tables. When a PK migrates to another table, it becomes a foreign key in the other table. When each cell can contain only one value and the PK migrates into a regular entity table, this design pattern can represent either a one-to-one or one-to-many relationship. Most relational database designs resolve many-to-many relationships by creating an additional table that contains the PKs from both of the other entity tables –  the relationship becomes an entity; the resolution table is then named appropriately and the two FKs are combined to form a PK. The migration of PKs to other tables is the second major reason why system-assigned integers are used normally as PKs; there is usually neither efficiency nor clarity in migrating a bunch of other types of columns.\\n\\n\\n=== Relationships ===\\nRelationships are a logical connection between different tables, established on the basis of interaction among these tables.\\n\\n\\n== Transactions ==\\nIn order for a database management system (DBMS) to operate efficiently and accurately, it must use ACID transactions.\\n\\n\\n== Stored procedures ==\\nMost of the programming within a RDBMS is accomplished using stored procedures (SPs). Often procedures can be used to greatly reduce the amount of information transferred within and outside of a system. For increased security, the system design may grant access to only the stored procedures and not directly to the tables. Fundamental stored procedures contain the logic needed to insert new and update existing data. More complex procedures may be written to implement additional rules and logic related to processing or selecting the data.\\n\\n\\n== Terminology ==\\n\\nThe relational database was first defined in June 1970 by Edgar Codd, of IBM\\'s San Jose Research Laboratory. Codd\\'s view of what qualifies as an RDBMS is summarized in Codd\\'s 12 rules. A relational database has become the predominant type of database. Other models besides the relational model include the hierarchical database model and the network model.\\nThe table below summarizes some of the most important relational database terms and the corresponding SQL term:\\n\\n\\n== Relations or tables ==\\n\\nA relation is defined as a set of tuples that have the same attributes. A tuple usually represents an object and information about that object. Objects are typically physical objects or concepts. A relation is usually described as a table, which is organized into rows and columns. All the data referenced by an attribute are in the same domain and conform to the same constraints.\\nThe relational model specifies that the tuples of a relation have no specific order and that the tuples, in turn, impose no order on the attributes. Applications access data by specifying queries, which use operations such as select to identify tuples, project to identify attributes, and join to combine relations. Relations can be modified using the insert, delete, and update operators. New tuples can supply explicit values or be derived from a query. Similarly, queries identify tuples for updating or deleting.\\nTuples by definition are unique. If the tuple contains a candidate or primary key then obviously it is unique; however, a primary key need not be defined for a row or record to be a tuple. The definition of a tuple requires that it be unique, but does not require a primary key to be defined. Because a tuple is unique, its attributes by definition constitute a superkey.\\n\\n\\n== Base and derived relations ==\\n\\nIn a relational database, all data are stored and accessed via relations. Relations that store data are called \"base relations\", and in implementations are called \"tables\". Other relations do not store data, but are computed by applying relational operations to other relations. These relations are sometimes called \"derived relations\". In implementations these are called \"views\" or \"queries\". Derived relations are convenient in that they act as a single relation, even though they may grab information from several relations. Also, derived relations can be used as an abstraction layer.\\n\\n\\n=== Domain ===\\n\\nA domain describes the set of possible values for a given attribute, and can be considered a constraint on the value of the attribute. Mathematically, attaching a domain to an attribute means that any value for the attribute must be an element of the specified set. The character string \"ABC\", for instance, is not in the integer domain, but the integer value 123 is. Another example of domain describes the possible values for the field \"CoinFace\" as (\"Heads\",\"Tails\"). So, the field \"CoinFace\" will not accept input values like (0,1) or (H,T).\\n\\n\\n== Constraints ==\\nConstraints make it possible to further restrict the domain of an attribute. For instance, a constraint can restrict a given integer attribute to values between 1 and 10. Constraints provide one method of implementing business rules in the database and support subsequent data use within the application layer. SQL implements constraint functionality in the form of check constraints.\\nConstraints restrict the data that can be stored in relations. These are usually defined using expressions that result in a boolean value, indicating whether or not the data satisfies the constraint. Constraints can apply to single attributes, to a tuple (restricting combinations of attributes) or to an entire relation.\\nSince every attribute has an associated domain, there are constraints (domain constraints). The two principal rules for the relational model are known as entity integrity and referential integrity.\\n\\n\\n=== Primary key ===\\n\\nEach relation/table has a primary key, this being a consequence of a relation being a set. A primary key uniquely specifies a tuple within a table. While natural attributes (attributes used to describe the data being entered) are sometimes good primary keys, surrogate keys are often used instead. A surrogate key is an artificial attribute assigned to an object which uniquely identifies it (for instance, in a table of information about students at a school they might all be assigned a student ID in order to differentiate them). The surrogate key has no intrinsic (inherent) meaning, but rather is useful through its ability to uniquely identify a tuple.\\nAnother common occurrence, especially in regard to N:M cardinality is the composite key. A composite key is a key made up of two or more attributes within a table that (together) uniquely identify a record.\\n\\n\\n=== Foreign key ===\\n\\nA foreign key is a field in a relational table that matches the primary key column of another table. It relates the two keys. Foreign keys need not have unique values in the referencing relation. A foreign key can be used to cross-reference tables, and it effectively uses the values of attributes in the referenced relation to restrict the domain of one or more attributes in the referencing relation. The concept is described formally as: \"For all tuples in the referencing relation projected over the referencing attributes, there must exist a tuple in the referenced relation projected over those same attributes such that the values in each of the referencing attributes match the corresponding values in the referenced attributes.\"\\n\\n\\n=== Stored procedures ===\\n\\nA stored procedure is executable code that is associated with, and generally stored in, the database. Stored procedures usually collect and customize common operations, like inserting a tuple into a relation, gathering statistical information about usage patterns, or encapsulating complex business logic and calculations. Frequently they are used as an application programming interface (API) for security or simplicity. Implementations of stored procedures on SQL RDBMS\\'s often allow developers to take advantage of procedural extensions (often vendor-specific) to the standard declarative SQL syntax.\\nStored procedures are not part of the relational database model, but all commercial implementations include them.\\n\\n\\n=== Index ===\\n\\nAn index is one way of providing quicker access to data. Indices can be created on any combination of attributes on a relation. Queries that filter using those attributes can find matching tuples directly using the index (similar to Hash table lookup), without having to check each tuple in turn. This is analogous to using the index of a book to go directly to the page on which the information you are looking for is found, so that you do not have to read the entire book to find what you are looking for. Relational databases typically supply multiple indexing techniques, each of which is optimal for some combination of data distribution, relation size, and typical access pattern. Indices are usually implemented via B+ trees, R-trees, and bitmaps.\\nIndices are usually not considered part of the database, as they are considered an implementation detail, though indices are usually maintained by the same group that maintains the other parts of the database. The use of efficient indexes on both primary and foreign keys can dramatically improve query performance. This is because B-tree indexes result in query times proportional to log(n) where n is the number of rows in a table and hash indexes result in constant time queries (no size dependency as long as the relevant part of the index fits into memory).\\n\\n\\n== Relational operations ==\\n\\nQueries made against the relational database, and the derived relvars in the database are expressed in a relational calculus or a relational algebra. In his original relational algebra, Codd introduced eight relational operators in two groups of four operators each. The first four operators were based on the traditional mathematical set operations:\\n\\nThe union operator (υ) combines the tuples of two relations and removes all duplicate tuples from the result. The relational union operator is equivalent to the SQL UNION operator.\\nThe intersection operator (∩) produces the set of tuples that two relations share in common. Intersection is implemented in SQL in the form of the INTERSECT operator.\\nThe set difference operator (-) acts on two relations and produces the set of tuples from the first relation that do not exist in the second relation. Difference is implemented in SQL in the form of the EXCEPT or MINUS operator.\\nThe cartesian product (X) of two relations is a join that is not restricted by any criteria, resulting in every tuple of the first relation being matched with every tuple of the second relation. The cartesian product is implemented in SQL as the Cross join operator.The remaining operators proposed by Codd involve special operations specific to relational databases:\\n\\nThe selection, or restriction, operation (σ) retrieves tuples from a relation, limiting the results to only those that meet a specific criterion, i.e. a subset in terms of set theory. The SQL equivalent of selection is the SELECT query statement with a WHERE clause.\\nThe projection operation (π) extracts only the specified attributes from a tuple or set of tuples.\\nThe join operation defined for relational databases is often referred to as a natural join (⋈). In this type of join, two relations are connected by their common attributes. MySQL\\'s approximation of a natural join is the Inner join operator. In SQL, an INNER JOIN prevents a cartesian product from occurring when there are two tables in a query. For each table added to a SQL Query, one additional INNER JOIN is added to prevent a cartesian product. Thus, for N tables in an SQL query, there must be N−1 INNER JOINS to prevent a cartesian product.\\nThe relational division (÷) operation is a slightly more complex operation and essentially involves using the tuples of one relation (the dividend) to partition a second relation (the divisor). The relational division operator is effectively the opposite of the cartesian product operator (hence the name).Other operators have been introduced or proposed since Codd\\'s introduction of the original eight including relational comparison operators and extensions that offer support for nesting and hierarchical data, among others.\\n\\n\\n== Normalization ==\\n\\nNormalization was first proposed by Codd as an integral part of the relational model. It encompasses a set of procedures designed to eliminate non-simple domains (non-atomic values) and the redundancy (duplication) of data, which in turn prevents data manipulation anomalies and loss of data integrity. The most common forms of normalization applied to databases are called the normal forms.\\n\\n\\n== RDBMS ==\\n\\nConnolly and Begg define Database Management System (DBMS) as a \"software system that enables users to define, create, maintain and control access to the database\". RDBMS is an extension of that acronym that is sometimes used when the underlying database is relational.\\nAn alternative definition for a relational database management system is a database management system (DBMS) based on the relational model. Most databases in widespread use today are based on this model.RDBMSs have been a common option for the storage of information in databases used for financial records, manufacturing and logistical information, personnel data, and other applications since the 1980s. Relational databases have often replaced legacy hierarchical databases and network databases, because RDBMS were easier to implement and administer. Nonetheless, relational stored data received continued, unsuccessful challenges by object database management systems in the 1980s and 1990s, (which were introduced in an attempt to address the so-called object–relational impedance mismatch between relational databases and object-oriented application programs), as well as by XML database management systems in the 1990s. However, due to the expanse of technologies, such as horizontal scaling of computer clusters, NoSQL databases have recently become popular as an alternative to RDBMS databases.\\n\\n\\n== Distributed relational databases ==\\nDistributed Relational Database Architecture (DRDA) was designed by a workgroup within IBM in the period 1988 to 1994. DRDA enables network connected relational databases to cooperate to fulfill SQL requests.\\nThe messages, protocols, and structural components of DRDA are defined by the Distributed Data Management Architecture.\\n\\n\\n== Market share ==\\nAccording to DB-Engines, in March 2021, the most widely used systems were:\\nOracle Database\\nMySQL\\nMicrosoft SQL Server\\nPostgreSQL (free software)\\nIBM Db2\\nSQLite (free software)\\nMicrosoft Access\\nMariaDB (free software)\\nHive (free software; specialized for data warehouses).\\nTeradata\\nMicrosoft Azure SQL DatabaseAccording to research company Gartner, in 2011, the five leading proprietary software relational database vendors by revenue were Oracle (48.8%), IBM (20.2%), Microsoft (17.0%), SAP including Sybase (4.6%), and Teradata (3.7%).\\n\\n\\n== See also ==\\nSQL\\nObject database (OODBMS)\\nOnline analytical processing (OLAP) and ROLAP (Relational Online Analytical Processing)\\nData warehouse\\nStar schema\\nSnowflake schema\\nList of relational database management systems\\nComparison of relational database management systems\\n\\n\\n== References ==\\n\\n\\n== Sources ==\\nDate, C. J. (1984). A Guide to DB2 (student ed.). Addison-Wesley. ISBN 0201113171. OCLC 256383726. OL 2838595M.\\n\\n\\n== External links ==', 'Set theory is the branch of mathematical logic that studies sets, which can be informally described as collections of objects. Although objects of any kind can be collected into a set, set theory, as a branch of mathematics, is mostly concerned with those that are relevant to mathematics as a whole.\\nThe modern study of set theory was initiated by the German mathematicians Richard Dedekind and Georg Cantor in the 1870s. In particular, Georg Cantor is commonly considered the founder of set theory. The non-formalized systems investigated during this early stage go under the name of naive set theory. After the discovery of paradoxes within naive set theory (such as Russell\\'s paradox, Cantor\\'s paradox and Burali-Forti paradox) various axiomatic systems were proposed in the early twentieth century, of which Zermelo–Fraenkel set theory (with or without the axiom of choice) is still the best-known and most studied.\\nSet theory is commonly employed as a foundational system for the whole of mathematics, particularly in the form of Zermelo–Fraenkel set theory with the axiom of choice. Beside its foundational role, set theory also provides the framework to develop a mathematical theory of infinity, and has various applications in computer science (such as in the theory of relational algebra), philosophy and formal semantics. Its foundational appeal, together with its paradoxes, its implications for the concept of infinity and its multiple applications, have made set theory an area of major interest for logicians and philosophers of mathematics. Contemporary research into set theory covers a vast array of topics, ranging from the structure of the real number line to the study of the consistency of large cardinals.\\n\\n\\n== History ==\\n\\nMathematical topics typically emerge and evolve through interactions among many researchers. Set theory, however, was founded by a single paper in 1874 by Georg Cantor: \"On a Property of the Collection of All Real Algebraic Numbers\".Since the 5th century BC, beginning with Greek mathematician Zeno of Elea in the West and early Indian mathematicians in the East, mathematicians had struggled with the concept of infinity. Especially notable is the work of Bernard Bolzano in the first half of the 19th century. Modern understanding of infinity began in 1870–1874, and was motivated by Cantor\\'s work in real analysis. An 1872 meeting between Cantor and Richard Dedekind influenced Cantor\\'s thinking, and culminated in Cantor\\'s 1874 paper.\\nCantor\\'s work initially polarized the mathematicians of his day. While Karl Weierstrass and Dedekind supported Cantor, Leopold Kronecker, now seen as a founder of mathematical constructivism, did not. Cantorian set theory eventually became widespread, due to the utility of Cantorian concepts, such as one-to-one correspondence among sets, his proof that there are more real numbers than integers, and the \"infinity of infinities\" (\"Cantor\\'s paradise\") resulting from the power set operation. This utility of set theory led to the article \"Mengenlehre\", contributed in 1898 by Arthur Schoenflies to Klein\\'s encyclopedia.\\nThe next wave of excitement in set theory came around 1900, when it was discovered that some interpretations of Cantorian set theory gave rise to several contradictions, called antinomies or paradoxes. Bertrand Russell and Ernst Zermelo independently found the simplest and best known paradox, now called Russell\\'s paradox: consider \"the set of all sets that are not members of themselves\", which leads to a contradiction since it must be a member of itself and not a member of itself. In 1899, Cantor had himself posed the question \"What is the cardinal number of the set of all sets?\", and obtained a related paradox. Russell used his paradox as a theme in his 1903 review of continental mathematics in his The Principles of Mathematics. Rather than the term set, Russell used the term Class, which has subsequently been used more technically.\\nIn 1906, the term set appeared in the book Theory of Sets of Points by husband and wife William Henry Young and Grace Chisholm Young, published by Cambridge University Press.\\nThe momentum of set theory was such that debate on the paradoxes did not lead to its abandonment. The work of Zermelo in 1908 and the work of Abraham Fraenkel and Thoralf Skolem in 1922 resulted in the set of axioms ZFC, which became the most commonly used set of axioms for set theory. The work of analysts, such as that of Henri Lebesgue, demonstrated the great mathematical utility of set theory, which has since become woven into the fabric of modern mathematics. Set theory is commonly used as a foundational system, although in some areas—such as algebraic geometry and algebraic topology—category theory is thought to be a preferred foundation.\\n\\n\\n== Basic concepts and notation ==\\n\\nSet theory begins with a fundamental binary relation between an object o and a set A. If o is a member (or element) of A, the notation o ∈ A is used. A set is described by listing elements separated by commas, or by a characterizing property of its elements, within braces { }. Since sets are objects, the membership relation can relate sets as well.\\nA derived binary relation between two sets is the subset relation, also called set inclusion. If all the members of set A are also members of set B, then A is a subset of B, denoted A ⊆ B. For example, {1, 2} is a subset of {1, 2, 3}, and so is {2} but {1, 4} is not. As implied by this definition, a set is a subset of itself. For cases where this possibility is unsuitable or would make sense to be rejected, the term proper subset is defined. A is called a proper subset of B if and only if A is a subset of B, but A is not equal to B. Also, 1, 2, and 3 are members (elements) of the set {1, 2, 3}, but are not subsets of it; and in turn, the subsets, such as {1}, are not members of the set {1, 2, 3}.\\nJust as arithmetic features binary operations on numbers, set theory features binary operations on sets. The following is a partial list of them:\\n\\nUnion of the sets A and B, denoted A ∪ B, is the set of all objects that are a member of A, or B, or both. For example, the union of {1, 2, 3} and {2, 3, 4} is the set {1, 2, 3, 4}.\\nIntersection of the sets A and B, denoted A ∩ B, is the set of all objects that are members of both A and B. For example, the intersection of {1, 2, 3} and {2, 3, 4} is the set {2, 3}.\\nSet difference of U and A, denoted U \\\\ A, is the set of all members of U that are not members of A. The set difference {1, 2, 3} \\\\ {2, 3, 4}  is {1}, while conversely, the set difference {2, 3, 4} \\\\ {1, 2, 3} is {4}. When A is a subset of U, the set difference U \\\\ A is also called the complement of A in U. In this case, if the choice of U is clear from the context, the notation Ac is sometimes used instead of U \\\\ A, particularly if U is a universal set as in the study of Venn diagrams.\\nSymmetric difference of sets A and B, denoted A △ B or A ⊖ B, is the set of all objects that are a member of exactly one of A and B (elements which are in one of the sets, but not in both). For instance, for the sets {1, 2, 3} and {2, 3, 4}, the symmetric difference set is {1, 4}. It is the set difference of the union and the intersection, (A ∪ B) \\\\ (A ∩ B) or (A \\\\ B) ∪ (B \\\\ A).\\nCartesian product of A and B, denoted A × B, is the set whose members are all possible ordered pairs (a, b), where a is a member of A and b is a member of B. For example, the Cartesian product of {1, 2} and {red, white} is {(1, red), (1, white), (2, red), (2, white)}.\\nPower set of a set A, denoted \\n  \\n    \\n      \\n        \\n          \\n            P\\n          \\n        \\n        (\\n        A\\n        )\\n      \\n    \\n    {\\\\displaystyle {\\\\mathcal {P}}(A)}\\n  , is the set whose members are all of the possible subsets of A. For example, the power set of {1, 2} is { {}, {1}, {2}, {1, 2} }.Some basic sets of central importance are the set of natural numbers, the set of real numbers and the empty set—the unique set containing no elements. The empty set is also occasionally called the null set, though this name is ambiguous and can lead to several interpretations.\\n\\n\\n== Some ontology ==\\n\\nA set is pure if all of its members are sets, all members of its members are sets, and so on. For example, the set {{}} containing only the empty set is a nonempty pure set. In modern set theory, it is common to restrict attention to the von Neumann universe of pure sets, and many systems of axiomatic set theory are designed to axiomatize the pure sets only. There are many technical advantages to this restriction, and little generality is lost, because essentially all mathematical concepts can be modeled by pure sets. Sets in the von Neumann universe are organized into a cumulative hierarchy, based on how deeply their members, members of members, etc. are nested. Each set in this hierarchy is assigned (by transfinite recursion) an ordinal number \\n  \\n    \\n      \\n        α\\n      \\n    \\n    {\\\\displaystyle \\\\alpha }\\n  , known as its rank. The rank of a pure set \\n  \\n    \\n      \\n        X\\n      \\n    \\n    {\\\\displaystyle X}\\n   is defined to be the least ordinal that is strictly greater than the rank of any of its elements. For example, the empty set is assigned rank 0, while the set  {{}}  containing only the empty set is assigned rank 1. For each ordinal \\n  \\n    \\n      \\n        α\\n      \\n    \\n    {\\\\displaystyle \\\\alpha }\\n  , the set \\n  \\n    \\n      \\n        \\n          V\\n          \\n            α\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle V_{\\\\alpha }}\\n   is defined to consist of all pure sets with rank less than \\n  \\n    \\n      \\n        α\\n      \\n    \\n    {\\\\displaystyle \\\\alpha }\\n  . The entire von Neumann universe is denoted \\n  \\n    \\n      \\n        V\\n      \\n    \\n    {\\\\displaystyle V}\\n  .\\n\\n\\n== Formalized set theory ==\\nElementary set theory can be studied informally and intuitively, and so can be taught in primary schools using Venn diagrams. The intuitive approach tacitly assumes that a set may be formed from the class of all objects satisfying any particular defining condition. This assumption gives rise to paradoxes, the simplest and best known of which are Russell\\'s paradox and the Burali-Forti paradox. Axiomatic set theory was originally devised to rid set theory of such paradoxes.The most widely studied systems of axiomatic set theory imply that all sets form a cumulative hierarchy. Such systems come in two flavors, those whose ontology consists of:\\n\\nSets alone. This includes the most common axiomatic set theory, Zermelo–Fraenkel set theory with the Axiom of Choice (ZFC). Fragments of ZFC include:\\nZermelo set theory, which replaces the axiom schema of replacement with that of separation;\\nGeneral set theory, a small fragment of Zermelo set theory sufficient for the Peano axioms and finite sets;\\nKripke–Platek set theory, which omits the axioms of infinity, powerset, and choice, and weakens the axiom schemata of separation and replacement.\\nSets and proper classes. These include Von Neumann–Bernays–Gödel set theory, which has the same strength as ZFC for theorems about sets alone, and Morse–Kelley set theory and Tarski–Grothendieck set theory, both of which are stronger than ZFC.The above systems can be modified to allow urelements, objects that can be members of sets but that are not themselves sets and do not have any members.\\nThe New Foundations systems of NFU (allowing urelements) and NF (lacking them) are not based on a cumulative hierarchy. NF and NFU include a \"set of everything\", relative to which every set has a complement. In these systems urelements matter, because NF, but not NFU, produces sets for which the axiom of choice does not hold.\\nSystems of constructive set theory, such as CST, CZF, and IZF, embed their set axioms in intuitionistic instead of classical logic. Yet other systems accept classical logic but feature a nonstandard membership relation. These include rough set theory and fuzzy set theory, in which the value of an atomic formula embodying the membership relation is not simply True or False. The Boolean-valued models of ZFC are a related subject.\\nAn enrichment of ZFC called internal set theory was proposed by Edward Nelson in 1977.\\n\\n\\n== Applications ==\\nMany mathematical concepts can be defined precisely using only set theoretic concepts. For example, mathematical structures as diverse as graphs, manifolds, rings, vector spaces, and relational algebras can all be defined as sets satisfying various (axiomatic) properties. Equivalence and order relations are ubiquitous in mathematics, and the theory of mathematical relations can be described in set theory.Set theory is also a promising foundational system for much of mathematics. Since the publication of the first volume of Principia Mathematica, it has been claimed that most (or even all) mathematical theorems can be derived using an aptly designed set of axioms for set theory, augmented with many definitions, using first or second-order logic. For example, properties of the natural and real numbers can be derived within set theory, as each number system can be identified with a set of equivalence classes under a suitable equivalence relation whose field is some infinite set.Set theory as a foundation for mathematical analysis, topology, abstract algebra, and discrete mathematics is likewise uncontroversial; mathematicians accept (in principle) that theorems in these areas can be derived from the relevant definitions and the axioms of set theory. However, it remains that few full derivations of complex mathematical theorems from set theory have been formally verified, since such formal derivations are often much longer than the natural language proofs mathematicians commonly present. One verification project, Metamath, includes human-written, computer-verified derivations of more than 12,000 theorems starting from ZFC set theory, first-order logic and propositional logic.\\n\\n\\n== Areas of study ==\\nSet theory is a major area of research in mathematics, with many interrelated subfields.\\n\\n\\n=== Combinatorial set theory ===\\n\\nCombinatorial set theory concerns extensions of finite combinatorics to infinite sets. This includes the study of cardinal arithmetic and the study of extensions of Ramsey\\'s theorem such as the Erdős–Rado theorem. Double extension set theory (DEST) is an axiomatic set theory proposed by Andrzej Kisielewicz consisting of two separate membership relations on the universe of sets.\\n\\n\\n=== Descriptive set theory ===\\n\\nDescriptive set theory is the study of subsets of the real line and, more generally, subsets of Polish spaces. It begins with the study of pointclasses in the Borel hierarchy and extends to the study of more complex hierarchies such as the projective hierarchy and the Wadge hierarchy. Many properties of Borel sets can be established in ZFC, but proving these properties hold for more complicated sets requires additional axioms related to determinacy and large cardinals.\\nThe field of effective descriptive set theory is between set theory and recursion theory. It includes the study of lightface pointclasses, and is closely related to hyperarithmetical theory. In many cases, results of classical descriptive set theory have effective versions; in some cases, new results are obtained by proving the effective version first and then extending (\"relativizing\") it to make it more broadly applicable.\\nA recent area of research concerns Borel equivalence relations and more complicated definable equivalence relations. This has important applications to the study of invariants in many fields of mathematics.\\n\\n\\n=== Fuzzy set theory ===\\n\\nIn set theory as Cantor defined and Zermelo and Fraenkel axiomatized, an object is either a member of a set or not. In fuzzy set theory this condition was relaxed by Lotfi A. Zadeh so an object has a degree of membership in a set, a number between 0 and 1. For example, the degree of membership of a person in the set of \"tall people\" is more flexible than a simple yes or no answer and can be a real number such as 0.75.\\n\\n\\n=== Inner model theory ===\\n\\nAn inner model of Zermelo–Fraenkel set theory (ZF) is a transitive class that includes all the ordinals and satisfies all the axioms of ZF. The canonical example is the constructible universe L developed by Gödel.\\nOne reason that the study of inner models is of interest is that it can be used to prove consistency results. For example, it can be shown that regardless of whether a model V of ZF satisfies the continuum hypothesis or the axiom of choice, the inner model L constructed inside the original model will satisfy both the generalized continuum hypothesis and the axiom of choice. Thus the assumption that ZF is consistent (has at least one model) implies that ZF together with these two principles is consistent.\\nThe study of inner models is common in the study of determinacy and large cardinals, especially when considering axioms such as the axiom of determinacy that contradict the axiom of choice. Even if a fixed model of set theory satisfies the axiom of choice, it is possible for an inner model to fail to satisfy the axiom of choice. For example, the existence of sufficiently large cardinals implies that there is an inner model satisfying the axiom of determinacy (and thus not satisfying the axiom of choice).\\n\\n\\n=== Large cardinals ===\\n\\nA large cardinal is a cardinal number with an extra property. Many such properties are studied, including inaccessible cardinals, measurable cardinals, and many more. These properties typically imply the cardinal number must be very large, with the existence of a cardinal with the specified property unprovable in Zermelo–Fraenkel set theory.\\n\\n\\n=== Determinacy ===\\n\\nDeterminacy refers to the fact that, under appropriate assumptions, certain two-player games of perfect information are determined from the start in the sense that one player must have a winning strategy. The existence of these strategies has important consequences in descriptive set theory, as the assumption that a broader class of games is determined often implies that a broader class of sets will have a topological property. The axiom of determinacy (AD) is an important object of study; although incompatible with the axiom of choice, AD implies that all subsets of the real line are well behaved (in particular, measurable and with the perfect set property). AD can be used to prove that the Wadge degrees have an elegant structure.\\n\\n\\n=== Forcing ===\\n\\nPaul Cohen invented the method of forcing while searching for a model of ZFC in which the continuum hypothesis fails, or a model of ZF in which the axiom of choice fails. Forcing adjoins to some given model of set theory additional sets in order to create a larger model with properties determined (i.e. \"forced\") by the construction and the original model. For example, Cohen\\'s construction adjoins additional subsets of the natural numbers without changing any of the cardinal numbers of the original model. Forcing is also one of two methods for proving relative consistency by finitistic methods, the other method being Boolean-valued models.\\n\\n\\n=== Cardinal invariants ===\\n\\nA cardinal invariant is a property of the real line measured by a cardinal number. For example, a well-studied invariant is the smallest cardinality of a collection of meagre sets of reals whose union is the entire real line. These are invariants in the sense that any two isomorphic models of set theory must give the same cardinal for each invariant. Many cardinal invariants have been studied, and the relationships between them are often complex and related to axioms of set theory.\\n\\n\\n=== Set-theoretic topology ===\\n\\nSet-theoretic topology studies questions of general topology that are set-theoretic in nature or that require advanced methods of set theory for their solution. Many of these theorems are independent of ZFC, requiring stronger axioms for their proof. A famous problem is the normal Moore space question, a question in general topology that was the subject of intense research. The answer to the normal Moore space question was eventually proved to be independent of ZFC.\\n\\n\\n== Objections to set theory ==\\nFrom set theory\\'s inception, some mathematicians have objected to it as a foundation for mathematics. The most common objection to set theory, one Kronecker voiced in set theory\\'s earliest years, starts from the constructivist view that mathematics is loosely related to computation. If this view is granted, then the treatment of infinite sets, both in naive and in axiomatic set theory, introduces into mathematics methods and objects that are not computable even in principle. The feasibility of constructivism as a substitute foundation for mathematics was greatly increased by Errett Bishop\\'s influential book Foundations of Constructive Analysis.A different objection put forth by Henri Poincaré is that defining sets using the axiom schemas of specification and replacement, as well as the axiom of power set, introduces impredicativity, a type of circularity, into the definitions of mathematical objects. The scope of predicatively founded mathematics, while less than that of the commonly accepted Zermelo–Fraenkel theory, is much greater than that of constructive mathematics, to the point that Solomon Feferman has said that \"all of scientifically applicable analysis can be developed [using predicative methods]\".Ludwig Wittgenstein condemned set theory philosophically for its connotations of Mathematical platonism.  He wrote that \"set theory is wrong\", since it builds on the \"nonsense\" of fictitious symbolism, has \"pernicious idioms\", and that it is nonsensical to talk about \"all numbers\".  Wittgenstein identified mathematics with algorithmic human deduction; the need for a secure foundation for mathematics seemed, to him, nonsensical.  Moreover, since human effort is necessarily finite, Wittgenstein\\'s philosophy required an ontological commitment to radical constructivism and finitism.  Meta-mathematical statements — which, for Wittgenstein, included any statement quantifying over infinite domains, and thus almost all modern set theory — are not mathematics.  Few modern philosophers have adopted Wittgenstein\\'s views after a spectacular blunder in Remarks on the Foundations of Mathematics: Wittgenstein attempted to refute Gödel\\'s incompleteness theorems after having only read the abstract.  As reviewers Kreisel, Bernays, Dummett, and Goodstein all pointed out, many of his critiques did not apply to the paper in full.  Only recently have philosophers such as Crispin Wright begun to rehabilitate Wittgenstein\\'s arguments.Category theorists have proposed topos theory as an alternative to traditional axiomatic set theory. Topos theory can interpret various alternatives to that theory, such as constructivism, finite set theory, and computable set theory. Topoi also give a natural setting for forcing and discussions of the independence of choice from ZF, as well as providing the framework for pointless topology and Stone spaces.An active area of research is the univalent foundations and related to it homotopy type theory. Within homotopy type theory, a set may be regarded as a homotopy 0-type, with universal properties of sets arising from the inductive and recursive properties of higher inductive types. Principles such as the axiom of choice and the law of the excluded middle can be formulated in a manner corresponding to the classical formulation in set theory or perhaps in a spectrum of distinct ways unique to type theory. Some of these principles may be proven to be a consequence of other principles. The variety of formulations of these axiomatic principles allows for a detailed analysis of the formulations required in order to derive various mathematical results.\\n\\n\\n== Set theory in mathematical education ==\\nAs set theory gained popularity as a foundation for modern mathematics, there has been support for the idea of introducing the basics of naive set theory early in mathematics education.\\nIn the US in the 1960s, the New Math experiment aimed to teach basic set theory, among other abstract concepts, to primary school students, but was met with much criticism. The math syllabus in European schools followed this trend, and currently includes the subject at different levels in all grades. Venn diagrams are widely employed to explain basic set-theoretic relationships to primary school students (even though John Venn originally devised them as part of a procedure to assess the validity of inferences in term logic).\\nSet theory is used to introduce students to logical operators (NOT, AND, OR), and semantic or rule description (technically intensional definition) of sets (e.g. \"months starting with the letter A\"), which may be useful when learning computer programming, since boolean logic is used in various programming languages. Likewise, sets and other collection-like objects, such as multisets and lists, are common datatypes in computer science and programming.\\nIn addition to that, sets are commonly referred to in mathematical teaching when talking about different types of numbers (N, Z, R, ...), and when defining a mathematical function as a relation from one set (the domain) to another set (the range).\\n\\n\\n== See also ==\\n\\nGlossary of set theory\\nClass (set theory)\\nList of set theory topics\\nRelational model – borrows from set theory\\n\\n\\n== Notes ==\\n\\n\\n== References ==\\n\\n\\n== Further reading ==\\nDevlin, Keith (1993), The Joy of Sets (2nd ed.), Springer Verlag, ISBN 0-387-94094-4\\nFerreirós, Jose (2007), Labyrinth of Thought: A history of set theory and its role in modern mathematics, Basel: Birkhäuser, ISBN 978-3-7643-8349-7\\nJohnson, Philip (1972), A History of Set Theory, Prindle, Weber & Schmidt, ISBN 0-87150-154-6\\nKunen, Kenneth (1980), Set Theory: An Introduction to Independence Proofs, North-Holland, ISBN 0-444-85401-0\\nPotter, Michael (2004), Set Theory and Its Philosophy: A Critical Introduction, Oxford University Press\\nTiles, Mary (2004), The Philosophy of Set Theory: An Historical Introduction to Cantor\\'s Paradise, Dover Publications, ISBN 978-0-486-43520-6\\nSmullyan, Raymond M.; Fitting, Melvin (2010), Set Theory And The Continuum Problem, Dover Publications, ISBN 978-0-486-47484-7\\nMonk, J. Donald (1969), Introduction to Set Theory, McGraw-Hill Book Company, ISBN 978-0898740066\\n\\n\\n== External links ==\\n\\nDaniel Cunningham, Set Theory article in the Internet Encyclopedia of Philosophy.\\nJose Ferreiros, The Early Development of Set Theory article in the [Stanford Encyclopedia of Philosophy].\\nForeman, Matthew, Akihiro Kanamori, eds. Handbook of Set Theory. 3 vols., 2010. Each chapter surveys some aspect of contemporary research in set theory. Does not cover established elementary set theory, on which see Devlin (1993).\\n\"Axiomatic set theory\", Encyclopedia of Mathematics, EMS Press, 2001 [1994]\\n\"Set theory\", Encyclopedia of Mathematics, EMS Press, 2001 [1994]\\nSchoenflies, Arthur (1898). Mengenlehre in Klein\\'s encyclopedia.\\nOnline books, and library resources in your library and in other libraries about set theory\\nRudin, Walter B. (April 6, 1990). \"Set Theory: An Offspring of Analysis\". Marden Lecture in Mathematics. University of Wisconsin-Milwaukee. Archived from the original on 2021-10-31 – via YouTube.', \"Structured storage is computer storage for structured data, often in the form of a distributed database. Computer software formally known as structured storage systems include Apache Cassandra, Google's Bigtable and Apache HBase.\\n\\n\\n== Comparison ==\\nThe following is a comparison of notable structured storage systems.\\n\\n\\n== See also ==\\nNoSQL\\n\\n\\n== References ==\", 'A NoSQL (originally referring to \"non-SQL\" or \"non-relational\") database provides a mechanism for storage and retrieval of data that is modeled in means other than the tabular relations used in relational databases. Such databases have existed since the late 1960s, but the name \"NoSQL\" was only coined in the early 21st century, triggered by the needs of Web 2.0 companies. NoSQL databases are increasingly used in big data and real-time web applications. NoSQL systems are also sometimes called \"Not only SQL\" to emphasize that they may support SQL-like query languages or sit alongside SQL databases in polyglot-persistent architectures.Motivations for this approach include simplicity of design, simpler \"horizontal\" scaling to clusters of machines (which is a problem for relational databases), finer control over availability and limiting the object-relational impedance mismatch. The data structures used by NoSQL databases (e.g. key–value pair, wide column, graph, or document) are different from those used by default in relational databases, making some operations faster in NoSQL. The particular suitability of a given NoSQL database depends on the problem it must solve.  Sometimes the data structures used by NoSQL databases are also viewed as \"more flexible\" than relational database tables.Many NoSQL stores compromise consistency (in the sense of the CAP theorem) in favor of availability, partition tolerance, and speed. Barriers to the greater adoption of NoSQL stores include the use of low-level query languages (instead of SQL, for instance), lack of ability to perform ad hoc joins across tables, lack of standardized interfaces, and huge previous investments in existing relational databases. Most NoSQL stores lack true ACID transactions, although a few databases have made them central to their designs.\\nInstead, most NoSQL databases offer a concept of \"eventual consistency\", in which database changes are propagated to all nodes \"eventually\" (typically within milliseconds), so queries for data might not return updated data immediately or might result in reading data that is not accurate, a problem known as stale reads. Additionally, some NoSQL systems may exhibit lost writes and other forms of data loss. Some NoSQL systems provide concepts such as write-ahead logging to avoid data loss. For distributed transaction processing across multiple databases, data consistency is an even bigger challenge that is difficult for both NoSQL and relational databases. Relational databases \"do not allow referential integrity constraints to span databases\". Few systems maintain both ACID transactions and X/Open XA standards for distributed transaction processing. Interactive relational databases share conformational relay analysis techniques as a common feature. Limitations within the interface environment are overcome using semantic virtualization protocols, such that NoSQL services are accessible to most operating systems.\\n\\n\\n== History ==\\nThe term NoSQL was used by Carlo Strozzi in 1998 to name his lightweight Strozzi NoSQL open-source relational database that did not expose the standard Structured Query Language (SQL) interface, but was still relational. His NoSQL RDBMS is distinct from the around-2009 general concept of NoSQL databases.  Strozzi suggests that, because the current NoSQL movement \"departs from the relational model altogether, it should therefore have been called more appropriately \\'NoREL\\'\", referring to \"not relational\".\\nJohan Oskarsson, then a developer at Last.fm, reintroduced the term NoSQL in early 2009 when he organized an event to discuss \"open-source distributed, non-relational databases\". The name attempted to label the emergence of an increasing number of non-relational, distributed data stores, including open source clones of Google\\'s Bigtable/MapReduce and Amazon\\'s DynamoDB.\\n\\n\\n== Types and examples ==\\nThere are various ways to classify NoSQL databases, with different categories and subcategories, some of which overlap. What follows is a basic classification by data model, with examples:\\n\\nWide column: Azure Cosmos DB, Accumulo, Cassandra, ScyllaDB, HBase.\\nDocument: Azure Cosmos DB, Apache CouchDB, ArangoDB, BaseX, Clusterpoint, Couchbase, eXist-db, IBM Domino, MarkLogic, MongoDB, OrientDB, Qizx, RethinkDB\\nKey–value: Azure Cosmos DB, Aerospike, Apache Ignite, ArangoDB, Berkeley DB, Couchbase, Dynamo, FoundationDB, InfinityDB, MemcacheDB, MUMPS, Oracle NoSQL Database, OrientDB, Redis, Riak, SciDB, SDBM/Flat File dbm, ZooKeeper\\nGraph: Azure Cosmos DB, AllegroGraph, ArangoDB, InfiniteGraph, Apache Giraph, MarkLogic, Neo4J, AgensGraph, OrientDB, VirtuosoA more detailed classification is the following, based on one from Stephen Yen:\\nCorrelation databases are model-independent, and instead of row-based or column-based storage, use value-based storage.\\n\\n\\n=== Key–value store ===\\n\\nKey–value (KV) stores use the associative array (also called a map or dictionary) as their fundamental data model. In this model, data is represented as a collection of key–value pairs, such that each possible key appears at most once in the collection.The key–value model is one of the simplest non-trivial data models, and richer data models are often implemented as an extension of it. The key–value model can be extended to a discretely ordered model that maintains keys in lexicographic order. This extension is computationally powerful, in that it can efficiently retrieve selective key ranges.Key–value stores can use consistency models ranging from eventual consistency to serializability. Some databases support ordering of keys. There are various hardware implementations, and some users store data in memory (RAM), while others on solid-state drives (SSD) or rotating disks (aka hard disk drive (HDD)).\\n\\n\\n=== Document store ===\\n\\nThe central concept of a document store is that of a \"document\". While the details of this definition differ among document-oriented databases, they all assume that documents encapsulate and encode data (or information) in some standard formats or encodings. Encodings in use include XML, YAML, and JSON and binary forms like BSON. Documents are addressed in the database via a unique key that represents that document. Another defining characteristic of a document-oriented database is an API or query language to retrieve documents based on their contents.\\nDifferent implementations offer different ways of organizing and/or grouping documents:\\n\\nCollections\\nTags\\nNon-visible metadata\\nDirectory hierarchiesCompared to relational databases, collections could be considered analogous to tables and documents analogous to records. But they are different: every record in a table has the same sequence of fields, while documents in a collection may have fields that are completely different.\\n\\n\\n=== Graph ===\\n\\nGraph databases are designed for data whose relations are well represented as a graph consisting of elements connected by a finite number of relations. Examples of data include social relations, public transport links, road maps, network topologies, etc.\\n\\nGraph databases and their query language\\n\\n\\n=== Object database ===\\n\\ndb4o\\nGemStone/S\\nInterSystems Caché\\nJADE\\nObjectDatabase++\\nObjectDB\\nObjectivity/DB\\nObjectStore\\nODABA\\nPerst\\nRealm\\nOpenLink Virtuoso\\nVersant Object Database\\nZODB\\n\\n\\n=== Tabular ===\\nApache Accumulo\\nBigtable\\nApache Hbase\\nHypertable\\nMnesia\\nOpenLink Virtuoso\\n\\n\\n=== Tuple store ===\\nApache River\\nGigaSpaces\\nTarantool\\nTIBCO ActiveSpaces\\nOpenLink Virtuoso\\n\\n\\n=== Triple/quad store (RDF) database ===\\n\\nAllegroGraph\\nApache JENA (It is a framework, not a database)\\nMarkLogic\\nOntotext-OWLIM\\nOracle NoSQL database\\nProfium Sense\\nVirtuoso Universal Server\\n\\n\\n=== Hosted ===\\nAzure Cosmos DB\\nAmazon DynamoDB\\nAmazon DocumentDB\\nAmazon SimpleDB\\nClusterpoint database\\nCloudant Data Layer (CouchDB)\\nFreebase\\nGoogle Cloud Datastore\\nMicrosoft Azure Storage services\\nOpenLink Virtuoso\\nMongoDB Atlas\\n\\n\\n=== Multivalue databases ===\\nD3 Pick database\\nExtensible Storage Engine (ESE/NT)\\nInfinityDB\\nInterSystems Caché\\njBASE Pick database\\nmvBase Rocket Software\\nmvEnterprise Rocket Software\\nNorthgate Information Solutions Reality, the original Pick/MV Database\\nOpenQM\\nRevelation Software\\'s OpenInsight (Windows) and Advanced Revelation (DOS)\\nUniData Rocket U2\\nUniVerse Rocket U2\\n\\n\\n=== Multimodel database ===\\nAzure Cosmos DB\\nApache Ignite\\nArangoDB\\nCouchbase\\nFoundationDB\\nMarkLogic\\nOrientDB\\nOracle Database\\n\\n\\n== Performance ==\\nThe performance of NoSQL databases is usually evaluated using the metric of throughput, which is measured as operations/second. Performance evaluation must pay attention to the right benchmarks such as production configurations, parameters of the databases, anticipated data volume, and concurrent user workloads.\\nBen Scofield rated different categories of NoSQL databases as follows:\\nPerformance and scalability comparisons are most commonly done using the YCSB benchmark.\\n\\n\\n== Handling relational data ==\\nSince most NoSQL databases lack ability for joins in queries, the database schema generally needs to be designed differently. There are three main techniques for handling relational data in a NoSQL database. (See table Join and ACID Support for NoSQL databases that support joins.)\\n\\n\\n=== Multiple queries ===\\nInstead of retrieving all the data with one query, it is common to do several queries to get the desired data. NoSQL queries are often faster than traditional SQL queries so the cost of additional queries may be acceptable. If an excessive number of queries would be necessary, one of the other two approaches is more appropriate.\\n\\n\\n=== Caching, replication and non-normalized data ===\\nInstead of only storing foreign keys, it is common to store actual foreign values along with the model\\'s data. For example, each blog comment might include the username in addition to a user id, thus providing easy access to the username without requiring another lookup. When a username changes however, this will now need to be changed in many places in the database. Thus this approach works better when reads are much more common than writes.\\n\\n\\n=== Nesting data ===\\nWith document databases like MongoDB it is common to put more data in a smaller number of collections. For example, in a blogging application, one might choose to store comments within the blog post document so that with a single retrieval one gets all the comments. Thus in this approach a single document contains all the data you need for a specific task.\\n\\n\\n== ACID and join support ==\\nA database is marked as supporting ACID properties (Atomicity, Consistency, Isolation, Durability) or join operations if the documentation for the database makes that claim. However, this doesn\\'t necessarily mean that the capability is fully supported in a manner similar to most SQL databases.\\n\\n\\n== See also ==\\nCAP theorem\\nComparison of object database management systems\\nComparison of structured storage software\\nCorrelation database\\nC++\\nDatabase scalability\\nDistributed cache\\nFaceted search\\nMultiValue database\\nMulti-model database\\nTriplestore\\nSchema-agnostic databases\\n\\n\\n== References ==\\n\\n\\n== Further reading ==\\nSadalage, Pramod; Fowler, Martin (2012). NoSQL Distilled: A Brief Guide to the Emerging World of Polyglot Persistence. Addison-Wesley. ISBN 978-0-321-82662-6.\\nMcCreary, Dan; Kelly, Ann (2013). Making Sense of NoSQL: A guide for managers and the rest of us. ISBN 9781617291074.\\nWiese, Lena (2015). Advanced Data Management for SQL, NoSQL, Cloud and Distributed Databases. DeGruyter/Oldenbourg. ISBN 978-3-11-044140-6.\\nStrauch, Christof (2012). \"NoSQL Databases\" (PDF).\\nMoniruzzaman, A. B.; Hossain, S. A. (2013). \"NoSQL Database: New Era of Databases for Big data Analytics - Classification, Characteristics and Comparison\". arXiv:1307.0191. Bibcode:2013arXiv1307.0191M. \\nOrend, Kai (2013). \"Analysis and Classification of NoSQL Databases and Evaluation of their Ability to Replace an Object-relational Persistence Layer\". CiteSeerX 10.1.1.184.483. \\nKrishnan, Ganesh; Kulkarni, Sarang; Dadbhawala, Dharmesh Kirit. \"Method and system for versioned sharing, consolidating and reporting information\".\\n\\n\\n== External links ==\\nStrauch, Christoph. \"NoSQL whitepaper\" (PDF). Stuttgart: Hochschule der Medien.\\nEdlich, Stefan. \"NoSQL database List\".\\nNeubauer, Peter (2010). \"Graph Databases, NOSQL and Neo4j\".\\nBushik, Sergey (2012). \"A vendor-independent comparison of NoSQL databases: Cassandra, HBase, MongoDB, Riak\". NetworkWorld.\\nZicari, Roberto V. (2014). \"NoSQL Data Stores – Articles, Papers, Presentations\". odbms.org.', 'In computing, a compiler is a computer program that translates computer code written in one programming language (the source language) into another language (the target language). The name \"compiler\" is primarily used for programs that translate source code from a high-level programming language to a lower level language (e.g. assembly language, object code, or machine code) to create an executable program.:\\u200ap1\\u200aThere are many different types of compilers which produce output in different useful forms. A cross-compiler  produces code for a different CPU or operating system than the one on which the cross-compiler itself runs. A bootstrap compiler is written in the language that it intends to compile. A program that translates from a low-level language to a higher level one is a decompiler. A program that translates between high-level languages is usually called a source-to-source compiler or transpiler. A language rewriter is usually a program that translates the form of expressions without a change of language. A compiler-compiler is a compiler that produces a compiler (or part of one).\\nA compiler is likely to perform some or all of the following operations, often called phases: preprocessing, lexical analysis, parsing, semantic analysis (syntax-directed translation), conversion of input programs to an intermediate representation, code optimization and code generation. Compilers generally implement these phases as modular components, promoting efficient design and correctness of transformations of source input to target output. Program faults caused by incorrect compiler behavior can be very difficult to track down and work around; therefore, compiler implementers invest significant effort to ensure compiler correctness.Compilers are not the only language processor used to transform source programs. An interpreter is computer software that transforms and then executes the indicated operations.:\\u200ap2\\u200a The translation process influences the design of computer languages, which leads to a preference of compilation or interpretation. In theory, a programming language can have both a compiler and an interpreter. In practice, programming languages tend to be associated with just one (a compiler or an interpreter).\\n\\n\\n== History ==\\n\\nTheoretical computing concepts developed by scientists, mathematicians, and engineers formed the basis of digital modern computing development during World War II. Primitive binary languages evolved because digital devices only understand ones and zeros and the circuit patterns in the underlying machine architecture. In the late 1940s, assembly languages were created to offer a more workable abstraction of the computer architectures. Limited memory capacity of early computers led to substantial technical challenges when the first compilers were designed. Therefore, the compilation process needed to be divided into several small programs. The front end programs produce the analysis products used by the back end programs to generate target code. As computer technology provided more resources, compiler designs could align better with the compilation process.\\nIt is usually more productive for a programmer to use a high-level language, so the development of high-level languages followed naturally from the capabilities offered by digital computers. High-level languages are formal languages that are strictly defined by their syntax and semantics which form the high-level language architecture. Elements of these formal languages include:\\n\\nAlphabet, any finite set of symbols;\\nString, a finite sequence of symbols;\\nLanguage, any set of strings on an alphabet.The sentences in a language may be defined by a set of rules called a grammar.Backus–Naur form (BNF) describes the syntax of \"sentences\" of a language and was used for the syntax of Algol 60 by John Backus. The ideas derive from the context-free grammar concepts by Noam Chomsky, a linguist. \"BNF and its extensions have become standard tools for describing the syntax of programming notations, and in many cases parts of compilers are generated automatically from a BNF description.\"In the 1940s, Konrad Zuse designed an algorithmic programming language called Plankalkül (\"Plan Calculus\"). While no actual implementation occurred until the 1970s, it presented concepts later seen in APL designed by Ken Iverson in the late 1950s. APL is a language for mathematical computations. \\nHigh-level language design during the formative years of digital computing provided useful programming tools for a variety of applications:\\n\\nFORTRAN (Formula Translation) for engineering and science applications is considered to be the first high-level language.\\nCOBOL (Common Business-Oriented Language) evolved from A-0 and FLOW-MATIC to become the dominant high-level language for business applications.\\nLISP (List Processor) for symbolic computation.Compiler technology evolved from the need for a strictly defined transformation of the high-level source program into a low-level target program for the digital computer. The compiler could be viewed as a front end to deal with the analysis of the source code and a back end to synthesize the analysis into the target code. Optimization between the front end and back end could produce more efficient target code.Some early milestones in the development of compiler technology:\\n\\n1952: An Autocode compiler developed by Alick Glennie for the Manchester Mark I computer at the University of Manchester is considered by some to be the first compiled programming language.\\n1952: Grace Hopper\\'s team at Remington Rand wrote the compiler for the A-0 programming language (and coined the term compiler to describe it), although the A-0 compiler functioned more as a loader or linker than the modern notion of a full compiler.\\n1954–1957: A team led by John Backus at IBM developed FORTRAN which is usually considered the first high-level language. In 1957, they completed a FORTRAN compiler that is generally credited as having introduced the first unambiguously complete compiler.\\n1959: The Conference on Data Systems Language (CODASYL) initiated development of COBOL. The COBOL design drew on A-0 and FLOW-MATIC. By the early 1960s COBOL was compiled on multiple architectures.\\n1958–1962: John McCarthy at MIT designed LISP. The symbol processing capabilities provided useful features for artificial intelligence research. In 1962, LISP 1.5 release noted some tools: an interpreter written by Stephen Russell and Daniel J. Edwards, a compiler and assembler written by Tim Hart and Mike Levin.Early operating systems and software were written in assembly language. In the 1960s and early 1970s, the use of high-level languages for system programming was still controversial due to resource limitations. However, several research and industry efforts began the shift toward high-level systems programming languages, for example, BCPL, BLISS, B, and C.\\nBCPL (Basic Combined Programming Language) designed in 1966 by Martin Richards at the University of Cambridge was originally developed as a compiler writing tool. Several compilers have been implemented, Richards\\' book provides insights to the language and its compiler. BCPL was not only an influential systems programming language that is still used in research but also provided a basis for the design of B and C languages.\\nBLISS (Basic Language for Implementation of System Software) was developed for a Digital Equipment Corporation (DEC) PDP-10 computer by W.A. Wulf\\'s Carnegie Mellon University (CMU) research team. The CMU team went on to develop BLISS-11 compiler one year later in 1970.\\nMultics (Multiplexed Information and Computing Service), a time-sharing operating system project, involved MIT, Bell Labs, General Electric (later Honeywell) and was led by Fernando Corbató from MIT. Multics was written in the PL/I language developed by IBM and IBM User Group. IBM\\'s goal was to satisfy business, scientific, and systems programming requirements. There were other languages that could have been considered but PL/I offered the most complete solution even though it had not been implemented. For the first few years of the Multics project, a subset of the language could be compiled to assembly language with the Early PL/I (EPL) compiler by Doug McIlory and Bob Morris from Bell Labs. EPL supported the project until a boot-strapping compiler for the full PL/I could be developed.Bell Labs left the Multics project in 1969: \"Over time, hope was replaced by frustration as the group effort initially failed to produce an economically useful system.\" Continued participation would drive up project support costs. So researchers turned to other development efforts. A system programming language B based on BCPL concepts was written by Dennis Ritchie and Ken Thompson. Ritchie created a boot-strapping compiler for B and wrote Unics (Uniplexed Information and Computing Service) operating system for a PDP-7 in B. Unics eventually became spelled Unix.\\nBell Labs started the development and expansion of C based on B and BCPL. The BCPL compiler had been transported to Multics by Bell Labs and BCPL was a preferred language at Bell Labs. Initially, a front-end program to Bell Labs\\' B compiler was used while a C compiler was developed. In 1971, a new PDP-11 provided the resource to define extensions to B and rewrite the compiler. By 1973 the design of C language was essentially complete and the Unix kernel for a PDP-11 was rewritten in C. Steve Johnson started development of Portable C Compiler (PCC) to support retargeting of C compilers to new machines.Object-oriented programming (OOP) offered some interesting possibilities for application development and maintenance. OOP concepts go further back but were part of LISP and Simula language science. At Bell Labs, the development of C++ became interested in OOP. C++ was first used in 1980 for systems programming. The initial design leveraged C language systems programming capabilities with Simula concepts. Object-oriented facilities were added in 1983. The Cfront program implemented a C++ front-end for C84 language compiler. In subsequent years several C++ compilers were developed as C++ popularity grew.\\nIn many application domains, the idea of using a higher-level language quickly caught on. Because of the expanding functionality supported by newer programming languages and the increasing complexity of computer architectures, compilers became more complex.\\nDARPA (Defense Advanced Research Projects Agency) sponsored a compiler project with Wulf\\'s CMU research team in 1970. The Production Quality Compiler-Compiler PQCC design would produce a Production Quality Compiler (PQC) from formal definitions of source language and the target. PQCC tried to extend the term compiler-compiler beyond the traditional meaning as a parser generator (e.g., Yacc) without much success. PQCC might more properly be referred to as a compiler generator.\\nPQCC research into code generation process sought to build a truly automatic compiler-writing system. The effort discovered and designed the phase structure of the PQC. The BLISS-11 compiler provided the initial structure. The phases included analyses (front end), intermediate translation to virtual machine (middle end), and translation to the target (back end). TCOL was developed for the PQCC research to handle language specific constructs in the intermediate representation. Variations of TCOL supported various languages. The PQCC project investigated techniques of automated compiler construction. The design concepts proved useful in optimizing compilers and compilers for the object-oriented programming language Ada.\\nThe Ada Stoneman Document formalized the program support environment (APSE) along with the kernel (KAPSE) and minimal (MAPSE). An Ada interpreter NYU/ED supported development and standardization efforts with the American National Standards Institute (ANSI) and the International Standards Organization (ISO). Initial Ada compiler development by the U.S. Military Services included the compilers in a complete integrated design environment along the lines of the Stoneman Document. Army and Navy worked on the Ada Language System (ALS) project targeted to DEC/VAX architecture while the Air Force started on the Ada Integrated Environment (AIE) targeted to IBM 370 series. While the projects did not provide the desired results, they did contribute to the overall effort on Ada development.Other Ada compiler efforts got underway in Britain at the University of York and in Germany at the University of Karlsruhe. In the U. S., Verdix (later acquired by Rational) delivered the Verdix Ada Development System (VADS) to the Army. VADS provided a set of development tools including a compiler. Unix/VADS could be hosted on a variety of Unix platforms such as DEC Ultrix and the Sun 3/60 Solaris targeted to Motorola 68020 in an Army CECOM evaluation. There were soon many Ada compilers available that passed the Ada Validation tests. The Free Software Foundation GNU project developed the GNU Compiler Collection (GCC) which provides a core capability to support multiple languages and targets. The Ada version GNAT is one of the most widely used Ada compilers. GNAT is free but there is also commercial support, for example, AdaCore, was founded in 1994 to provide commercial software solutions for Ada. GNAT Pro includes the GNU GCC based GNAT with a tool suite to provide an integrated development environment.\\nHigh-level languages continued to drive compiler research and development. Focus areas included optimization and automatic code generation. Trends in programming languages and development environments influenced compiler technology. More compilers became included in language distributions (PERL, Java Development Kit) and as a component of an IDE (VADS, Eclipse, Ada Pro). The interrelationship and interdependence of technologies grew. The advent of web services promoted growth of web languages and scripting languages. Scripts trace back to the early days of Command Line Interfaces (CLI) where the user could enter commands to be executed by the system. User Shell concepts developed with languages to write shell programs. Early Windows designs offered a simple batch programming capability. The conventional transformation of these language used an interpreter. While not widely used, Bash and Batch compilers have been written. More recently sophisticated interpreted languages became part of the developers tool kit. Modern scripting languages include PHP, Python, Ruby and Lua. (Lua is widely used in game development.) All of these have interpreter and compiler support.\"When the field of compiling began in the late 50s, its focus was limited to the translation of high-level language programs into machine code ... The compiler field is increasingly intertwined with other disciplines including computer architecture, programming languages, formal methods, software engineering, and computer security.\" The \"Compiler Research: The Next 50 Years\" article noted the importance of object-oriented languages and Java. Security and parallel computing were cited among the future research targets.\\n\\n\\n== Compiler construction ==\\nA compiler implements a formal transformation from a high-level source program to a low-level target program. Compiler design can define an end-to-end solution or tackle a defined subset that interfaces with other compilation tools e.g. preprocessors, assemblers, linkers. Design requirements include rigorously defined interfaces both internally between compiler components and externally between supporting toolsets.\\nIn the early days, the approach taken to compiler design was directly affected by the complexity of the computer language to be processed, the experience of the person(s) designing it, and the resources available. Resource limitations led to the need to pass through the source code more than once.\\nA compiler for a relatively simple language written by one person might be a single, monolithic piece of software. However, as the source language grows in complexity the design may be split into a number of interdependent phases. Separate phases provide design improvements that focus development on the functions in the compilation process.\\n\\n\\n=== One-pass versus multi-pass compilers ===\\nClassifying compilers by number of passes has its background in the hardware resource limitations of computers. Compiling involves performing much work and early computers did not have enough memory to contain one program that did all of this work. So compilers were split up into smaller programs which each made a pass over the source (or some representation of it) performing some of the required analysis and translations.\\nThe ability to compile in a single pass has classically been seen as a benefit because it simplifies the job of writing a compiler and one-pass compilers generally perform compilations faster than multi-pass compilers. Thus, partly driven by the resource limitations of early systems, many early languages were specifically designed so that they could be compiled in a single pass (e.g., Pascal).\\nIn some cases, the design of a language feature may require a compiler to perform more than one pass over the source. For instance, consider a declaration appearing on line 20 of the source which affects the translation of a statement appearing on line 10. In this case, the first pass needs to gather information about declarations appearing after statements that they affect, with the actual translation happening during a subsequent pass.\\nThe disadvantage of compiling in a single pass is that it is not possible to perform many of the sophisticated optimizations needed to generate high quality code. It can be difficult to count exactly how many passes an optimizing compiler makes. For instance, different phases of optimization may analyse one expression many times but only analyse another expression once.\\nSplitting a compiler up into small programs is a technique used by researchers interested in producing provably correct compilers. Proving the correctness of a set of small programs often requires less effort than proving the correctness of a larger, single, equivalent program.\\n\\n\\n=== Three-stage compiler structure ===\\n\\nRegardless of the exact number of phases in the compiler design, the phases can be assigned to one of three stages. The stages include a front end, a middle end, and a back end.\\n\\nThe front end scans the input and verifies syntax and semantics according to a specific source language. For statically typed languages it performs type checking by collecting type information. If the input program is syntactically incorrect or has a type error, it generates error and/or warning messages, usually identifying the location in the source code where the problem was detected; in some cases the actual error may be (much) earlier in the program. Aspects of the front end include lexical analysis, syntax analysis, and semantic analysis. The front end transforms the input program into an intermediate representation (IR) for further processing by the middle end. This IR is usually a lower-level representation of the program with respect to the source code.\\nThe middle end performs optimizations on the IR that are independent of the CPU architecture being targeted. This source code/machine code independence is intended to enable generic optimizations to be shared between versions of the compiler supporting different languages and target processors. Examples of middle end optimizations are removal of useless (dead code elimination) or unreachable code (reachability analysis), discovery and propagation of constant values (constant propagation), relocation of computation to a less frequently executed place (e.g., out of a loop), or specialization of computation based on the context. Eventually producing the \"optimized\" IR that is used by the back end.\\nThe back end takes the optimized IR from the middle end. It may perform more analysis, transformations and optimizations that are specific for the target CPU architecture. The back end generates the target-dependent assembly code, performing register allocation in the process. The back end performs instruction scheduling, which re-orders instructions to keep parallel execution units busy by filling delay slots. Although most optimization problems are NP-hard, heuristic techniques for solving them are well-developed and currently implemented in production-quality compilers. Typically the output of a back end is machine code specialized for a particular processor and operating system.This front/middle/back-end approach makes it possible to combine front ends for different languages with back ends for different CPUs while sharing the optimizations of the middle end. Practical examples of this approach are the GNU Compiler Collection, Clang (LLVM-based C/C++ compiler), and the Amsterdam Compiler Kit, which have multiple front-ends, shared optimizations and multiple back-ends.\\n\\n\\n==== Front end ====\\n\\nThe front end analyzes the source code to build an internal representation of the program, called the intermediate representation (IR). It also manages the symbol table, a data structure mapping each symbol in the source code to associated information such as location, type and scope.\\nWhile the frontend can be a single monolithic function or program, as in a scannerless parser, it was traditionally implemented and analyzed as several phases, which may execute sequentially or concurrently. This method is favored due to its modularity and separation of concerns. Most commonly today, the frontend is broken into three phases: lexical analysis (also known as lexing or scanning), syntax analysis (also known as scanning or parsing), and semantic analysis. Lexing and parsing comprise the syntactic analysis (word syntax and phrase syntax, respectively), and in simple cases, these modules (the lexer and parser) can be automatically generated from a grammar for the language, though in more complex cases these require manual modification. The lexical grammar and phrase grammar are usually context-free grammars, which simplifies analysis significantly, with context-sensitivity handled at the semantic analysis phase. The semantic analysis phase is generally more complex and written by hand, but can be partially or fully automated using attribute grammars. These phases themselves can be further broken down: lexing as scanning and evaluating, and parsing as building a concrete syntax tree (CST, parse tree) and then transforming it into an abstract syntax tree (AST, syntax tree). In some cases additional phases are used, notably line reconstruction and preprocessing, but these are rare.\\nThe main phases of the front end include the following:\\n\\nLine reconstruction converts the input character sequence to a canonical form ready for the parser. Languages which strop their keywords or allow arbitrary spaces within identifiers require this phase. The top-down, recursive-descent, table-driven parsers used in the 1960s typically read the source one character at a time and did not require a separate tokenizing phase. Atlas Autocode and Imp (and some implementations of ALGOL and Coral 66) are examples of stropped languages whose compilers would have a Line Reconstruction phase.\\nPreprocessing supports macro substitution and conditional compilation. Typically the preprocessing phase occurs before syntactic or semantic analysis; e.g. in the case of C, the preprocessor manipulates lexical tokens rather than syntactic forms. However, some languages such as Scheme support macro substitutions based on syntactic forms.\\nLexical analysis (also known as lexing or tokenization) breaks the source code text into a sequence of small pieces called lexical tokens. This phase can be divided into two stages: the scanning, which segments the input text into syntactic units called lexemes and assigns them a category; and the evaluating, which converts lexemes into a processed value. A token is a pair consisting of a token name and an optional token value. Common token categories may include identifiers, keywords, separators, operators, literals and comments, although the set of token categories varies in different programming languages. The lexeme syntax is typically a regular language, so a finite state automaton constructed from a regular expression can be used to recognize it. The software doing lexical analysis is called a lexical analyzer. This may not be a separate step—it can be combined with the parsing step in scannerless parsing, in which case parsing is done at the character level, not the token level.\\nSyntax analysis (also known as parsing) involves parsing the token sequence to identify the syntactic structure of the program. This phase typically builds a parse tree, which replaces the linear sequence of tokens with a tree structure built according to the rules of a formal grammar which define the language\\'s syntax. The parse tree is often analyzed, augmented, and transformed by later phases in the compiler.\\nSemantic analysis adds semantic information to the parse tree and builds the symbol table. This phase performs semantic checks such as type checking (checking for type errors), or object binding (associating variable and function references with their definitions), or definite assignment (requiring all local variables to be initialized before use), rejecting incorrect programs or issuing warnings. Semantic analysis usually requires a complete parse tree, meaning that this phase logically follows the parsing phase, and logically precedes the code generation phase, though it is often possible to fold multiple phases into one pass over the code in a compiler implementation.\\n\\n\\n==== Middle end ====\\nThe middle end, also known as optimizer, performs optimizations on the intermediate representation in order to improve the performance and the quality of the produced machine code. The middle end contains those optimizations that are independent of the CPU architecture being targeted.\\nThe main phases of the middle end include the following:\\n\\nAnalysis: This is the gathering of program information from the intermediate representation derived from the input; data-flow analysis is used to build use-define chains, together with dependence analysis, alias analysis, pointer analysis, escape analysis, etc. Accurate analysis is the basis for any compiler optimization. The control-flow graph of every compiled function and the call graph of the program are usually also built during the analysis phase.\\nOptimization: the intermediate language representation is transformed into functionally equivalent but faster (or smaller) forms. Popular optimizations are inline expansion, dead code elimination, constant propagation, loop transformation and even automatic parallelization.Compiler analysis is the prerequisite for any compiler optimization, and they tightly work together. For example, dependence analysis is crucial for loop transformation.\\nThe scope of compiler analysis and optimizations vary greatly; their scope may range from operating within a basic block, to whole procedures, or even the whole program.  There is a trade-off between the granularity of the optimizations and the cost of compilation.  For example, peephole optimizations are fast to perform during compilation but only affect a small local fragment of the code, and can be performed independently of the context in which the code fragment appears.  In contrast, interprocedural optimization requires more compilation time and memory space, but enable optimizations that are only possible by considering the behavior of multiple functions simultaneously.\\nInterprocedural analysis and optimizations are common in modern commercial compilers from HP, IBM, SGI, Intel, Microsoft, and Sun Microsystems. The free software GCC was criticized for a long time for lacking powerful interprocedural optimizations, but it is changing in this respect. Another open source compiler with full analysis and optimization infrastructure is Open64, which is used by many organizations for research and commercial purposes.\\nDue to the extra time and space needed for compiler analysis and optimizations, some compilers skip them by default. Users have to use compilation options to explicitly tell the compiler which optimizations should be enabled.\\n\\n\\n==== Back end ====\\nThe back end is responsible for the CPU architecture specific optimizations and for code generation.\\nThe main phases of the back end include the following:\\n\\nMachine dependent optimizations: optimizations that depend on the details of the CPU architecture that the compiler targets. A prominent example is peephole optimizations, which rewrites short sequences of assembler instructions into more efficient instructions.\\nCode generation: the transformed intermediate language is translated into the output language, usually the native machine language of the system. This involves resource and storage decisions, such as deciding which variables to fit into registers and memory and the selection and scheduling of appropriate machine instructions along with their associated addressing modes (see also Sethi–Ullman algorithm). Debug data may also need to be generated to facilitate debugging.\\n\\n\\n=== Compiler correctness ===\\n\\nCompiler correctness is the branch of software engineering that deals with trying to show that a compiler behaves according to its language specification. Techniques include developing the compiler using formal methods and using rigorous testing (often called compiler validation) on an existing compiler.\\n\\n\\n== Compiled versus interpreted languages ==\\nHigher-level programming languages usually appear with a type of translation in mind: either designed as compiled language or interpreted language. However, in practice there is rarely anything about a language that requires it to be exclusively compiled or exclusively interpreted, although it is possible to design languages that rely on re-interpretation at run time. The categorization usually reflects the most popular or widespread implementations of a language – for instance, BASIC is sometimes called an interpreted language, and C a compiled one, despite the existence of BASIC compilers and C interpreters.\\nInterpretation does not replace compilation completely. It only hides it from the user and makes it gradual. Even though an interpreter can itself be interpreted, a directly executed program is needed somewhere at the bottom of the execution stack (see machine language).\\nFurthermore, for optimization compilers can contain interpreter functionality, and interpreters may include ahead of time compilation techniques. For example, where an expression can be executed during compilation and the results inserted into the output program, then it prevents it having to be recalculated each time the program runs, which can greatly speed up the final program. Modern trends toward just-in-time compilation and bytecode interpretation at times blur the traditional categorizations of compilers and interpreters even further.\\nSome language specifications spell out that implementations must include a compilation facility; for example, Common Lisp. However, there is nothing inherent in the definition of Common Lisp that stops it from being interpreted. Other languages have features that are very easy to implement in an interpreter, but make writing a compiler much harder; for example, APL, SNOBOL4, and many scripting languages allow programs to construct arbitrary source code at runtime with regular string operations, and then execute that code by passing it to a special evaluation function. To implement these features in a compiled language, programs must usually be shipped with a runtime library that includes a version of the compiler itself.\\n\\n\\n== Types ==\\nOne classification of compilers is by the platform on which their generated code executes. This is known as the target platform.\\nA native or hosted compiler is one whose output is intended to directly run on the same type of computer and operating system that the compiler itself runs on. The output of a cross compiler is designed to run on a different platform. Cross compilers are often used when developing software for embedded systems that are not intended to support a software development environment.\\nThe output of a compiler that produces code for a virtual machine (VM) may or may not be executed on the same platform as the compiler that produced it. For this reason, such compilers are not usually classified as native or cross compilers.\\nThe lower level language that is the target of a compiler may itself be a high-level programming language. C, viewed by some as a sort of portable assembly language, is frequently the target language of such compilers. For example, Cfront, the original compiler for C++, used C as its target language. The C code generated by such a compiler is usually not intended to be readable and maintained by humans, so indent style and creating pretty C intermediate code are ignored. Some of the features of C that make it a good target language include the #line directive, which can be generated by the compiler to support debugging of the original source, and the wide platform support available with C compilers.\\nWhile a common compiler type outputs machine code, there are many other types:\\n\\nSource-to-source compilers are a type of compiler that takes a high-level language as its input and outputs a high-level language. For example, an automatic parallelizing compiler will frequently take in a high-level language program as an input and then transform the code and annotate it with parallel code annotations (e.g. OpenMP) or language constructs (e.g. Fortran\\'s DOALL statements). Other terms for source-to-source compilers are language translator, language converter, or language rewriter. The last term is usually applied to translations that do not involve a change of language.\\nBytecode compilers compile to assembly language of a theoretical machine, like some Prolog implementations\\nThis Prolog machine is also known as the Warren Abstract Machine (or WAM).\\nBytecode compilers for Java, Python are also examples of this category.\\nJust-in-time compilers (JIT compiler) defer compilation until runtime. JIT compilers exist for many modern languages including Python, JavaScript, Smalltalk, Java, Microsoft .NET\\'s Common Intermediate Language (CIL) and others.  A JIT compiler generally runs inside an interpreter.  When the interpreter detects that a code path is \"hot\", meaning it is executed frequently, the JIT compiler will be invoked and compile the \"hot\" code for increased performance.\\nFor some languages, such as Java, applications are first compiled using a bytecode compiler and delivered in a machine-independent intermediate representation.  A bytecode interpreter executes the bytecode, but the JIT compiler will translate the bytecode to machine code when increased performance is necessary.\\nHardware compilers (also known as synthesis tools) are compilers whose input is a hardware description language and whose output is a description, in the form of a netlist or otherwise, of a hardware configuration.\\nThe output of these compilers target computer hardware at a very low level, for example a field-programmable gate array (FPGA) or structured application-specific integrated circuit (ASIC). Such compilers are said to be hardware compilers, because the source code they compile effectively controls the final configuration of the hardware and how it operates. The output of the compilation is only an interconnection of transistors or lookup tables.\\nAn example of hardware compiler is XST, the Xilinx Synthesis Tool used for configuring FPGAs. Similar tools are available from Altera, Synplicity, Synopsys and other hardware vendors.\\nAn assembler is a program that compiles human readable assembly language to machine code, the actual instructions executed by hardware.  The inverse program that translates machine code to assembly language is called a disassembler.\\nA program that translates from a low-level language to a higher level one is a decompiler.\\nA program that translates into an object code format that is not supported on the compilation machine is called a cross compiler and is commonly used to prepare code for embedded applications.\\nA program that rewrites object code back into the same type of object code while applying optimisations and transformations is a binary recompiler.\\n\\n\\n== See also ==\\n\\n\\n== References ==\\n\\n\\n== Further reading ==\\n\\n\\n== External links ==\\nCompilers at Curlie\\nIncremental Approach to Compiler Construction – a PDF tutorial\\nCompile-Howto\\nBasics of Compiler Design at the Wayback Machine (archived 15 May 2018)\\nShort animation on YouTube explaining the key conceptual difference between compilers and interpreters\\nSyntax Analysis & LL1 Parsing on YouTube\\nLet\\'s Build a Compiler, by Jack Crenshaw\\nForum about compiler development at the Wayback Machine (archived 10 October 2014)', 'In computing, a compiler is a computer program that translates computer code written in one programming language (the source language) into another language (the target language). The name \"compiler\" is primarily used for programs that translate source code from a high-level programming language to a lower level language (e.g. assembly language, object code, or machine code) to create an executable program.:\\u200ap1\\u200aThere are many different types of compilers which produce output in different useful forms. A cross-compiler  produces code for a different CPU or operating system than the one on which the cross-compiler itself runs. A bootstrap compiler is written in the language that it intends to compile. A program that translates from a low-level language to a higher level one is a decompiler. A program that translates between high-level languages is usually called a source-to-source compiler or transpiler. A language rewriter is usually a program that translates the form of expressions without a change of language. A compiler-compiler is a compiler that produces a compiler (or part of one).\\nA compiler is likely to perform some or all of the following operations, often called phases: preprocessing, lexical analysis, parsing, semantic analysis (syntax-directed translation), conversion of input programs to an intermediate representation, code optimization and code generation. Compilers generally implement these phases as modular components, promoting efficient design and correctness of transformations of source input to target output. Program faults caused by incorrect compiler behavior can be very difficult to track down and work around; therefore, compiler implementers invest significant effort to ensure compiler correctness.Compilers are not the only language processor used to transform source programs. An interpreter is computer software that transforms and then executes the indicated operations.:\\u200ap2\\u200a The translation process influences the design of computer languages, which leads to a preference of compilation or interpretation. In theory, a programming language can have both a compiler and an interpreter. In practice, programming languages tend to be associated with just one (a compiler or an interpreter).\\n\\n\\n== History ==\\n\\nTheoretical computing concepts developed by scientists, mathematicians, and engineers formed the basis of digital modern computing development during World War II. Primitive binary languages evolved because digital devices only understand ones and zeros and the circuit patterns in the underlying machine architecture. In the late 1940s, assembly languages were created to offer a more workable abstraction of the computer architectures. Limited memory capacity of early computers led to substantial technical challenges when the first compilers were designed. Therefore, the compilation process needed to be divided into several small programs. The front end programs produce the analysis products used by the back end programs to generate target code. As computer technology provided more resources, compiler designs could align better with the compilation process.\\nIt is usually more productive for a programmer to use a high-level language, so the development of high-level languages followed naturally from the capabilities offered by digital computers. High-level languages are formal languages that are strictly defined by their syntax and semantics which form the high-level language architecture. Elements of these formal languages include:\\n\\nAlphabet, any finite set of symbols;\\nString, a finite sequence of symbols;\\nLanguage, any set of strings on an alphabet.The sentences in a language may be defined by a set of rules called a grammar.Backus–Naur form (BNF) describes the syntax of \"sentences\" of a language and was used for the syntax of Algol 60 by John Backus. The ideas derive from the context-free grammar concepts by Noam Chomsky, a linguist. \"BNF and its extensions have become standard tools for describing the syntax of programming notations, and in many cases parts of compilers are generated automatically from a BNF description.\"In the 1940s, Konrad Zuse designed an algorithmic programming language called Plankalkül (\"Plan Calculus\"). While no actual implementation occurred until the 1970s, it presented concepts later seen in APL designed by Ken Iverson in the late 1950s. APL is a language for mathematical computations. \\nHigh-level language design during the formative years of digital computing provided useful programming tools for a variety of applications:\\n\\nFORTRAN (Formula Translation) for engineering and science applications is considered to be the first high-level language.\\nCOBOL (Common Business-Oriented Language) evolved from A-0 and FLOW-MATIC to become the dominant high-level language for business applications.\\nLISP (List Processor) for symbolic computation.Compiler technology evolved from the need for a strictly defined transformation of the high-level source program into a low-level target program for the digital computer. The compiler could be viewed as a front end to deal with the analysis of the source code and a back end to synthesize the analysis into the target code. Optimization between the front end and back end could produce more efficient target code.Some early milestones in the development of compiler technology:\\n\\n1952: An Autocode compiler developed by Alick Glennie for the Manchester Mark I computer at the University of Manchester is considered by some to be the first compiled programming language.\\n1952: Grace Hopper\\'s team at Remington Rand wrote the compiler for the A-0 programming language (and coined the term compiler to describe it), although the A-0 compiler functioned more as a loader or linker than the modern notion of a full compiler.\\n1954–1957: A team led by John Backus at IBM developed FORTRAN which is usually considered the first high-level language. In 1957, they completed a FORTRAN compiler that is generally credited as having introduced the first unambiguously complete compiler.\\n1959: The Conference on Data Systems Language (CODASYL) initiated development of COBOL. The COBOL design drew on A-0 and FLOW-MATIC. By the early 1960s COBOL was compiled on multiple architectures.\\n1958–1962: John McCarthy at MIT designed LISP. The symbol processing capabilities provided useful features for artificial intelligence research. In 1962, LISP 1.5 release noted some tools: an interpreter written by Stephen Russell and Daniel J. Edwards, a compiler and assembler written by Tim Hart and Mike Levin.Early operating systems and software were written in assembly language. In the 1960s and early 1970s, the use of high-level languages for system programming was still controversial due to resource limitations. However, several research and industry efforts began the shift toward high-level systems programming languages, for example, BCPL, BLISS, B, and C.\\nBCPL (Basic Combined Programming Language) designed in 1966 by Martin Richards at the University of Cambridge was originally developed as a compiler writing tool. Several compilers have been implemented, Richards\\' book provides insights to the language and its compiler. BCPL was not only an influential systems programming language that is still used in research but also provided a basis for the design of B and C languages.\\nBLISS (Basic Language for Implementation of System Software) was developed for a Digital Equipment Corporation (DEC) PDP-10 computer by W.A. Wulf\\'s Carnegie Mellon University (CMU) research team. The CMU team went on to develop BLISS-11 compiler one year later in 1970.\\nMultics (Multiplexed Information and Computing Service), a time-sharing operating system project, involved MIT, Bell Labs, General Electric (later Honeywell) and was led by Fernando Corbató from MIT. Multics was written in the PL/I language developed by IBM and IBM User Group. IBM\\'s goal was to satisfy business, scientific, and systems programming requirements. There were other languages that could have been considered but PL/I offered the most complete solution even though it had not been implemented. For the first few years of the Multics project, a subset of the language could be compiled to assembly language with the Early PL/I (EPL) compiler by Doug McIlory and Bob Morris from Bell Labs. EPL supported the project until a boot-strapping compiler for the full PL/I could be developed.Bell Labs left the Multics project in 1969: \"Over time, hope was replaced by frustration as the group effort initially failed to produce an economically useful system.\" Continued participation would drive up project support costs. So researchers turned to other development efforts. A system programming language B based on BCPL concepts was written by Dennis Ritchie and Ken Thompson. Ritchie created a boot-strapping compiler for B and wrote Unics (Uniplexed Information and Computing Service) operating system for a PDP-7 in B. Unics eventually became spelled Unix.\\nBell Labs started the development and expansion of C based on B and BCPL. The BCPL compiler had been transported to Multics by Bell Labs and BCPL was a preferred language at Bell Labs. Initially, a front-end program to Bell Labs\\' B compiler was used while a C compiler was developed. In 1971, a new PDP-11 provided the resource to define extensions to B and rewrite the compiler. By 1973 the design of C language was essentially complete and the Unix kernel for a PDP-11 was rewritten in C. Steve Johnson started development of Portable C Compiler (PCC) to support retargeting of C compilers to new machines.Object-oriented programming (OOP) offered some interesting possibilities for application development and maintenance. OOP concepts go further back but were part of LISP and Simula language science. At Bell Labs, the development of C++ became interested in OOP. C++ was first used in 1980 for systems programming. The initial design leveraged C language systems programming capabilities with Simula concepts. Object-oriented facilities were added in 1983. The Cfront program implemented a C++ front-end for C84 language compiler. In subsequent years several C++ compilers were developed as C++ popularity grew.\\nIn many application domains, the idea of using a higher-level language quickly caught on. Because of the expanding functionality supported by newer programming languages and the increasing complexity of computer architectures, compilers became more complex.\\nDARPA (Defense Advanced Research Projects Agency) sponsored a compiler project with Wulf\\'s CMU research team in 1970. The Production Quality Compiler-Compiler PQCC design would produce a Production Quality Compiler (PQC) from formal definitions of source language and the target. PQCC tried to extend the term compiler-compiler beyond the traditional meaning as a parser generator (e.g., Yacc) without much success. PQCC might more properly be referred to as a compiler generator.\\nPQCC research into code generation process sought to build a truly automatic compiler-writing system. The effort discovered and designed the phase structure of the PQC. The BLISS-11 compiler provided the initial structure. The phases included analyses (front end), intermediate translation to virtual machine (middle end), and translation to the target (back end). TCOL was developed for the PQCC research to handle language specific constructs in the intermediate representation. Variations of TCOL supported various languages. The PQCC project investigated techniques of automated compiler construction. The design concepts proved useful in optimizing compilers and compilers for the object-oriented programming language Ada.\\nThe Ada Stoneman Document formalized the program support environment (APSE) along with the kernel (KAPSE) and minimal (MAPSE). An Ada interpreter NYU/ED supported development and standardization efforts with the American National Standards Institute (ANSI) and the International Standards Organization (ISO). Initial Ada compiler development by the U.S. Military Services included the compilers in a complete integrated design environment along the lines of the Stoneman Document. Army and Navy worked on the Ada Language System (ALS) project targeted to DEC/VAX architecture while the Air Force started on the Ada Integrated Environment (AIE) targeted to IBM 370 series. While the projects did not provide the desired results, they did contribute to the overall effort on Ada development.Other Ada compiler efforts got underway in Britain at the University of York and in Germany at the University of Karlsruhe. In the U. S., Verdix (later acquired by Rational) delivered the Verdix Ada Development System (VADS) to the Army. VADS provided a set of development tools including a compiler. Unix/VADS could be hosted on a variety of Unix platforms such as DEC Ultrix and the Sun 3/60 Solaris targeted to Motorola 68020 in an Army CECOM evaluation. There were soon many Ada compilers available that passed the Ada Validation tests. The Free Software Foundation GNU project developed the GNU Compiler Collection (GCC) which provides a core capability to support multiple languages and targets. The Ada version GNAT is one of the most widely used Ada compilers. GNAT is free but there is also commercial support, for example, AdaCore, was founded in 1994 to provide commercial software solutions for Ada. GNAT Pro includes the GNU GCC based GNAT with a tool suite to provide an integrated development environment.\\nHigh-level languages continued to drive compiler research and development. Focus areas included optimization and automatic code generation. Trends in programming languages and development environments influenced compiler technology. More compilers became included in language distributions (PERL, Java Development Kit) and as a component of an IDE (VADS, Eclipse, Ada Pro). The interrelationship and interdependence of technologies grew. The advent of web services promoted growth of web languages and scripting languages. Scripts trace back to the early days of Command Line Interfaces (CLI) where the user could enter commands to be executed by the system. User Shell concepts developed with languages to write shell programs. Early Windows designs offered a simple batch programming capability. The conventional transformation of these language used an interpreter. While not widely used, Bash and Batch compilers have been written. More recently sophisticated interpreted languages became part of the developers tool kit. Modern scripting languages include PHP, Python, Ruby and Lua. (Lua is widely used in game development.) All of these have interpreter and compiler support.\"When the field of compiling began in the late 50s, its focus was limited to the translation of high-level language programs into machine code ... The compiler field is increasingly intertwined with other disciplines including computer architecture, programming languages, formal methods, software engineering, and computer security.\" The \"Compiler Research: The Next 50 Years\" article noted the importance of object-oriented languages and Java. Security and parallel computing were cited among the future research targets.\\n\\n\\n== Compiler construction ==\\nA compiler implements a formal transformation from a high-level source program to a low-level target program. Compiler design can define an end-to-end solution or tackle a defined subset that interfaces with other compilation tools e.g. preprocessors, assemblers, linkers. Design requirements include rigorously defined interfaces both internally between compiler components and externally between supporting toolsets.\\nIn the early days, the approach taken to compiler design was directly affected by the complexity of the computer language to be processed, the experience of the person(s) designing it, and the resources available. Resource limitations led to the need to pass through the source code more than once.\\nA compiler for a relatively simple language written by one person might be a single, monolithic piece of software. However, as the source language grows in complexity the design may be split into a number of interdependent phases. Separate phases provide design improvements that focus development on the functions in the compilation process.\\n\\n\\n=== One-pass versus multi-pass compilers ===\\nClassifying compilers by number of passes has its background in the hardware resource limitations of computers. Compiling involves performing much work and early computers did not have enough memory to contain one program that did all of this work. So compilers were split up into smaller programs which each made a pass over the source (or some representation of it) performing some of the required analysis and translations.\\nThe ability to compile in a single pass has classically been seen as a benefit because it simplifies the job of writing a compiler and one-pass compilers generally perform compilations faster than multi-pass compilers. Thus, partly driven by the resource limitations of early systems, many early languages were specifically designed so that they could be compiled in a single pass (e.g., Pascal).\\nIn some cases, the design of a language feature may require a compiler to perform more than one pass over the source. For instance, consider a declaration appearing on line 20 of the source which affects the translation of a statement appearing on line 10. In this case, the first pass needs to gather information about declarations appearing after statements that they affect, with the actual translation happening during a subsequent pass.\\nThe disadvantage of compiling in a single pass is that it is not possible to perform many of the sophisticated optimizations needed to generate high quality code. It can be difficult to count exactly how many passes an optimizing compiler makes. For instance, different phases of optimization may analyse one expression many times but only analyse another expression once.\\nSplitting a compiler up into small programs is a technique used by researchers interested in producing provably correct compilers. Proving the correctness of a set of small programs often requires less effort than proving the correctness of a larger, single, equivalent program.\\n\\n\\n=== Three-stage compiler structure ===\\n\\nRegardless of the exact number of phases in the compiler design, the phases can be assigned to one of three stages. The stages include a front end, a middle end, and a back end.\\n\\nThe front end scans the input and verifies syntax and semantics according to a specific source language. For statically typed languages it performs type checking by collecting type information. If the input program is syntactically incorrect or has a type error, it generates error and/or warning messages, usually identifying the location in the source code where the problem was detected; in some cases the actual error may be (much) earlier in the program. Aspects of the front end include lexical analysis, syntax analysis, and semantic analysis. The front end transforms the input program into an intermediate representation (IR) for further processing by the middle end. This IR is usually a lower-level representation of the program with respect to the source code.\\nThe middle end performs optimizations on the IR that are independent of the CPU architecture being targeted. This source code/machine code independence is intended to enable generic optimizations to be shared between versions of the compiler supporting different languages and target processors. Examples of middle end optimizations are removal of useless (dead code elimination) or unreachable code (reachability analysis), discovery and propagation of constant values (constant propagation), relocation of computation to a less frequently executed place (e.g., out of a loop), or specialization of computation based on the context. Eventually producing the \"optimized\" IR that is used by the back end.\\nThe back end takes the optimized IR from the middle end. It may perform more analysis, transformations and optimizations that are specific for the target CPU architecture. The back end generates the target-dependent assembly code, performing register allocation in the process. The back end performs instruction scheduling, which re-orders instructions to keep parallel execution units busy by filling delay slots. Although most optimization problems are NP-hard, heuristic techniques for solving them are well-developed and currently implemented in production-quality compilers. Typically the output of a back end is machine code specialized for a particular processor and operating system.This front/middle/back-end approach makes it possible to combine front ends for different languages with back ends for different CPUs while sharing the optimizations of the middle end. Practical examples of this approach are the GNU Compiler Collection, Clang (LLVM-based C/C++ compiler), and the Amsterdam Compiler Kit, which have multiple front-ends, shared optimizations and multiple back-ends.\\n\\n\\n==== Front end ====\\n\\nThe front end analyzes the source code to build an internal representation of the program, called the intermediate representation (IR). It also manages the symbol table, a data structure mapping each symbol in the source code to associated information such as location, type and scope.\\nWhile the frontend can be a single monolithic function or program, as in a scannerless parser, it was traditionally implemented and analyzed as several phases, which may execute sequentially or concurrently. This method is favored due to its modularity and separation of concerns. Most commonly today, the frontend is broken into three phases: lexical analysis (also known as lexing or scanning), syntax analysis (also known as scanning or parsing), and semantic analysis. Lexing and parsing comprise the syntactic analysis (word syntax and phrase syntax, respectively), and in simple cases, these modules (the lexer and parser) can be automatically generated from a grammar for the language, though in more complex cases these require manual modification. The lexical grammar and phrase grammar are usually context-free grammars, which simplifies analysis significantly, with context-sensitivity handled at the semantic analysis phase. The semantic analysis phase is generally more complex and written by hand, but can be partially or fully automated using attribute grammars. These phases themselves can be further broken down: lexing as scanning and evaluating, and parsing as building a concrete syntax tree (CST, parse tree) and then transforming it into an abstract syntax tree (AST, syntax tree). In some cases additional phases are used, notably line reconstruction and preprocessing, but these are rare.\\nThe main phases of the front end include the following:\\n\\nLine reconstruction converts the input character sequence to a canonical form ready for the parser. Languages which strop their keywords or allow arbitrary spaces within identifiers require this phase. The top-down, recursive-descent, table-driven parsers used in the 1960s typically read the source one character at a time and did not require a separate tokenizing phase. Atlas Autocode and Imp (and some implementations of ALGOL and Coral 66) are examples of stropped languages whose compilers would have a Line Reconstruction phase.\\nPreprocessing supports macro substitution and conditional compilation. Typically the preprocessing phase occurs before syntactic or semantic analysis; e.g. in the case of C, the preprocessor manipulates lexical tokens rather than syntactic forms. However, some languages such as Scheme support macro substitutions based on syntactic forms.\\nLexical analysis (also known as lexing or tokenization) breaks the source code text into a sequence of small pieces called lexical tokens. This phase can be divided into two stages: the scanning, which segments the input text into syntactic units called lexemes and assigns them a category; and the evaluating, which converts lexemes into a processed value. A token is a pair consisting of a token name and an optional token value. Common token categories may include identifiers, keywords, separators, operators, literals and comments, although the set of token categories varies in different programming languages. The lexeme syntax is typically a regular language, so a finite state automaton constructed from a regular expression can be used to recognize it. The software doing lexical analysis is called a lexical analyzer. This may not be a separate step—it can be combined with the parsing step in scannerless parsing, in which case parsing is done at the character level, not the token level.\\nSyntax analysis (also known as parsing) involves parsing the token sequence to identify the syntactic structure of the program. This phase typically builds a parse tree, which replaces the linear sequence of tokens with a tree structure built according to the rules of a formal grammar which define the language\\'s syntax. The parse tree is often analyzed, augmented, and transformed by later phases in the compiler.\\nSemantic analysis adds semantic information to the parse tree and builds the symbol table. This phase performs semantic checks such as type checking (checking for type errors), or object binding (associating variable and function references with their definitions), or definite assignment (requiring all local variables to be initialized before use), rejecting incorrect programs or issuing warnings. Semantic analysis usually requires a complete parse tree, meaning that this phase logically follows the parsing phase, and logically precedes the code generation phase, though it is often possible to fold multiple phases into one pass over the code in a compiler implementation.\\n\\n\\n==== Middle end ====\\nThe middle end, also known as optimizer, performs optimizations on the intermediate representation in order to improve the performance and the quality of the produced machine code. The middle end contains those optimizations that are independent of the CPU architecture being targeted.\\nThe main phases of the middle end include the following:\\n\\nAnalysis: This is the gathering of program information from the intermediate representation derived from the input; data-flow analysis is used to build use-define chains, together with dependence analysis, alias analysis, pointer analysis, escape analysis, etc. Accurate analysis is the basis for any compiler optimization. The control-flow graph of every compiled function and the call graph of the program are usually also built during the analysis phase.\\nOptimization: the intermediate language representation is transformed into functionally equivalent but faster (or smaller) forms. Popular optimizations are inline expansion, dead code elimination, constant propagation, loop transformation and even automatic parallelization.Compiler analysis is the prerequisite for any compiler optimization, and they tightly work together. For example, dependence analysis is crucial for loop transformation.\\nThe scope of compiler analysis and optimizations vary greatly; their scope may range from operating within a basic block, to whole procedures, or even the whole program.  There is a trade-off between the granularity of the optimizations and the cost of compilation.  For example, peephole optimizations are fast to perform during compilation but only affect a small local fragment of the code, and can be performed independently of the context in which the code fragment appears.  In contrast, interprocedural optimization requires more compilation time and memory space, but enable optimizations that are only possible by considering the behavior of multiple functions simultaneously.\\nInterprocedural analysis and optimizations are common in modern commercial compilers from HP, IBM, SGI, Intel, Microsoft, and Sun Microsystems. The free software GCC was criticized for a long time for lacking powerful interprocedural optimizations, but it is changing in this respect. Another open source compiler with full analysis and optimization infrastructure is Open64, which is used by many organizations for research and commercial purposes.\\nDue to the extra time and space needed for compiler analysis and optimizations, some compilers skip them by default. Users have to use compilation options to explicitly tell the compiler which optimizations should be enabled.\\n\\n\\n==== Back end ====\\nThe back end is responsible for the CPU architecture specific optimizations and for code generation.\\nThe main phases of the back end include the following:\\n\\nMachine dependent optimizations: optimizations that depend on the details of the CPU architecture that the compiler targets. A prominent example is peephole optimizations, which rewrites short sequences of assembler instructions into more efficient instructions.\\nCode generation: the transformed intermediate language is translated into the output language, usually the native machine language of the system. This involves resource and storage decisions, such as deciding which variables to fit into registers and memory and the selection and scheduling of appropriate machine instructions along with their associated addressing modes (see also Sethi–Ullman algorithm). Debug data may also need to be generated to facilitate debugging.\\n\\n\\n=== Compiler correctness ===\\n\\nCompiler correctness is the branch of software engineering that deals with trying to show that a compiler behaves according to its language specification. Techniques include developing the compiler using formal methods and using rigorous testing (often called compiler validation) on an existing compiler.\\n\\n\\n== Compiled versus interpreted languages ==\\nHigher-level programming languages usually appear with a type of translation in mind: either designed as compiled language or interpreted language. However, in practice there is rarely anything about a language that requires it to be exclusively compiled or exclusively interpreted, although it is possible to design languages that rely on re-interpretation at run time. The categorization usually reflects the most popular or widespread implementations of a language – for instance, BASIC is sometimes called an interpreted language, and C a compiled one, despite the existence of BASIC compilers and C interpreters.\\nInterpretation does not replace compilation completely. It only hides it from the user and makes it gradual. Even though an interpreter can itself be interpreted, a directly executed program is needed somewhere at the bottom of the execution stack (see machine language).\\nFurthermore, for optimization compilers can contain interpreter functionality, and interpreters may include ahead of time compilation techniques. For example, where an expression can be executed during compilation and the results inserted into the output program, then it prevents it having to be recalculated each time the program runs, which can greatly speed up the final program. Modern trends toward just-in-time compilation and bytecode interpretation at times blur the traditional categorizations of compilers and interpreters even further.\\nSome language specifications spell out that implementations must include a compilation facility; for example, Common Lisp. However, there is nothing inherent in the definition of Common Lisp that stops it from being interpreted. Other languages have features that are very easy to implement in an interpreter, but make writing a compiler much harder; for example, APL, SNOBOL4, and many scripting languages allow programs to construct arbitrary source code at runtime with regular string operations, and then execute that code by passing it to a special evaluation function. To implement these features in a compiled language, programs must usually be shipped with a runtime library that includes a version of the compiler itself.\\n\\n\\n== Types ==\\nOne classification of compilers is by the platform on which their generated code executes. This is known as the target platform.\\nA native or hosted compiler is one whose output is intended to directly run on the same type of computer and operating system that the compiler itself runs on. The output of a cross compiler is designed to run on a different platform. Cross compilers are often used when developing software for embedded systems that are not intended to support a software development environment.\\nThe output of a compiler that produces code for a virtual machine (VM) may or may not be executed on the same platform as the compiler that produced it. For this reason, such compilers are not usually classified as native or cross compilers.\\nThe lower level language that is the target of a compiler may itself be a high-level programming language. C, viewed by some as a sort of portable assembly language, is frequently the target language of such compilers. For example, Cfront, the original compiler for C++, used C as its target language. The C code generated by such a compiler is usually not intended to be readable and maintained by humans, so indent style and creating pretty C intermediate code are ignored. Some of the features of C that make it a good target language include the #line directive, which can be generated by the compiler to support debugging of the original source, and the wide platform support available with C compilers.\\nWhile a common compiler type outputs machine code, there are many other types:\\n\\nSource-to-source compilers are a type of compiler that takes a high-level language as its input and outputs a high-level language. For example, an automatic parallelizing compiler will frequently take in a high-level language program as an input and then transform the code and annotate it with parallel code annotations (e.g. OpenMP) or language constructs (e.g. Fortran\\'s DOALL statements). Other terms for source-to-source compilers are language translator, language converter, or language rewriter. The last term is usually applied to translations that do not involve a change of language.\\nBytecode compilers compile to assembly language of a theoretical machine, like some Prolog implementations\\nThis Prolog machine is also known as the Warren Abstract Machine (or WAM).\\nBytecode compilers for Java, Python are also examples of this category.\\nJust-in-time compilers (JIT compiler) defer compilation until runtime. JIT compilers exist for many modern languages including Python, JavaScript, Smalltalk, Java, Microsoft .NET\\'s Common Intermediate Language (CIL) and others.  A JIT compiler generally runs inside an interpreter.  When the interpreter detects that a code path is \"hot\", meaning it is executed frequently, the JIT compiler will be invoked and compile the \"hot\" code for increased performance.\\nFor some languages, such as Java, applications are first compiled using a bytecode compiler and delivered in a machine-independent intermediate representation.  A bytecode interpreter executes the bytecode, but the JIT compiler will translate the bytecode to machine code when increased performance is necessary.\\nHardware compilers (also known as synthesis tools) are compilers whose input is a hardware description language and whose output is a description, in the form of a netlist or otherwise, of a hardware configuration.\\nThe output of these compilers target computer hardware at a very low level, for example a field-programmable gate array (FPGA) or structured application-specific integrated circuit (ASIC). Such compilers are said to be hardware compilers, because the source code they compile effectively controls the final configuration of the hardware and how it operates. The output of the compilation is only an interconnection of transistors or lookup tables.\\nAn example of hardware compiler is XST, the Xilinx Synthesis Tool used for configuring FPGAs. Similar tools are available from Altera, Synplicity, Synopsys and other hardware vendors.\\nAn assembler is a program that compiles human readable assembly language to machine code, the actual instructions executed by hardware.  The inverse program that translates machine code to assembly language is called a disassembler.\\nA program that translates from a low-level language to a higher level one is a decompiler.\\nA program that translates into an object code format that is not supported on the compilation machine is called a cross compiler and is commonly used to prepare code for embedded applications.\\nA program that rewrites object code back into the same type of object code while applying optimisations and transformations is a binary recompiler.\\n\\n\\n== See also ==\\n\\n\\n== References ==\\n\\n\\n== Further reading ==\\n\\n\\n== External links ==\\nCompilers at Curlie\\nIncremental Approach to Compiler Construction – a PDF tutorial\\nCompile-Howto\\nBasics of Compiler Design at the Wayback Machine (archived 15 May 2018)\\nShort animation on YouTube explaining the key conceptual difference between compilers and interpreters\\nSyntax Analysis & LL1 Parsing on YouTube\\nLet\\'s Build a Compiler, by Jack Crenshaw\\nForum about compiler development at the Wayback Machine (archived 10 October 2014)', 'Automata theory is the study of abstract machines and automata, as well as the computational problems that can be solved using them. It is a theory in theoretical computer science. The word automata (the plural of automaton) comes from the Greek word αὐτόματος, which means \"self-acting, self-willed, self-moving\". An automaton (Automata in plural) is an abstract self-propelled computing device which follows a predetermined sequence of operations automatically. An automaton with a finite number of states is called a Finite Automaton (FA) or Finite-State Machine (FSM).\\nThe figure at right illustrates a finite-state machine, which belongs to a well-known type of automaton.  This automaton consists of states (represented in the figure by circles) and transitions (represented by arrows).  As the automaton sees a symbol of input, it makes a transition (or jump) to another state, according to its transition function, which takes the previous state and current input symbol as its arguments.\\nAutomata theory is closely related to formal language theory. In this context, automata are used as finite representations of formal languages that may be infinite. Automata are often classified by the class of formal languages they can recognize, as in the Chomsky hierarchy, which describes a nesting relationship between major classes of automata. Automata play a major role in theory of computation, compiler construction, artificial intelligence, parsing and formal verification.\\n\\n\\n== History ==\\nThe theory of abstract automata was developed in the mid-20th century in connection with finite automata. Automata theory was initially considered a branch of mathematical systems theory, studying the behavior of discrete-parameter systems. Early work in automata theory differed from previous work on systems by using abstract algebra to describe information systems rather than differential calculus to describe material systems. The theory of the finite-state transducer was developed under different names by different research communities. The earlier concept of Turing machines were also included in the discipline along with new forms of infinite-state automaton, such as pushdown automata.\\n1956 saw the publication of Automata Studies, which collected work by scientists including Claude Shannon, W. Ross Ashby, John von Neumann, Marvin Minsky, Edward F. Moore, and Stephen Cole Kleene. With the publication of this volume, \"automata theory emerged as a relatively autonomous discipline\". The book included Kleene\\'s description of the set of regular events, or regular languages, and a relatively stable measure of complexity in Turing machine programs by Shannon. \\nIn the same year, Noam Chomsky described the Chomsky hierarchy, a correspondence between automata and formal grammars, and Ross Ashby published An Introduction to Cybernetics, an accessible textbook explaining automata and information using basic set theory.\\nThe study of linear automata led to the Myhill-Nerode theorem, which gives a necessary and sufficient condition for a formal language to be regular, and an exact count of the number of states in a minimal machine for the language. The pumping lemma for regular languages, also useful in regularity proofs, was proven in this period by Michael O. Rabin and Dana Scott, along with the computational equivalence of deterministic and nondeterministic finite automata.In the 1960s, a body of algebraic results known as \"structure theory\" or \"algebraic decomposition theory\" emerged, which dealt with the realization of sequential machines from smaller machines by interconnection. While any finite automaton can be simulated using a  universal gate set, this requires that the simulating circuit contain loops of arbitrary complexity. Structure theory deals with the \"loop-free\" realizability of machines.\\nThe theory of computational complexity also took shape in the 1960s. By the end of the decade, automata theory came to be seen as \"the pure mathematics of computer science\".\\n\\n\\n== Automata ==\\nWhat follows is a general definition of automaton, which restricts a broader definition of system to one viewed as acting in discrete time-steps, with its state behavior and outputs defined at each step by unchanging functions of only its state and input.\\n\\n\\n=== Informal description ===\\nAn automaton runs when it is given some sequence of inputs in discrete (individual) time steps or steps. An automaton processes one input picked from a set of symbols or letters, which is called an input alphabet. The symbols received by the automaton as input at any step are a sequence of symbols called words. An automaton has a set of states. At each moment during a run of the automaton, the automaton is in one of its states. When the automaton receives new input it moves to another state (or transitions) based on a transition function that takes the previous state and current input symbol as parameters. At the same time, another function called the output function produces symbols from the output alphabet, also according to the previous state and current input symbol. The automaton reads the symbols of the input word and transitions between states until the word is read completely, if it is finite in length, at which point the automaton halts. A state at which the automaton halts is called the final state.\\nTo investigate the possible state/input/output sequences in an automaton using formal language theory, a machine can be assigned a starting state and a set of accepting states. Then, depending on whether a run starting from the starting state ends in an accepting state, the automaton can be said to accept or reject an input sequence. The set of all the words accepted by an automaton is called the language recognized by the automaton. A familiar example of a machine recognizing a language is an electronic lock which accepts or rejects attempts to enter the correct code.\\n\\n\\n=== Formal definition ===\\nAutomatonAn automaton can be represented formally by a 5-tuple \\n  \\n    \\n      \\n        M\\n        =\\n        ⟨\\n        Σ\\n        ,\\n        Γ\\n        ,\\n        Q\\n        ,\\n        δ\\n        ,\\n        λ\\n        ⟩\\n      \\n    \\n    {\\\\displaystyle M=\\\\langle \\\\Sigma ,\\\\Gamma ,Q,\\\\delta ,\\\\lambda \\\\rangle }\\n  , where:\\n\\n  \\n    \\n      \\n        Σ\\n      \\n    \\n    {\\\\displaystyle \\\\Sigma }\\n   is a finite set of symbols, called the input alphabet of the automaton,\\n\\n  \\n    \\n      \\n        Γ\\n      \\n    \\n    {\\\\displaystyle \\\\Gamma }\\n   is another finite set of symbols, called the output alphabet of the automaton,\\n\\n  \\n    \\n      \\n        Q\\n      \\n    \\n    {\\\\displaystyle Q}\\n   is a set of states,\\n\\n  \\n    \\n      \\n        δ\\n      \\n    \\n    {\\\\displaystyle \\\\delta }\\n   is the next-state function or transition function \\n  \\n    \\n      \\n        δ\\n        :\\n        Q\\n        ×\\n        Σ\\n        →\\n        Q\\n      \\n    \\n    {\\\\displaystyle \\\\delta :Q\\\\times \\\\Sigma \\\\to Q}\\n   mapping state-input pairs to successor states,\\n\\n  \\n    \\n      \\n        λ\\n      \\n    \\n    {\\\\displaystyle \\\\lambda }\\n   is the next-output function \\n  \\n    \\n      \\n        λ\\n        :\\n        Q\\n        ×\\n        Σ\\n        →\\n        Γ\\n      \\n    \\n    {\\\\displaystyle \\\\lambda :Q\\\\times \\\\Sigma \\\\to \\\\Gamma }\\n   mapping state-input pairs to outputs.\\nIf \\n  \\n    \\n      \\n        Q\\n      \\n    \\n    {\\\\displaystyle Q}\\n   is finite, then \\n  \\n    \\n      \\n        M\\n      \\n    \\n    {\\\\displaystyle M}\\n   is a finite automaton.Input word\\nAn automaton reads a finite string of symbols \\n  \\n    \\n      \\n        \\n          a\\n          \\n            1\\n          \\n        \\n        \\n          a\\n          \\n            2\\n          \\n        \\n        .\\n        .\\n        .\\n        \\n          a\\n          \\n            n\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle a_{1}a_{2}...a_{n}}\\n  , where \\n  \\n    \\n      \\n        \\n          a\\n          \\n            i\\n          \\n        \\n        ∈\\n        Σ\\n      \\n    \\n    {\\\\displaystyle a_{i}\\\\in \\\\Sigma }\\n  , which is called an input word. The set of all words is denoted by \\n  \\n    \\n      \\n        \\n          Σ\\n          \\n            ∗\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle \\\\Sigma ^{*}}\\n  .Run\\nA sequence of states \\n  \\n    \\n      \\n        \\n          q\\n          \\n            0\\n          \\n        \\n        ,\\n        \\n          q\\n          \\n            1\\n          \\n        \\n        ,\\n        .\\n        .\\n        .\\n        \\n          q\\n          \\n            n\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle q_{0},q_{1},...q_{n}}\\n  , where \\n  \\n    \\n      \\n        \\n          q\\n          \\n            i\\n          \\n        \\n        ∈\\n        Q\\n      \\n    \\n    {\\\\displaystyle q_{i}\\\\in Q}\\n   such that \\n  \\n    \\n      \\n        \\n          q\\n          \\n            i\\n          \\n        \\n        =\\n        δ\\n        (\\n        \\n          q\\n          \\n            i\\n            −\\n            1\\n          \\n        \\n        ,\\n        \\n          a\\n          \\n            i\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle q_{i}=\\\\delta (q_{i-1},a_{i})}\\n   for \\n  \\n    \\n      \\n        0\\n        <\\n        i\\n        ≤\\n        n\\n      \\n    \\n    {\\\\displaystyle 0<i\\\\leq n}\\n  , is a run of the automaton on an input \\n  \\n    \\n      \\n        \\n          a\\n          \\n            1\\n          \\n        \\n        \\n          a\\n          \\n            2\\n          \\n        \\n        .\\n        .\\n        .\\n        \\n          a\\n          \\n            n\\n          \\n        \\n        ∈\\n        \\n          Σ\\n          \\n            ∗\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle a_{1}a_{2}...a_{n}\\\\in \\\\Sigma ^{*}}\\n   starting from state \\n  \\n    \\n      \\n        \\n          q\\n          \\n            0\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle q_{0}}\\n  . In other words, at first the automaton is at the start state \\n  \\n    \\n      \\n        \\n          q\\n          \\n            0\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle q_{0}}\\n  , and receives input \\n  \\n    \\n      \\n        \\n          a\\n          \\n            1\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle a_{1}}\\n  . For \\n  \\n    \\n      \\n        \\n          a\\n          \\n            1\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle a_{1}}\\n   and every following \\n  \\n    \\n      \\n        \\n          a\\n          \\n            i\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle a_{i}}\\n   in the input string, the automaton picks the next state \\n  \\n    \\n      \\n        \\n          q\\n          \\n            i\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle q_{i}}\\n   according to the transition function \\n  \\n    \\n      \\n        δ\\n        (\\n        \\n          q\\n          \\n            i\\n            −\\n            1\\n          \\n        \\n        ,\\n        \\n          a\\n          \\n            i\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle \\\\delta (q_{i-1},a_{i})}\\n  , until the last symbol \\n  \\n    \\n      \\n        \\n          a\\n          \\n            n\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle a_{n}}\\n   has been read, leaving the machine in the final state of the run, \\n  \\n    \\n      \\n        \\n          q\\n          \\n            n\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle q_{n}}\\n  . Similarly, at each step, the automaton emits an output symbol according to the output function \\n  \\n    \\n      \\n        λ\\n        (\\n        \\n          q\\n          \\n            i\\n            −\\n            1\\n          \\n        \\n        ,\\n        \\n          a\\n          \\n            i\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle \\\\lambda (q_{i-1},a_{i})}\\n  .The transition function \\n  \\n    \\n      \\n        δ\\n      \\n    \\n    {\\\\displaystyle \\\\delta }\\n   is extended inductively into \\n  \\n    \\n      \\n        \\n          \\n            δ\\n            ¯\\n          \\n        \\n        :\\n        Q\\n        ×\\n        \\n          Σ\\n          \\n            ∗\\n          \\n        \\n        →\\n        Q\\n      \\n    \\n    {\\\\displaystyle {\\\\overline {\\\\delta }}:Q\\\\times \\\\Sigma ^{*}\\\\to Q}\\n   to describe the machine\\'s behavior when fed whole input words. For the empty string \\n  \\n    \\n      \\n        ε\\n      \\n    \\n    {\\\\displaystyle \\\\varepsilon }\\n  , \\n  \\n    \\n      \\n        \\n          \\n            δ\\n            ¯\\n          \\n        \\n        (\\n        q\\n        ,\\n        ε\\n        )\\n        =\\n        q\\n      \\n    \\n    {\\\\displaystyle {\\\\overline {\\\\delta }}(q,\\\\varepsilon )=q}\\n   for all states \\n  \\n    \\n      \\n        q\\n      \\n    \\n    {\\\\displaystyle q}\\n  , and for strings \\n  \\n    \\n      \\n        w\\n        a\\n      \\n    \\n    {\\\\displaystyle wa}\\n   where \\n  \\n    \\n      \\n        a\\n      \\n    \\n    {\\\\displaystyle a}\\n   is the last symbol and \\n  \\n    \\n      \\n        w\\n      \\n    \\n    {\\\\displaystyle w}\\n   is the (possibly empty) rest of the string, \\n  \\n    \\n      \\n        \\n          \\n            δ\\n            ¯\\n          \\n        \\n        (\\n        q\\n        ,\\n        w\\n        a\\n        )\\n        =\\n        δ\\n        (\\n        \\n          \\n            δ\\n            ¯\\n          \\n        \\n        (\\n        q\\n        ,\\n        w\\n        )\\n        ,\\n        a\\n        )\\n      \\n    \\n    {\\\\displaystyle {\\\\overline {\\\\delta }}(q,wa)=\\\\delta ({\\\\overline {\\\\delta }}(q,w),a)}\\n  . The output function \\n  \\n    \\n      \\n        λ\\n      \\n    \\n    {\\\\displaystyle \\\\lambda }\\n   may be extended similarly into \\n  \\n    \\n      \\n        \\n          \\n            λ\\n            ¯\\n          \\n        \\n        (\\n        q\\n        ,\\n        w\\n        )\\n      \\n    \\n    {\\\\displaystyle {\\\\overline {\\\\lambda }}(q,w)}\\n  , which gives the complete output of the machine when run on word \\n  \\n    \\n      \\n        w\\n      \\n    \\n    {\\\\displaystyle w}\\n   from state \\n  \\n    \\n      \\n        q\\n      \\n    \\n    {\\\\displaystyle q}\\n  .\\nAcceptorIn order to study an automaton with the theory of formal languages, an automaton may be considered as an acceptor, replacing the output alphabet and function \\n  \\n    \\n      \\n        Γ\\n      \\n    \\n    {\\\\displaystyle \\\\Gamma }\\n   and \\n  \\n    \\n      \\n        λ\\n      \\n    \\n    {\\\\displaystyle \\\\lambda }\\n   with\\n\\n  \\n    \\n      \\n        \\n          q\\n          \\n            0\\n          \\n        \\n        ∈\\n        Q\\n      \\n    \\n    {\\\\displaystyle q_{0}\\\\in Q}\\n  , a designated start state, and\\n\\n  \\n    \\n      \\n        F\\n      \\n    \\n    {\\\\displaystyle F}\\n  , a set of states of \\n  \\n    \\n      \\n        Q\\n      \\n    \\n    {\\\\displaystyle Q}\\n   (i.e. \\n  \\n    \\n      \\n        F\\n        ⊆\\n        Q\\n      \\n    \\n    {\\\\displaystyle F\\\\subseteq Q}\\n  ) called accept states.\\nThis allows the following to be defined:Accepting word\\nA word \\n  \\n    \\n      \\n        w\\n        =\\n        \\n          a\\n          \\n            1\\n          \\n        \\n        \\n          a\\n          \\n            2\\n          \\n        \\n        .\\n        .\\n        .\\n        \\n          a\\n          \\n            n\\n          \\n        \\n        ∈\\n        \\n          Σ\\n          \\n            ∗\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle w=a_{1}a_{2}...a_{n}\\\\in \\\\Sigma ^{*}}\\n   is an accepting word for the automaton if \\n  \\n    \\n      \\n        \\n          \\n            δ\\n            ¯\\n          \\n        \\n        (\\n        \\n          q\\n          \\n            0\\n          \\n        \\n        ,\\n        w\\n        )\\n        ∈\\n        F\\n      \\n    \\n    {\\\\displaystyle {\\\\overline {\\\\delta }}(q_{0},w)\\\\in F}\\n  , that is, if after consuming the whole string \\n  \\n    \\n      \\n        w\\n      \\n    \\n    {\\\\displaystyle w}\\n   the machine is in an accept state.Recognized language\\nThe language \\n  \\n    \\n      \\n        L\\n        ⊆\\n        \\n          Σ\\n          \\n            ∗\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle L\\\\subseteq \\\\Sigma ^{*}}\\n   recognized by an automaton is the set of all the words that are accepted by the automaton, \\n  \\n    \\n      \\n        L\\n        =\\n        {\\n        w\\n        ∈\\n        \\n          Σ\\n          \\n            ∗\\n          \\n        \\n         \\n        \\n          |\\n        \\n         \\n        \\n          \\n            δ\\n            ¯\\n          \\n        \\n        (\\n        \\n          q\\n          \\n            0\\n          \\n        \\n        ,\\n        w\\n        )\\n        ∈\\n        F\\n        }\\n      \\n    \\n    {\\\\displaystyle L=\\\\{w\\\\in \\\\Sigma ^{*}\\\\ |\\\\ {\\\\overline {\\\\delta }}(q_{0},w)\\\\in F\\\\}}\\n  .Recognizable languages\\nThe recognizable languages are the set of languages that are recognized by some automaton. For finite automata the recognizable languages are regular languages. For different types of automata, the recognizable languages are different.\\n\\n\\n== Variant definitions of automata ==\\nAutomata are defined to study useful machines under mathematical formalism. So, the definition of an automaton is open to variations according to the \"real world machine\", which we want to model using the automaton. People have studied many variations of automata. The following are some popular variations in the definition of different components of automata.\\n\\nInputFinite input: An automaton that accepts only finite sequence of symbols. The above introductory definition only encompasses finite words.\\nInfinite input: An automaton that accepts infinite words (ω-words). Such automata are called ω-automata.\\nTree word input: The input may be a tree of symbols instead of sequence of symbols. In this case after reading each symbol, the automaton reads all the successor symbols in the input tree. It is said that the automaton makes one copy of itself for each successor and each such copy starts running on one of the successor symbols from the state according to the transition relation of the automaton. Such an automaton is called a tree automaton.\\nInfinite tree input : The two extensions above can be combined, so the automaton reads a tree structure with (in)finite branches. Such an automaton is called an infinite tree automatonStatesSingle state: An automaton with one state, also called a combinational circuit, performs a transformation which may implement combinational logic.\\nFinite states: An automaton that contains only a finite number of states.\\nInfinite states: An automaton that may not have a finite number of states, or even a countable number of states. Different kinds of abstract memory may be used to give such machines finite descriptions.\\nStack memory: An automaton may also contain some extra memory in the form of a stack in which symbols can be pushed and popped. This kind of automaton is called a pushdown automaton\\nQueue memory: An automaton may have memory in the form of a queue. Such a machine is called queue machine and is Turing-complete.\\nTape memory: The inputs and outputs of automata are often described as input and output tapes. Some machines have additional working tapes, including the Turing machine, linear bounded automaton, and log-space transducer.Transition functionDeterministic: For a given current state and an input symbol, if an automaton can only jump to one and only one state then it is a deterministic automaton.\\nNondeterministic: An automaton that, after reading an input symbol, may jump into any of a number of states, as licensed by its transition relation. Notice that the term transition function is replaced by transition relation: The automaton non-deterministically decides to jump into one of the allowed choices. Such automata are called nondeterministic automata.\\nAlternation: This idea is quite similar to tree automaton but orthogonal. The automaton may run its multiple copies on the same next read symbol. Such automata are called alternating automata. Acceptance condition must satisfy all runs of such copies to accept the input.Acceptance conditionAcceptance of finite words: Same as described in the informal definition above.\\nAcceptance of infinite words: an omega automaton cannot have final states, as infinite words never terminate. Rather, acceptance of the word is decided by looking at the infinite sequence of visited states during the run.\\nProbabilistic acceptance: An automaton need not strictly accept or reject an input. It may accept the input with some probability between zero and one. For example, quantum finite automaton, geometric automaton and metric automaton have probabilistic acceptance.Different combinations of the above variations produce many classes of automaton.\\nAutomata theory is a subject matter that studies properties of various types of automata. For example, the following questions are studied about a given type of automata.\\n\\nWhich class of formal languages is recognizable by some type of automata? (Recognizable languages)\\nAre certain automata closed under union, intersection, or complementation of formal languages? (Closure properties)\\nHow expressive is a type of automata in terms of recognizing a class of formal languages? And, their relative expressive power? (Language hierarchy)Automata theory also studies the existence or nonexistence of any effective algorithms to solve problems similar to the following list:\\n\\nDoes an automaton accept any input word? (Emptiness checking)\\nIs it possible to transform a given non-deterministic automaton into deterministic automaton without changing the recognizable language? (Determinization)\\nFor a given formal language, what is the smallest automaton that recognizes it? (Minimization)\\n\\n\\n== Classes of automata ==\\nThe following is an incomplete list of types of automata.\\n\\n\\n=== Discrete, continuous, and hybrid automata ===\\nNormally automata theory describes the states of abstract machines but there are discrete automata, analog automata or continuous automata, or hybrid discrete-continuous automata, which use digital data, analog data or continuous time, or digital and analog data, respectively.\\n\\n\\n== Hierarchy in terms of powers ==\\nThe following is an incomplete hierarchy in terms of powers of different types of virtual machines. The hierarchy reflects the nested categories of languages the machines are able to accept.\\n\\n\\n== Applications ==\\nEach model in automata theory plays important roles in several applied areas. Finite automata are used in text processing, compilers, and hardware design. Context-free grammar (CFGs) are used in programming languages and artificial intelligence. Originally, CFGs were used in the study of the human languages. Cellular automata are used in the field of artificial life, the most famous example being John Conway\\'s Game of Life. Some other examples which could be explained using automata theory in biology include mollusk and pine cones growth and pigmentation patterns. Going further, a theory suggesting that the whole universe is computed by some sort of a discrete automaton, is advocated by some scientists. The idea originated in the work of Konrad Zuse, and was popularized in America by Edward Fredkin. Automata also appear in the theory of finite fields: the set of irreducible polynomials which can be written as composition of degree two polynomials is in fact a regular language.\\nAnother problem for which automata can be used is the induction of regular languages.\\n\\n\\n== Automata simulators ==\\nAutomata simulators are pedagogical tools used to teach, learn and research automata theory. An automata simulator takes as input the description of an automaton and then simulates its working for an arbitrary input string. The description of the automaton can be entered in several ways. An automaton can be defined in a symbolic language  or its specification may be entered in a predesigned form or its transition diagram may be drawn by clicking and dragging the mouse. Well known automata simulators include Turing\\'s World, JFLAP, VAS, TAGS and SimStudio.\\n\\n\\n== Connection to category theory ==\\nOne can define several distinct categories of automata following the automata classification into different types described in the previous section. The mathematical category of deterministic automata, sequential machines or sequential automata, and Turing machines with automata homomorphisms defining the arrows between automata is a Cartesian closed category, it has both categorical limits and colimits. An automata homomorphism maps a quintuple of an automaton Ai onto the quintuple of another automaton \\n Aj. Automata homomorphisms can also be considered as automata transformations or as semigroup homomorphisms, when the state space, S, of the automaton is defined as a semigroup Sg. Monoids are also considered as a suitable setting for automata in monoidal categories.\\nCategories of variable automataOne could also define a variable automaton, in the sense of Norbert Wiener in his book on The Human Use of Human Beings via the endomorphisms \\n  \\n    \\n      \\n        \\n          A\\n          \\n            i\\n          \\n        \\n        →\\n        \\n          A\\n          \\n            i\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle A_{i}\\\\to A_{i}}\\n  . Then, one can show that such variable automata homomorphisms form a mathematical group. In the case of non-deterministic, or other complex kinds of automata, the latter set of endomorphisms may become, however, a variable automaton groupoid. Therefore, in the most general case, categories of variable automata of any kind are categories of groupoids or groupoid categories. Moreover, the category of reversible automata is then a \\n2-category, and also a subcategory of the 2-category of groupoids, or the groupoid category.\\n\\n\\n== See also ==\\nBoolean differential calculus\\n\\n\\n== References ==\\n\\n\\n== Further reading ==\\nJohn E. Hopcroft, Rajeev Motwani, Jeffrey D. Ullman (2000). Introduction to Automata Theory, Languages, and Computation (2nd ed.). Pearson Education. ISBN 978-0-201-44124-6.CS1 maint: uses authors parameter (link)\\nMichael Sipser (1997). Introduction to the Theory of Computation. PWS Publishing. ISBN 978-0-534-94728-6. Part One: Automata and Languages, chapters 1–2, pp. 29–122. Section 4.1: Decidable Languages, pp. 152–159. Section 5.1: Undecidable Problems from Language Theory, pp. 172–183.\\nElaine Rich (2008). Automata, Computability and Complexity: Theory and Applications. Pearson. ISBN 978-0-13-228806-4.\\nSalomaa, Arto (1985). Computation and automata. Encyclopedia of Mathematics and Its Applications. 25. Cambridge University Press. ISBN 978-0-521-30245-6. Zbl 0565.68046.\\nAnderson, James A. (2006). Automata theory with modern applications. With contributions by Tom Head. Cambridge: Cambridge University Press. ISBN 978-0-521-61324-8. Zbl 1127.68049.\\nConway, J.H. (1971). Regular algebra and finite machines. Chapman and Hall Mathematics Series. London: Chapman & Hall. Zbl 0231.94041.\\nJohn M. Howie (1991) Automata and Languages, Clarendon Press ISBN 0-19-853424-8 MR1254435\\nSakarovitch, Jacques (2009). Elements of automata theory. Translated from the French by Reuben Thomas. Cambridge University Press. ISBN 978-0-521-84425-3. Zbl 1188.68177.\\nJames P. Schmeiser, David T. Barnard (1995). Producing a top-down parse order with bottom-up parsing. Elsevier North-Holland.CS1 maint: uses authors parameter (link)\\nIgor Aleksander, F.Keith Hanna (1975). Automata Theory : An Engineering Approach. New York: Crane Russak. ISBN 978-0-8448-0657-0.CS1 maint: uses authors parameter (link)\\nMarvin Minsky (1967). Computation : Finite and infinite machines. Princeton, N.J.: Prentice Hall.\\nJohn C. Martin (2011). Introduction to Languages and The Theory of Computation. New York, NY 10020: McGraw Hill. ISBN 978-0-07-319146-1.CS1 maint: location (link)\\n\\n\\n== External links ==\\nVisual Automata Simulator, a tool for simulating, visualizing and transforming finite-state automata and Turing Machines, by Jean Bovet\\nJFLAP\\ndk.brics.automaton\\nlibfa', 'A programming language is a formal language comprising a set of strings that produce various kinds of machine code output. Programming languages are one kind of computer language, and are used in computer programming to implement algorithms.\\nMost programming languages consist of instructions for computers. There are programmable machines that use a set of specific instructions, rather than general programming languages. Since the early 1800s, programs have been used to direct the behavior of machines such as Jacquard looms, music boxes and player pianos. The programs for these machines (such as a player piano\\'s scrolls) did not produce different behavior in response to different inputs or conditions.\\nThousands of different programming languages have been created, and more are being created every year. Many programming languages are written in an imperative form (i.e., as a sequence of operations to perform) while other languages use the declarative form (i.e. the desired result is specified, not how to achieve it).\\nThe description of a programming language is usually split into the two components of syntax (form) and semantics (meaning). Some languages are defined by a specification document (for example, the C programming language is specified by an ISO Standard) while other languages (such as Perl) have a dominant implementation that is treated as a reference. Some languages have both, with the basic language defined by a standard and extensions taken from the dominant implementation being common. \\nProgramming language theory is a subfield of computer science that deals with the design, implementation, analysis, characterization, and classification of programming languages.\\n\\n\\n== Definitions ==\\nA programming language is a notation for writing programs, which are specifications of a computation or algorithm. Some authors restrict the term \"programming language\" to those languages that can express all possible algorithms. Traits often considered important for what constitutes a programming language include:\\n\\nFunction and target\\nA computer programming language is a language used to write computer programs, which involves a computer performing some kind of computation or algorithm and possibly control external devices such as printers, disk drives, robots, and so on. For example, PostScript programs are frequently created by another program to control a computer printer or display. More generally, a programming language may describe computation on some, possibly abstract, machine. It is generally accepted that a complete specification for a programming language includes a description, possibly idealized, of a machine or processor for that language. In most practical contexts, a programming language involves a computer; consequently, programming languages are usually defined and studied this way. Programming languages differ from natural languages in that natural languages are only used for interaction between people, while programming languages also allow humans to communicate instructions to machines.\\nAbstractions\\nProgramming languages usually contain abstractions for defining and manipulating data structures or controlling the flow of execution. The practical necessity that a programming language support adequate abstractions is expressed by the abstraction principle. This principle is sometimes formulated as a recommendation to the programmer to make proper use of such abstractions.\\nExpressive power\\nThe theory of computation classifies languages by the computations they are capable of expressing. All Turing-complete languages can implement the same set of algorithms. ANSI/ISO SQL-92 and Charity are examples of languages that are not Turing complete, yet are often called programming languages.Markup languages like XML, HTML, or troff, which define structured data, are not usually considered programming languages. Programming languages may, however, share the syntax with markup languages if a computational semantics is defined. XSLT, for example, is a Turing complete language entirely using XML syntax. Moreover, LaTeX, which is mostly used for structuring documents, also contains a Turing complete subset.The term computer language is sometimes used interchangeably with programming language. However, the usage of both terms varies among authors, including the exact scope of each. One usage describes programming languages as a subset of computer languages. Similarly, languages used in computing that have a different goal than expressing computer programs are generically designated computer languages. For instance, markup languages are sometimes referred to as computer languages to emphasize that they are not meant to be used for programming.Another usage regards programming languages as theoretical constructs for programming abstract machines, and computer languages as the subset thereof that runs on physical computers, which have finite hardware resources. John C. Reynolds emphasizes that formal specification languages are just as much programming languages as are the languages intended for execution. He also argues that textual and even graphical input formats that affect the behavior of a computer are programming languages, despite the fact they are commonly not Turing-complete, and remarks that ignorance of programming language concepts is the reason for many flaws in input formats.\\n\\n\\n== History ==\\n\\n\\n=== Early developments ===\\nVery early computers, such as Colossus, were programmed without the help of a stored program, by modifying their circuitry or setting banks of physical controls.\\nSlightly later, programs could be written in machine language, where the programmer writes each instruction in a numeric form the hardware can execute directly. For example, the instruction to add the value in two memory locations might consist of 3 numbers: an \"opcode\" that selects the \"add\" operation, and two memory locations. The programs, in decimal or binary form, were read in from punched cards, paper tape, magnetic tape or toggled in on switches on the front panel of the computer.  Machine languages were later termed first-generation programming languages (1GL).\\nThe next step was the development of the so-called second-generation programming languages (2GL) or assembly languages, which were still closely tied to the instruction set architecture of the specific computer. These served to make the program much more human-readable and relieved the programmer of tedious and error-prone address calculations.\\nThe first high-level programming languages, or third-generation programming languages (3GL), were written in the 1950s. An early high-level programming language to be designed for a computer was Plankalkül, developed for the German Z3 by Konrad Zuse between 1943 and 1945. However, it was not implemented until 1998 and 2000.John Mauchly\\'s Short Code, proposed in 1949, was one of the first high-level languages ever developed for an electronic computer. Unlike machine code, Short Code statements represented mathematical expressions in understandable form. However, the program had to be translated into machine code every time it ran, making the process much slower than running the equivalent machine code.\\nAt the University of Manchester, Alick Glennie developed Autocode in the early 1950s. As a programming language, it used a compiler to automatically convert the language into machine code. The first code and compiler was developed in 1952 for the Mark 1 computer at the University of Manchester and is considered to be the first compiled high-level programming language.The second autocode was developed for the Mark 1 by R. A. Brooker in 1954 and was called the \"Mark 1 Autocode\". Brooker also developed an autocode for the Ferranti Mercury in the 1950s in conjunction with the University of Manchester. The version for the EDSAC 2 was devised by D. F. Hartley of University of Cambridge Mathematical Laboratory in 1961. Known as EDSAC 2 Autocode, it was a straight development from Mercury Autocode adapted for local circumstances and was noted for its object code optimisation and source-language diagnostics which were advanced for the time. A contemporary but separate thread of development, Atlas Autocode was developed for the University of Manchester Atlas 1 machine.\\nIn 1954, FORTRAN was invented at IBM by John Backus. It was the first widely used high-level general purpose programming language to have a functional implementation, as opposed to just a design on paper. It is still a popular language for high-performance computing and is used for programs that benchmark and rank the world\\'s fastest supercomputers.Another early programming language was devised by Grace Hopper in the US, called FLOW-MATIC. It was developed for the UNIVAC I at Remington Rand during the period from 1955 until 1959. Hopper found that business data processing customers were uncomfortable with mathematical notation, and in early 1955, she and her team wrote a specification for an English programming language and implemented a prototype. The FLOW-MATIC compiler became publicly available in early 1958 and was substantially complete in 1959. FLOW-MATIC was a major influence in the design of COBOL, since only it and its direct descendant AIMACO were in actual use at the time.\\n\\n\\n=== Refinement ===\\nThe increased use of high-level languages introduced a requirement for low-level programming languages or system programming languages. These languages, to varying degrees, provide facilities between assembly languages and high-level languages. They can be used to perform tasks that require direct access to hardware facilities but still provide higher-level control structures and error-checking.\\nThe period from the 1960s to the late 1970s brought the development of the major language paradigms now in use:\\n\\nAPL introduced array programming and influenced functional programming.\\nALGOL refined both structured procedural programming and the discipline of language specification; the \"Revised Report on the Algorithmic Language ALGOL 60\" became a model for how later language specifications were written.\\nLisp, implemented in 1958, was the first dynamically typed functional programming language.\\nIn the 1960s, Simula was the first language designed to support object-oriented programming; in the mid-1970s, Smalltalk followed with the first \"purely\" object-oriented language.\\nC was developed between 1969 and 1973 as a system programming language for the Unix operating system and remains popular.\\nProlog, designed in 1972, was the first logic programming language.\\nIn 1978, ML built a polymorphic type system on top of Lisp, pioneering statically typed functional programming languages.Each of these languages spawned descendants, and most modern programming languages count at least one of them in their ancestry.\\nThe 1960s and 1970s also saw considerable debate over the merits of structured programming, and whether programming languages should be designed to support it. Edsger Dijkstra, in a famous 1968 letter published in the Communications of the ACM, argued that Goto statements should be eliminated from all \"higher level\" programming languages.\\n\\n\\n=== Consolidation and growth ===\\n\\nThe 1980s were years of relative consolidation. C++ combined object-oriented and systems programming. The United States government standardized Ada, a systems programming language derived from Pascal and intended for use by defense contractors. In Japan and elsewhere, vast sums were spent investigating the so-called \"fifth-generation\" languages that incorporated logic programming constructs. The functional languages community moved to standardize ML and Lisp. Rather than inventing new paradigms, all of these movements elaborated upon the ideas invented in the previous decades.\\nOne important trend in language design for programming large-scale systems during the 1980s was an increased focus on the use of modules or large-scale organizational units of code. Modula-2, Ada, and ML all developed notable module systems in the 1980s, which were often wedded to generic programming constructs.The rapid growth of the Internet in the mid-1990s created opportunities for new languages. Perl, originally a Unix scripting tool first released in 1987, became common in dynamic websites. Java came to be used for server-side programming, and bytecode virtual machines became popular again in commercial settings with their promise of \"Write once, run anywhere\" (UCSD Pascal had been popular for a time in the early 1980s). These developments were not fundamentally novel; rather, they were refinements of many existing languages and paradigms (although their syntax was often based on the C family of programming languages).\\nProgramming language evolution continues, in both industry and research. Current directions include security and reliability verification, new kinds of modularity (mixins, delegates, aspects), and database integration such as Microsoft\\'s LINQ.\\nFourth-generation programming languages (4GL) are computer programming languages that aim to provide a higher level of abstraction of the internal computer hardware details than 3GLs. Fifth-generation programming languages (5GL) are programming languages based on solving problems using constraints given to the program, rather than using an algorithm written by a programmer.\\n\\n\\n== Elements ==\\nAll programming languages have some primitive building blocks for the description of data and the processes or transformations applied to them (like the addition of two numbers or the selection of an item from a collection). These primitives are defined by syntactic and semantic rules which describe their structure and meaning respectively.\\n\\n\\n=== Syntax ===\\n\\nA programming language\\'s surface form is known as its syntax. Most programming languages are purely textual; they use sequences of text including words, numbers, and punctuation, much like written natural languages. On the other hand, there are some programming languages which are more graphical in nature, using visual relationships between symbols to specify a program.\\nThe syntax of a language describes the possible combinations of symbols that form a syntactically correct program. The meaning given to a combination of symbols is handled by semantics (either formal or hard-coded in a reference implementation). Since most languages are textual, this article discusses textual syntax.\\nProgramming language syntax is usually defined using a combination of regular expressions (for lexical structure) and Backus–Naur form (for grammatical structure). Below is a simple grammar, based on Lisp:\\n\\nThis grammar specifies the following:\\n\\nan expression is either an atom or a list;\\nan atom is either a number or a symbol;\\na number is an unbroken sequence of one or more decimal digits, optionally preceded by a plus or minus sign;\\na symbol is a letter followed by zero or more of any characters (excluding whitespace); and\\na list is a matched pair of parentheses, with zero or more expressions inside it.The following are examples of well-formed token sequences in this grammar: 12345, () and (a b c232 (1)).\\nNot all syntactically correct programs are semantically correct. Many syntactically correct programs are nonetheless ill-formed, per the language\\'s rules; and may (depending on the language specification and the soundness of the implementation) result in an error on translation or execution. In some cases, such programs may exhibit undefined behavior. Even when a program is well-defined within a language, it may still have a meaning that is not intended by the person who wrote it.\\nUsing natural language as an example, it may not be possible to assign a meaning to a grammatically correct sentence or the sentence may be false:\\n\\n\"Colorless green ideas sleep furiously.\" is grammatically well-formed but has no generally accepted meaning.\\n\"John is a married bachelor.\" is grammatically well-formed but expresses a meaning that cannot be true.The following C language fragment is syntactically correct, but performs operations that are not semantically defined (the operation *p >> 4 has no meaning for a value having a complex type and p->im is not defined because the value of p is the null pointer):\\n\\nIf the type declaration on the first line were omitted, the program would trigger an error on undefined variable p during compilation. However, the program would still be syntactically correct since type declarations provide only semantic information.\\nThe grammar needed to specify a programming language can be classified by its position in the Chomsky hierarchy. The syntax of most programming languages can be specified using a Type-2 grammar, i.e., they are context-free grammars. Some languages, including Perl and Lisp, contain constructs that allow execution during the parsing phase. Languages that have constructs that allow the programmer to alter the behavior of the parser make syntax analysis an undecidable problem, and generally blur the distinction between parsing and execution. In contrast to Lisp\\'s macro system and Perl\\'s BEGIN blocks, which may contain general computations, C macros are merely string replacements and do not require code execution.\\n\\n\\n=== Semantics ===\\nThe term semantics refers to the meaning of languages, as opposed to their form (syntax).\\n\\n\\n==== Static semantics ====\\nThe static semantics defines restrictions on the structure of valid texts that are hard or impossible to express in standard syntactic formalisms. For compiled languages, static semantics essentially include those semantic rules that can be checked at compile time. Examples include checking that every identifier is declared before it is used (in languages that require such declarations) or that the labels on the arms of a case statement are distinct. Many important restrictions of this type, like checking that identifiers are used in the appropriate context (e.g. not adding an integer to a function name), or that subroutine calls have the appropriate number and type of arguments, can be enforced by defining them as rules in a logic called a type system. Other forms of static analyses like data flow analysis may also be part of static semantics. Newer programming languages like Java and C# have definite assignment analysis, a form of data flow analysis, as part of their static semantics.\\n\\n\\n==== Dynamic semantics ====\\n\\nOnce data has been specified, the machine must be instructed to perform operations on the data. For example, the semantics may define the strategy by which expressions are evaluated to values, or the manner in which control structures conditionally execute statements. The dynamic semantics (also known as execution semantics) of a language defines how and when the various constructs of a language should produce a program behavior. There are many ways of defining execution semantics. Natural language is often used to specify the execution semantics of languages commonly used in practice. A significant amount of academic research went into formal semantics of programming languages, which allow execution semantics to be specified in a formal manner. Results from this field of research have seen limited application to programming language design and implementation outside academia.\\n\\n\\n=== Type system ===\\n\\nA type system defines how a programming language classifies values and expressions into types, how it can manipulate those types and how they interact. The goal of a type system is to verify and usually enforce a certain level of correctness in programs written in that language by detecting certain incorrect operations. Any decidable type system involves a trade-off: while it rejects many incorrect programs, it can also prohibit some correct, albeit unusual programs. In order to bypass this downside, a number of languages have type loopholes, usually unchecked casts that may be used by the programmer to explicitly allow a normally disallowed operation between different types. In most typed languages, the type system is used only to type check programs, but a number of languages, usually functional ones, infer types, relieving the programmer from the need to write type annotations. The formal design and study of type systems is known as type theory.\\n\\n\\n==== Typed versus untyped languages ====\\nA language is typed if the specification of every operation defines types of data to which the operation is applicable. For example, the data represented by \"this text between the quotes\" is a string, and in many programming languages dividing a number by a string has no meaning and will not be executed. The invalid operation may be detected when the program is compiled (\"static\" type checking) and will be rejected by the compiler with a compilation error message, or it may be detected while the program is running (\"dynamic\" type checking), resulting in a run-time exception. Many languages allow a function called an exception handler to handle this exception and, for example, always return \"-1\" as the result.\\nA special case of typed languages are the single-typed languages. These are often scripting or markup languages, such as REXX or SGML, and have only one data type–—most commonly character strings which are used for both symbolic and numeric data.\\nIn contrast, an untyped language, such as most assembly languages, allows any operation to be performed on any data, generally sequences of bits of various lengths. High-level untyped languages include BCPL, Tcl, and some varieties of Forth.\\nIn practice, while few languages are considered typed from the type theory (verifying or rejecting all operations), most modern languages offer a degree of typing. Many production languages provide means to bypass or subvert the type system, trading type-safety for finer control over the program\\'s execution (see casting).\\n\\n\\n==== Static versus dynamic typing ====\\nIn static typing, all expressions have their types determined prior to when the program is executed, typically at compile-time. For example, 1 and (2+2) are integer expressions; they cannot be passed to a function that expects a string, or stored in a variable that is defined to hold dates.Statically typed languages can be either manifestly typed or type-inferred. In the first case, the programmer must explicitly write types at certain textual positions (for example, at variable declarations). In the second case, the compiler infers the types of expressions and declarations based on context. Most mainstream statically typed languages, such as C++, C# and Java, are manifestly typed. Complete type inference has traditionally been associated with less mainstream languages, such as Haskell and ML. However, many manifestly typed languages support partial type inference; for example, C++, Java and C# all infer types in certain limited cases. Additionally, some programming languages allow for some types to be automatically converted to other types; for example, an int can be used where the program expects a float.\\nDynamic typing, also called latent typing, determines the type-safety of operations at run time; in other words, types are associated with run-time values rather than textual expressions. As with type-inferred languages, dynamically typed languages do not require the programmer to write explicit type annotations on expressions. Among other things, this may permit a single variable to refer to values of different types at different points in the program execution. However, type errors cannot be automatically detected until a piece of code is actually executed, potentially making debugging more difficult. Lisp, Smalltalk, Perl, Python, JavaScript, and Ruby are all examples of dynamically typed languages.\\n\\n\\n==== Weak and strong typing ====\\nWeak typing allows a value of one type to be treated as another, for example treating a string as a number. This can occasionally be useful, but it can also allow some kinds of program faults to go undetected at compile time and even at run time.\\nStrong typing prevents these program faults. An attempt to perform an operation on the wrong type of value raises an error. Strongly typed languages are often termed type-safe or safe.\\nAn alternative definition for \"weakly typed\" refers to languages, such as Perl and JavaScript, which permit a large number of implicit type conversions. In JavaScript, for example, the expression 2 * x implicitly converts x to a number, and this conversion succeeds even if x is null, undefined, an Array, or a string of letters. Such implicit conversions are often useful, but they can mask programming errors.\\nStrong and static are now generally considered orthogonal concepts, but usage in the literature differs. Some use the term strongly typed to mean strongly, statically typed, or, even more confusingly, to mean simply statically typed. Thus C has been called both strongly typed and weakly, statically typed.It may seem odd to some professional programmers that C could be \"weakly, statically typed\". However, notice that the use of the generic pointer, the void* pointer, does allow for casting of pointers to other pointers without needing to do an explicit cast. This is extremely similar to somehow casting an array of bytes to any kind of datatype in C without using an explicit cast, such as (int) or (char).\\n\\n\\n=== Standard library and run-time system ===\\n\\nMost programming languages have an associated core library (sometimes known as the \\'standard library\\', especially if it is included as part of the published language standard), which is conventionally made available by all implementations of the language. Core libraries typically include definitions for commonly used algorithms, data structures, and mechanisms for input and output.\\nThe line between a language and its core library differs from language to language. In some cases, the language designers may treat the library as a separate entity from the language. However, a language\\'s core library is often treated as part of the language by its users, and some language specifications even require that this library be made available in all implementations. Indeed, some languages are designed so that the meanings of certain syntactic constructs cannot even be described without referring to the core library. For example, in Java, a string literal is defined as an instance of the java.lang.String class; similarly, in Smalltalk, an anonymous function expression (a \"block\") constructs an instance of the library\\'s BlockContext class. Conversely, Scheme contains multiple coherent subsets that suffice to construct the rest of the language as library macros, and so the language designers do not even bother to say which portions of the language must be implemented as language constructs, and which must be implemented as parts of a library.\\n\\n\\n== Design and implementation ==\\nProgramming languages share properties with natural languages related to their purpose as vehicles for communication, having a syntactic form separate from its semantics, and showing language families of related languages branching one from another. But as artificial constructs, they also differ in fundamental ways from languages that have evolved through usage. A significant difference is that a programming language can be fully described and studied in its entirety since it has a precise and finite definition. By contrast, natural languages have changing meanings given by their users in different communities. While constructed languages are also artificial languages designed from the ground up with a specific purpose, they lack the precise and complete semantic definition that a programming language has.\\nMany programming languages have been designed from scratch, altered to meet new needs, and combined with other languages. Many have eventually fallen into disuse. Although there have been attempts to design one \"universal\" programming language that serves all purposes, all of them have failed to be generally accepted as filling this role. The need for diverse programming languages arises from the diversity of contexts in which languages are used:\\n\\nPrograms range from tiny scripts written by individual hobbyists to huge systems written by hundreds of programmers.\\nProgrammers range in expertise from novices who need simplicity above all else to experts who may be comfortable with considerable complexity.\\nPrograms must balance speed, size, and simplicity on systems ranging from microcontrollers to supercomputers.\\nPrograms may be written once and not change for generations, or they may undergo continual modification.\\nProgrammers may simply differ in their tastes: they may be accustomed to discussing problems and expressing them in a particular language.One common trend in the development of programming languages has been to add more ability to solve problems using a higher level of abstraction. The earliest programming languages were tied very closely to the underlying hardware of the computer. As new programming languages have developed, features have been added that let programmers express ideas that are more remote from simple translation into underlying hardware instructions. Because programmers are less tied to the complexity of the computer, their programs can do more computing with less effort from the programmer. This lets them write more functionality per time unit.\\nNatural language programming has been proposed as a way to eliminate the need for a specialized language for programming. However, this goal remains distant and its benefits are open to debate. Edsger W. Dijkstra took the position that the use of a formal language is essential to prevent the introduction of meaningless constructs, and dismissed natural language programming as \"foolish\". Alan Perlis was similarly dismissive of the idea. Hybrid approaches have been taken in Structured English and SQL.\\nA language\\'s designers and users must construct a number of artifacts that govern and enable the practice of programming. The most important of these artifacts are the language specification and implementation.\\n\\n\\n=== Specification ===\\n\\nThe specification of a programming language is an artifact that the language users and the implementors can use to agree upon whether a piece of source code is a valid program in that language, and if so what its behavior shall be.\\nA programming language specification can take several forms, including the following:\\n\\nAn explicit definition of the syntax, static semantics, and execution semantics of the language. While syntax is commonly specified using a formal grammar, semantic definitions may be written in natural language (e.g., as in the C language), or a formal semantics (e.g., as in Standard ML and Scheme specifications).\\nA description of the behavior of a translator for the language (e.g., the C++ and Fortran specifications). The syntax and semantics of the language have to be inferred from this description, which may be written in natural or a formal language.\\nA reference or model implementation, sometimes written in the language being specified (e.g., Prolog or ANSI REXX). The syntax and semantics of the language are explicit in the behavior of the reference implementation.\\n\\n\\n=== Implementation ===\\n\\nAn implementation of a programming language provides a way to write programs in that language and execute them on one or more configurations of hardware and software. There are, broadly, two approaches to programming language implementation: compilation and interpretation. It is generally possible to implement a language using either technique.\\nThe output of a compiler may be executed by hardware or a program called an interpreter. In some implementations that make use of the interpreter approach there is no distinct boundary between compiling and interpreting. For instance, some implementations of BASIC compile and then execute the source a line at a time.\\nPrograms that are executed directly on the hardware usually run much faster than those that are interpreted in software.One technique for improving the performance of interpreted programs is just-in-time compilation. Here the virtual machine, just before execution, translates the blocks of bytecode which are going to be used to machine code, for direct execution on the hardware.\\n\\n\\n== Proprietary languages ==\\nAlthough most of the most commonly used programming languages have fully open specifications and implementations, many programming languages exist only as proprietary programming languages with the implementation available only from a single vendor, which may claim that such a proprietary language is their intellectual property. Proprietary programming languages are commonly domain specific languages or internal scripting languages for a single product; some proprietary languages are used only internally within a vendor, while others are available to external users.\\nSome programming languages exist on the border between proprietary and open; for example, Oracle Corporation asserts proprietary rights to some aspects of the Java programming language, and Microsoft\\'s C# programming language, which has open implementations of most parts of the system, also has Common Language Runtime (CLR) as a closed environment.Many proprietary languages are widely used, in spite of their proprietary nature; examples include MATLAB, VBScript, and Wolfram Language.  Some languages may make the transition from closed to open; for example, Erlang was originally an Ericsson\\'s internal programming language.\\n\\n\\n== Use ==\\nThousands of different programming languages have been created, mainly in the computing field.\\nIndividual software projects commonly use five programming languages or more.Programming languages differ from most other forms of human expression in that they require a greater degree of precision and completeness. When using a natural language to communicate with other people, human authors and speakers can be ambiguous and make small errors, and still expect their intent to be understood. However, figuratively speaking, computers \"do exactly what they are told to do\", and cannot \"understand\" what code the programmer intended to write. The combination of the language definition, a program, and the program\\'s inputs must fully specify the external behavior that occurs when the program is executed, within the domain of control of that program. On the other hand, ideas about an algorithm can be communicated to humans without the precision required for execution by using pseudocode, which interleaves natural language with code written in a programming language.\\nA programming language provides a structured mechanism for defining pieces of data, and the operations or transformations that may be carried out automatically on that data. A programmer uses the abstractions present in the language to represent the concepts involved in a computation. These concepts are represented as a collection of the simplest elements available (called primitives). Programming is the process by which programmers combine these primitives to compose new programs, or adapt existing ones to new uses or a changing environment.\\nPrograms for a computer might be executed in a batch process without human interaction, or a user might type commands in an interactive session of an interpreter. In this case the \"commands\" are simply programs, whose execution is chained together. When a language can run its commands through an interpreter (such as a Unix shell or other command-line interface), without compiling, it is called a scripting language.\\n\\n\\n=== Measuring language usage ===\\n\\nDetermining which is the most widely used programming language is difficult since the definition of usage varies by context. One language may occupy the greater number of programmer hours, a different one has more lines of code, and a third may consume the most CPU time. Some languages are very popular for particular kinds of applications. For example, COBOL is still strong in the corporate data center, often on large mainframes; Fortran in scientific and engineering applications; Ada in aerospace, transportation, military, real-time and embedded applications; and C in embedded applications and operating systems. Other languages are regularly used to write many different kinds of applications.\\nVarious methods of measuring language popularity, each subject to a different bias over what is measured, have been proposed:\\n\\ncounting the number of job advertisements that mention the language\\nthe number of books sold that teach or describe the language\\nestimates of the number of existing lines of code written in the language –  which may underestimate languages not often found in public searches\\ncounts of language references (i.e., to the name of the language) found using a web search engine.Combining and averaging information from various internet sites, stackify.com reported the ten most popular programming languages as (in descending order by overall popularity): Java, C, C++, Python, C#, JavaScript, VB .NET, R, PHP, and MATLAB.\\n\\n\\n== Dialects, flavors and implementations ==\\nA dialect of a programming language or a data exchange language is a (relatively small) variation or extension of the language that does not change its intrinsic nature. With languages such as Scheme and Forth, standards may be considered insufficient, inadequate or illegitimate by implementors, so often they will deviate from the standard, making a new dialect. In other cases, a dialect is created for use in a domain-specific language, often a subset. In the Lisp world, most languages that use basic S-expression syntax and Lisp-like semantics are considered Lisp dialects, although they vary wildly, as do, say, Racket and Clojure. As it is common for one language to have several dialects, it can become quite difficult for an inexperienced programmer to find the right documentation. The BASIC programming language has many dialects.\\nThe explosion of Forth dialects led to the saying \"If you\\'ve seen one Forth... you\\'ve seen one Forth.\"\\n\\n\\n== Taxonomies ==\\n\\nThere is no overarching classification scheme for programming languages. A given programming language does not usually have a single ancestor language. Languages commonly arise by combining the elements of several predecessor languages with new ideas in circulation at the time. Ideas that originate in one language will diffuse throughout a family of related languages, and then leap suddenly across familial gaps to appear in an entirely different family.\\nThe task is further complicated by the fact that languages can be classified along multiple axes. For example, Java is both an object-oriented language (because it encourages object-oriented organization) and a concurrent language (because it contains built-in constructs for running multiple threads in parallel). Python is an object-oriented scripting language.\\nIn broad strokes, programming languages divide into programming paradigms and a classification by intended domain of use, with general-purpose programming languages distinguished from domain-specific programming languages. Traditionally, programming languages have been regarded as describing computation in terms of imperative sentences, i.e. issuing commands. These are generally called imperative programming languages. A great deal of research in programming languages has been aimed at blurring the distinction between a program as a set of instructions and a program as an assertion about the desired answer, which is the main feature of declarative programming. More refined paradigms include procedural programming, object-oriented programming, functional programming, and logic programming; some languages are hybrids of paradigms or multi-paradigmatic. An assembly language is not so much a paradigm as a direct model of an underlying machine architecture. By purpose, programming languages might be considered general purpose, system programming languages, scripting languages, domain-specific languages, or concurrent/distributed languages (or a combination of these). Some general purpose languages were designed largely with educational goals.A programming language may also be classified by factors unrelated to programming paradigm. For instance, most programming languages use English language keywords, while a minority do not. Other languages may be classified as being deliberately esoteric or not.\\n\\n\\n== See also ==\\n\\n\\n== References ==\\n\\n\\n== Further reading ==\\n\\n\\n== External links ==', 'Programming paradigms are a way to classify programming languages based on their features. Languages can be classified into multiple paradigms.\\nSome paradigms are concerned mainly with implications for the execution model of the language, such as allowing side effects, or whether the sequence of operations is defined by the execution model.  Other paradigms are concerned mainly with the way that code is organized, such as grouping a code into units along with the state that is modified by the code.  Yet others are concerned mainly with the style of syntax and grammar.\\nCommon programming paradigms include:\\nimperative in which the programmer instructs the machine how to change its state,\\nprocedural which groups instructions into procedures,\\nobject-oriented which groups instructions with the part of the state they operate on,\\ndeclarative in which the programmer merely declares properties of the desired result, but not how to compute it\\nfunctional in which the desired result is declared as the value of a series of function applications,\\nlogic in which the desired result is declared as the answer to a question about a system of facts and rules,\\nmathematical in which the desired result is declared as the solution of an optimization problem\\nreactive in which the desired result is declared with data streams and the propagation of changeSymbolic techniques such as reflection, which allow the program to refer to itself, might also be considered as a programming paradigm. However, this is compatible with the major paradigms and thus is not a real paradigm in its own right.\\nFor example, languages that fall into the imperative paradigm have two main features: they state the order in which operations occur, with constructs that explicitly control that order, and they allow side effects, in which state can be modified at one point in time, within one unit of code, and then later read at a different point in time inside a different unit of code.  The communication between the units of code is not explicit.  Meanwhile, in object-oriented programming, code is organized into objects that contain a state that is only modified by the code that is part of the object.  Most object-oriented languages are also imperative languages.  In contrast, languages that fit the declarative paradigm do not state the order in which to execute operations.  Instead, they supply a number of available operations in the system, along with the conditions under which each is allowed to execute.  The implementation of the language\\'s execution model tracks which operations are free to execute and chooses the order independently. More at  Comparison of multi-paradigm programming languages.\\n\\n\\n== Overview ==\\n\\nJust as software engineering (as a process) is defined by differing methodologies, so the programming languages (as models of computation) are defined by differing paradigms. Some languages are designed to support one paradigm (Smalltalk supports object-oriented programming, Haskell supports functional programming), while other programming languages support multiple paradigms (such as Object Pascal, C++, Java, JavaScript, C#, Scala, Visual Basic, Common Lisp, Scheme, Perl, PHP, Python, Ruby, Oz, and F#). For example, programs written in C++, Object Pascal or PHP can be purely procedural, purely object-oriented, or can contain elements of both or other paradigms. Software designers and programmers decide how to use those paradigm elements.\\nIn object-oriented programming, programs are treated as a set of interacting objects. In functional programming, programs are treated as a sequence of stateless function evaluations. When programming computers or systems with many processors, in process-oriented programming, programs are treated as sets of concurrent processes that act on a logical shared data structures.\\nMany programming paradigms are as well known for the techniques they forbid as for those they enable. For instance, pure functional programming disallows use of side-effects, while structured programming disallows use of the goto statement. Partly for this reason, new paradigms are often regarded as doctrinaire or overly rigid by those accustomed to earlier styles. Yet, avoiding certain techniques can make it easier to understand program behavior, and to prove theorems about program correctness.\\nProgramming paradigms can also be compared with programming models, which allows invoking an execution model by using only an API. Programming models can also be classified into paradigms based on features of the execution model.\\nFor parallel computing, using a programming model instead of a language is common.  The reason is that details of the parallel hardware leak into the abstractions used to program the hardware.  This causes the programmer to have to map patterns in the algorithm onto patterns in the execution model (which have been inserted due to leakage of hardware into the abstraction).  As a consequence, no one parallel programming language maps well to all computation problems.  Thus, it is more convenient to use a base sequential language and insert API calls to parallel execution models via a programming model.  Such parallel programming models can be classified according to abstractions that reflect the hardware, such as shared memory, distributed memory with message passing, notions of place visible in the code, and so forth.  These can be considered flavors of programming paradigm that apply to only parallel languages and programming models.\\n\\n\\n== Criticism ==\\nSome programming language researchers criticise the notion of paradigms as a classification of programming languages, e.g. Harper, and Krishnamurthi.  They argue that many programming languages cannot be strictly classified into one paradigm, but rather include features from several paradigms. See Comparison of multi-paradigm programming languages.\\n\\n\\n== History ==\\nDifferent approaches to programming have developed over time, being identified as such either at the time or retrospectively. An early approach consciously identified as such is structured programming, advocated since the mid 1960s. The concept of a \"programming paradigm\" as such dates at least to 1978, in the Turing Award lecture of Robert W. Floyd, entitled The Paradigms of Programming, which cites the notion of paradigm as used by Thomas Kuhn in his The Structure of Scientific Revolutions (1962).\\n\\n\\n=== Machine code ===\\nThe lowest-level programming paradigms are machine code, which directly represents the instructions (the contents of program memory) as a sequence of numbers, and assembly language where the machine instructions are represented by mnemonics and memory addresses can be given symbolic labels. These are sometimes called first- and second-generation languages.\\nIn the 1960s, assembly languages were developed to support library COPY and quite sophisticated conditional macro generation and preprocessing abilities, CALL to (subroutines), external variables and common sections (globals), enabling significant code re-use and isolation from hardware specifics via the use of logical operators such as READ/WRITE/GET/PUT. Assembly was and still is, used for time-critical systems and often in embedded systems as it gives the most direct control of what the machine does.\\n\\n\\n=== Procedural languages ===\\nThe next advance was the development of procedural languages. These third-generation languages (the first described as high-level languages) use vocabulary related to the problem being solved. For example,\\n\\nCOmmon Business Oriented Language (COBOL) –  uses terms like file, move and copy.\\nFORmula TRANslation (FORTRAN) –  using mathematical language terminology, it was developed mainly for scientific and engineering problems.\\nALGOrithmic Language (ALGOL) –  focused on being an appropriate language to define algorithms, while using mathematical language terminology, targeting scientific and engineering problems, just like FORTRAN.\\nProgramming Language One (PL/I) –  a hybrid commercial-scientific general purpose language supporting pointers.\\nBeginners All purpose Symbolic Instruction Code (BASIC) –  it was developed to enable more people to write programs.\\nC –  a general-purpose programming language, initially developed by Dennis Ritchie between 1969 and 1973 at AT&T Bell Labs.All these languages follow the procedural paradigm. That is, they describe, step by step, exactly the procedure that should, according to the particular programmer at least, be followed to solve a specific problem. The efficacy and efficiency of any such solution are both therefore entirely subjective and highly dependent on that programmer\\'s experience, inventiveness, and ability.\\n\\n\\n=== Object-oriented programming ===\\n\\nFollowing the widespread use of procedural languages, object-oriented programming (OOP) languages were created, such as Simula, Smalltalk, C++, Eiffel, Python, PHP, Java, and C#. In these languages, data and methods to manipulate it are kept as one unit called an object.  With perfect encapsulation, one of the distinguishing features of OOP, the only way that another object or user would be able to access the data is via the object\\'s methods. Thus, an object\\'s inner workings may be changed without affecting any code that uses the object. There is still some controversy raised by Alexander Stepanov, Richard Stallman and other programmers, concerning the efficacy of the OOP paradigm versus the procedural paradigm. The need for every object to have associative methods leads some skeptics to associate OOP with software bloat; an attempt to resolve this dilemma came through polymorphism.\\nBecause object-oriented programming is considered a paradigm, not a language, it is possible to create even an object-oriented assembler language. High Level Assembly (HLA) is an example of this that fully supports advanced data types and object-oriented assembly language programming –  despite its early origins. Thus, differing programming paradigms can be seen rather like motivational memes of their advocates, rather than necessarily representing progress from one level to the next. Precise comparisons of competing paradigms\\' efficacy are frequently made more difficult because of new and differing terminology applied to similar entities and processes together with numerous implementation distinctions across languages.\\n\\n\\n=== Further paradigms ===\\nLiterate programming, as a form of imperative programming, structures programs as a human-centered web, as in a hypertext essay: documentation is integral to the program, and the program is structured following the logic of prose exposition, rather than compiler convenience.\\nIndependent of the imperative branch, declarative programming paradigms were developed. In these languages, the computer is told what the problem is, not how to solve the problem –  the program is structured as a set of properties to find in the expected result, not as a procedure to follow. Given a database or a set of rules, the computer tries to find a solution matching all the desired properties. An archetype of a declarative language is the fourth generation language SQL, and the family of functional languages and logic programming.\\nFunctional programming is a subset of declarative programming. Programs written using this paradigm use functions, blocks of code intended to behave like mathematical functions. Functional languages discourage changes in the value of variables through assignment, making a great deal of use of recursion instead.\\nThe logic programming paradigm views computation as automated reasoning over a body of knowledge. Facts about the problem domain are expressed as logic formulas, and programs are executed by applying inference rules over them until an answer to the problem is found, or the set of formulas is proved inconsistent.\\nSymbolic programming is a paradigm that describes programs able to manipulate formulas and program components as data.  Programs can thus effectively modify themselves, and appear to \"learn\", making them suited for applications such as artificial intelligence, expert systems, natural-language processing and computer games.  Languages that support this paradigm include Lisp and Prolog.Differentiable programming structures programs so that they can be differentiated throughout, usually via automatic differentiation.\\n\\n\\n== Support for multiple paradigms ==\\n\\nMost programming languages support more than one programming paradigm to allow programmers to use the most suitable programming style and associated language constructs for a given job.\\n\\n\\n== See also ==\\n\\n\\n== References ==\\n\\n\\n== External links ==\\nClassification of the principal programming paradigms\\nHow programming paradigms evolve and get adopted?', 'Object-oriented programming (OOP) is a programming paradigm based on the concept of \"objects\", which can contain data and code: data in the form of fields (often known as attributes or properties), and code, in the form of procedures (often known as methods). \\nA feature of objects is that an object\\'s own procedures can access and often modify the data fields of itself (objects have a notion of this or self). In OOP, computer programs are designed by making them out of objects that interact with one another. OOP languages are diverse, but the most popular ones are class-based, meaning that objects are instances of classes, which also determine their types.\\nMany of the most widely used programming languages (such as C++, Java, Python, etc.) are multi-paradigm and they support object-oriented programming to a greater or lesser degree, typically in combination with imperative, procedural programming. Significant object-oriented languages include:\\nJava,\\nC++,\\nC#,\\nPython,\\nR,\\nPHP,\\nVisual Basic.NET,\\nJavaScript,\\nRuby,\\nPerl,\\nSIMSCRIPT,\\nObject Pascal,\\nObjective-C,\\nDart,\\nSwift,\\nScala,\\nKotlin,\\nCommon Lisp,\\nMATLAB,\\nand\\nSmalltalk.\\n\\n\\n== History ==\\n\\nTerminology invoking \"objects\" and \"oriented\" in the modern sense of object-oriented programming made its first appearance at MIT in the late 1950s and early 1960s. In the environment of the artificial intelligence group, as early as 1960, \"object\" could refer to identified items (LISP atoms) with properties (attributes);Alan Kay later cited a detailed understanding of LISP internals as a strong influence on his thinking in 1966.\\n\\nAnother early MIT example was Sketchpad created by Ivan Sutherland in 1960–1961; in the glossary of the 1963 technical report based on his dissertation about Sketchpad, Sutherland defined notions of \"object\" and \"instance\" (with the class concept covered by \"master\" or \"definition\"), albeit specialized to graphical interaction.\\nAlso, an MIT ALGOL version, AED-0, established a direct link between data structures (\"plexes\", in that dialect) and procedures, prefiguring what were later termed \"messages\", \"methods\", and \"member functions\".Simula introduced important concepts that are today an essential part of object-oriented programming, such as class and object, inheritance, and dynamic binding. \\nThe object-oriented Simula programming language was used mainly by researchers involved with physical modelling, such as models to study and improve the movement of ships and their content through cargo ports.In the 1970s, the first version of the Smalltalk programming language was developed at Xerox PARC by Alan Kay, Dan Ingalls and Adele Goldberg. Smaltalk-72 included a programming environment and was dynamically typed, and at first was interpreted, not compiled. Smalltalk became noted for its application of object orientation at the language-level and its graphical development environment. Smalltalk went through various versions and interest in the language grew. While Smalltalk was influenced by the ideas introduced in Simula 67 it was designed to be a fully dynamic system in which classes could be created and modified dynamically.In the 1970s, Smalltalk influenced the Lisp community to incorporate object-based techniques that were introduced to developers via the Lisp machine. Experimentation with various extensions to Lisp (such as LOOPS and Flavors introducing multiple inheritance and mixins) eventually led to the Common Lisp Object System, which integrates functional programming and object-oriented programming and allows extension via a Meta-object protocol. In the 1980s, there were a few attempts to design processor architectures that included hardware support for objects in memory but these were not successful. Examples include the Intel iAPX 432 and the Linn Smart Rekursiv.\\nIn 1981, Goldberg edited the August issue of Byte Magazine, introducing Smalltalk and object-oriented programming to a wider audience. In 1986, the Association for Computing Machinery organised the first Conference on Object-Oriented Programming, Systems, Languages, and Applications (OOPSLA), which was unexpectedly attended by 1,000 people. In the mid-1980s Objective-C was developed by Brad Cox, who had used Smalltalk at ITT Inc., and Bjarne Stroustrup, who had used Simula for his PhD thesis, eventually went to create the object-oriented C++. In 1985, Bertrand Meyer also produced the first design of the Eiffel language. Focused on software quality, Eiffel is a purely object-oriented programming language and a notation supporting the entire software lifecycle. Meyer described the Eiffel software development method, based on a small number of key ideas from software engineering and computer science, in Object-Oriented Software Construction. Essential to the quality focus of Eiffel is Meyer\\'s reliability mechanism, Design by Contract, which is an integral part of both the method and language.\\n\\nIn the early and mid-1990s object-oriented programming developed as the dominant programming paradigm when programming languages supporting the techniques became widely available. These included Visual FoxPro 3.0, C++, and Delphi. Its dominance was further enhanced by the rising popularity of graphical user interfaces, which rely heavily upon object-oriented programming techniques. An example of a closely related dynamic GUI library and OOP language can be found in the Cocoa frameworks on Mac OS X, written in Objective-C, an object-oriented, dynamic messaging extension to C based on Smalltalk. OOP toolkits also enhanced the popularity of event-driven programming (although this concept is not limited to OOP).\\nAt ETH Zürich, Niklaus Wirth and his colleagues had also been investigating such topics as data abstraction and modular programming (although this had been in common use in the 1960s or earlier). Modula-2 (1978) included both, and their succeeding design, Oberon, included a distinctive approach to object orientation, classes, and such.\\nObject-oriented features have been added to many previously existing languages, including Ada, BASIC, Fortran, Pascal, and COBOL. Adding these features to languages that were not initially designed for them often led to problems with compatibility and maintainability of code.\\nMore recently, a number of languages have emerged that are primarily object-oriented, but that are also compatible with procedural methodology. Two such languages are Python and Ruby. Probably the most commercially important recent object-oriented languages are Java, developed by Sun Microsystems, as well as C# and Visual Basic.NET (VB.NET), both designed for Microsoft\\'s .NET platform. Each of these two frameworks shows, in its own way, the benefit of using OOP by creating an abstraction from implementation. VB.NET and C# support cross-language inheritance, allowing classes defined in one language to subclass classes defined in the other language.\\n\\n\\n== Features ==\\nObject-oriented programming uses objects, but not all of the associated techniques and structures are supported directly in languages that claim to support OOP.  The features listed below are common among languages considered to be strongly class- and object-oriented (or multi-paradigm with OOP support), with notable exceptions mentioned.\\n\\n\\n=== Shared with non-OOP languages ===\\nVariables that can store information formatted in a small number of built-in data types like integers and alphanumeric characters.  This may include data structures like strings, lists, and hash tables that are either built-in or result from combining variables using memory pointers.\\nProcedures – also known as functions, methods, routines, or subroutines – that take input, generate output, and manipulate data.  Modern languages include structured programming constructs like loops and conditionals.Modular programming support provides the ability to group procedures into files and modules for organizational purposes.  Modules are namespaced so identifiers in one module will not conflict with a procedure or variable sharing the same name in another file or module.\\n\\n\\n=== Objects and classes ===\\nLanguages that support object-oriented programming (OOP) typically use inheritance for code reuse and extensibility in the form of either classes or prototypes. Those that use classes support two main concepts:\\n\\nClasses – the definitions for the data format and available procedures for a given type or class of object; may also contain data and procedures (known as class methods) themselves, i.e. classes contain the data members and member functions\\nObjects – instances of classesObjects sometimes correspond to things found in the real world. For example, a graphics program may have objects such as \"circle\", \"square\", \"menu\". An online shopping system might have objects such as \"shopping cart\", \"customer\", and \"product\". Sometimes objects represent more abstract entities, like an object that represents an open file, or an object that provides the service of translating measurements from U.S. customary to metric.\\n\\nEach object is said to be an instance of a particular class (for example, an object with its name field set to \"Mary\" might be an instance of class Employee).  Procedures in object-oriented programming are known as methods; variables are also known as fields, members, attributes, or properties.  This leads to the following terms:\\n\\nClass variables – belong to the class as a whole; there is only one copy of each one\\nInstance variables or attributes – data that belongs to individual objects; every object has its own copy of each one\\nMember variables – refers to both the class and instance variables that are defined by a particular class\\nClass methods – belong to the class as a whole and have access to only class variables and inputs from the procedure call\\nInstance methods – belong to individual objects, and have access to instance variables for the specific object they are called on, inputs, and class variablesObjects are accessed somewhat like variables with complex internal structure, and in many languages are effectively pointers, serving as actual references to a single instance of said object in memory within a heap or stack.  They provide a layer of abstraction which can be used to separate internal from external code. External code can use an object by calling a specific instance method with a certain set of input parameters, read an instance variable, or write to an instance variable. Objects are created by calling a special type of method in the class known as a constructor.  A program may create many instances of the same class as it runs, which operate independently.  This is an easy way for the same procedures to be used on different sets of data.\\nObject-oriented programming that uses classes is sometimes called class-based programming, while prototype-based programming does not typically use classes. As a result, significantly different yet analogous terminology is used to define the concepts of object and instance.\\nIn some languages classes and objects can be composed using other concepts like traits and mixins.\\n\\n\\n=== Class-based vs prototype-based ===\\nIn class-based languages the classes are defined beforehand and the objects are instantiated based on the classes. If two objects apple and orange are instantiated from the class Fruit, they are inherently fruits and it is guaranteed that you may handle them in the same way; e.g. a programmer can expect the existence of the same attributes such as color or sugar_content or is_ripe.\\nIn prototype-based languages the objects are the primary entities. No classes even exist. The prototype of an object is just another object to which the object is linked. Every object has one prototype link (and only one).  New objects can be created based on already existing objects chosen as their prototype. You may call two different objects apple and orange a fruit, if the object fruit exists, and both apple and orange have fruit as their prototype. The idea of the fruit class doesn\\'t exist explicitly, but as the equivalence class of the objects sharing the same prototype. The attributes and methods of the prototype are delegated to all the objects of the equivalence class defined by this prototype. The attributes and methods owned individually by the object may not be shared by other objects of the same equivalence class; e.g. the attribute sugar_content may be unexpectedly not present in apple. Only single inheritance can be implemented through the prototype.\\n\\n\\n=== Dynamic dispatch/message passing ===\\nIt is the responsibility of the object, not any external code, to select the procedural code to execute in response to a method call, typically by looking up the method at run time in a table associated with the object.  This feature is known as dynamic dispatch, and distinguishes an object from an abstract data type (or module), which has a fixed (static) implementation of the operations for all instances.  If the call variability relies on more than the single type of the object on which it is called (i.e. at least one other parameter object is involved in the method choice), one speaks of multiple dispatch.\\nA method call is also known as message passing.  It is conceptualized as a message (the name of the method and its input parameters) being passed to the object for dispatch.\\n\\n\\n=== Encapsulation ===\\nEncapsulation is an object-oriented programming concept that binds together the data and functions that manipulate the data, and that keeps both safe from outside interference and misuse. Data encapsulation led to the important OOP concept of data hiding.\\nIf a class does not allow calling code to access internal object data and permits access through methods only, this is a strong form of abstraction or information hiding known as encapsulation.  Some languages (Java, for example) let classes enforce access restrictions explicitly, for example denoting internal data with the private keyword and designating methods intended for use by code outside the class with the public keyword.  Methods may also be designed public, private, or intermediate levels such as protected (which allows access from the same class and its subclasses, but not objects of a different class).  In other languages (like Python) this is enforced only by convention (for example, private methods may have names that start with an underscore).  Encapsulation prevents external code from being concerned with the internal workings of an object.  This facilitates code refactoring, for example allowing the author of the class to change how objects of that class represent their data internally without changing any external code (as long as \"public\" method calls work the same way).  It also encourages programmers to put all the code that is concerned with a certain set of data in the same class, which organizes it for easy comprehension by other programmers.  Encapsulation is a technique that encourages decoupling.\\n\\n\\n=== Composition, inheritance, and delegation ===\\nObjects can contain other objects in their instance variables; this is known as object composition.  For example, an object in the Employee class might contain (either directly or through a pointer) an object in the Address class, in addition to its own instance variables like \"first_name\" and \"position\".  Object composition is used to represent \"has-a\" relationships: every employee has an address, so every Employee object has access to a place to store an Address object (either directly embedded within itself, or at a separate location addressed via a pointer).\\nLanguages that support classes almost always support inheritance.  This allows classes to be arranged in a hierarchy that represents \"is-a-type-of\" relationships.  For example, class Employee might inherit from class Person.  All the data and methods available to the parent class also appear in the child class with the same names.  For example, class Person might define variables \"first_name\" and \"last_name\" with method \"make_full_name()\".  These will also be available in class Employee, which might add the variables \"position\" and \"salary\".  This technique allows easy re-use of the same procedures and data definitions, in addition to potentially mirroring real-world relationships in an intuitive way. Rather than utilizing database tables and programming subroutines, the developer utilizes objects the user may be more familiar with: objects from their application domain.Subclasses can override the methods defined by superclasses. Multiple inheritance is allowed in some languages, though this can make resolving overrides complicated.  Some languages have special support for mixins, though in any language with multiple inheritance, a mixin is simply a class that does not represent an is-a-type-of relationship.  Mixins are typically used to add the same methods to multiple classes.  For example, class UnicodeConversionMixin might provide a method unicode_to_ascii() when included in class FileReader and class WebPageScraper, which don\\'t share a common parent.\\nAbstract classes cannot be instantiated into objects; they exist only for the purpose of inheritance into other \"concrete\" classes that can be instantiated.  In Java, the final keyword can be used to prevent a class from being subclassed.\\nThe doctrine of composition over inheritance advocates implementing has-a relationships using composition instead of inheritance.  For example, instead of inheriting from class Person, class Employee could give each Employee object an internal Person object, which it then has the opportunity to hide from external code even if class Person has many public attributes or methods.  Some languages, like Go do not support inheritance at all.\\nThe \"open/closed principle\" advocates that classes and functions \"should be open for extension, but closed for modification\".\\nDelegation is another language feature that can be used as an alternative to inheritance.\\n\\n\\n=== Polymorphism ===\\nSubtyping – a form of polymorphism – is when calling code can be agnostic as to which class in the supported hierarchy it is operating on – the parent class or one of its descendants.  Meanwhile, the same operation name among objects in an inheritance hierarchy may behave differently.\\nFor example, objects of type Circle and Square are derived from a common class called Shape.  The Draw function for each type of Shape implements what is necessary to draw itself while calling code can remain indifferent to the particular type of Shape being drawn.\\nThis is another type of abstraction that simplifies code external to the class hierarchy and enables strong separation of concerns.\\n\\n\\n=== Open recursion ===\\nIn languages that support open recursion, object methods can call other methods on the same object (including themselves), typically using a special variable or keyword called this or self.  This variable is late-bound; it allows a method defined in one class to invoke another method that is defined later, in some subclass thereof.\\n\\n\\n== OOP languages ==\\n\\nSimula (1967) is generally accepted as being the first language with the primary features of an object-oriented language. It was created for making simulation programs, in which what came to be called objects were the most important information representation. Smalltalk (1972 to 1980) is another early example, and the one with which much of the theory of OOP was developed. Concerning the degree of object orientation, the following distinctions can be made:\\n\\nLanguages called \"pure\" OO languages, because everything in them is treated consistently as an object, from primitives such as characters and punctuation, all the way up to whole classes, prototypes, blocks, modules, etc. They were designed specifically to facilitate, even enforce, OO methods. Examples: Ruby, Scala, Smalltalk, Eiffel, Emerald, JADE, Self, Raku.\\nLanguages designed mainly for OO programming, but with some procedural elements. Examples: Java, Python, C++, C#, Delphi/Object Pascal, VB.NET.\\nLanguages that are historically procedural languages, but have been extended with some OO features. Examples: PHP, Perl, Visual Basic (derived from BASIC), MATLAB, COBOL 2002, Fortran 2003, ABAP, Ada 95, Pascal.\\nLanguages with most of the features of objects (classes, methods, inheritance), but in a distinctly original form. Examples: Oberon (Oberon-1 or Oberon-2).\\nLanguages with abstract data type support which may be used to resemble OO programming, but without all features of object-orientation. This includes object-based and prototype-based languages. Examples: JavaScript, Lua, Modula-2, CLU.\\nChameleon languages that support multiple paradigms, including OO. Tcl stands out among these for TclOO, a hybrid object system that supports both prototype-based programming and class-based OO.\\n\\n\\n=== OOP in dynamic languages ===\\nIn recent years, object-oriented programming has become especially popular in dynamic programming languages. Python, PowerShell, Ruby and Groovy are dynamic languages built on OOP principles, while Perl and PHP have been adding object-oriented features since Perl 5 and PHP 4, and ColdFusion since version 6.\\nThe Document Object Model of HTML, XHTML, and XML documents on the Internet has bindings to the popular JavaScript/ECMAScript language. JavaScript is perhaps the best known prototype-based programming language, which employs cloning from prototypes rather than inheriting from a class (contrast to class-based programming). Another scripting language that takes this approach is Lua.\\n\\n\\n=== OOP in a network protocol ===\\nThe messages that flow between computers to request services in a client-server environment can be designed as the linearizations of objects defined by class objects known to both the client and the server.  For example, a simple linearized object would consist of a length field, a code point identifying the class, and a data value.  A more complex example would be a command consisting of the length and code point of the command and values consisting of linearized objects representing the command\\'s parameters.  Each such command must be directed by the server to an object whose class (or superclass) recognizes the command and is able to provide the requested service. Clients and servers are best modeled as complex object-oriented structures. Distributed Data Management Architecture (DDM) took this approach and used class objects to define objects at four levels of a formal hierarchy:\\n\\nFields defining the data values that form messages, such as their length, code point and data values.\\nObjects and collections of objects similar to what would be found in a Smalltalk program for messages and parameters.\\nManagers similar to IBM i Objects, such as a directory to files and files consisting of metadata and records. Managers conceptually provide memory and processing resources for their contained objects.\\nA client or server consisting of all the managers necessary to implement a full processing environment, supporting such aspects as directory services, security and concurrency control.The initial version of DDM defined distributed file services.  It was later extended to be the foundation of Distributed Relational Database Architecture (DRDA).\\n\\n\\n== Design patterns ==\\nChallenges of object-oriented design are addressed by several approaches. Most common is known as the design patterns codified by Gamma et al.. More broadly, the term \"design patterns\" can be used to refer to any general, repeatable, solution pattern to a commonly occurring problem in software design. Some of these commonly occurring problems have implications and solutions particular to object-oriented development.\\n\\n\\n=== Inheritance and behavioral subtyping ===\\n\\nIt is intuitive to assume that inheritance creates a semantic \"is a\" relationship, and thus to infer that objects instantiated from subclasses can always be safely used instead of those instantiated from the superclass. This intuition is unfortunately false in most OOP languages, in particular in all those that allow mutable objects. Subtype polymorphism as enforced by the type checker in OOP languages (with mutable objects) cannot guarantee behavioral subtyping in any context. Behavioral subtyping is undecidable in general, so it cannot be implemented by a program (compiler). Class or object hierarchies must be carefully designed, considering possible incorrect uses that cannot be detected syntactically. This issue is known as the Liskov substitution principle.\\n\\n\\n=== Gang of Four design patterns ===\\n\\nDesign Patterns: Elements of Reusable Object-Oriented Software is an influential book published in 1994 by Erich Gamma, Richard Helm, Ralph Johnson, and John Vlissides, often referred to humorously as the \"Gang of Four\". Along with exploring the capabilities and pitfalls of object-oriented programming, it describes 23 common programming problems and patterns for solving them.\\nAs of April 2007, the book was in its 36th printing.\\nThe book describes the following patterns:\\n\\nCreational patterns (5): Factory method pattern, Abstract factory pattern, Singleton pattern, Builder pattern, Prototype pattern\\nStructural patterns (7): Adapter pattern, Bridge pattern, Composite pattern, Decorator pattern, Facade pattern, Flyweight pattern, Proxy pattern\\nBehavioral patterns (11): Chain-of-responsibility pattern, Command pattern, Interpreter pattern, Iterator pattern, Mediator pattern, Memento pattern, Observer pattern, State pattern, Strategy pattern, Template method pattern, Visitor pattern\\n\\n\\n=== Object-orientation and databases ===\\n\\nBoth object-oriented programming and relational database management systems (RDBMSs) are extremely common in software today. Since relational databases don\\'t store objects directly (though some RDBMSs have object-oriented features to approximate this), there is a general need to bridge the two worlds. The problem of bridging object-oriented programming accesses and data patterns with relational databases is known as object-relational impedance mismatch. There are a number of approaches to cope with this problem, but no general solution without downsides. One of the most common approaches is object-relational mapping, as found in IDE languages such as Visual FoxPro and libraries such as Java Data Objects and Ruby on Rails\\' ActiveRecord.\\nThere are also object databases that can be used to replace RDBMSs, but these have not been as technically and commercially successful as RDBMSs.\\n\\n\\n=== Real-world modeling and relationships ===\\nOOP can be used to associate real-world objects and processes with digital counterparts. However, not everyone agrees that OOP facilitates direct real-world mapping (see Criticism section) or that real-world mapping is even a worthy goal; Bertrand Meyer argues in Object-Oriented Software Construction that a program is not a model of the world but a model of some part of the world; \"Reality is a cousin twice removed\". At the same time, some principal limitations of OOP have been noted.\\nFor example, the circle-ellipse problem is difficult to handle using OOP\\'s concept of inheritance.\\nHowever, Niklaus Wirth (who popularized the adage now known as Wirth\\'s law: \"Software is getting slower more rapidly than hardware becomes faster\") said of OOP in his paper, \"Good Ideas through the Looking Glass\", \"This paradigm closely reflects the structure of systems \\'in the real world\\', and it is therefore well suited to model complex systems with complex behaviours\" (contrast KISS principle).\\nSteve Yegge and others noted that natural languages lack the OOP approach of strictly prioritizing things (objects/nouns) before actions (methods/verbs). This problem may cause OOP to suffer more convoluted solutions than procedural programming.\\n\\n\\n=== OOP and control flow ===\\nOOP was developed to increase the reusability and maintainability of source code. Transparent representation of the control flow had no priority and was meant to be handled by a compiler. With the increasing relevance of parallel hardware and multithreaded coding, developing transparent control flow becomes more important, something hard to achieve with OOP.\\n\\n\\n=== Responsibility- vs. data-driven design ===\\nResponsibility-driven design defines classes in terms of a contract, that is, a class should be defined around a responsibility and the information that it shares. This is contrasted by Wirfs-Brock and Wilkerson with data-driven design, where classes are defined around the data-structures that must be held. The authors hold that responsibility-driven design is preferable.\\n\\n\\n=== SOLID and GRASP guidelines ===\\nSOLID is a mnemonic invented by Michael Feathers that stands for and advocates five programming practices:\\n\\nSingle responsibility principle\\nOpen/closed principle\\nLiskov substitution principle\\nInterface segregation principle\\nDependency inversion principleGRASP (General Responsibility Assignment Software Patterns) is another set of guidelines advocated by Craig Larman.\\n\\n\\n== Criticism ==\\nThe OOP paradigm has been criticised for a number of reasons, including not meeting its stated goals of reusability and modularity, and for overemphasizing one aspect of software design and modeling (data/objects) at the expense of other important aspects (computation/algorithms).Luca Cardelli has claimed that OOP code is \"intrinsically less efficient\" than procedural code, that OOP can take longer to compile, and that OOP languages have \"extremely poor modularity properties with respect to class extension and modification\", and tend to be extremely complex. The latter point is reiterated by Joe Armstrong, the principal inventor of Erlang, who is quoted as saying:\\nThe problem with object-oriented languages is they\\'ve got all this implicit environment that they carry around with them. You wanted a banana but what you got was a gorilla holding the banana and the entire jungle.\\nA study by Potok et al. has shown no significant difference in productivity between OOP and procedural approaches.Christopher J. Date stated that critical comparison of OOP to other technologies, relational in particular, is difficult because of lack of an agreed-upon and rigorous definition of OOP; however, Date and Darwen have proposed a theoretical foundation on OOP that uses OOP as a kind of customizable type system to support RDBMS.In an article Lawrence Krubner claimed that compared to other languages (LISP dialects, functional languages, etc.) OOP languages have no unique strengths, and inflict a heavy burden of unneeded complexity.Alexander Stepanov compares object orientation unfavourably to generic programming:\\nI find OOP technically unsound. It attempts to decompose the world in terms of interfaces that vary on a single type. To deal with the real problems you need multisorted algebras — families of interfaces that span multiple types. I find OOP philosophically unsound. It claims that everything is an object. Even if it is true it is not very interesting — saying that everything is an object is saying nothing at all.\\nPaul Graham has suggested that OOP\\'s popularity within large companies is due to \"large (and frequently changing) groups of mediocre programmers\". According to Graham, the discipline imposed by OOP prevents any one programmer from \"doing too much damage\".Leo Brodie has suggested a connection between the standalone nature of objects and a tendency to duplicate code in violation of the don\\'t repeat yourself principle of software development.\\nSteve Yegge noted that, as opposed to functional programming:\\nObject Oriented Programming puts the Nouns first and foremost. Why would you go to such lengths to put one part of speech on a pedestal? Why should one kind of concept take precedence over another? It\\'s not as if OOP has suddenly made verbs less important in the way we actually think. It\\'s a strangely skewed perspective.\\nRich Hickey, creator of Clojure, described object systems as overly simplistic models of the real world. He emphasized the inability of OOP to model time properly, which is getting increasingly problematic as software systems become more concurrent.Eric S. Raymond, a Unix programmer and open-source software advocate, has been critical of claims that present object-oriented programming as the \"One True Solution\", and has written that object-oriented programming languages tend to encourage thickly layered programs that destroy transparency. Raymond compares this unfavourably to the approach taken with Unix and the C programming language.Rob Pike, a programmer involved in the creation of UTF-8 and Go, has called object-oriented programming \"the Roman numerals of computing\" and has said that OOP languages frequently shift the focus from data structures and algorithms to types. Furthermore, he cites an instance of a Java professor whose \"idiomatic\" solution to a problem was to create six new classes, rather than to simply use a lookup table.\\n\\n\\n== Formal semantics ==\\n\\nObjects are the run-time entities in an object-oriented system. They may represent a person, a place, a bank account, a table of data, or any item that the program has to handle.\\nThere have been several attempts at formalizing the concepts used in object-oriented programming. The following concepts and constructs have been used as interpretations of OOP concepts:\\n\\nco algebraic data types\\nabstract data types (which have existential types) allow the definition of modules but these do not support dynamic dispatch\\nrecursive types\\nencapsulated state\\ninheritance\\nrecords are basis for understanding objects if function literals can be stored in fields (like in functional-programming languages), but the actual calculi need be considerably more complex to incorporate essential features of OOP. Several extensions of System F<: that deal with mutable objects have been studied; these allow both subtype polymorphism and parametric polymorphism (generics)Attempts to find a consensus definition or theory behind objects have not proven very successful (however, see Abadi & Cardelli, A Theory of Objects for formal definitions of many OOP concepts and constructs), and often diverge widely. For example, some definitions focus on mental activities, and some on program structuring. One of the simpler definitions is that OOP is the act of using \"map\" data structures or arrays that can contain functions and pointers to other maps, all with some syntactic and scoping sugar on top. Inheritance can be performed by cloning the maps (sometimes called \"prototyping\").\\n\\n\\n== See also ==\\n\\nComparison of programming languages (object-oriented programming)\\nComparison of programming paradigms\\nComponent-based software engineering\\nDesign by contract\\nObject association\\nObject database\\nObject model reference\\nObject modeling language\\nObject-oriented analysis and design\\nObject-relational impedance mismatch (and The Third Manifesto)\\nObject-relational mapping\\n\\n\\n=== Systems ===\\nCADES\\nCommon Object Request Broker Architecture (CORBA)\\nDistributed Component Object Model\\nDistributed Data Management Architecture\\nJeroo\\n\\n\\n=== Modeling languages ===\\nIDEF4\\nInterface description language\\nLepus3\\nUML\\n\\n\\n== References ==\\n\\n\\n== Further reading ==\\nAbadi, Martin; Luca Cardelli (1998). A Theory of Objects. Springer Verlag. ISBN 978-0-387-94775-4.\\nAbelson, Harold; Gerald Jay Sussman (1997). Structure and Interpretation of Computer Programs. MIT Press. ISBN 978-0-262-01153-2.\\nArmstrong, Deborah J. (February 2006). \"The Quarks of Object-Oriented Development\". Communications of the ACM. 49 (2): 123–128. doi:10.1145/1113034.1113040. ISSN 0001-0782. S2CID 11485502.\\nBooch, Grady (1997). Object-Oriented Analysis and Design with Applications. Addison-Wesley. ISBN 978-0-8053-5340-2.\\nEeles, Peter; Oliver Sims (1998). Building Business Objects. John Wiley & Sons. ISBN 978-0-471-19176-6.\\nGamma, Erich; Richard Helm; Ralph Johnson; John Vlissides (1995). Design Patterns: Elements of Reusable Object Oriented Software. Addison-Wesley. Bibcode:1995dper.book.....G. ISBN 978-0-201-63361-0.\\nHarmon, Paul; William Morrissey (1996). The Object Technology Casebook – Lessons from Award-Winning Business Applications. John Wiley & Sons. ISBN 978-0-471-14717-6.\\nJacobson, Ivar (1992). Object-Oriented Software Engineering: A Use Case-Driven Approach. Addison-Wesley. Bibcode:1992oose.book.....J. ISBN 978-0-201-54435-0.\\nKay, Alan. The Early History of Smalltalk. Archived from the original on 4 April 2005. Retrieved 18 April 2005.\\nMeyer, Bertrand (1997). Object-Oriented Software Construction. Prentice Hall. ISBN 978-0-13-629155-8.\\nPecinovsky, Rudolf (2013). OOP – Learn Object Oriented Thinking & Programming. Bruckner Publishing. ISBN 978-80-904661-8-0.\\nRumbaugh, James; Michael Blaha; William Premerlani; Frederick Eddy; William Lorensen (1991). Object-Oriented Modeling and Design. Prentice Hall. ISBN 978-0-13-629841-0.\\nSchach, Stephen (2006). Object-Oriented and Classical Software Engineering, Seventh Edition. McGraw-Hill. ISBN 978-0-07-319126-3.\\nSchreiner, Axel-Tobias (1993). Object oriented programming with ANSI-C. Hanser. hdl:1850/8544. ISBN 978-3-446-17426-9.\\nTaylor, David A. (1992). Object-Oriented Information Systems – Planning and Implementation. John Wiley & Sons. ISBN 978-0-471-54364-0.\\nWeisfeld, Matt (2009). The Object-Oriented Thought Process, Third Edition. Addison-Wesley. ISBN 978-0-672-33016-2.\\nWest, David (2004). Object Thinking (Developer Reference). Microsoft Press. ISBN 978-0-7356-1965-4.\\n\\n\\n== External links ==\\nIntroduction to Object Oriented Programming Concepts (OOP) and More by L.W.C. Nirosh\\nDiscussion about the flaws of OOD\\nOOP Concepts (Java Tutorials)', 'Programming language theory (PLT) is a branch of computer science that deals with the design, implementation, analysis, characterization, and classification of formal languages known as programming languages and of their individual features. It falls within the discipline of computer science, both depending on and affecting mathematics, software engineering, linguistics and even cognitive science. It has become a well-recognized branch of computer science, and an active research area, with results published in numerous  journals dedicated to PLT, as well as in general computer science and engineering publications.\\n\\n\\n== History ==\\nIn some ways, the history of programming language theory predates even the development of programming languages themselves. The lambda calculus, developed by Alonzo Church and Stephen Cole Kleene in the 1930s, is considered by some to be the world\\'s first programming language, even though it was intended to model computation rather than being a means for programmers to describe algorithms to a computer system. Many modern functional programming languages have been described as providing a \"thin veneer\" over the lambda calculus, and many are easily described in terms of it.\\nThe first programming language to be invented was Plankalkül, which was designed by Konrad Zuse in the 1940s, but not publicly known until 1972 (and not implemented until 1998). The first widely known and successful high-level programming language was Fortran, developed from 1954 to 1957 by a team of IBM researchers led by John Backus. The success of FORTRAN led to the formation of a committee of scientists to develop a \"universal\" computer language; the result of their effort was ALGOL 58. Separately, John McCarthy of MIT developed Lisp, the first language with origins in academia to be successful. With the success of these initial efforts, programming languages became an active topic of research in the 1960s and beyond.\\nSome other key events in the history of programming language theory since then:\\n\\n\\n=== 1950s ===\\nNoam Chomsky developed the Chomsky hierarchy in the field of linguistics, a discovery which has directly impacted programming language theory and other branches of computer science.\\n\\n\\n=== 1960s ===\\nThe Simula language was developed by Ole-Johan Dahl and Kristen Nygaard; it is widely considered to be the first example of an object-oriented programming language; Simula also introduced the concept of coroutines.\\nIn 1964, Peter Landin is the first to realize Church\\'s lambda calculus can be used to model programming languages. He introduces the SECD machine which \"interprets\" lambda expressions.\\nIn 1965, Landin introduces the J operator, essentially a form of continuation.\\nIn 1966, Landin introduces ISWIM, an abstract computer programming language in his article The Next 700 Programming Languages. It is influential in the design of languages leading to the Haskell programming language.\\nIn 1966, Corrado Böhm introduced the programming language CUCH (Curry-Church).\\nIn 1967, Christopher Strachey publishes his influential set of lecture notes Fundamental Concepts in Programming Languages, introducing the terminology R-values, L-values, parametric polymorphism, and ad hoc polymorphism.\\nIn 1969, J. Roger Hindley publishes The Principal Type-Scheme of an Object in Combinatory Logic, later generalized into the Hindley–Milner type inference algorithm.\\nIn 1969, Tony Hoare introduces the Hoare logic, a form of axiomatic semantics.\\nIn 1969, William Alvin Howard observed that a \"high-level\" proof system, referred to as natural deduction, can be directly interpreted in its intuitionistic version as a typed variant of the model of computation known as lambda calculus. This became known as the Curry–Howard correspondence.\\n\\n\\n=== 1970s ===\\nIn 1970, Dana Scott first publishes his work on denotational semantics.\\nIn 1972, logic programming and Prolog were developed thus allowing computer programs to be expressed as mathematical logic.\\nA team of scientists at Xerox PARC led by Alan Kay develop Smalltalk, an object-oriented language widely known for its innovative development environment.\\nIn 1974, John C. Reynolds discovers System F. It had already been discovered in 1971 by the mathematical logician Jean-Yves Girard.\\nFrom 1975, Gerald Jay Sussman and Guy Steele develop the Scheme programming language, a Lisp dialect incorporating lexical scoping, a unified namespace, and elements from the actor model including first-class continuations.\\nBackus, at the 1977 Turing Award lecture, assailed the current state of industrial languages and proposed a new class of programming languages now known as function-level programming languages.\\nIn 1977, Gordon Plotkin introduces Programming Computable Functions, an abstract typed functional language.\\nIn 1978, Robin Milner introduces the Hindley–Milner type inference algorithm for ML. Type theory became applied as a discipline to programming languages, this application has led to tremendous advances in type theory over the years.\\n\\n\\n=== 1980s ===\\nIn 1981, Gordon Plotkin publishes his paper on structured operational semantics.\\nIn 1988, Gilles Kahn published his paper on natural semantics.\\nThere emerged process calculi, such as the Calculus of Communicating Systems of Robin Milner, and the Communicating sequential processes model of C. A. R. Hoare, as well as similar models of concurrency such as the actor model of Carl Hewitt.\\nIn 1985, the release of Miranda sparks an academic interest in lazy-evaluated pure functional programming languages. A committee was formed to define an open standard resulting in the release of the Haskell 1.0 standard in 1990.\\nBertrand Meyer created the methodology Design by contract and incorporated it into the Eiffel programming language.\\n\\n\\n=== 1990s ===\\nGregor Kiczales, Jim Des Rivieres and Daniel G. Bobrow published the book The Art of the Metaobject Protocol.\\nEugenio Moggi and Philip Wadler introduced the use of monads for structuring programs written in functional programming languages.\\n\\n\\n== Sub-disciplines and related fields ==\\nThere are several fields of study which either lie within programming language theory, or which have a profound influence on it; many of these have considerable overlap. In addition, PLT makes use of many other branches of mathematics, including computability theory, category theory, and set theory.\\n\\n\\n=== Formal semantics ===\\n\\nFormal semantics is the formal specification of the behaviour of computer programs and programming languages. Three common approaches to describe the semantics or \"meaning\" of a computer program are denotational semantics, operational semantics and axiomatic semantics.\\n\\n\\n=== Type theory ===\\n\\nType theory is the study of type systems; which are \"a tractable syntactic method for proving the absence of certain program behaviors by classifying phrases according to the kinds of values they compute\". Many programming languages are distinguished by the characteristics of their type systems.\\n\\n\\n=== Program analysis and transformation ===\\n\\nProgram analysis is the general problem of examining a program and determining key characteristics (such as the absence of classes of program errors). Program transformation is the process of transforming a program in one form (language) to another form.\\n\\n\\n=== Comparative programming language analysis ===\\nComparative programming language analysis seeks to classify programming languages into different types based on their characteristics; broad categories of programming languages are often known as programming paradigms.\\n\\n\\n=== Generic and metaprogramming ===\\nMetaprogramming is the generation of higher-order programs which, when executed, produce programs (possibly in a different language, or in a subset of the original language) as a result.\\n\\n\\n=== Domain-specific languages ===\\nDomain-specific languages are languages constructed to efficiently solve problems of a particular part of domain.\\n\\n\\n=== Compiler construction ===\\n\\nCompiler theory is the theory of writing compilers (or more generally, translators); programs which translate a program written in one language into another form. The actions of a compiler are traditionally broken up into syntax analysis (scanning and parsing), semantic analysis (determining what a program should do), optimization (improving the performance of a program as indicated by some metric; typically execution speed) and code generation (generation and output of an equivalent program in some target language; often the instruction set of a CPU).\\n\\n\\n=== Run-time systems ===\\nRun-time systems refer to the development of programming language runtime environments and their components, including virtual machines, garbage collection, and foreign function interfaces.\\n\\n\\n== Journals, publications, and conferences ==\\nConferences are the primary venue for presenting research in programming languages. The most well known conferences include the Symposium on Principles of Programming Languages (POPL), Programming Language Design and Implementation (PLDI), the International Conference on Functional Programming (ICFP), the International Conference on Object Oriented Programming, Systems, Languages and Applications (OOPSLA) and the International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS). \\nNotable journals that publish PLT research include the ACM Transactions on Programming Languages and Systems (TOPLAS), Journal of Functional Programming (JFP), Journal of Functional and Logic Programming, and Higher-Order and Symbolic Computation.\\n\\n\\n== See also ==\\nSIGPLAN\\nTimeline of programming languages\\nVery high-level programming language\\n\\n\\n== References ==\\n\\n\\n== Further reading ==\\n\\nAbadi, Martín and Cardelli, Luca. A Theory of Objects. Springer-Verlag.\\nMichael J. C. Gordon. Programming Language Theory and Its Implementation. Prentice Hall.\\nGunter, Carl and Mitchell, John C. (eds.). Theoretical Aspects of Object Oriented Programming Languages: Types, Semantics, and Language Design. MIT Press.\\nHarper, Robert. Practical Foundations for Programming Languages. Draft version.\\nKnuth, Donald E. (2003). Selected Papers on Computer Languages. Stanford, California: Center for the Study of Language and Information.\\nMitchell, John C.. Foundations for Programming Languages.\\nMitchell, John C.. Introduction to Programming Language Theory.\\nO\\'Hearn, Peter. W. and Tennent, Robert. D. (1997). Algol-like Languages. Progress in Theoretical Computer Science. Birkhauser, Boston.\\nPierce, Benjamin C. (2002). Types and Programming Languages. MIT Press.\\nPierce, Benjamin C. Advanced Topics in Types and Programming Languages.\\nPierce, Benjamin C. et al. (2010). Software Foundations.\\n\\n\\n== External links ==\\nLambda the Ultimate, a community weblog for professional discussion and repository of documents on programming language theory.\\nGreat Works in Programming Languages. Collected by Benjamin C. Pierce (University of Pennsylvania).\\nClassic Papers in Programming Languages and Logic. Collected by Karl Crary (Carnegie Mellon University).\\nProgramming Language Research. Directory by Mark Leone.\\nProgramming Language Theory Texts Online. At Utrecht University.\\nλ-Calculus: Then & Now by Dana S. Scott for the ACM Turing Centenary Celebration\\nGrand Challenges in Programming Languages. Panel session at POPL 2009.', 'In programming language theory, semantics is the field concerned with the rigorous mathematical study of the meaning of programming languages. \\nIt does so by evaluating the meaning of syntactically valid strings defined by a specific programming language, showing the computation involved. In such a case that the evaluation would be of syntactically invalid strings, the result would be non-computation. Semantics describes the processes a computer follows when executing a program in that specific language. This can be shown by describing the relationship between the input and output of a program, or an explanation of how the program will be executed on a certain platform, hence creating a model of computation.\\n\\n\\n== Overview ==\\nThe field of formal semantics encompasses all of the following:\\n\\nThe definition of semantic models\\nThe relations between different semantic models\\nThe relations between different approaches to meaning\\nThe relation between computation and the underlying mathematical structures from fields such as logic, set theory, model theory, category theory, etc.It has close links with other areas of computer science such as programming language design, type theory, compilers and interpreters, program verification and model checking.\\n\\n\\n== Approaches ==\\nThere are many approaches to formal semantics; these belong to three major classes:\\n\\nDenotational semantics, whereby each phrase in the language is interpreted as a denotation, i.e. a conceptual meaning that can be thought of abstractly.  Such denotations are often mathematical objects inhabiting a mathematical space, but it is not a requirement that they should be so.  As a practical necessity, denotations are described using some form of mathematical notation, which can in turn be formalized as a denotational metalanguage.  For example, denotational semantics of functional languages often translate the language into domain theory. Denotational semantic descriptions can also serve as compositional translations from a programming language into the denotational metalanguage and used as a basis for designing compilers.\\nOperational semantics, whereby the execution of the language is described directly (rather than by translation).  Operational semantics loosely corresponds to interpretation, although again the \"implementation language\" of the interpreter is generally a mathematical formalism.  Operational semantics may define an abstract machine (such as the SECD machine), and give meaning to phrases by describing the transitions they induce on states of the machine.  Alternatively, as with the pure lambda calculus, operational semantics can be defined via syntactic transformations on phrases of the language itself;\\nAxiomatic semantics, whereby one gives meaning to phrases by describing the axioms that apply to them.  Axiomatic semantics makes no distinction between a phrase\\'s meaning and the logical formulas that describe it; its meaning is exactly what can be proven about it in some logic.  The canonical example of axiomatic semantics is Hoare logic.Apart from the choice between denotational, operational, or axiomatic approaches, most variations in formal semantic systems arise from the choice of supporting mathematical formalism.\\n\\n\\n== Variations ==\\nSome variations of formal semantics include the following:\\n\\nAction semantics is an approach that tries to modularize denotational semantics, splitting the formalization process in two layers (macro and microsemantics) and predefining three semantic entities (actions, data and yielders) to simplify the specification;\\nAlgebraic semantics is a form of axiomatic semantics based on algebraic laws for describing and reasoning about program semantics in a formal manner;\\nAttribute grammars define systems that systematically compute \"metadata\" (called attributes) for the various cases of the language\\'s syntax.  Attribute grammars can be understood as a denotational semantics where the target language is simply the original language enriched with attribute annotations.  Aside from formal semantics, attribute grammars have also been used for code generation in compilers, and to augment regular or context-free grammars with context-sensitive conditions;\\nCategorical (or \"functorial\") semantics uses category theory as the core mathematical formalism. A categorical semantics is usually proven to correspond to some axiomatic semantics that gives a syntactic presentation of the categorical structures. Also, denotational semantics are often instances of a general categorical semantics, \\nConcurrency semantics is a catch-all term for any formal semantics that describes concurrent computations.  Historically important concurrent formalisms have included the actor model and process calculi;\\nGame semantics uses a metaphor inspired by game theory.\\nPredicate transformer semantics, developed by Edsger W. Dijkstra, describes the meaning of a program fragment as the function transforming a postcondition to the precondition needed to establish it.\\n\\n\\n== Describing relationships ==\\nFor a variety of reasons, one might wish to describe the relationships between different formal semantics.  For example:\\n\\nTo prove that a particular operational semantics for a language satisfies the logical formulas of an axiomatic semantics for that language.  Such a proof demonstrates that it is \"sound\" to reason about a particular (operational) interpretation strategy using a particular (axiomatic) proof system.\\nTo prove that operational semantics over a high-level machine is related by a simulation with the semantics over a low-level machine, whereby the low-level abstract machine contains more primitive operations than the high-level abstract machine definition of a given language. Such a proof demonstrates that the low-level machine \"faithfully implements\" the high-level machine.It is also possible to relate multiple semantics through abstractions via the theory of abstract interpretation.\\n\\n\\n== History ==\\nRobert W. Floyd is credited with founding the field of programming language semantics in Floyd (1967).\\n\\n\\n== See also ==\\nComputational semantics\\nFormal semantics (logic)\\nFormal semantics (linguistics)\\nOntology\\nOntology (information science)\\nSemantic equivalence\\nSemantic technology\\n\\n\\n== References ==\\n\\n\\n== Further reading ==\\nTextbooks', 'In mathematics, logic, and computer science, a type system is a formal system in which every term has a \"type\" which defines its meaning and the operations that may be performed on it. Type theory is the academic study of type systems.\\nSome type theories serve as alternatives to set theory as a foundation of mathematics. Two well-known such theories are Alonzo Church\\'s typed λ-calculus and Per Martin-Löf\\'s intuitionistic type theory.\\nType theory was created to avoid paradoxes in previous foundations such as naive set theory, formal logics and rewrite systems.\\nType theory is closely related to, and in some cases overlaps with, computational type systems, which are a programming language feature used to reduce bugs and facilitate certain compiler optimizations.\\n\\n\\n== History ==\\n\\nBetween 1902 and 1908 Bertrand Russell proposed various \"theories of type\" in response to his discovery that Gottlob Frege\\'s version of naive set theory was afflicted with Russell\\'s paradox. By 1908 Russell arrived at a \"ramified\" theory of types together with an \"axiom of reducibility\" both of which featured prominently in Whitehead and Russell\\'s Principia Mathematica published between 1910 and 1913. They attempted to resolve Russell\\'s paradox by first creating a hierarchy of types, then assigning each concrete mathematical (and possibly other) entity to a type. Entities of a given type are built exclusively from entities of those types that are lower in their hierarchy, thus preventing an entity from being assigned to itself.\\nIn the 1920s, Leon Chwistek and Frank P. Ramsey proposed an unramified type theory, now known as the \"theory of simple types\" or simple type theory, which collapsed the hierarchy of the types in the earlier ramified theory and as such did not require the axiom of reducibility.\\nThe common usage of \"type theory\" is when those types are used with a term rewrite system. The most famous early example is Alonzo Church\\'s simply typed lambda calculus. Church\\'s theory of types helped the formal system avoid the Kleene–Rosser paradox that afflicted the original untyped lambda calculus. Church demonstrated that it could serve as a foundation of mathematics and it was referred to as a higher-order logic.\\nSome other type theories include Per Martin-Löf\\'s intuitionistic type theory, which has been the foundation used in some areas of constructive mathematics. Thierry Coquand\\'s calculus of constructions and its derivatives are the foundation used by Coq, Lean, and others. The field is an area of active research, as demonstrated by homotopy type theory.\\n\\n\\n== Basic concepts ==\\n\\nThe contemporary presentation of type systems in the context of type theory has been made systematic by a conceptual framework introduced by Henk Barendregt.\\n\\n\\n=== Type, term, value ===\\nIn a system of type theory, a term is opposed to a type.  For example, 4, 2 + 2, and \\n  \\n    \\n      \\n        2\\n        ⋅\\n        2\\n      \\n    \\n    {\\\\displaystyle 2\\\\cdot 2}\\n   are all separate terms with the type nat for natural numbers.  Traditionally, the term is followed by a colon and its type, such as 2 : nat - this means that the number 2 is of type nat. Beyond this opposition and syntax, little can be said about types in this generality, but often, they are interpreted as some kind of collection (not necessarily sets) of the values that the term might evaluate to. It is usual to denote terms by e and types by τ. How terms and types are shaped depends on the particular type system and is made precise by some syntax and additional restrictions of well-formedness.\\n\\n\\n=== Typing environment, type assignment, type judgement ===\\n\\nTyping usually takes place in some context or environment denoted by the symbol \\n  \\n    \\n      \\n        Γ\\n      \\n    \\n    {\\\\displaystyle \\\\Gamma }\\n  . Often, an environment is a list of pairs \\n  \\n    \\n      \\n        e\\n        :\\n        τ\\n      \\n    \\n    {\\\\displaystyle e:\\\\tau }\\n  . This pair is sometimes called an assignment. The context completes the above opposition. Together they form a judgement denoted \\n  \\n    \\n      \\n        Γ\\n        ⊢\\n        e\\n        :\\n        τ\\n      \\n    \\n    {\\\\displaystyle \\\\Gamma \\\\vdash e:\\\\tau }\\n  .\\n\\n\\n=== Rewriting rules, conversion, reduction ===\\nType theories have explicit computation and it is encoded in rules for rewriting terms. These are called conversion rules or, if the rule only works in one direction, a reduction rule. For example, \\n  \\n    \\n      \\n        2\\n        +\\n        2\\n      \\n    \\n    {\\\\displaystyle 2+2}\\n   and \\n  \\n    \\n      \\n        4\\n      \\n    \\n    {\\\\displaystyle 4}\\n   are syntactically different terms, but the former reduces to the latter. This reduction is written \\n  \\n    \\n      \\n        2\\n        +\\n        2\\n        ↠\\n        4\\n      \\n    \\n    {\\\\displaystyle 2+2\\\\twoheadrightarrow 4}\\n  . These rules also establish corresponding equivalences between the terms, written \\n  \\n    \\n      \\n        2\\n        +\\n        2\\n        ≡\\n        4\\n      \\n    \\n    {\\\\displaystyle 2+2\\\\equiv 4}\\n  .\\nThe term \\n  \\n    \\n      \\n        2\\n        +\\n        1\\n      \\n    \\n    {\\\\displaystyle 2+1}\\n   reduces to \\n  \\n    \\n      \\n        3\\n      \\n    \\n    {\\\\displaystyle 3}\\n  . Since \\n  \\n    \\n      \\n        3\\n      \\n    \\n    {\\\\displaystyle 3}\\n   cannot be reduced further, it is called a normal form. Various systems of typed lambda calculus including the simply typed lambda calculus, Jean-Yves Girard\\'s System F, and Thierry Coquand\\'s calculus of constructions are strongly normalizing. In such systems, a successful type check implies a termination proof of the term.\\n\\n\\n=== Type rules ===\\n\\nBased on the judgements and equivalences type inference rules can be used to describe how a type system assigns a type to syntactic constructions (terms), much like in natural deduction. To be meaningful, conversion and type rules are usually closely related as in e.g. by a subject reduction property, which might establish a part of the soundness of a type system.\\n\\n\\n== Decision problems ==\\nA type system  is naturally associated with the decision problems of type checking, typability, and type inhabitation.\\n\\n\\n=== Type checking ===\\n\\nThe decision problem of type checking (abbreviated by \\n  \\n    \\n      \\n        Γ\\n        ⊢\\n        e\\n        :\\n        τ\\n        ?\\n      \\n    \\n    {\\\\displaystyle \\\\Gamma \\\\vdash e:\\\\tau ?}\\n  ) is: \\n\\nGiven a type environment \\n  \\n    \\n      \\n        Γ\\n      \\n    \\n    {\\\\displaystyle \\\\Gamma }\\n  , a term \\n  \\n    \\n      \\n        e\\n      \\n    \\n    {\\\\displaystyle e}\\n  , and a type \\n  \\n    \\n      \\n        τ\\n      \\n    \\n    {\\\\displaystyle \\\\tau }\\n  , decide whether the term \\n  \\n    \\n      \\n        e\\n      \\n    \\n    {\\\\displaystyle e}\\n   can be assigned the type \\n  \\n    \\n      \\n        τ\\n      \\n    \\n    {\\\\displaystyle \\\\tau }\\n   in the type environment \\n  \\n    \\n      \\n        Γ\\n      \\n    \\n    {\\\\displaystyle \\\\Gamma }\\n  .Decidability of type checking means that type safety of any given program text (source code) can be verified.\\n\\n\\n=== Typability ===\\n\\nThe decision problem of typability (abbreviated by \\n  \\n    \\n      \\n        ∃\\n        Γ\\n        ,\\n        τ\\n        .\\n        Γ\\n        ⊢\\n        e\\n        :\\n        τ\\n        ?\\n      \\n    \\n    {\\\\displaystyle \\\\exists \\\\Gamma ,\\\\tau .\\\\Gamma \\\\vdash e:\\\\tau ?}\\n  ) is: \\n\\nGiven a term \\n  \\n    \\n      \\n        e\\n      \\n    \\n    {\\\\displaystyle e}\\n  , decide whether there exists a type environment \\n  \\n    \\n      \\n        Γ\\n      \\n    \\n    {\\\\displaystyle \\\\Gamma }\\n   and a type \\n  \\n    \\n      \\n        τ\\n      \\n    \\n    {\\\\displaystyle \\\\tau }\\n   such that the term \\n  \\n    \\n      \\n        e\\n      \\n    \\n    {\\\\displaystyle e}\\n   can be assigned the type \\n  \\n    \\n      \\n        τ\\n      \\n    \\n    {\\\\displaystyle \\\\tau }\\n   in the type environment \\n  \\n    \\n      \\n        Γ\\n      \\n    \\n    {\\\\displaystyle \\\\Gamma }\\n  .A variant of typability is typability wrt. a type environment (abbreviated by \\n  \\n    \\n      \\n        ∃\\n        τ\\n        .\\n        Γ\\n        ⊢\\n        e\\n        :\\n        τ\\n        ?\\n      \\n    \\n    {\\\\displaystyle \\\\exists \\\\tau .\\\\Gamma \\\\vdash e:\\\\tau ?}\\n  ), for which a type environment is part of the input.\\nIf the given term does not contain external references (such as free term variables), then typability coincides with typability wrt. the empty type environment.\\nTypability is closely related to type inference. Whereas typability (as a decision problem) addresses the existence of a type for a given term, type inference (as a computation problem) requires an actual type to be computed.\\n\\n\\n=== Type inhabitation ===\\n\\nThe decision problem of type inhabitation (abbreviated by \\n  \\n    \\n      \\n        ∃\\n        e\\n        .\\n        Γ\\n        ⊢\\n        e\\n        :\\n        τ\\n        ?\\n      \\n    \\n    {\\\\displaystyle \\\\exists e.\\\\Gamma \\\\vdash e:\\\\tau ?}\\n  ) is: \\n\\nGiven a type environment \\n  \\n    \\n      \\n        Γ\\n      \\n    \\n    {\\\\displaystyle \\\\Gamma }\\n   and a type \\n  \\n    \\n      \\n        τ\\n      \\n    \\n    {\\\\displaystyle \\\\tau }\\n  , decide whether there exists a term \\n  \\n    \\n      \\n        e\\n      \\n    \\n    {\\\\displaystyle e}\\n   that can be assigned the type \\n  \\n    \\n      \\n        τ\\n      \\n    \\n    {\\\\displaystyle \\\\tau }\\n   in the type environment \\n  \\n    \\n      \\n        Γ\\n      \\n    \\n    {\\\\displaystyle \\\\Gamma }\\n  .Girard\\'s paradox shows that type inhabitation is strongly related to the consistency of a type system with Curry–Howard correspondence. To be sound, such a system must have uninhabited types.\\nThe opposition of terms and types can also be views as one of implementation and specification. By program synthesis (the computational counterpart of) type inhabitation (see below) can be used to construct (all or parts of) programs from specification given in form of type information.\\n\\n\\n== Interpretations of type theory ==\\nType theory is closely linked to many fields of active research. Most particular, the Curry–Howard correspondence provides a deep isomorphism between intuitionistic logic, typed lambda calculus and cartesian closed categories.\\n\\n\\n=== Intuitionistic logic ===\\n\\nBeside the view of types as collection of values of a term, type theory offers a second interpretation of the opposition of term and types. Types can be seen as propositions and terms as proofs. In this way of reading a typing, a function type \\n  \\n    \\n      \\n        α\\n        →\\n        β\\n      \\n    \\n    {\\\\displaystyle \\\\alpha \\\\rightarrow \\\\beta }\\n   is viewed as an implication, i.e. as the proposition, that \\n  \\n    \\n      \\n        β\\n      \\n    \\n    {\\\\displaystyle \\\\beta }\\n   follows from \\n  \\n    \\n      \\n        α\\n      \\n    \\n    {\\\\displaystyle \\\\alpha }\\n  .\\n\\n\\n=== Category theory ===\\n\\nThe internal language of the cartesian closed category is the simply typed lambda calculus. This view can be extended to other typed lambda calculi.\\nCertain Cartesian closed categories, the topoi, have been proposed as a general setting for mathematics, instead of traditional set theory.\\n\\n\\n== Difference from set theory ==\\nThere are many different set theories and many different systems of type theory, so what follows are generalizations.\\n\\nSet theory is built on top of logic. It requires a separate system like predicate logic underneath it. In type theory, concepts like \"and\" and \"or\" can be encoded as types in the type theory itself.\\nIn set theory, an element is not restricted to one set. In type theory, terms (generally) belong to only one type. (Where a subset would be used, type theory tends to use a predicate function that returns true if the term is in the subset and returns false if the value is not.  The union of two types can be defined as a new type called a sum type, which contains new terms.)\\nSet theory usually encodes numbers as sets.  (0 is the empty set, 1 is a set containing the empty set, etc.  See Set-theoretic definition of natural numbers.) Type theory can encode numbers as functions using Church encoding or more naturally as inductive types.  Inductive types create new constants for the successor function and zero, closely resembling Peano\\'s axioms.\\nType theory has a simple connection to constructive mathematics through the BHK interpretation.  It can be connected to logic by the Curry–Howard isomorphism.  And some type theories are closely connected to Category theory.\\n\\n\\n== Optional features ==\\n\\n\\n=== Dependent types ===\\n\\nA dependent type is a type that depends on a term or another type. Thus, the type returned by a function may depend on the argument to the function.\\nFor example, a list of \\n  \\n    \\n      \\n        \\n          n\\n          a\\n          t\\n        \\n      \\n    \\n    {\\\\displaystyle \\\\mathrm {nat} }\\n  s of length 4 may be a different type than a list of \\n  \\n    \\n      \\n        \\n          n\\n          a\\n          t\\n        \\n      \\n    \\n    {\\\\displaystyle \\\\mathrm {nat} }\\n  s of length 5. In a type theory with dependent types, it is possible to define a function that takes a parameter \"n\" and returns a list containing \"n\" zeros. Calling the function with 4 would produce a term with a different type than if the function was called with 5.\\nAnother example is the type consisting of the proofs that the argument term has a certain property, such as the term of  \\n  \\n    \\n      \\n        \\n          n\\n          a\\n          t\\n        \\n      \\n    \\n    {\\\\displaystyle \\\\mathrm {nat} }\\n    type, e.g., a given natural number, is prime.  See Curry-Howard Correspondence.\\nDependent types play a central role in intuitionistic type theory and in the design of functional programming languages like Idris, ATS, Agda and Epigram.\\n\\n\\n=== Equality types ===\\nMany systems of type theory have a type that represents equality of types and of terms. This type is different from convertibility, and is often denoted propositional equality.\\nIn intuitionistic type theory, the equality type (also called the identity type) is known as \\n  \\n    \\n      \\n        I\\n      \\n    \\n    {\\\\displaystyle I}\\n   for identity. There is a type \\n  \\n    \\n      \\n        I\\n         \\n        A\\n         \\n        a\\n         \\n        b\\n      \\n    \\n    {\\\\displaystyle I\\\\ A\\\\ a\\\\ b}\\n   when \\n  \\n    \\n      \\n        A\\n      \\n    \\n    {\\\\displaystyle A}\\n   is a type and \\n  \\n    \\n      \\n        a\\n      \\n    \\n    {\\\\displaystyle a}\\n   and \\n  \\n    \\n      \\n        b\\n      \\n    \\n    {\\\\displaystyle b}\\n   are both terms of type \\n  \\n    \\n      \\n        A\\n      \\n    \\n    {\\\\displaystyle A}\\n  . A term of type \\n  \\n    \\n      \\n        I\\n         \\n        A\\n         \\n        a\\n         \\n        b\\n      \\n    \\n    {\\\\displaystyle I\\\\ A\\\\ a\\\\ b}\\n   is interpreted as meaning that \\n  \\n    \\n      \\n        a\\n      \\n    \\n    {\\\\displaystyle a}\\n   is equal to \\n  \\n    \\n      \\n        b\\n      \\n    \\n    {\\\\displaystyle b}\\n  .\\nIn practice, it is possible to build a type \\n  \\n    \\n      \\n        I\\n         \\n        \\n          n\\n          a\\n          t\\n        \\n         \\n        3\\n         \\n        4\\n      \\n    \\n    {\\\\displaystyle I\\\\ \\\\mathrm {nat} \\\\ 3\\\\ 4}\\n   but there will not exist a term of that type. In intuitionistic type theory, new terms of equality start with reflexivity. If \\n  \\n    \\n      \\n        3\\n      \\n    \\n    {\\\\displaystyle 3}\\n   is a term of type \\n  \\n    \\n      \\n        \\n          n\\n          a\\n          t\\n        \\n      \\n    \\n    {\\\\displaystyle \\\\mathrm {nat} }\\n  , then there exists a term of type \\n  \\n    \\n      \\n        I\\n         \\n        \\n          n\\n          a\\n          t\\n        \\n         \\n        3\\n         \\n        3\\n      \\n    \\n    {\\\\displaystyle I\\\\ \\\\mathrm {nat} \\\\ 3\\\\ 3}\\n  . More complicated equalities can be created by creating a reflexive term and then doing a reduction on one side. So if \\n  \\n    \\n      \\n        2\\n        +\\n        1\\n      \\n    \\n    {\\\\displaystyle 2+1}\\n   is a term of type \\n  \\n    \\n      \\n        \\n          n\\n          a\\n          t\\n        \\n      \\n    \\n    {\\\\displaystyle \\\\mathrm {nat} }\\n  , then there is a term of type \\n  \\n    \\n      \\n        I\\n         \\n        \\n          n\\n          a\\n          t\\n        \\n         \\n        (\\n        2\\n        +\\n        1\\n        )\\n         \\n        (\\n        2\\n        +\\n        1\\n        )\\n      \\n    \\n    {\\\\displaystyle I\\\\ \\\\mathrm {nat} \\\\ (2+1)\\\\ (2+1)}\\n   and, by reduction, generate a term of type \\n  \\n    \\n      \\n        I\\n         \\n        \\n          n\\n          a\\n          t\\n        \\n         \\n        (\\n        2\\n        +\\n        1\\n        )\\n         \\n        3\\n      \\n    \\n    {\\\\displaystyle I\\\\ \\\\mathrm {nat} \\\\ (2+1)\\\\ 3}\\n  . Thus, in this system, the equality type denotes that two values of the same type are convertible by reductions.\\nHaving a type for equality is important because it can be manipulated inside the system. There is usually no judgement to say two terms are not equal; instead, as in the Brouwer–Heyting–Kolmogorov interpretation, we map \\n  \\n    \\n      \\n        ¬\\n        (\\n        a\\n        =\\n        b\\n        )\\n      \\n    \\n    {\\\\displaystyle \\\\neg (a=b)}\\n   to \\n  \\n    \\n      \\n        (\\n        a\\n        =\\n        b\\n        )\\n        →\\n        ⊥\\n      \\n    \\n    {\\\\displaystyle (a=b)\\\\to \\\\bot }\\n  , where \\n  \\n    \\n      \\n        ⊥\\n      \\n    \\n    {\\\\displaystyle \\\\bot }\\n   is the bottom type having no values. There exists a term with type \\n  \\n    \\n      \\n        (\\n        I\\n         \\n        \\n          n\\n          a\\n          t\\n        \\n         \\n        3\\n         \\n        4\\n        )\\n        →\\n        ⊥\\n      \\n    \\n    {\\\\displaystyle (I\\\\ \\\\mathrm {nat} \\\\ 3\\\\ 4)\\\\to \\\\bot }\\n  , but not one of type \\n  \\n    \\n      \\n        (\\n        I\\n         \\n        \\n          n\\n          a\\n          t\\n        \\n         \\n        3\\n         \\n        3\\n        )\\n        →\\n        ⊥\\n      \\n    \\n    {\\\\displaystyle (I\\\\ \\\\mathrm {nat} \\\\ 3\\\\ 3)\\\\to \\\\bot }\\n  .\\nHomotopy type theory differs from intuitionistic type theory mostly by its handling of the equality type.\\n\\n\\n=== Inductive types ===\\n\\nA system of type theory requires some basic terms and types to operate on. Some systems build them out of functions using Church encoding. Other systems have inductive types: a set of base types and a set of type constructors that generate types with well-behaved properties. For example, certain recursive functions called on inductive types are guaranteed to terminate.\\nCoinductive types are infinite data types created by giving a function that generates the next element(s). See Coinduction and Corecursion.\\nInduction-induction  is a feature for declaring an inductive type and a family of types which depends on the inductive type.\\nInduction recursion allows a wider range of well-behaved types, allowing the type and recursive functions operating on it to be defined at the same time.\\n\\n\\n=== Universe types ===\\nTypes were created to prevent paradoxes, such as Russell\\'s paradox. However, the motives that lead to those paradoxes—being able to say things about all types—still exist. So, many type theories have a \"universe type\", which contains all other types (and not itself).\\nIn systems where you might want to say something about universe types, there is a hierarchy of universe types, each containing the one below it in the hierarchy. The hierarchy is defined as being infinite, but statements must only refer to a finite number of universe levels.\\nType universes are particularly tricky in type theory. The initial proposal of intuitionistic type theory suffered from Girard\\'s paradox.\\n\\n\\n=== Computational component ===\\nMany systems of type theory, such as the simply-typed lambda calculus, intuitionistic type theory, and the calculus of constructions, are also programming languages. That is, they are said to have a \"computational component\". The computation is the reduction of terms of the language using rewriting rules.\\nA system of type theory that has a well-behaved computational component also has a simple connection to constructive mathematics through the BHK interpretation.\\nNon-constructive mathematics in these systems is possible by adding operators on continuations such as call with current continuation. However, these operators tend to break desirable properties such as canonicity and parametricity.\\n\\n\\n== Type theories ==\\n\\n\\n=== Major ===\\nSimply typed lambda calculus which is a higher-order logic;\\nintuitionistic type theory;\\nsystem F;\\nLF is often used to define other type theories;\\ncalculus of constructions and its derivatives.\\n\\n\\n=== Minor ===\\nAutomath;\\nST type theory;\\nUTT (Luo\\'s Unified Theory of dependent Types)\\nsome forms of combinatory logic;\\nothers defined in the lambda cube;\\nothers under the name typed lambda calculus;\\nothers under the name pure type system.\\n\\n\\n=== Active ===\\nHomotopy type theory is being researched.\\n\\n\\n== Practical impact ==\\n\\n\\n=== Programming languages ===\\n\\nThere is extensive overlap and interaction between the fields of type theory and type systems. Type systems are a programming language feature designed to identify bugs. Any static program analysis, such as the type checking algorithms in the semantic analysis phase of compiler, has a connection to type theory.\\nA prime example is Agda, a programming language which uses UTT (Luo\\'s Unified Theory of dependent Types) for its type system. The programming language ML was developed for manipulating type theories (see LCF) and its own type system was heavily influenced by them.\\n\\n\\n=== Mathematical foundations ===\\nThe first computer proof assistant, called Automath, used type theory to encode mathematics on a computer. Martin-Löf specifically developed intuitionistic type theory to encode all mathematics to serve as a new foundation for mathematics. There is ongoing research into mathematical foundations using homotopy type theory.\\nMathematicians working in category theory already had difficulty working with the widely accepted foundation of Zermelo–Fraenkel set theory. This led to proposals such as Lawvere\\'s Elementary Theory of the Category of Sets (ETCS). Homotopy type theory continues in this line using type theory. Researchers are exploring connections between dependent types (especially the identity type) and algebraic topology (specifically homotopy).\\n\\n\\n=== Proof assistants ===\\n\\nMuch of the current research into type theory is driven by proof checkers, interactive proof assistants, and automated theorem provers. Most of these systems use a type theory as the mathematical foundation for encoding proofs, which is not surprising, given the close connection between type theory and programming languages:\\n\\nLF is used by Twelf, often to define other type theories;\\nmany type theories which fall under higher-order logic are used by the HOL family of provers and PVS;\\ncomputational type theory is used by NuPRL;\\ncalculus of constructions and its derivatives are used by Coq, Matita, and Lean;\\nUTT (Luo\\'s Unified Theory of dependent Types) is used by Agda which is both a programming language and proof assistantMany type theories are supported by LEGO and Isabelle. Isabelle also supports foundations besides type theories, such as ZFC. Mizar is an example of a proof system that only supports set theory.\\n\\n\\n=== Linguistics ===\\nType theory is also widely used in formal theories of semantics of natural languages, especially Montague grammar and its descendants. In particular, categorial grammars and pregroup grammars extensively use type constructors to define the types (noun, verb, etc.) of words.\\nThe most common construction takes the basic types \\n  \\n    \\n      \\n        e\\n      \\n    \\n    {\\\\displaystyle e}\\n   and \\n  \\n    \\n      \\n        t\\n      \\n    \\n    {\\\\displaystyle t}\\n   for individuals and truth-values, respectively, and defines the set of types recursively as follows:\\n\\nif \\n  \\n    \\n      \\n        a\\n      \\n    \\n    {\\\\displaystyle a}\\n   and \\n  \\n    \\n      \\n        b\\n      \\n    \\n    {\\\\displaystyle b}\\n   are types, then so is \\n  \\n    \\n      \\n        ⟨\\n        a\\n        ,\\n        b\\n        ⟩\\n      \\n    \\n    {\\\\displaystyle \\\\langle a,b\\\\rangle }\\n  ;\\nnothing except the basic types, and what can be constructed from them by means of the previous clause are types.A complex type \\n  \\n    \\n      \\n        ⟨\\n        a\\n        ,\\n        b\\n        ⟩\\n      \\n    \\n    {\\\\displaystyle \\\\langle a,b\\\\rangle }\\n   is the type of functions from entities of type \\n  \\n    \\n      \\n        a\\n      \\n    \\n    {\\\\displaystyle a}\\n   to entities of type \\n  \\n    \\n      \\n        b\\n      \\n    \\n    {\\\\displaystyle b}\\n  . Thus one has types like \\n  \\n    \\n      \\n        ⟨\\n        e\\n        ,\\n        t\\n        ⟩\\n      \\n    \\n    {\\\\displaystyle \\\\langle e,t\\\\rangle }\\n   which are interpreted as elements of the set of functions from entities to truth-values, i.e. indicator functions of sets of entities. An expression of type \\n  \\n    \\n      \\n        ⟨\\n        ⟨\\n        e\\n        ,\\n        t\\n        ⟩\\n        ,\\n        t\\n        ⟩\\n      \\n    \\n    {\\\\displaystyle \\\\langle \\\\langle e,t\\\\rangle ,t\\\\rangle }\\n   is a function from sets of entities to truth-values, i.e. a (indicator function of a) set of sets. This latter type is standardly taken to be the type of natural language quantifiers, like  everybody or  nobody (Montague 1973, Barwise and Cooper 1981).\\n\\n\\n=== Social sciences ===\\nGregory Bateson introduced a theory of logical types into the social sciences; his notions of double bind and logical levels are based on Russell\\'s theory of types.\\n\\n\\n== Relation to category theory ==\\nAlthough the initial motivation for category theory was far removed from foundationalism, the two fields turned out to have deep connections. As John Lane Bell writes: \"In fact categories can themselves be viewed as type theories of a certain kind; this fact alone indicates that type theory is much more closely related to category theory than it is to set theory.\" In brief, a category can be viewed as a type theory by regarding its objects as types (or sorts), i.e. \"Roughly speaking, a category may be thought of as a type theory shorn of its syntax.\" A number of significant results follow in this way:\\ncartesian closed categories correspond to the typed λ-calculus (Lambek, 1970);\\nC-monoids (categories with products and exponentials and one non-terminal object) correspond to the untyped λ-calculus (observed independently by Lambek and Dana Scott around 1980);\\nlocally cartesian closed categories correspond to Martin-Löf type theories (Seely, 1984).The interplay, known as categorical logic, has been a subject of active research since then; see the monograph of Jacobs (1999) for instance.\\n\\n\\n== See also ==\\nData type for concrete types of data in programming\\nDomain theory\\nType (model theory)\\nType system for a more practical discussion of type systems for programming languages\\nUnivalent foundations\\n\\n\\n== Notes ==\\n\\n\\n== References ==\\n\\n\\n== Further reading ==\\n\\n\\n== External links ==\\nRobert L. Constable (ed.). \"Computational type theory\". Scholarpedia.\\nThe TYPES Forum — moderated e-mail forum focusing on type theory in computer science, operating since 1987.\\nThe Nuprl Book: \"Introduction to Type Theory.\"\\nTypes Project lecture notes of summer schools 2005–2008\\nThe 2005 summer school has introductory lectures\\nType theory in nLab, which has articles on many related topics.\\nOregon Programming Languages Summer School, many lectures and some notes.\\nSummer 2015 Types, Logic, Semantics, and Verification', 'Computational science, also known as scientific computing or scientific computation (SC), is a rapidly growing field that uses advanced computing capabilities to understand and solve complex problems. It is an area of science which spans many disciplines, but at its core, it involves the development of models and simulations to understand natural systems.\\n\\nAlgorithms (numerical and non-numerical): mathematical models, computational models, and computer simulations developed to solve science (e.g., biological, physical, and social), engineering, and humanities problems\\nComputer hardware that develops and optimizes the advanced system hardware, firmware, networking, and data management components needed to solve computationally demanding problems\\nThe computing infrastructure that supports both the science and engineering problem solving and the developmental computer and information scienceIn practical use, it is typically the application of computer simulation and other forms of computation from numerical analysis and theoretical computer science to solve problems in various scientific disciplines. The field is different from theory and laboratory experiment which are the traditional forms of science and engineering. The scientific computing approach is to gain understanding, mainly through the analysis of mathematical models implemented on computers. Scientists and engineers develop computer programs, application software, that model systems being studied and run these programs with various sets of input parameters. The essence of computational science is the application of numerical algorithms and/or computational mathematics. In some cases, these models require massive amounts of calculations (usually floating-point) and are often executed on supercomputers or distributed computing platforms.\\n\\n\\n== The computational scientist ==\\n\\nThe term computational scientist is used to describe someone skilled in scientific computing. This person is usually a scientist, an engineer or an applied mathematician who applies high-performance computing in different ways to advance the state-of-the-art in their respective applied disciplines in physics, chemistry or engineering.\\nComputational science is now commonly considered a third mode of science, complementing and adding to experimentation/observation and theory (see image on the right). Here, we define a system as a potential source of data, an experiment as a process of extracting data from a system by exerting it through its inputs and a model (M) for a system (S) and an experiment (E) as anything to which E can be applied in order to answer questions about S.  A computational scientist should be capable of:\\n\\nrecognizing complex problems\\nadequately conceptualising the system containing these problems\\ndesigning a framework of algorithms suitable for studying this system: the simulation\\nchoosing a suitable computing infrastructure (parallel computing/grid computing/supercomputers)\\nhereby, maximising the computational power of the simulation\\nassessing to what level the output of the simulation resembles the systems: the model is validated\\nadjusting the conceptualisation of the system accordingly\\nrepeating cycle until a suitable level of validation is obtained: the computational scientists trusts that the simulation generates adequately realistic results for the system, under the studied conditionsIn fact, substantial effort in computational sciences has been devoted to the development of algorithms, the efficient implementation in programming languages, and validation of computational results. A collection of problems and solutions in computational science can be found in Steeb, Hardy, Hardy and Stoop (2004).Philosophers of science addressed the question to what degree computational science qualifies as science, among them Humphreys and Gelfert. They address the general question of epistemology: how do we gain insight from such computational science approaches. Tolk uses these insights to show the epistemological constraints of computer-based simulation research. As computational science uses mathematical models representing the underlying theory in executable form, in essence, they apply modeling (theory building) and simulation (implementation and execution). While simulation and computational science are our most sophisticated way to express our knowledge and understanding, they also come with all constraints and limits already known for computational solutions.\\n\\n\\n== Applications of computational science ==\\nProblem domains for computational science/scientific computing include:\\n\\n\\n=== Predictive computational science ===\\nPredictive computational science is a scientific discipline concerned with the formulation, calibration, numerical solution and validation of mathematical models designed to predict specific aspects of physical events, given initial and boundary conditions and a set of characterizing parameters and associated uncertainties. In typical cases, the predictive statement is formulated in terms of probabilities.  For example, given a mechanical component and a periodic loading condition, “the probability is (say) 90% that the number of cycles at failure (Nf) will be in the interval N1<Nf<N2”.\\n\\n\\n=== Urban complex systems ===\\nIn 2015, over half the world\\'s population live in cities. By the middle of the 21st century, it is estimated that 75% of the world\\'s population will be urban. This urban growth is focused in the urban populations of developing countries where city dwellers will more than double, increasing from 2.5 billion in 2009 to almost 5.2 billion in 2050. Cities are massive complex systems created by humans, made up of humans and governed by humans. Trying to predict, understand and somehow shape the development of cities in the future requires complex thinking, and requires computational models and simulations to help mitigate challenges and possible disasters. The focus of research in urban complex systems is, through modeling and simulation, to build a greater understanding of city dynamics and help prepare for the coming urbanisation.\\n\\n\\n=== Computational finance ===\\n\\nIn today\\'s financial markets huge volumes of interdependent assets are traded by a large number of interacting market participants in different locations and time zones. Their behavior is of unprecedented complexity and the characterization and measurement of the risk inherent to these highly diverse set of instruments is typically based on complicated mathematical and computational models. Solving these models exactly in closed form, even at a single instrument level, is typically not possible, and therefore we have to look for efficient numerical algorithms. This has become even more urgent and complex recently, as the credit crisis has clearly demonstrated the role of cascading effects going from single instruments through portfolios of single institutions to even the interconnected trading network. Understanding this requires a multi-scale and holistic approach where interdependent risk factors such as market, credit and liquidity risk are modelled simultaneously and at different interconnected scales.\\n\\n\\n=== Computational biology ===\\n\\nExciting new developments in biotechnology are now revolutionizing biology and biomedical research. Examples of these techniques are high-throughput sequencing, high-throughput quantitative PCR, intra-cellular imaging, in-situ hybridization of gene expression, three-dimensional imaging techniques like Light Sheet Fluorescence Microscopy and Optical Projection, (micro)-Computer Tomography. Given the massive amounts of complicated data that is generated by these techniques, their meaningful interpretation, and even their storage, form major challenges calling for new approaches. Going beyond current bioinformatics approaches, computational biology needs to develop new methods to discover meaningful patterns in these large data sets. Model-based reconstruction of gene networks can be used to organize the gene expression data in a systematic way and to guide future data collection. A major challenge here is to understand how gene regulation is controlling fundamental biological processes like biomineralisation and embryogenesis. The sub-processes like gene regulation, organic molecules interacting with the mineral deposition process, cellular processes, physiology and other processes at the tissue and environmental levels are linked. Rather than being directed by a central control mechanism, biomineralisation and embryogenesis can be viewed as an emergent behavior resulting from a complex system in which several sub-processes on very different temporal and spatial scales (ranging from nanometer and nanoseconds to meters and years) are connected into a multi-scale system. One of the few available options to understand such systems is by developing a multi-scale model of the system.\\n\\n\\n=== Complex systems theory ===\\n\\nUsing information theory, non-equilibrium dynamics and explicit simulations computational systems theory tries to uncover the true nature of complex adaptive systems.\\n\\n\\n=== Computational science in engineering ===\\n\\nComputational science and engineering (CSE) is a relatively new discipline that deals with the development and application of computational models and simulations, often coupled with high-performance computing, to solve complex physical problems arising in engineering analysis and design (computational engineering) as well as natural phenomena (computational science). CSE has been described as the \"third mode of discovery\" (next to theory and experimentation). In many fields, computer simulation is integral and therefore essential to business and research. Computer simulation provides the capability to enter fields that are either inaccessible to traditional experimentation or where carrying out traditional empirical inquiries is prohibitively expensive. CSE should neither be confused with pure computer science, nor with computer engineering, although a wide domain in the former is used in CSE (e.g., certain algorithms, data structures, parallel programming, high performance computing) and some problems in the latter can be modeled and solved with CSE methods (as an application area).\\n\\n\\n== Methods and algorithms ==\\nAlgorithms and mathematical methods used in computational science are varied. Commonly applied methods include:\\n\\nBoth historically and today, Fortran remains popular for most applications of scientific computing. Other programming languages and computer algebra systems commonly used for the more mathematical aspects of scientific computing applications include GNU Octave, Haskell, Julia, Maple, Mathematica, MATLAB, Python (with third-party SciPy library), Perl (with third-party PDL library), R, Scilab, and TK Solver. The more computationally intensive aspects of scientific computing will often use some variation of C or Fortran and optimized algebra libraries such as BLAS or LAPACK. In addition, parallel computing is heavily used in scientific computing to achieve solutions of large problems in a reasonable amount of time. In this framework, the problem is either divided over many cores on a single CPU node (such as with OpenMP), divided over many CPU nodes networked together (such as with MPI), or is run on one or more GPUs (typically using either CUDA or OpenCL).\\nComputational science application programs often model real-world changing conditions, such as weather, airflow around a plane, automobile body distortions in a crash, the motion of stars in a galaxy, an explosive device, etc. Such programs might create a \\'logical mesh\\' in computer memory where each item corresponds to an area in space and contains information about that space relevant to the model. For example, in weather models, each item might be a square kilometer; with land elevation, current wind direction, humidity, temperature, pressure, etc. The program would calculate the likely next state based on the current state, in simulated time steps, solving differential equations that describe how the system operates; and then repeat the process to calculate the next state.\\n\\n\\n== Conferences and journals ==\\nIn the year 2001, the International Conference on Computational Science (ICCS) was first organised. Since then it has been organised yearly. ICCS is an A-rank conference in CORE classification.\\nThe international Journal of Computational Science published its first issue in May 2010. A new initiative was launched in 2012, the Journal of Open Research Software.\\nIn 2015, ReScience C dedicated to the replication of computational results has been started on GitHub.\\n\\n\\n== Education ==\\nAt some institutions, a specialization in scientific computation can be earned as a \"minor\" within another program (which may be at varying levels). However, there are increasingly many bachelor\\'s, master\\'s and doctoral programs in computational science. The joint degree programme master program computational science at the University of Amsterdam and the Vrije Universiteit in computational science was first offered in 2004. In this programme, students:\\n\\nlearn to build computational models from real-life observations;\\ndevelop skills in turning these models into computational structures and in performing large-scale simulations;\\nlearn theory that will give a firm basis for the analysis of complex systems;\\nlearn to analyse the results of simulations in a virtual laboratory using advanced numerical algorithms.George Mason University was one of the early pioneers first offering a multidisciplinary doctorate Ph.D program in Computational Sciences and Informatics in 1992 that focused on a number of specialty areas including bioinformatics, computational chemistry, earth systems and global changes, computational mathematics, computational physics, space sciences, and computational statistics\\nSchool of Computational and Integrative Sciences, Jawaharlal Nehru University (erstwhile School of Information Technology) also offers a vibrant master\\'s science program for computational science with two specialities namely- Computational Biology and Complex Systems.\\n\\n\\n== Related fields ==\\n\\n\\n== See also ==\\n\\nComputer simulations in science\\nComputational science and engineering\\nComparison of computer algebra systems\\nDifferentiable programming\\nList of molecular modeling software\\nList of numerical-analysis software\\nList of statistical packages\\nTimeline of scientific computing\\nSimulated reality\\nExtensions for Scientific Computation (XSC)\\n\\n\\n== References ==\\n\\n\\n== Additional sources ==\\nE. Gallopoulos and A. Sameh, \"CSE: Content and Product\". IEEE Computational Science and Engineering Magazine, 4(2):39–43 (1997)\\nG. Hager and G. Wellein, Introduction to High Performance Computing for Scientists and Engineers, Chapman and Hall (2010)\\nA.K. Hartmann, Practical Guide to Computer Simulations, World Scientific (2009)\\nJournal Computational Methods in Science and Technology (open access), Polish Academy of Sciences\\nJournal Computational Science and Discovery, Institute of Physics\\nR.H. Landau, C.C. Bordeianu, and M. Jose Paez, A Survey of Computational Physics: Introductory Computational Science, Princeton University Press (2008)\\n\\n\\n== External links ==\\nJohn von Neumann-Institut for Computing (NIC) at Juelich (Germany)\\nThe National Center for Computational Science at Oak Ridge National Laboratory\\nCenter for Simulation and Modeling at George Mason University\\nEducational Materials for Undergraduate Computational Studies\\nComputational Science at the National Laboratories\\nBachelor in Computational Science, University of Medellin, Colombia, South America\\nSimulation Optimization Systems (SOS) Research Laboratory, McMaster University, Hamilton, ON\\nComputational Sciences and Informatics, Ph.D Program, George Mason University', 'Scientific modelling is a scientific activity, the aim of which is to make a particular part or feature of the world easier to understand, define, quantify, visualize, or simulate by referencing it to existing and usually commonly accepted knowledge. It requires selecting and identifying relevant aspects of a situation in the real world and then using different types of models for different aims, such as conceptual models to better understand, operational models to operationalize, mathematical models to quantify, computational models to simulate, and graphical models to visualize the subject. \\nModelling is an essential and inseparable part of many scientific disciplines, each of which has its own ideas about specific types of modelling. The following was said by John von Neumann.\\n... the sciences do not try to explain, they hardly even try to interpret, they mainly make models. By a model is meant a mathematical construct which, with the addition of certain verbal interpretations, describes observed phenomena. The justification of such a mathematical construct is solely and precisely that it is expected to work—that is, correctly to describe phenomena from a reasonably wide area.\\nThere is also an increasing attention to scientific modelling in fields such as science education, philosophy of science, systems theory, and knowledge visualization. There is a growing collection of methods, techniques and meta-theory about all kinds of specialized scientific modelling.\\n\\n\\n== Overview ==\\n\\nA scientific model seeks to represent empirical objects, phenomena, and physical processes in a logical and objective way. All models are in simulacra, that is, simplified reflections of reality that, despite being approximations, can be extremely useful. Building and disputing models is fundamental to the scientific enterprise. Complete and true representation may be impossible, but scientific debate often concerns which is the better model for a given task, e.g., which is the more accurate climate model for seasonal forecasting.Attempts to formalize the principles of the empirical sciences use an interpretation to model reality, in the same way logicians axiomatize the principles of logic. The aim of these attempts is to construct a formal system that will not produce theoretical consequences that are contrary to what is found in reality. Predictions or other statements drawn from such a formal system mirror or map the real world only insofar as these scientific models are true.For the scientist, a model is also a way in which the human thought processes can be amplified. For instance, models that are rendered in software allow scientists to leverage computational power to simulate, visualize, manipulate and gain intuition about the entity, phenomenon, or process being represented. Such computer models are in silico. Other types of scientific models are in vivo (living models, such as laboratory rats) and in vitro (in glassware, such as tissue culture).\\n\\n\\n== Basics ==\\n\\n\\n=== Modelling as a substitute for direct measurement and experimentation ===\\nModels are typically used when it is either impossible or impractical to create experimental conditions in which scientists can directly measure outcomes. Direct measurement of outcomes under controlled conditions (see Scientific method) will always be more reliable than modeled estimates of outcomes.\\nWithin  modeling and simulation, a model is a task-driven, purposeful simplification and abstraction of a perception of reality, shaped by physical, legal, and cognitive constraints. It is task-driven because a model is captured with a certain question or task in mind. Simplifications leave all the known and observed entities and their relation out that are not important for the task. Abstraction aggregates information that is important but not needed in the same detail as the object of interest. Both activities, simplification, and abstraction, are done purposefully. However, they are done based on a perception of reality. This perception is already a model in itself, as it comes with a physical constraint. There are also constraints on what we are able to legally observe with our current tools and methods, and cognitive constraints that limit what we are able to explain with our current theories. This model comprises the concepts, their behavior, and their relations informal form and is often referred to as a conceptual model. In order to execute the model, it needs to be implemented as a computer simulation. This requires more choices, such as numerical approximations or the use of heuristics. Despite all these epistemological and computational constraints, simulation has been recognized as the third pillar of scientific methods: theory building, simulation, and experimentation.\\n\\n\\n=== Simulation ===\\nA simulation is a way to implement the model, often employed when the model is too complex for the analytical solution. A steady-state simulation provides information about the system at a specific instant in time (usually at equilibrium, if such a state exists).  A dynamic simulation provides information over time. A simulation shows how a particular object or phenomenon will behave. Such a simulation can be useful for testing, analysis, or training in those cases where real-world systems or concepts can be represented by models.\\n\\n\\n=== Structure ===\\nStructure is a fundamental and sometimes intangible notion covering the recognition, observation, nature, and stability of patterns and relationships of entities. From a child\\'s verbal description of a snowflake, to the detailed scientific analysis of the properties of magnetic fields, the concept of structure is an essential foundation of nearly every mode of inquiry and discovery in science, philosophy, and art.\\n\\n\\n=== Systems ===\\nA system is a set of interacting or interdependent entities, real or abstract, forming an integrated whole. In general, a system is a construct or collection of different elements that together can produce results not obtainable by the elements alone.  The concept of an \\'integrated whole\\' can also be stated in terms of a system embodying a set of relationships which are differentiated from relationships of the set to other elements, and form relationships between an element of the set and elements not a part of the relational regime.  There are two types of system models: 1) discrete in which the variables change instantaneously at separate points in time and, 2) continuous where the state variables change continuously with respect to time.\\n\\n\\n=== Generating a model ===\\nModelling is the process of generating a model as a conceptual representation of some phenomenon. Typically a model will deal with only some aspects of the phenomenon in question, and two models of the same phenomenon may be essentially different—that is to say, that the differences between them comprise more than just a simple renaming of components.\\nSuch differences may be due to differing requirements of the model\\'s end users, or to conceptual or aesthetic differences among the modelers and to contingent decisions made during the modelling process. Considerations that may influence the structure of a model might be the modeler\\'s preference for a reduced ontology, preferences regarding statistical models versus deterministic models, discrete versus continuous time, etc. In any case, users of a model need to understand the assumptions made that are pertinent to its validity for a given use.\\nBuilding a model requires abstraction.  Assumptions are used in modelling in order to specify the domain of application of the model. For example, the special theory of relativity assumes an inertial frame of reference. This assumption was contextualized and further explained by the general theory of relativity. A model makes accurate predictions when its assumptions are valid, and might well not make accurate predictions when its assumptions do not hold. Such assumptions are often the point with which older theories are succeeded by new ones (the general theory of relativity works in non-inertial reference frames as well).\\n\\n\\n=== Evaluating a model ===\\n\\nA model is evaluated first and foremost by its consistency to empirical data; any model inconsistent with reproducible observations must be modified or rejected. One way to modify the model is by restricting the domain over which it is credited with having high validity. A case in point is Newtonian physics, which is highly useful except for the very small, the very fast, and the very massive phenomena of the universe. However, a fit to empirical data alone is not sufficient for a model to be accepted as valid.  Factors important in evaluating a model include:\\nAbility to explain past observations\\nAbility to predict future observations\\nCost of use, especially in combination with other models\\nRefutability, enabling estimation of the degree of confidence in the model\\nSimplicity, or even aesthetic appealPeople may attempt to quantify the evaluation of a model using a utility function.\\n\\n\\n=== Visualization ===\\nVisualization is any technique for creating images, diagrams, or animations to communicate a message. Visualization through visual imagery has been an effective way to communicate both abstract and concrete ideas since the dawn of man. Examples from history include cave paintings, Egyptian hieroglyphs, Greek geometry, and Leonardo da Vinci\\'s revolutionary methods of technical drawing for engineering and scientific purposes.\\n\\n\\n=== Space mapping ===\\nSpace mapping refers to a methodology that employs a \"quasi-global\" modelling formulation to link companion \"coarse\" (ideal or low-fidelity) with \"fine\" (practical or high-fidelity) models of different complexities. In engineering optimization, space mapping aligns (maps) a very fast coarse model with its related expensive-to-compute fine model so as to avoid direct expensive optimization of the fine model. The alignment process iteratively refines a \"mapped\" coarse model (surrogate model).\\n\\n\\n== Types ==\\n\\n\\n== Applications ==\\n\\n\\n=== Modelling and simulation ===\\nOne application of scientific modelling is the field of modelling and simulation, generally referred to as \"M&S\".  M&S has a spectrum of applications which range from concept development and analysis, through experimentation, measurement, and verification, to disposal analysis.  Projects and programs may use hundreds of different simulations, simulators and model analysis tools.\\n\\nThe figure shows how Modelling and Simulation is used as a central part of an integrated program in a Defence capability development process.\\n\\n\\n=== Model-based learning in education ===\\n\\nModel–based learning in education, particularly in relation to learning science involves students creating models for scientific concepts in order to:\\nGain insight of the scientific idea(s)\\nAcquire deeper understanding of the subject through visualization of the model\\nImprove student engagement in the courseDifferent types of model based learning techniques include:\\nPhysical macrocosms\\nRepresentational systems\\nSyntactic models\\nEmergent modelsModel–making in education is an iterative exercise with students refining, developing and evaluating their models over time. This shifts learning from the rigidity and monotony of traditional curriculum to an exercise of students\\' creativity and curiosity. This approach utilizes the constructive strategy of social collaboration and learning scaffold theory. Model based learning includes cognitive reasoning skills where existing models can be improved upon by the construction of newer models using the old models as a basis.\"Model–based learning entails determining target models and a learning pathway that provide realistic chances of understanding.\" Model making can also incorporate blended learning strategies by using web based tools and simulators, thereby allowing students to:\\n\\nFamiliarize themselves with on-line or digital resources\\nCreate different models with various virtual materials at little or no cost\\nPractice model making activity any time and any place\\nRefine existing models\"A well-designed simulation simplifies a real-world system while heightening awareness of the complexity of the system. Students can participate in the simplified system and learn how the real system operates without spending days, weeks or years it would take to undergo this experience in the real world.\"The teacher\\'s role in the overall teaching and learning process is primarily that of a facilitator and arranger of the learning experience. He or she would assign the students, a model making activity for a particular concept and provide relevant information or support for the activity. For virtual model making activities, the teacher can also provide information on the usage of the digital tool and render troubleshooting support in case of glitches while using the same. The teacher can also arrange the group discussion activity between the students and provide the platform necessary for students to share their observations and knowledge extracted from the model making activity.\\nModel-based learning evaluation could include the use of rubrics that assess the ingenuity and creativity of the student in the model construction and also the overall classroom participation of the student vis-a-vis the knowledge constructed through the activity.\\nIt is important, however, to give due consideration to the following for successful model-based learning to occur:\\n\\nUse of the right tool at the right time for a particular concept\\nProvision within the educational setup for the model–making activity: e.g., computer room with internet facility or software installed to access simulator or digital tool\\n\\n\\n== See also ==\\nAbductive reasoning\\nAll models are wrong\\nHeuristic\\nInverse model\\nScientific visualization\\nStatistical model\\n\\n\\n== References ==\\n\\n\\n== Further reading ==\\nNowadays there are some 40 magazines about scientific modelling which offer all kinds of international forums. Since the 1960s there is a strongly growing number of books and magazines about specific forms of scientific modelling. There is also a lot of discussion about scientific modelling in the philosophy-of-science literature. A selection:\\n\\nRainer Hegselmann, Ulrich Müller and Klaus Troitzsch (eds.) (1996). Modelling and Simulation in the Social Sciences from the Philosophy of Science Point of View. Theory and Decision Library. Dordrecht: Kluwer.\\nPaul Humphreys (2004). Extending Ourselves: Computational Science, Empiricism, and Scientific Method. Oxford: Oxford University Press.\\nJohannes Lenhard, Günter Küppers and Terry Shinn (Eds.) (2006) \"Simulation: Pragmatic Constructions of Reality\", Springer Berlin.\\nTom Ritchey (2012). \"Outline for a Morphology of Modelling Methods:  Contribution to a General Theory of Modelling\". In: Acta Morphologica Generalis, Vol 1. No 1. pp. 1–20.\\nWilliam Silvert (2001). \"Modelling as a Discipline\". In: Int. J. General Systems. Vol. 30(3), pp. 261.\\nSergio Sismondo and Snait Gissis (eds.) (1999). Modeling and Simulation. Special Issue of Science in Context 12.\\nEric Winsberg (2018) \"Philosophy and Climate Science\" Cambridge: Cambridge University Press\\nEric Winsberg (2010) \"Science in the Age of Computer Simulation\" Chicago: University of Chicago Press\\nEric Winsberg (2003). \"Simulated Experiments: Methodology for a Virtual World\". In: Philosophy of Science 70: 105–125.\\nTomáš Helikar, Jim A Rogers (2009). \"ChemChains: a platform for simulation and analysis of biochemical networks aimed to laboratory scientists\". BioMed Central.\\n\\n\\n== External links ==\\nModels. Entry in the Internet Encyclopedia of Philosophy\\nModels in Science. Entry in the Stanford Encyclopedia of Philosophy\\nThe World as a Process: Simulations in the Natural and Social Sciences, in: R. Hegselmann et al. (eds.), Modelling and Simulation in the Social Sciences from the Philosophy of Science Point of View, Theory and Decision Library. Dordrecht: Kluwer 1996, 77-100.\\nResearch in simulation and modelling of various physical systems\\nModelling Water Quality Information Center, U.S. Department of Agriculture\\nEcotoxicology & Models\\nA Morphology of Modelling Methods. Acta Morphologica Generalis, Vol 1. No 1. pp. 1–20.', 'Numerical analysis is the study of algorithms that use numerical approximation (as opposed to symbolic manipulations) for the problems of mathematical analysis (as distinguished from discrete mathematics). Numerical analysis finds application in all fields of engineering and the physical sciences, and in the 21st century also the life and social sciences, medicine, business and even the arts. Current growth in computing power has enabled the use of more complex numerical analysis, providing detailed and realistic mathematical models in science and engineering. Examples of numerical analysis include: ordinary differential equations as found in celestial mechanics (predicting the motions of planets, stars and galaxies), numerical linear algebra in data analysis, and stochastic differential equations and Markov chains for simulating living cells in medicine and biology.\\nBefore modern computers, numerical methods often relied on hand interpolation formulas, using data from large printed tables. Since the mid 20th century, computers calculate the required functions instead, but many of the same formulas continue to be used in software algorithms.The numerical point of view goes back to the earliest mathematical writings. A tablet from the Yale Babylonian Collection (YBC 7289), gives a sexagesimal numerical approximation of the square root of 2, the length of the diagonal in a unit square.\\nNumerical analysis continues this long tradition: rather than giving exact symbolic answers translated into digits and applicable only to real-world measurements, approximate solutions within specified error bounds are used.\\n\\n\\n== General introduction ==\\nThe overall goal of the field of numerical analysis is the design and analysis of techniques to give approximate but accurate solutions to hard problems, the variety of which is suggested by the following:\\n\\nAdvanced numerical methods are essential in making numerical weather prediction feasible.\\nComputing the trajectory of a spacecraft requires the accurate numerical solution of a system of ordinary differential equations.\\nCar companies can improve the crash safety of their vehicles by using computer simulations of car crashes. Such simulations essentially consist of solving partial differential equations numerically.\\nHedge funds (private investment funds) use tools from all fields of numerical analysis to attempt to calculate the value of stocks and derivatives more precisely than other market participants.\\nAirlines use sophisticated optimization algorithms to decide ticket prices, airplane and crew assignments and fuel needs. Historically, such algorithms were developed within the overlapping field of operations research.\\nInsurance companies use numerical programs for actuarial analysis.The rest of this section outlines several important themes of numerical analysis.\\n\\n\\n=== History ===\\nThe field of numerical analysis predates the invention of modern computers by many centuries. Linear interpolation was already in use more than 2000 years ago. Many great mathematicians of the past were preoccupied by numerical analysis, as is obvious from the names of important algorithms like Newton\\'s method, Lagrange interpolation polynomial, Gaussian elimination, or Euler\\'s method.\\nTo facilitate computations by hand, large books were produced with formulas and tables of data such as interpolation points and function coefficients. Using these tables, often calculated out to 16 decimal places or more for some functions, one could look up values to plug into the formulas given and achieve very good numerical estimates of some functions. The canonical work in the field is the NIST publication edited by Abramowitz and Stegun, a 1000-plus page book of a very large number of commonly used formulas and functions and their values at many points. The function values are no longer very useful when a computer is available, but the large listing of formulas can still be very handy.\\nThe mechanical calculator was also developed as a tool for hand computation. These calculators evolved into electronic computers in the 1940s, and it was then found that these computers were also useful for administrative purposes. But the invention of the computer also influenced the field of numerical analysis, since now longer and more complicated calculations could be done.\\n\\n\\n=== Direct and iterative methods ===\\nConsider the problem of solving\\n\\n3x3 + 4 = 28for the unknown quantity x.\\n\\nFor the iterative method, apply the bisection method to f(x) = 3x3 − 24. The initial values are a = 0, b = 3, f(a) = −24, f(b) = 57.\\n\\nFrom this table it can be concluded that the solution is between 1.875 and 2.0625. The algorithm might return any number in that range with an error less than 0.2.\\n\\n\\n==== Discretization and numerical integration ====\\n\\nIn a two-hour race, the speed of the car is measured at three instants and recorded in the following table.\\n\\nA discretization would be to say that the speed of the car was constant from 0:00 to 0:40, then from 0:40 to 1:20 and finally from 1:20 to 2:00. For instance, the total distance traveled in the first 40 minutes is approximately (2/3 h × 140 km/h) = 93.3 km. This would allow us to estimate the total distance traveled as 93.3 km + 100 km + 120 km = 313.3 km, which is an example of numerical integration (see below) using a Riemann sum, because displacement is the integral of velocity.\\nIll-conditioned problem: Take the function f(x) = 1/(x − 1). Note that f(1.1) = 10 and f(1.001) = 1000: a change in x of less than 0.1 turns into a change in f(x) of nearly 1000. Evaluating f(x) near x = 1 is an ill-conditioned problem.\\nWell-conditioned problem: By contrast, evaluating the same function f(x) = 1/(x − 1) near x = 10 is a well-conditioned problem. For instance, f(10) = 1/9 ≈ 0.111 and f(11) = 0.1: a modest change in x leads to a modest change in f(x).\\nDirect methods compute the solution to a problem in a finite number of steps. These methods would give the precise answer if they were performed in infinite precision arithmetic. Examples include Gaussian elimination, the QR factorization method for solving systems of linear equations, and the simplex method of linear programming. In practice, finite precision is used and the result is an approximation of the true solution (assuming stability).\\nIn contrast to direct methods, iterative methods are not expected to terminate in a finite number of steps. Starting from an initial guess, iterative methods form successive approximations that converge to the exact solution only in the limit. A convergence test, often involving the residual, is specified in order to decide when a sufficiently accurate solution has (hopefully) been found. Even using infinite precision arithmetic these methods would not reach the solution within a finite number of steps (in general). Examples include Newton\\'s method, the bisection method, and Jacobi iteration. In computational matrix algebra, iterative methods are generally needed for large problems.Iterative methods are more common than direct methods in numerical analysis. Some methods are direct in principle but are usually used as though they were not, e.g. GMRES and the conjugate gradient method. For these methods the number of steps needed to obtain the exact solution is so large that an approximation is accepted in the same manner as for an iterative method.\\n\\n\\n=== Discretization ===\\nFurthermore, continuous problems must sometimes be replaced by a discrete problem whose solution is known to approximate that of the continuous problem; this process is called \\'discretization\\'. For example, the solution of a differential equation is a function. This function must be represented by a finite amount of data, for instance by its value at a finite number of points at its domain, even though this domain is a continuum.\\n\\n\\n== Generation and propagation of errors ==\\nThe study of errors forms an important part of numerical analysis. There are several ways in which error can be introduced in the solution of the problem.\\n\\n\\n=== Round-off ===\\nRound-off errors arise because it is impossible to represent all real numbers exactly on a machine with finite memory (which is what all practical digital computers are).\\n\\n\\n=== Truncation and discretization error ===\\nTruncation errors are committed when an iterative method is terminated or a mathematical procedure is approximated, and the approximate solution differs from the exact solution. Similarly, discretization induces a discretization error because the solution of the discrete problem does not coincide with the solution of the continuous problem. For instance, in the iteration in the sidebar to compute the solution of \\n  \\n    \\n      \\n        3\\n        \\n          x\\n          \\n            3\\n          \\n        \\n        +\\n        4\\n        =\\n        28\\n      \\n    \\n    {\\\\displaystyle 3x^{3}+4=28}\\n  , after 10 or so iterations, it can be concluded that the root is roughly 1.99 (for example). Therefore, there is a truncation error of 0.01.\\nOnce an error is generated, it will generally propagate through the calculation. For instance, already noted is that the operation + on a calculator (or a computer) is inexact. It follows that a calculation of the type \\n  \\n    \\n      \\n        a\\n        +\\n        b\\n        +\\n        c\\n        +\\n        d\\n        +\\n        e\\n      \\n    \\n    {\\\\displaystyle a+b+c+d+e}\\n   is even more inexact.\\nThe truncation error is created when a mathematical procedure is approximated. To integrate a function exactly it is required to find the sum of infinite trapezoids, but numerically only the sum of only finite trapezoids can be found, and hence the approximation of the mathematical procedure. Similarly, to differentiate a function, the differential element approaches zero but numerically only a finite value of the differential element can be chosen.\\n\\n\\n=== Numerical stability and well-posed problems ===\\nNumerical stability is a notion in numerical analysis. An algorithm is called \\'numerically stable\\' if an error, whatever its cause, does not grow to be much larger during the calculation. This happens if the problem is \\'well-conditioned\\', meaning that the solution changes by only a small amount if the problem data are changed by a small amount. To the contrary, if a problem is \\'ill-conditioned\\', then any small error in the data will grow to be a large error.Both the original problem and the algorithm used to solve that problem can be \\'well-conditioned\\' or \\'ill-conditioned\\', and any combination is possible.\\nSo an algorithm that solves a well-conditioned problem may be either numerically stable or numerically unstable. An art of numerical analysis is to find a stable algorithm for solving a well-posed mathematical problem. For instance, computing the square root of 2 (which is roughly 1.41421) is a well-posed problem. Many algorithms solve this problem by starting with an initial approximation x0 to \\n  \\n    \\n      \\n        \\n          \\n            2\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\sqrt {2}}}\\n  , for instance x0 = 1.4, and then computing improved guesses x1, x2, etc. One such method is the famous Babylonian method, which is given by xk+1 = xk/2 + 1/xk. Another method, called \\'method X\\', is given by xk+1 = (xk2 − 2)2 + xk. A few iterations of each scheme are calculated in table form below, with initial guesses x0 = 1.4 and x0 = 1.42.\\n\\nObserve that the Babylonian method converges quickly regardless of the initial guess, whereas Method X converges extremely slowly with initial guess x0 = 1.4 and diverges for initial guess x0 = 1.42. Hence, the Babylonian method is numerically stable, while Method X is numerically unstable.\\n\\nNumerical stability is affected by the number of the significant digits the machine keeps on, if a machine is used that keeps only the four most significant decimal digits, a good example on loss of significance can be given by these two equivalent functions\\n\\n  \\n    \\n      \\n        f\\n        (\\n        x\\n        )\\n        =\\n        x\\n        \\n          (\\n          \\n            \\n              \\n                x\\n                +\\n                1\\n              \\n            \\n            −\\n            \\n              \\n                x\\n              \\n            \\n          \\n          )\\n        \\n        \\n           and \\n        \\n        g\\n        (\\n        x\\n        )\\n        =\\n        \\n          \\n            x\\n            \\n              \\n                \\n                  x\\n                  +\\n                  1\\n                \\n              \\n              +\\n              \\n                \\n                  x\\n                \\n              \\n            \\n          \\n        \\n        .\\n      \\n    \\n    {\\\\displaystyle f(x)=x\\\\left({\\\\sqrt {x+1}}-{\\\\sqrt {x}}\\\\right){\\\\text{ and }}g(x)={\\\\frac {x}{{\\\\sqrt {x+1}}+{\\\\sqrt {x}}}}.}\\n  \\nComparing the results of\\n\\n  \\n    \\n      \\n        f\\n        (\\n        500\\n        )\\n        =\\n        500\\n        \\n          (\\n          \\n            \\n              \\n                501\\n              \\n            \\n            −\\n            \\n              \\n                500\\n              \\n            \\n          \\n          )\\n        \\n        =\\n        500\\n        \\n          (\\n          \\n            22.38\\n            −\\n            22.36\\n          \\n          )\\n        \\n        =\\n        500\\n        (\\n        0.02\\n        )\\n        =\\n        10\\n      \\n    \\n    {\\\\displaystyle f(500)=500\\\\left({\\\\sqrt {501}}-{\\\\sqrt {500}}\\\\right)=500\\\\left(22.38-22.36\\\\right)=500(0.02)=10}\\n  \\nand\\n\\n  \\n    \\n      \\n        \\n          \\n            \\n              \\n                g\\n                (\\n                500\\n                )\\n              \\n              \\n                \\n                =\\n                \\n                  \\n                    500\\n                    \\n                      \\n                        \\n                          501\\n                        \\n                      \\n                      +\\n                      \\n                        \\n                          500\\n                        \\n                      \\n                    \\n                  \\n                \\n              \\n            \\n            \\n              \\n              \\n                \\n                =\\n                \\n                  \\n                    500\\n                    \\n                      22.38\\n                      +\\n                      22.36\\n                    \\n                  \\n                \\n              \\n            \\n            \\n              \\n              \\n                \\n                =\\n                \\n                  \\n                    500\\n                    44.74\\n                  \\n                \\n                =\\n                11.17\\n              \\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\begin{alignedat}{3}g(500)&={\\\\frac {500}{{\\\\sqrt {501}}+{\\\\sqrt {500}}}}\\\\\\\\&={\\\\frac {500}{22.38+22.36}}\\\\\\\\&={\\\\frac {500}{44.74}}=11.17\\\\end{alignedat}}}\\n  \\nby comparing the two results above, it is clear that loss of significance (caused here by catastrophic cancellation from subtracting approximations to the nearby numbers \\n  \\n    \\n      \\n        \\n          \\n            501\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\sqrt {501}}}\\n   and \\n  \\n    \\n      \\n        \\n          \\n            500\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\sqrt {500}}}\\n  , despite the subtraction being computed exactly) has a huge effect on the results, even though both functions are equivalent, as shown below\\n\\n  \\n    \\n      \\n        \\n          \\n            \\n              \\n                f\\n                (\\n                x\\n                )\\n              \\n              \\n                \\n                =\\n                x\\n                \\n                  (\\n                  \\n                    \\n                      \\n                        x\\n                        +\\n                        1\\n                      \\n                    \\n                    −\\n                    \\n                      \\n                        x\\n                      \\n                    \\n                  \\n                  )\\n                \\n              \\n            \\n            \\n              \\n              \\n                \\n                =\\n                x\\n                \\n                  (\\n                  \\n                    \\n                      \\n                        x\\n                        +\\n                        1\\n                      \\n                    \\n                    −\\n                    \\n                      \\n                        x\\n                      \\n                    \\n                  \\n                  )\\n                \\n                \\n                  \\n                    \\n                      \\n                        \\n                          x\\n                          +\\n                          1\\n                        \\n                      \\n                      +\\n                      \\n                        \\n                          x\\n                        \\n                      \\n                    \\n                    \\n                      \\n                        \\n                          x\\n                          +\\n                          1\\n                        \\n                      \\n                      +\\n                      \\n                        \\n                          x\\n                        \\n                      \\n                    \\n                  \\n                \\n              \\n            \\n            \\n              \\n              \\n                \\n                =\\n                x\\n                \\n                  \\n                    \\n                      (\\n                      \\n                        \\n                          x\\n                          +\\n                          1\\n                        \\n                      \\n                      \\n                        )\\n                        \\n                          2\\n                        \\n                      \\n                      −\\n                      (\\n                      \\n                        \\n                          x\\n                        \\n                      \\n                      \\n                        )\\n                        \\n                          2\\n                        \\n                      \\n                    \\n                    \\n                      \\n                        \\n                          x\\n                          +\\n                          1\\n                        \\n                      \\n                      +\\n                      \\n                        \\n                          x\\n                        \\n                      \\n                    \\n                  \\n                \\n              \\n            \\n            \\n              \\n              \\n                \\n                =\\n                x\\n                \\n                  \\n                    \\n                      x\\n                      +\\n                      1\\n                      −\\n                      x\\n                    \\n                    \\n                      \\n                        \\n                          x\\n                          +\\n                          1\\n                        \\n                      \\n                      +\\n                      \\n                        \\n                          x\\n                        \\n                      \\n                    \\n                  \\n                \\n              \\n            \\n            \\n              \\n              \\n                \\n                =\\n                x\\n                \\n                  \\n                    1\\n                    \\n                      \\n                        \\n                          x\\n                          +\\n                          1\\n                        \\n                      \\n                      +\\n                      \\n                        \\n                          x\\n                        \\n                      \\n                    \\n                  \\n                \\n              \\n            \\n            \\n              \\n              \\n                \\n                =\\n                \\n                  \\n                    x\\n                    \\n                      \\n                        \\n                          x\\n                          +\\n                          1\\n                        \\n                      \\n                      +\\n                      \\n                        \\n                          x\\n                        \\n                      \\n                    \\n                  \\n                \\n              \\n            \\n            \\n              \\n              \\n                \\n                =\\n                g\\n                (\\n                x\\n                )\\n              \\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\begin{alignedat}{4}f(x)&=x\\\\left({\\\\sqrt {x+1}}-{\\\\sqrt {x}}\\\\right)\\\\\\\\&=x\\\\left({\\\\sqrt {x+1}}-{\\\\sqrt {x}}\\\\right){\\\\frac {{\\\\sqrt {x+1}}+{\\\\sqrt {x}}}{{\\\\sqrt {x+1}}+{\\\\sqrt {x}}}}\\\\\\\\&=x{\\\\frac {({\\\\sqrt {x+1}})^{2}-({\\\\sqrt {x}})^{2}}{{\\\\sqrt {x+1}}+{\\\\sqrt {x}}}}\\\\\\\\&=x{\\\\frac {x+1-x}{{\\\\sqrt {x+1}}+{\\\\sqrt {x}}}}\\\\\\\\&=x{\\\\frac {1}{{\\\\sqrt {x+1}}+{\\\\sqrt {x}}}}\\\\\\\\&={\\\\frac {x}{{\\\\sqrt {x+1}}+{\\\\sqrt {x}}}}\\\\\\\\&=g(x)\\\\end{alignedat}}}\\n  The desired value, computed using infinite precision, is 11.174755...The example is a modification of one taken from Mathew; Numerical methods using Matlab, 3rd ed.\\n\\n\\n== Areas of study ==\\nThe field of numerical analysis includes many sub-disciplines. Some of the major ones are:\\n\\n\\n=== Computing values of functions ===\\nOne of the simplest problems is the evaluation of a function at a given point. The most straightforward approach, of just plugging in the number in the formula is sometimes not very efficient. For polynomials, a better approach is using the Horner scheme, since it reduces the necessary number of multiplications and additions. Generally, it is important to estimate and control round-off errors arising from the use of floating point arithmetic.\\n\\n\\n=== Interpolation, extrapolation, and regression ===\\nInterpolation solves the following problem: given the value of some unknown function at a number of points, what value does that function have at some other point between the given points?\\nExtrapolation is very similar to interpolation, except that now the value of the unknown function at a point which is outside the given points must be found.Regression is also similar, but it takes into account that the data is imprecise. Given some points, and a measurement of the value of some function at these points (with an error), the unknown function can be found. The least squares-method is one way to achieve this.\\n\\n\\n=== Solving equations and systems of equations ===\\nAnother fundamental problem is computing the solution of some given equation. Two cases are commonly distinguished, depending on whether the equation is linear or not. For instance, the equation \\n  \\n    \\n      \\n        2\\n        x\\n        +\\n        5\\n        =\\n        3\\n      \\n    \\n    {\\\\displaystyle 2x+5=3}\\n   is linear while \\n  \\n    \\n      \\n        2\\n        \\n          x\\n          \\n            2\\n          \\n        \\n        +\\n        5\\n        =\\n        3\\n      \\n    \\n    {\\\\displaystyle 2x^{2}+5=3}\\n   is not.\\nMuch effort has been put in the development of methods for solving systems of linear equations. Standard direct methods, i.e., methods that use some matrix decomposition are Gaussian elimination, LU decomposition, Cholesky decomposition for symmetric (or hermitian) and positive-definite matrix, and QR decomposition for non-square matrices. Iterative methods such as the Jacobi method, Gauss–Seidel method, successive over-relaxation and conjugate gradient method are usually preferred for large systems. General iterative methods can be developed using a matrix splitting.\\nRoot-finding algorithms are used to solve nonlinear equations (they are so named since a root of a function is an argument for which the function yields zero). If the function is differentiable and the derivative is known, then Newton\\'s method is a popular choice. Linearization is another technique for solving nonlinear equations.\\n\\n\\n=== Solving eigenvalue or singular value problems ===\\nSeveral important problems can be phrased in terms of eigenvalue decompositions or singular value decompositions. For instance, the spectral image compression algorithm is based on the singular value decomposition. The corresponding tool in statistics is called principal component analysis.\\n\\n\\n=== Optimization ===\\n\\nOptimization problems ask for the point at which a given function is maximized (or minimized). Often, the point also has to satisfy some constraints.\\nThe field of optimization is further split in several subfields, depending on the form of the objective function and the constraint. For instance, linear programming deals with the case that both the objective function and the constraints are linear. A famous method in linear programming is the simplex method.\\nThe method of Lagrange multipliers can be used to reduce optimization problems with constraints to unconstrained optimization problems.\\n\\n\\n=== Evaluating integrals ===\\n\\nNumerical integration, in some instances also known as numerical quadrature, asks for the value of a definite integral. Popular methods use one of the Newton–Cotes formulas (like the midpoint rule or Simpson\\'s rule) or Gaussian quadrature. These methods rely on a \"divide and conquer\" strategy, whereby an integral on a relatively large set is broken down into integrals on smaller sets. In higher dimensions, where these methods become prohibitively expensive in terms of computational effort, one may use Monte Carlo or quasi-Monte Carlo methods (see Monte Carlo integration), or, in modestly large dimensions, the method of sparse grids.\\n\\n\\n=== Differential equations ===\\n\\nNumerical analysis is also concerned with computing (in an approximate way) the solution of differential equations, both ordinary differential equations and partial differential equations.Partial differential equations are solved by first discretizing the equation, bringing it into a finite-dimensional subspace. This can be done by a finite element method, a finite difference method, or (particularly in engineering) a finite volume method. The theoretical justification of these methods often involves theorems from functional analysis. This reduces the problem to the solution of an algebraic equation.\\n\\n\\n== Software ==\\n\\nSince the late twentieth century, most algorithms are implemented in a variety of programming languages. The Netlib repository contains various collections of software routines for numerical problems, mostly in Fortran and C. Commercial products implementing many different numerical algorithms include the IMSL and NAG libraries; a free-software alternative is the GNU Scientific Library.\\nOver the years the Royal Statistical Society published numerous algorithms in its Applied Statistics (code for these \"AS\" functions is here); \\nACM similarly, in its Transactions on Mathematical Software (\"TOMS\" code is here).\\nThe Naval Surface Warfare Center several times published its Library of Mathematics Subroutines (code here).\\nThere are several popular numerical computing applications such as MATLAB, TK Solver, S-PLUS, and IDL as well as free and open source alternatives such as FreeMat, Scilab, GNU Octave (similar to Matlab), and IT++ (a C++ library). There are also programming languages such as R (similar to S-PLUS), Julia, and Python with libraries such as NumPy, SciPy and SymPy. Performance varies widely: while vector and matrix operations are usually fast, scalar loops may vary in speed by more than an order of magnitude.Many computer algebra systems such as Mathematica also benefit from the availability of arbitrary-precision arithmetic which can provide more accurate results.Also, any spreadsheet software can be used to solve simple problems relating to numerical analysis. \\nExcel, for example, has hundreds of available functions, including for matrices, which may be used in conjunction with its built in \"solver\".\\n\\n\\n== See also ==\\nAnalysis of algorithms\\nComputational science\\nInterval arithmetic\\nList of numerical analysis topics\\nLocal linearization method\\nNumerical differentiation\\nNumerical Recipes\\nSymbolic-numeric computation\\nValidated numerics\\n\\n\\n== Notes ==\\n\\n\\n== References ==\\n\\n\\n=== Citations ===\\n\\n\\n=== Sources ===\\n\\n\\n== External links ==\\n\\n\\n=== Journals ===\\ngdz.sub.uni-goettingen, Numerische Mathematik, volumes 1-66, Springer, 1959-1994 (searchable; pages are images). (in English and German)\\nNumerische Mathematik, volumes 1–112, Springer, 1959–2009\\nJournal on Numerical Analysis, volumes 1-47, SIAM, 1964–2009\\n\\n\\n=== Online texts ===\\n\"Numerical analysis\", Encyclopedia of Mathematics, EMS Press, 2001 [1994]\\nNumerical Recipes, William H. Press (free, downloadable previous editions)\\nFirst Steps in Numerical Analysis (archived), R.J.Hosking, S.Joe, D.C.Joyce, and J.C.Turner\\nCSEP (Computational Science Education Project), U.S. Department of Energy (archived 2017-08-01)\\nNumerical Methods, ch 3. in the Digital Library of Mathematical Functions\\n\\n\\n=== Online course material ===\\nNumerical Methods Archived 28 July 2009 at the Wayback Machine, Stuart Dalziel University of Cambridge\\nLectures on Numerical Analysis, Dennis Deturck and Herbert S. Wilf University of Pennsylvania\\nNumerical methods, John D. Fenton University of Karlsruhe\\nNumerical Methods for Physicists, Anthony O’Hare Oxford University\\nLectures in Numerical Analysis (archived), R. Radok Mahidol University\\nIntroduction to Numerical Analysis for Engineering, Henrik Schmidt Massachusetts Institute of Technology\\nNumerical Analysis for Engineering, D. W. Harder University of Waterloo', 'In mathematics and computing, a root-finding algorithm is an algorithm for finding zeroes, also called \"roots\", of continuous functions. A zero of a function f, from the real numbers to real numbers or from the complex numbers to the complex numbers, is a number x such that f(x) = 0. As, generally, the zeroes of a function cannot be computed exactly nor expressed in closed form, root-finding algorithms provide approximations to zeroes, expressed either as floating point numbers or as small isolating intervals,  or disks for complex roots (an interval or disk output being equivalent to an approximate output together with an error bound).\\nSolving an equation f(x) = g(x) is the same as finding the roots of the function h(x) = f(x) – g(x). Thus root-finding algorithms allow solving any equation defined by continuous functions. However, most root-finding algorithms do not guarantee that they will find all the roots; in particular, if such an algorithm does not find any root, that does not mean that no root exists.\\nMost numerical root-finding methods use iteration, producing a sequence of numbers that hopefully converge towards the root as a limit. They require one or more initial guesses of the root as starting values, then each iteration of the algorithm produces a successively more accurate approximation to the root. Since the iteration must be stopped at some point these methods produce an approximation to the root, not an exact solution. Many methods compute subsequent values by evaluating an auxiliary function on the preceding values. The limit is thus a fixed point of the auxiliary function, which is chosen for having the roots of the original equation as fixed points, and for converging rapidly to these fixed points.\\nThe behaviour of general root-finding algorithms is studied in numerical analysis. However, for polynomials, root-finding study belongs generally to computer algebra, since algebraic properties of polynomials are fundamental for the most efficient algorithms. The efficiency of an algorithm may depend dramatically on the characteristics of the given functions. For example, many algorithms use the derivative of the input function, while others work on every continuous function. In general, numerical algorithms are not guaranteed to find all the roots of a function, so failing to find a root does not prove that there is no root. However, for polynomials, there are specific algorithms that use algebraic properties for certifying that no root is missed, and locating the roots in separate intervals (or disks for complex roots) that are small enough to ensure the convergence of numerical methods (typically Newton\\'s method) to the unique root so located.\\n\\n\\n== Bracketing methods ==\\nBracketing methods determine successively smaller intervals (brackets) that contain a root. When the interval is small enough, then a root has been found. They generally use the intermediate value theorem, which asserts that if a continuous function has values of opposite signs at the end points of an interval, then the function has at least one root in the interval. Therefore, they require to start with an interval such that the function takes opposite signs at the end points of the interval. However, in the case of polynomials there are other methods (Descartes\\' rule of signs, Budan\\'s theorem and Sturm\\'s theorem) for getting information on the number of roots in an interval. They lead to efficient algorithms for real-root isolation of polynomials, which ensure finding all real roots with a guaranteed accuracy.\\n\\n\\n=== Bisection method ===\\nThe simplest root-finding algorithm is the bisection method. Let f be a continuous function, for which one knows an interval [a, b] such that  f(a) and f(b) have opposite signs (a bracket). Let c = (a +b)/2 be the middle of the interval (the midpoint or the point that bisects the interval). Then either f(a) and f(c), or f(c) and f(b) have opposite signs, and one has divided by two the size of the interval. Although the bisection method is robust, it gains one and only one bit of accuracy with each iteration. Other methods, under appropriate conditions, can gain accuracy faster.\\n\\n\\n=== False position (regula falsi) ===\\nThe false position method, also called the regula falsi method, is similar to the bisection method, but instead of using bisection search\\'s middle of the interval it uses the x-intercept of the line that connects the plotted function values at the endpoints of the interval, that is \\n\\n  \\n    \\n      \\n        c\\n        =\\n        \\n          \\n            \\n              a\\n              f\\n              (\\n              b\\n              )\\n              −\\n              b\\n              f\\n              (\\n              a\\n              )\\n            \\n            \\n              f\\n              (\\n              b\\n              )\\n              −\\n              f\\n              (\\n              a\\n              )\\n            \\n          \\n        \\n        .\\n      \\n    \\n    {\\\\displaystyle c={\\\\frac {af(b)-bf(a)}{f(b)-f(a)}}.}\\n  False position is similar to the secant method, except that, instead of retaining the last two points, it makes sure to keep one point on either side of the root.  The false position method can be faster than the bisection method and will never diverge like the secant method; however, it may fail to converge in some naive implementations due to roundoff errors that may lead to a wrong sign for f(c); typically, this may occur if the rate of variation of f is large in the neighborhood of the root.\\n\\n\\n=== ITP method ===\\nThe ITP method is the only known method to bracket the root with the same worst case guarantees of the bisection method while guaranteeing a superlinear convergence to the root of smooth functions as the secant method. It is also the only known method guaranteed to outperform the bisection method on the average for any continuous distribution on the location of the root (see  ITP Method#Analysis). It does so by keeping track of both the bracketing interval as well as the minmax interval in which any point therein converges as fast as the bisection method. The construction of the queried point c follows three steps: interpolation (similar to the regula falsi), truncation (adjusting the regula falsi similar to Regula falsi § Improvements in regula falsi) and then projection onto the minmax interval. The combination of these steps produces a simultaneously minmax optimal method with guarantees similar to interpolation based methods for smooth functions, and, in practice will outperform both the bisection method and interpolation based methods under both smooth and non-smooth functions.\\n\\n\\n== Interpolation ==\\nMany root-finding processes work by interpolation. This consists in using the last computed approximate values of the root for approximating the function by a polynomial of low degree, which takes the same values at these approximate roots. Then the root of the polynomial is computed and used as a new approximate value of the root of the function, and the process is iterated.\\nTwo values allow interpolating a function by a polynomial of degree one (that is approximating the graph of the function by a line). This is the basis of the secant method. Three values define a quadratic function, which approximates the graph of the function by a parabola. This is Muller\\'s method.\\nRegula falsi is also an interpolation method, which differs from the secant method by using, for interpolating by a line, two points that are not necessarily the last two computed points.\\n\\n\\n== Iterative methods ==\\nAlthough all root-finding algorithms proceed by iteration, an iterative root-finding method generally uses a specific type of iteration, consisting of defining an auxiliary function, which is applied to the last computed approximations of a root for getting a new approximation. The iteration stops when a fixed point (up to the desired precision) of the auxiliary function is reached, that is when the new computed value is sufficiently close to the preceding ones.\\n\\n\\n=== Newton\\'s method (and similar derivative-based methods) ===\\nNewton\\'s method assumes the function f to have a continuous derivative. Newton\\'s method may not converge if started too far away from a root. However, when it does converge, it is faster than the bisection method, and is usually quadratic. Newton\\'s method is also important because it readily generalizes to higher-dimensional problems. Newton-like methods with higher orders of convergence are the Householder\\'s methods. The first one after Newton\\'s method is Halley\\'s method with cubic order of convergence.\\n\\n\\n=== Secant method ===\\nReplacing the derivative in Newton\\'s method with a finite difference, we get the secant method. This method does not require the computation (nor the existence) of a derivative, but the price is slower convergence (the order is approximately 1.6 (golden ratio)). A generalization of the secant method in higher dimensions is Broyden\\'s method.\\n\\n\\n=== Steffensen\\'s method ===\\nIf we use a polynomial fit to remove the quadratic part of the finite difference used in the Secant method, so that it better approximates the derivative, we obtain Steffensen\\'s method, which has quadratic convergence, and whose behavior (both good and bad) is essentially the same as Newton\\'s method but does not require a derivative.\\n\\n\\n=== Inverse interpolation ===\\nThe appearance of complex values in interpolation methods can be avoided by interpolating the inverse of f, resulting in the inverse quadratic interpolation method. Again, convergence is asymptotically faster than the secant method, but inverse quadratic interpolation often behaves poorly when the iterates are not close to the root.\\n\\n\\n== Combinations of methods ==\\n\\n\\n=== Brent\\'s method ===\\nBrent\\'s method is a combination of the bisection method, the secant method and inverse quadratic interpolation. At every iteration, Brent\\'s method decides which method out of these three is likely to do best, and proceeds by doing a step according to that method. This gives a robust and fast method, which therefore enjoys considerable popularity.\\n\\n\\n=== Ridders\\' method ===\\nRidders\\' method is a hybrid method that uses the value of function at the midpoint of the interval to perform an exponential interpolation to the root. This gives a fast convergence with a guaranteed convergence of at most twice the number of iterations as the bisection method.\\n\\n\\n== Roots of polynomials ==\\nFinding roots of polynomial is a long-standing problem that has been the object of much research throughout history. A testament to this is that up until the 19th century algebra meant essentially theory of polynomial equations.\\nFinding the root of a linear polynomial (degree one) is easy and needs only one division. For quadratic polynomials (degree two), the quadratic formula produces a solution, but its numerical evaluation may require some care for ensuring numerical stability. For degrees three and four, there are closed-form solutions in terms of radicals, which are generally not convenient for numerical evaluation, as being too complicated and involving the computation of several nth roots whose computation is not easier than the direct computation of the roots of the polynomial (for example the expression of the real roots of a cubic polynomial may involve non-real cube roots). For polynomials of degree five or higher Abel–Ruffini theorem asserts that there is, in general, no radical expression of the roots.\\nSo, except for very low degrees, root finding of polynomials consists of finding approximations of the roots. By the fundamental theorem of algebra, one knows that a polynomial of degree n has at most n real or complex roots, and this number is reached for almost all polynomials.\\nIt follows that the problem of root finding for polynomials may be split in three different subproblems;\\n\\nFinding one root\\nFinding all roots\\nFinding roots in a specific region of the complex plane, typically the real roots or the real roots in a given interval (for example, when roots represents a physical quantity, only the real positive ones are interesting).For finding one root, Newton\\'s method and other general iterative methods work generally well.\\nFor finding all the roots, the oldest method is, when a root r has been found, to divide the polynomial by x – r, and restart iteratively the search of a root of the quotient polynomial. However, except for low degrees, this does not work well because of the numerical instability: Wilkinson\\'s polynomial shows that a very small modification of one coefficient may change dramatically not only the value of the roots, but also their nature (real or complex). Also, even with a good approximation, when one evaluates a polynomial at an approximate root, one may get a result that is far to be close to zero. For example, if a polynomial of degree 20 (the degree of Wilkinson\\'s polynomial) has a root close to 10, the derivative of the polynomial at the root may be of the order of \\n  \\n    \\n      \\n        \\n          10\\n          \\n            20\\n          \\n        \\n        ;\\n      \\n    \\n    {\\\\displaystyle 10^{20};}\\n   this implies that an error of \\n  \\n    \\n      \\n        \\n          10\\n          \\n            −\\n            10\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle 10^{-10}}\\n   on the value of the root may produce a value of the polynomial at the approximate root that is of the order of \\n  \\n    \\n      \\n        \\n          10\\n          \\n            10\\n          \\n        \\n        .\\n      \\n    \\n    {\\\\displaystyle 10^{10}.}\\n  \\nFor avoiding these problems, methods have been elaborated, which compute all roots simultaneously, to any desired accuracy. Presently the most efficient method is Aberth method. A free implementation is available under the name of MPSolve. This is a reference implementation, which can find routinely the roots of polynomials of degree larger than 1,000, with more than 1,000 significant decimal digits.\\nThe methods for computing all roots may be used for computing real roots. However, it may be difficult to decide whether a root with a small imaginary part is real or not. Moreover, as the number of the real roots is, on the average, the logarithm of the degree, it is a waste of computer resources to compute the non-real roots when one is interested in real roots.\\nThe oldest method for computing the number of real roots, and the number of roots in an interval results from Sturm\\'s theorem, but the methods based on Descartes\\' rule of signs and its extensions—Budan\\'s and Vincent\\'s theorems—are generally more efficient. For root finding, all proceed by reducing the size of the intervals in which roots are searched until getting intervals containing zero or one root. Then the intervals containing one root may be further reduced for getting a quadratic convergence of Newton\\'s method to the isolated roots. The main computer algebra systems (Maple, Mathematica, SageMath, PARI/GP) have each a variant of this method as the default algorithm for the real roots of a polynomial.\\nAnother class of methods is based on converting the problem of finding polynomial roots to the problem of finding eigenvalues of the companion matrix of the polynomial. In principle, one can use any eigenvalue algorithm to find the roots of the polynomial. However, for efficiency reasons one prefers methods that employ the structure of the matrix, that is, can be implemented in matrix-free form. Among these methods are the power method, whose application to the transpose of the companion matrix is the classical Bernoulli\\'s method to find the root of greatest modulus. The inverse power method with shifts, which finds some smallest root first, is what drives the complex (cpoly) variant of the Jenkins–Traub algorithm and gives it its numerical stability. Additionally, it is insensitive to multiple roots and has fast convergence with order \\n  \\n    \\n      \\n        1\\n        +\\n        φ\\n        ≈\\n        2.6\\n      \\n    \\n    {\\\\displaystyle 1+\\\\varphi \\\\approx 2.6}\\n   (where \\n  \\n    \\n      \\n        φ\\n      \\n    \\n    {\\\\displaystyle \\\\varphi }\\n   is the golden ratio) even in the presence of clustered roots. This fast convergence comes with a cost of three polynomial evaluations per step, resulting in a residual of O(|f(x)|2+3φ), that is a slower convergence than with three steps of Newton\\'s method.\\n\\n\\n=== Finding one root ===\\nThe most widely used method for computing a root is Newton\\'s method, which consists of the iterations of the computation of \\n\\n  \\n    \\n      \\n        \\n          x\\n          \\n            n\\n            +\\n            1\\n          \\n        \\n        =\\n        \\n          x\\n          \\n            n\\n          \\n        \\n        −\\n        \\n          \\n            \\n              f\\n              (\\n              \\n                x\\n                \\n                  n\\n                \\n              \\n              )\\n            \\n            \\n              \\n                f\\n                ′\\n              \\n              (\\n              \\n                x\\n                \\n                  n\\n                \\n              \\n              )\\n            \\n          \\n        \\n        ,\\n      \\n    \\n    {\\\\displaystyle x_{n+1}=x_{n}-{\\\\frac {f(x_{n})}{f\\'(x_{n})}},}\\n  by starting from a well-chosen value \\n  \\n    \\n      \\n        \\n          x\\n          \\n            0\\n          \\n        \\n        .\\n      \\n    \\n    {\\\\displaystyle x_{0}.}\\n  \\nIf f is a polynomial, the computation is faster when using Horner\\'s method or evaluation with preprocessing for computing the polynomial and its derivative in each iteration.\\nThe convergence is generally quadratic, it may converge much slowly or even not converge at all. In particular, if the polynomial has no real root, and \\n  \\n    \\n      \\n        \\n          x\\n          \\n            0\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle x_{0}}\\n   is real, then Newton\\'s method cannot converge. However, if the polynomial has a real root, which is larger than the larger real root of its derivative, then Newton\\'s method converges quadratically to this largest root if \\n  \\n    \\n      \\n        \\n          x\\n          \\n            0\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle x_{0}}\\n   is larger that this larger root (there are easy ways for computing an upper bound of the roots, see Properties of polynomial roots). This is the starting point of Horner method for computing the roots.\\nWhen one root r has been found, one may use Euclidean division for removing the factor x – r from the polynomial. Computing a root of the resulting quotient, and repeating the process provides, in principle, a way for computing all roots. However, this iterative scheme is numerically unstable; the approximation errors accumulate during the successive factorizations, so that the last roots are determined with a polynomial that deviates widely from a factor of the original polynomial. To reduce this error, one may, for each root that is found, restart Newton\\'s method with the original polynomial, and this approximate root as starting value.\\nHowever, there is no warranty that this will allow finding all roots. In fact, the problem of finding the roots of a polynomial from its coefficients is in general highly ill-conditioned. This is illustrated by \\nWilkinson\\'s polynomial: the roots of this polynomial of degree 20 are the 20 first positive integers; changing the last bit of the 32-bit representation of one of its coefficient (equal to –210) produces a polynomial with only 10 real roots and 10 complex roots with imaginary parts larger than 0.6.\\nClosely related to Newton\\'s method are Halley\\'s method and Laguerre\\'s method. Both use the polynomial and its two first derivations for an iterative process that has a cubic convergence. Combining two consecutive steps of these methods into a single test, one gets a rate of convergence of 9, at the cost of 6 polynomial evaluations (with Horner rule). On the other hand, combining three steps of Newtons method gives a rate of convergence of 8 at the cost of the same number of polynomial evaluation. This gives a slight advantage to these methods (less clear for Laguerre\\'s method, as a square root has to be computed at each step).\\nWhen applying these methods to polynomials with real coefficients and real starting points, Newton\\'s and Halley\\'s method stay inside the real number line. One has to choose complex starting points to find complex roots. In contrast, the Laguerre method with a square root in its evaluation will leave the real axis of its own accord.\\n\\n\\n=== Finding roots in pairs ===\\nIf the given polynomial only has real coefficients, one may wish to avoid computations with complex numbers. To that effect, one has to find quadratic factors for pairs of conjugate complex roots. The application of the multidimensional Newton\\'s method to this task results in Bairstow\\'s method.\\nThe real variant of Jenkins–Traub algorithm is an improvement of this method.\\n\\n\\n=== Finding all roots at once ===\\nThe simple Durand–Kerner and the slightly more complicated Aberth method simultaneously find all of the roots using only simple complex number arithmetic. Accelerated algorithms for multi-point evaluation and interpolation similar to the fast Fourier transform can help speed them up for large degrees of the polynomial. It is advisable to choose an asymmetric, but evenly distributed set of initial points. The implementation of this method in the free software MPSolve is a reference for its efficiency and its accuracy.\\nAnother method with this style is the Dandelin–Gräffe method (sometimes also ascribed to Lobachevsky), which uses polynomial transformations to repeatedly and implicitly square the roots. This greatly magnifies variances in the roots. Applying Viète\\'s formulas, one obtains easy approximations for the modulus of the roots, and with some more effort, for the roots themselves.\\n\\n\\n=== Exclusion and enclosure methods ===\\nSeveral fast tests exist that tell if a segment of the real line or a region of the complex plane contains no roots. By bounding the modulus of the roots and recursively subdividing the initial region indicated by these bounds, one can isolate small regions that may contain roots and then apply other methods to locate them exactly.\\nAll these methods involve finding the coefficients of shifted and scaled versions of the polynomial. For large degrees, FFT-based accelerated methods become viable.\\nFor real roots, see next sections.\\nThe Lehmer–Schur algorithm uses the Schur–Cohn test for circles; a variant, Wilf\\'s global bisection algorithm uses a winding number computation for rectangular regions in the complex plane.\\nThe splitting circle method uses FFT-based polynomial transformations to find large-degree factors corresponding to clusters of roots. The precision of the factorization is maximized using a Newton-type iteration. This method is useful for finding the roots of polynomials of high degree to arbitrary precision; it has almost optimal complexity in this setting.\\n\\n\\n=== Real-root isolation ===\\n\\nFinding the real roots of a polynomial with real coefficients is a problem that has received much attention since the beginning of 19th century, and is still an active domain of research. Most root-finding algorithms can find some real roots, but cannot certify having found all the roots. Methods for finding all complex roots, such as Aberth method can provide the real roots. However, because of the numerical instability of polynomials (see Wilkinson\\'s polynomial), they may need arbitrary-precision arithmetic for deciding which roots are real. Moreover, they compute all complex roots when only few are real.\\nIt follows that the standard way of computing real roots is to compute first disjoint intervals, called isolating intervals, such that each one contains exactly one real root, and together they contain all the roots. This computation is called real-root isolation. Having isolating interval, one may use fast numerical methods, such as Newton\\'s method for improving the precision of the result.\\nThe oldest complete algorithm for real-root isolation results from Sturm\\'s theorem. However, it appears to be much less efficient than the methods based on Descartes\\' rule of signs and Vincent\\'s theorem. These methods divide into two main classes, one using continued fractions and the other using bisection. Both method have been dramatically improved since the beginning of 21st century. With these improvements they reach a computational complexity that is similar to that of the best algorithms for computing all the roots (even when all roots are real).\\nThese algorithms have been implemented and are available in Mathematica (continued fraction method) and Maple (bisection method). Both implementations can routinely find the real roots of polynomials of degree higher than 1,000.\\n\\n\\n=== Finding multiple roots of polynomials ===\\n\\nMost root-finding algorithms behave badly when there are multiple roots or very close roots. However, for polynomials whose coefficients are exactly given as integers or rational numbers, there is an efficient method to factorize them into factors that have only simple roots and whose coefficients are also exactly given. This method, called square-free factorization, is based on the multiple roots of a polynomial being the roots of the greatest common divisor of the polynomial and its derivative.\\nThe square-free factorization of a polynomial p is a factorization \\n  \\n    \\n      \\n        p\\n        =\\n        \\n          p\\n          \\n            1\\n          \\n        \\n        \\n          p\\n          \\n            2\\n          \\n          \\n            2\\n          \\n        \\n        ⋯\\n        \\n          p\\n          \\n            k\\n          \\n          \\n            k\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle p=p_{1}p_{2}^{2}\\\\cdots p_{k}^{k}}\\n   where each \\n  \\n    \\n      \\n        \\n          p\\n          \\n            i\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle p_{i}}\\n   is either 1 or a polynomial without multiple roots, and two different \\n  \\n    \\n      \\n        \\n          p\\n          \\n            i\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle p_{i}}\\n   do not have any common root.\\nAn efficient method to compute this factorization is Yun\\'s algorithm.\\n\\n\\n== See also ==\\nList of root finding algorithms\\nBroyden\\'s method – Quasi-Newton root-finding method for the multivariable case\\nCryptographically secure pseudorandom number generator\\nGNU Scientific Library\\nGraeffe\\'s method – Algorithm for finding polynomial roots\\nLill\\'s method – Graphical method for the real roots of a polynomial\\nMPSolve – Software for approximating the roots of a polynomial with arbitrarily high precision\\nMultiplicity (mathematics) – Number of times an object must be counted for making true a general formula\\nnth root algorithm\\nSystem of polynomial equations – Root-finding algorithms for common roots of several multivariate polynomials\\nKantorovich theorem – Initial conditions that insure the convergence of Newton\\'s method\\n\\n\\n== References ==', 'In analysis, numerical integration comprises a broad family of algorithms for calculating the numerical value of a definite integral, and by extension, the term is also sometimes used to describe the numerical solution of differential equations. This article focuses on calculation of definite integrals.\\nThe term numerical quadrature (often abbreviated to quadrature) is more or less a synonym for numerical integration, especially as applied to one-dimensional integrals. Some authors refer to numerical integration over more than one dimension as cubature; others take quadrature to include higher-dimensional integration.\\nThe basic problem in numerical integration is to compute an approximate solution to a definite integral\\n\\n  \\n    \\n      \\n        \\n          ∫\\n          \\n            a\\n          \\n          \\n            b\\n          \\n        \\n        f\\n        (\\n        x\\n        )\\n        \\n        d\\n        x\\n      \\n    \\n    {\\\\displaystyle \\\\int _{a}^{b}f(x)\\\\,dx}\\n  to a given degree of accuracy. If f\\u200a(x) is a smooth function integrated over a small number of dimensions, and the domain of integration is bounded, there are many methods for approximating the integral to the desired precision.\\n\\n\\n== Reasons for numerical integration ==\\nThere are several reasons for carrying out numerical integration, as opposed to analytical integration by finding the antiderivative:\\n\\nThe integrand f(x) may be known only at certain points, such as obtained by sampling. Some embedded systems and other computer applications may need numerical integration for this reason.\\nA formula for the integrand may be known, but it may be difficult or impossible to find an antiderivative that is an elementary function. An example of such an integrand is f(x) = exp(−x2), the antiderivative of which (the error function, times a constant) cannot be written in elementary form. \\nIt may be possible to find an antiderivative symbolically, but it may be easier to compute a numerical approximation than to compute the antiderivative. That may be the case if the antiderivative is given as an infinite series or product, or if its evaluation requires a special function that is not available.\\n\\n\\n== History ==\\n\\nThe term \"numerical integration\" first appears in 1915 in the publication A Course in Interpolation and Numeric Integration for the Mathematical Laboratory by David Gibb.Quadrature is a historical mathematical term that means calculating area. Quadrature problems have served as one of the main sources of mathematical analysis. Mathematicians of Ancient Greece, according to the Pythagorean doctrine, understood calculation of area as the process of constructing geometrically a square having the same area (squaring). That is why the process was named quadrature. For example, a quadrature of the circle, Lune of Hippocrates, The Quadrature of the Parabola. This construction must be performed only by means of compass and straightedge.\\nThe ancient Babylonians used the trapezoidal rule to integrate the motion of Jupiter along the ecliptic.\\n\\nFor a quadrature of a rectangle with the sides a and b it is necessary to construct a square with the side \\n  \\n    \\n      \\n        x\\n        =\\n        \\n          \\n            a\\n            b\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle x={\\\\sqrt {ab}}}\\n   (the Geometric mean of a and b). For this purpose it is possible to use the following fact: if we draw the circle with the sum of a and b as the diameter, then the height BH (from a point of their connection to crossing with a circle) equals their geometric mean. The similar geometrical construction solves a problem of a quadrature for a parallelogram and a triangle.\\n\\nProblems of quadrature for curvilinear figures are much more difficult. The quadrature of the circle with compass and straightedge had been proved in the 19th century to be impossible. Nevertheless, for some figures (for example the Lune of Hippocrates) a quadrature can be performed. The quadratures of a sphere surface and a parabola segment done by Archimedes became the highest achievement of the antique analysis.\\n\\nThe area of the surface of a sphere is equal to quadruple the area of a great circle of this sphere.\\nThe area of a segment of the parabola cut from it by a straight line is 4/3 the area of the triangle inscribed in this segment.For the proof of the results Archimedes used the Method of exhaustion of Eudoxus.\\nIn medieval Europe the quadrature meant calculation of area by any method. More often the Method of indivisibles was used; it was less rigorous, but more simple and powerful. With its help Galileo Galilei and Gilles de Roberval found the area of a cycloid arch, Grégoire de Saint-Vincent investigated the area under a hyperbola (Opus Geometricum, 1647), and Alphonse Antonio de Sarasa, de Saint-Vincent\\'s pupil and commentator, noted the relation of this area to logarithms.\\nJohn Wallis algebrised this method: he wrote in his Arithmetica Infinitorum (1656) series that we now call the definite integral, and he calculated their values. Isaac Barrow and James Gregory made further progress: quadratures for some algebraic curves and spirals. Christiaan Huygens successfully performed a quadrature of some Solids of revolution.\\nThe quadrature of the hyperbola by Saint-Vincent and de Sarasa provided a new function, the natural logarithm, of critical importance.\\nWith the invention of integral calculus came a universal method for area calculation. In response, the term quadrature has become traditional, and instead the modern phrase \"computation of a univariate definite integral\" is more common.\\n\\n\\n== Methods for one-dimensional integrals ==\\nNumerical integration methods can generally be described as combining evaluations of the integrand to get an approximation to the integral. The integrand is evaluated at a finite set of points called integration points and a weighted sum of these values is used to approximate the integral. The integration points and weights depend on the specific method used and the accuracy required from the approximation.\\nAn important part of the analysis of any numerical integration method is to study the behavior of the approximation error as a function of the number of integrand evaluations. A method that yields a small error for a small number of evaluations is usually considered superior. Reducing the number of evaluations of the integrand reduces the number of arithmetic operations involved, and therefore reduces the total round-off error. Also, each evaluation takes time, and the integrand may be arbitrarily complicated.\\nA \\'brute force\\' kind of numerical integration can be done, if the integrand is reasonably well-behaved (i.e. piecewise continuous and of bounded variation), by evaluating the integrand with very small increments.\\n\\n\\n=== Quadrature rules based on interpolating functions ===\\nA large class of quadrature rules can be derived by constructing  interpolating functions that are easy to integrate. Typically these interpolating functions are polynomials.  In practice, since polynomials of very high degree tend to oscillate wildly, only polynomials of low degree are used, typically linear and quadratic.\\n\\nThe simplest method of this type is to let the interpolating function be a constant function (a polynomial of degree zero) that passes through the point \\n  \\n    \\n      \\n        \\n          (\\n          \\n            \\n              \\n                \\n                  a\\n                  +\\n                  b\\n                \\n                2\\n              \\n            \\n            ,\\n            f\\n            \\n              (\\n              \\n                \\n                  \\n                    a\\n                    +\\n                    b\\n                  \\n                  2\\n                \\n              \\n              )\\n            \\n          \\n          )\\n        \\n      \\n    \\n    {\\\\textstyle \\\\left({\\\\frac {a+b}{2}},f\\\\left({\\\\frac {a+b}{2}}\\\\right)\\\\right)}\\n  . This is called the midpoint rule or rectangle rule\\n\\n  \\n    \\n      \\n        \\n          ∫\\n          \\n            a\\n          \\n          \\n            b\\n          \\n        \\n        f\\n        (\\n        x\\n        )\\n        \\n        d\\n        x\\n        ≈\\n        (\\n        b\\n        −\\n        a\\n        )\\n        f\\n        \\n          (\\n          \\n            \\n              \\n                a\\n                +\\n                b\\n              \\n              2\\n            \\n          \\n          )\\n        \\n        .\\n      \\n    \\n    {\\\\displaystyle \\\\int _{a}^{b}f(x)\\\\,dx\\\\approx (b-a)f\\\\left({\\\\frac {a+b}{2}}\\\\right).}\\n  \\nThe interpolating function may be a straight line (an affine function, i.e. a polynomial of degree 1)\\npassing through the points \\n  \\n    \\n      \\n        \\n          (\\n          \\n            a\\n            ,\\n            f\\n            (\\n            a\\n            )\\n          \\n          )\\n        \\n      \\n    \\n    {\\\\displaystyle \\\\left(a,f(a)\\\\right)}\\n   and \\n  \\n    \\n      \\n        \\n          (\\n          \\n            b\\n            ,\\n            f\\n            (\\n            b\\n            )\\n          \\n          )\\n        \\n      \\n    \\n    {\\\\displaystyle \\\\left(b,f(b)\\\\right)}\\n  .\\nThis is called the trapezoidal rule\\n\\n  \\n    \\n      \\n        \\n          ∫\\n          \\n            a\\n          \\n          \\n            b\\n          \\n        \\n        f\\n        (\\n        x\\n        )\\n        \\n        d\\n        x\\n        ≈\\n        (\\n        b\\n        −\\n        a\\n        )\\n        \\n          (\\n          \\n            \\n              \\n                f\\n                (\\n                a\\n                )\\n                +\\n                f\\n                (\\n                b\\n                )\\n              \\n              2\\n            \\n          \\n          )\\n        \\n        .\\n      \\n    \\n    {\\\\displaystyle \\\\int _{a}^{b}f(x)\\\\,dx\\\\approx (b-a)\\\\left({\\\\frac {f(a)+f(b)}{2}}\\\\right).}\\n  \\nFor either one of these rules, we can make a more accurate approximation by breaking  up the interval \\n  \\n    \\n      \\n        [\\n        a\\n        ,\\n        b\\n        ]\\n      \\n    \\n    {\\\\displaystyle [a,b]}\\n   into some number \\n  \\n    \\n      \\n        n\\n      \\n    \\n    {\\\\displaystyle n}\\n   of subintervals, computing an approximation for each subinterval, then adding up all the results. This is called a composite rule, extended rule, or iterated rule. For example, the composite trapezoidal rule can be stated as\\n\\n  \\n    \\n      \\n        \\n          ∫\\n          \\n            a\\n          \\n          \\n            b\\n          \\n        \\n        f\\n        (\\n        x\\n        )\\n        \\n        d\\n        x\\n        ≈\\n        \\n          \\n            \\n              b\\n              −\\n              a\\n            \\n            n\\n          \\n        \\n        \\n          (\\n          \\n            \\n              \\n                \\n                  f\\n                  (\\n                  a\\n                  )\\n                \\n                2\\n              \\n            \\n            +\\n            \\n              ∑\\n              \\n                k\\n                =\\n                1\\n              \\n              \\n                n\\n                −\\n                1\\n              \\n            \\n            \\n              (\\n              \\n                f\\n                \\n                  (\\n                  \\n                    a\\n                    +\\n                    k\\n                    \\n                      \\n                        \\n                          b\\n                          −\\n                          a\\n                        \\n                        n\\n                      \\n                    \\n                  \\n                  )\\n                \\n              \\n              )\\n            \\n            +\\n            \\n              \\n                \\n                  f\\n                  (\\n                  b\\n                  )\\n                \\n                2\\n              \\n            \\n          \\n          )\\n        \\n        ,\\n      \\n    \\n    {\\\\displaystyle \\\\int _{a}^{b}f(x)\\\\,dx\\\\approx {\\\\frac {b-a}{n}}\\\\left({f(a) \\\\over 2}+\\\\sum _{k=1}^{n-1}\\\\left(f\\\\left(a+k{\\\\frac {b-a}{n}}\\\\right)\\\\right)+{f(b) \\\\over 2}\\\\right),}\\n  where the subintervals have the form \\n  \\n    \\n      \\n        [\\n        a\\n        +\\n        k\\n        h\\n        ,\\n        a\\n        +\\n        (\\n        k\\n        +\\n        1\\n        )\\n        h\\n        ]\\n        ⊂\\n        [\\n        a\\n        ,\\n        b\\n        ]\\n        ,\\n      \\n    \\n    {\\\\displaystyle [a+kh,a+(k+1)h]\\\\subset [a,b],}\\n   with \\n  \\n    \\n      \\n        h\\n        =\\n        \\n          \\n            \\n              b\\n              −\\n              a\\n            \\n            n\\n          \\n        \\n      \\n    \\n    {\\\\textstyle h={\\\\frac {b-a}{n}}}\\n   and \\n  \\n    \\n      \\n        k\\n        =\\n        0\\n        ,\\n        …\\n        ,\\n        n\\n        −\\n        1.\\n      \\n    \\n    {\\\\displaystyle k=0,\\\\ldots ,n-1.}\\n   Here we used subintervals of the same length \\n  \\n    \\n      \\n        h\\n      \\n    \\n    {\\\\displaystyle h}\\n   but one could also use intervals of varying length \\n  \\n    \\n      \\n        \\n          \\n            (\\n            \\n              h\\n              \\n                k\\n              \\n            \\n            )\\n          \\n          \\n            k\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle \\\\left(h_{k}\\\\right)_{k}}\\n  .\\nInterpolation with polynomials evaluated at equally spaced points in \\n  \\n    \\n      \\n        [\\n        a\\n        ,\\n        b\\n        ]\\n      \\n    \\n    {\\\\displaystyle [a,b]}\\n   yields the Newton–Cotes formulas, of which the rectangle rule and the trapezoidal rule are examples. Simpson\\'s rule, which is based on a polynomial of order 2, is also a Newton–Cotes formula.\\nQuadrature rules with equally spaced points have the very convenient property of nesting.  The corresponding rule with each interval subdivided includes all the current points, so those integrand values can be re-used.\\nIf we allow the intervals between interpolation points to vary, we find another group of quadrature formulas, such as the Gaussian quadrature formulas. A Gaussian  quadrature rule is typically more accurate than a Newton–Cotes rule, which requires the same number of function evaluations, if the integrand is smooth (i.e., if it is sufficiently differentiable). Other quadrature methods with varying intervals include Clenshaw–Curtis quadrature (also called Fejér quadrature) methods, which do nest.\\nGaussian quadrature rules do not nest, but the related Gauss–Kronrod quadrature formulas do.\\n\\n\\n=== Generalized midpoint rule formula ===\\nA generalized midpoint rule formula is given by\\n\\n  \\n    \\n      \\n        \\n          ∫\\n          \\n            0\\n          \\n          \\n            1\\n          \\n        \\n        \\n          f\\n          (\\n          x\\n          )\\n          \\n          d\\n          x\\n        \\n        =\\n        \\n          ∑\\n          \\n            m\\n            =\\n            1\\n          \\n          \\n            M\\n          \\n        \\n        \\n          \\n            ∑\\n            \\n              n\\n              =\\n              0\\n            \\n            \\n              ∞\\n            \\n          \\n          \\n            \\n              \\n                \\n                  \\n                    \\n                      (\\n                      \\n                        −\\n                        1\\n                      \\n                      )\\n                    \\n                    \\n                      n\\n                    \\n                  \\n                  +\\n                  1\\n                \\n                \\n                  \\n                    \\n                      \\n                        (\\n                        \\n                          2\\n                          M\\n                        \\n                        )\\n                      \\n                      \\n                        n\\n                        +\\n                        1\\n                      \\n                    \\n                  \\n                  \\n                    (\\n                    \\n                      n\\n                      +\\n                      1\\n                    \\n                    )\\n                  \\n                  !\\n                \\n              \\n            \\n            \\n              \\n                \\n                  \\n                    \\n                    \\n                      \\n                        f\\n                        \\n                          (\\n                          n\\n                          )\\n                        \\n                      \\n                      (\\n                      x\\n                      )\\n                    \\n                    |\\n                  \\n                \\n                \\n                  x\\n                  =\\n                  \\n                    \\n                      \\n                        m\\n                        −\\n                        1\\n                        \\n                          /\\n                        \\n                        2\\n                      \\n                      M\\n                    \\n                  \\n                \\n              \\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle \\\\int _{0}^{1}{f(x)\\\\,dx}=\\\\sum _{m=1}^{M}{\\\\sum _{n=0}^{\\\\infty }{{\\\\frac {\\\\left({-1}\\\\right)^{n}+1}{{\\\\left(2M\\\\right)^{n+1}}\\\\left({n+1}\\\\right)!}}{{\\\\left.f^{(n)}(x)\\\\right|}_{x={\\\\frac {m-1/2}{M}}}}}}}\\n  or\\n\\n  \\n    \\n      \\n        \\n          ∫\\n          \\n            0\\n          \\n          \\n            1\\n          \\n        \\n        \\n          f\\n          (\\n          x\\n          )\\n          \\n          d\\n          x\\n        \\n        =\\n        \\n          lim\\n          \\n            M\\n            →\\n            ∞\\n          \\n        \\n        \\n          ∑\\n          \\n            m\\n            =\\n            1\\n          \\n          \\n            M\\n          \\n        \\n        \\n          \\n            ∑\\n            \\n              n\\n              =\\n              0\\n            \\n            \\n              N\\n            \\n          \\n          \\n            \\n              \\n                \\n                  \\n                    \\n                      (\\n                      \\n                        −\\n                        1\\n                      \\n                      )\\n                    \\n                    \\n                      n\\n                    \\n                  \\n                  +\\n                  1\\n                \\n                \\n                  \\n                    \\n                      \\n                        \\n                          (\\n                          \\n                            2\\n                            M\\n                          \\n                          )\\n                        \\n                      \\n                      \\n                        n\\n                        +\\n                        1\\n                      \\n                    \\n                  \\n                  \\n                    (\\n                    \\n                      n\\n                      +\\n                      1\\n                    \\n                    )\\n                  \\n                  !\\n                \\n              \\n            \\n            \\n              \\n                \\n                  \\n                  \\n                    \\n                      f\\n                      \\n                        (\\n                        n\\n                        )\\n                      \\n                    \\n                    (\\n                    x\\n                    )\\n                  \\n                  |\\n                \\n                \\n                  x\\n                  =\\n                  \\n                    \\n                      \\n                        m\\n                        −\\n                        1\\n                        \\n                          /\\n                        \\n                        2\\n                      \\n                      M\\n                    \\n                  \\n                \\n              \\n            \\n          \\n        \\n        ,\\n      \\n    \\n    {\\\\displaystyle \\\\int _{0}^{1}{f(x)\\\\,dx}=\\\\lim _{M\\\\to \\\\infty }\\\\sum _{m=1}^{M}{\\\\sum _{n=0}^{N}{{\\\\frac {\\\\left({-1}\\\\right)^{n}+1}{{{\\\\left({2M}\\\\right)}^{n+1}}\\\\left({n+1}\\\\right)!}}{\\\\left.f^{(n)}(x)\\\\right|_{x={\\\\frac {m-1/2}{M}}}}}},}\\n  where \\n  \\n    \\n      \\n        \\n          f\\n          \\n            (\\n            n\\n            )\\n          \\n        \\n        (\\n        x\\n        )\\n      \\n    \\n    {\\\\displaystyle f^{(n)}(x)}\\n   denotes \\n  \\n    \\n      \\n        n\\n      \\n    \\n    {\\\\displaystyle n}\\n  -th derivative. For example, substituting \\n  \\n    \\n      \\n        M\\n        =\\n        1\\n      \\n    \\n    {\\\\displaystyle M=1}\\n   and\\n\\n  \\n    \\n      \\n        f\\n        (\\n        x\\n        )\\n        =\\n        \\n          \\n            θ\\n            \\n              1\\n              +\\n              \\n                θ\\n                \\n                  2\\n                \\n              \\n              \\n                x\\n                \\n                  2\\n                \\n              \\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle f(x)={\\\\frac {\\\\theta }{1+\\\\theta ^{2}x^{2}}}}\\n  in the generalized midpoint rule formula, we obtain an equation of the inverse tangent\\n\\n  \\n    \\n      \\n        \\n          tan\\n          \\n            −\\n            1\\n          \\n        \\n        \\u2061\\n        (\\n        θ\\n        )\\n        =\\n        i\\n        \\n          ∑\\n          \\n            n\\n            =\\n            1\\n          \\n          \\n            ∞\\n          \\n        \\n        \\n          \\n            1\\n            \\n              2\\n              n\\n              −\\n              1\\n            \\n          \\n        \\n        \\n          (\\n          \\n            \\n              \\n                1\\n                \\n                  \\n                    (\\n                    \\n                      1\\n                      +\\n                      2\\n                      i\\n                      \\n                        /\\n                      \\n                      θ\\n                    \\n                    )\\n                  \\n                  \\n                    2\\n                    n\\n                    −\\n                    1\\n                  \\n                \\n              \\n            \\n            −\\n            \\n              \\n                1\\n                \\n                  \\n                    (\\n                    \\n                      1\\n                      −\\n                      2\\n                      i\\n                      \\n                        /\\n                      \\n                      θ\\n                    \\n                    )\\n                  \\n                  \\n                    2\\n                    n\\n                    −\\n                    1\\n                  \\n                \\n              \\n            \\n          \\n          )\\n        \\n        =\\n        2\\n        \\n          ∑\\n          \\n            n\\n            =\\n            1\\n          \\n          \\n            ∞\\n          \\n        \\n        \\n          \\n            \\n              1\\n              \\n                2\\n                n\\n                −\\n                1\\n              \\n            \\n          \\n          \\n            \\n              \\n                \\n                  a\\n                  \\n                    n\\n                  \\n                \\n                \\n                  (\\n                  θ\\n                  )\\n                \\n              \\n              \\n                \\n                  a\\n                  \\n                    n\\n                  \\n                  \\n                    2\\n                  \\n                \\n                \\n                  (\\n                  θ\\n                  )\\n                \\n                +\\n                \\n                  b\\n                  \\n                    n\\n                  \\n                  \\n                    2\\n                  \\n                \\n                \\n                  (\\n                  θ\\n                  )\\n                \\n              \\n            \\n          \\n        \\n        ,\\n      \\n    \\n    {\\\\displaystyle \\\\tan ^{-1}(\\\\theta )=i\\\\sum _{n=1}^{\\\\infty }{\\\\frac {1}{2n-1}}\\\\left({\\\\frac {1}{\\\\left(1+2i/\\\\theta \\\\right)^{2n-1}}}-{\\\\frac {1}{\\\\left(1-2i/\\\\theta \\\\right)^{2n-1}}}\\\\right)=2\\\\sum _{n=1}^{\\\\infty }{{\\\\frac {1}{2n-1}}{\\\\frac {a_{n}\\\\left(\\\\theta \\\\right)}{a_{n}^{2}\\\\left(\\\\theta \\\\right)+b_{n}^{2}\\\\left(\\\\theta \\\\right)}}},}\\n  where \\n  \\n    \\n      \\n        i\\n        =\\n        \\n          \\n            −\\n            1\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle i={\\\\sqrt {-1}}}\\n   is imaginary unit and\\n\\n  \\n    \\n      \\n        \\n          \\n            \\n              \\n                \\n                  a\\n                  \\n                    1\\n                  \\n                \\n                (\\n                θ\\n                )\\n              \\n              \\n                \\n                =\\n                \\n                  \\n                    2\\n                    θ\\n                  \\n                \\n                ,\\n              \\n            \\n            \\n              \\n                \\n                  b\\n                  \\n                    1\\n                  \\n                \\n                (\\n                θ\\n                )\\n              \\n              \\n                \\n                =\\n                1\\n                ,\\n              \\n            \\n            \\n              \\n                \\n                  a\\n                  \\n                    n\\n                  \\n                \\n                (\\n                θ\\n                )\\n              \\n              \\n                \\n                =\\n                \\n                  (\\n                  \\n                    1\\n                    −\\n                    \\n                      \\n                        4\\n                        \\n                          θ\\n                          \\n                            2\\n                          \\n                        \\n                      \\n                    \\n                  \\n                  )\\n                \\n                \\n                \\n                  a\\n                  \\n                    n\\n                    −\\n                    1\\n                  \\n                \\n                (\\n                θ\\n                )\\n                +\\n                \\n                  \\n                    4\\n                    θ\\n                  \\n                \\n                \\n                \\n                  b\\n                  \\n                    n\\n                    −\\n                    1\\n                  \\n                \\n                (\\n                θ\\n                )\\n                ,\\n              \\n            \\n            \\n              \\n                \\n                  b\\n                  \\n                    n\\n                  \\n                \\n                (\\n                θ\\n                )\\n              \\n              \\n                \\n                =\\n                \\n                  (\\n                  \\n                    1\\n                    −\\n                    \\n                      \\n                        4\\n                        \\n                          θ\\n                          \\n                            2\\n                          \\n                        \\n                      \\n                    \\n                  \\n                  )\\n                \\n                \\n                \\n                  b\\n                  \\n                    n\\n                    −\\n                    1\\n                  \\n                \\n                (\\n                θ\\n                )\\n                −\\n                \\n                  \\n                    4\\n                    θ\\n                  \\n                \\n                \\n                \\n                  a\\n                  \\n                    n\\n                    −\\n                    1\\n                  \\n                \\n                (\\n                θ\\n                )\\n                .\\n              \\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\begin{aligned}a_{1}(\\\\theta )&={\\\\frac {2}{\\\\theta }},\\\\\\\\b_{1}(\\\\theta )&=1,\\\\\\\\a_{n}(\\\\theta )&=\\\\left(1-{\\\\frac {4}{\\\\theta ^{2}}}\\\\right)\\\\,a_{n-1}(\\\\theta )+{\\\\frac {4}{\\\\theta }}\\\\,b_{n-1}(\\\\theta ),\\\\\\\\b_{n}(\\\\theta )&=\\\\left(1-{\\\\frac {4}{\\\\theta ^{2}}}\\\\right)\\\\,b_{n-1}(\\\\theta )-{\\\\frac {4}{\\\\theta }}\\\\,a_{n-1}(\\\\theta ).\\\\end{aligned}}}\\n  Since at each odd \\n  \\n    \\n      \\n        n\\n      \\n    \\n    {\\\\displaystyle n}\\n   the numerator of the integrand becomes \\n  \\n    \\n      \\n        (\\n        −\\n        1\\n        \\n          )\\n          \\n            n\\n          \\n        \\n        +\\n        1\\n        =\\n        0\\n      \\n    \\n    {\\\\displaystyle (-1)^{n}+1=0}\\n  , the generalized midpoint rule formula can be reorganized as\\n\\n  \\n    \\n      \\n        \\n          ∫\\n          \\n            0\\n          \\n          \\n            1\\n          \\n        \\n        \\n          f\\n          (\\n          x\\n          )\\n          \\n          d\\n          x\\n        \\n        =\\n        2\\n        \\n          ∑\\n          \\n            m\\n            =\\n            1\\n          \\n          \\n            M\\n          \\n        \\n        \\n          \\n            ∑\\n            \\n              n\\n              =\\n              0\\n            \\n            \\n              ∞\\n            \\n          \\n          \\n            \\n              \\n                1\\n                \\n                  \\n                    \\n                      \\n                        (\\n                        \\n                          2\\n                          M\\n                        \\n                        )\\n                      \\n                      \\n                        2\\n                        n\\n                        +\\n                        1\\n                      \\n                    \\n                  \\n                  \\n                    (\\n                    \\n                      2\\n                      n\\n                      +\\n                      1\\n                    \\n                    )\\n                  \\n                  !\\n                \\n              \\n            \\n            \\n              \\n                \\n                  \\n                    \\n                    \\n                      \\n                        f\\n                        \\n                          (\\n                          2\\n                          n\\n                          )\\n                        \\n                      \\n                      (\\n                      x\\n                      )\\n                    \\n                    |\\n                  \\n                \\n                \\n                  x\\n                  =\\n                  \\n                    \\n                      \\n                        m\\n                        −\\n                        1\\n                        \\n                          /\\n                        \\n                        2\\n                      \\n                      M\\n                    \\n                  \\n                \\n              \\n            \\n          \\n        \\n        \\n        \\n        .\\n      \\n    \\n    {\\\\displaystyle \\\\int _{0}^{1}{f(x)\\\\,dx}=2\\\\sum _{m=1}^{M}{\\\\sum _{n=0}^{\\\\infty }{{\\\\frac {1}{{\\\\left(2M\\\\right)^{2n+1}}\\\\left({2n+1}\\\\right)!}}{{\\\\left.f^{(2n)}(x)\\\\right|}_{x={\\\\frac {m-1/2}{M}}}}}}\\\\,\\\\,.}\\n  The following example of Mathematica code generates the plot showing difference between inverse tangent and its approximation truncated at \\n  \\n    \\n      \\n        M\\n        =\\n        5\\n      \\n    \\n    {\\\\displaystyle M=5}\\n   and \\n  \\n    \\n      \\n        N\\n        =\\n        10\\n      \\n    \\n    {\\\\displaystyle N=10}\\n  :\\n\\nFor a function \\n  \\n    \\n      \\n        g\\n        (\\n        t\\n        )\\n      \\n    \\n    {\\\\displaystyle g(t)}\\n   defined over interval \\n  \\n    \\n      \\n        (\\n        a\\n        ,\\n        b\\n        )\\n      \\n    \\n    {\\\\displaystyle (a,b)}\\n  , its integral is\\n\\n  \\n    \\n      \\n        \\n          ∫\\n          \\n            a\\n          \\n          \\n            b\\n          \\n        \\n        \\n          g\\n          (\\n          t\\n          )\\n          \\n          d\\n          t\\n        \\n        =\\n        \\n          ∫\\n          \\n            0\\n          \\n          \\n            b\\n            −\\n            a\\n          \\n        \\n        \\n          g\\n          (\\n          τ\\n          +\\n          a\\n          )\\n          \\n          d\\n          τ\\n        \\n        =\\n        (\\n        b\\n        −\\n        a\\n        )\\n        \\n          ∫\\n          \\n            0\\n          \\n          \\n            1\\n          \\n        \\n        \\n          g\\n          (\\n          (\\n          b\\n          −\\n          a\\n          )\\n          x\\n          +\\n          a\\n          )\\n          \\n          d\\n          x\\n        \\n        .\\n      \\n    \\n    {\\\\displaystyle \\\\int _{a}^{b}{g(t)\\\\,dt}=\\\\int _{0}^{b-a}{g(\\\\tau +a)\\\\,d\\\\tau }=(b-a)\\\\int _{0}^{1}{g((b-a)x+a)\\\\,dx}.}\\n  Therefore, we can apply the generalized midpoint integration formula above by assuming that \\n  \\n    \\n      \\n        f\\n        (\\n        x\\n        )\\n        =\\n        (\\n        b\\n        −\\n        a\\n        )\\n        \\n        g\\n        (\\n        (\\n        b\\n        −\\n        a\\n        )\\n        x\\n        +\\n        a\\n        )\\n      \\n    \\n    {\\\\displaystyle f(x)=(b-a)\\\\,g((b-a)x+a)}\\n  .\\n\\n\\n=== Adaptive algorithms ===\\n\\nIf f(x) does not have many derivatives at all points, or if the derivatives become large, then Gaussian quadrature is often insufficient. In this case, an algorithm similar to the following will perform better:\\n\\nSome details of the algorithm require careful thought. For many cases, estimating the error from quadrature over an interval for a function f(x) isn\\'t obvious. One popular solution is to use two different rules of quadrature, and use their difference as an estimate of the error from quadrature. The other problem is deciding what \"too large\" or \"very small\" signify. A local criterion for \"too large\" is that the quadrature error should not be larger than t ⋅ h where t, a real number, is the tolerance we wish to set for global error. Then again, if h is already tiny, it may not be worthwhile to make it even smaller even if the quadrature error is apparently large. A global criterion is that the sum of errors on all the intervals should be less than t.  This type of error analysis is usually called \"a posteriori\" since we compute the error after having computed the approximation.\\nHeuristics for adaptive quadrature are discussed by Forsythe et al. (Section 5.4).\\n\\n\\n=== Extrapolation methods ===\\nThe accuracy of a quadrature rule of the Newton–Cotes type is generally a function of the number of evaluation points. The result is usually more accurate as the number of evaluation points increases, or, equivalently, as the width of the step size between the points decreases. It is natural to ask what the result would be if the step size were allowed to approach zero. This can be answered by extrapolating the result from two or more nonzero step sizes, using series acceleration methods such as Richardson extrapolation. The extrapolation function may be a polynomial or rational function. Extrapolation methods are described in more detail by Stoer and Bulirsch (Section 3.4) and are implemented in many of the routines in the QUADPACK library.\\n\\n\\n=== Conservative (a priori) error estimation ===\\nLet \\n  \\n    \\n      \\n        f\\n      \\n    \\n    {\\\\displaystyle f}\\n   have a bounded first derivative over \\n  \\n    \\n      \\n        [\\n        a\\n        ,\\n        b\\n        ]\\n        ,\\n      \\n    \\n    {\\\\displaystyle [a,b],}\\n   i.e. \\n  \\n    \\n      \\n        f\\n        ∈\\n        \\n          C\\n          \\n            1\\n          \\n        \\n        (\\n        [\\n        a\\n        ,\\n        b\\n        ]\\n        )\\n        .\\n      \\n    \\n    {\\\\displaystyle f\\\\in C^{1}([a,b]).}\\n   The mean value theorem for \\n  \\n    \\n      \\n        f\\n        ,\\n      \\n    \\n    {\\\\displaystyle f,}\\n   where \\n  \\n    \\n      \\n        x\\n        ∈\\n        [\\n        a\\n        ,\\n        b\\n        )\\n        ,\\n      \\n    \\n    {\\\\displaystyle x\\\\in [a,b),}\\n   gives\\n\\n  \\n    \\n      \\n        (\\n        x\\n        −\\n        a\\n        )\\n        \\n          f\\n          ′\\n        \\n        (\\n        \\n          ξ\\n          \\n            x\\n          \\n        \\n        )\\n        =\\n        f\\n        (\\n        x\\n        )\\n        −\\n        f\\n        (\\n        a\\n        )\\n        ,\\n      \\n    \\n    {\\\\displaystyle (x-a)f\\'(\\\\xi _{x})=f(x)-f(a),}\\n  for some \\n  \\n    \\n      \\n        \\n          ξ\\n          \\n            x\\n          \\n        \\n        ∈\\n        (\\n        a\\n        ,\\n        x\\n        ]\\n      \\n    \\n    {\\\\displaystyle \\\\xi _{x}\\\\in (a,x]}\\n   depending on \\n  \\n    \\n      \\n        x\\n      \\n    \\n    {\\\\displaystyle x}\\n  .\\nIf we integrate in \\n  \\n    \\n      \\n        x\\n      \\n    \\n    {\\\\displaystyle x}\\n   from \\n  \\n    \\n      \\n        a\\n      \\n    \\n    {\\\\displaystyle a}\\n   to \\n  \\n    \\n      \\n        b\\n      \\n    \\n    {\\\\displaystyle b}\\n   on both sides and take the absolute values, we obtain\\n\\n  \\n    \\n      \\n        \\n          |\\n          \\n            \\n              ∫\\n              \\n                a\\n              \\n              \\n                b\\n              \\n            \\n            f\\n            (\\n            x\\n            )\\n            \\n            d\\n            x\\n            −\\n            (\\n            b\\n            −\\n            a\\n            )\\n            f\\n            (\\n            a\\n            )\\n          \\n          |\\n        \\n        =\\n        \\n          |\\n          \\n            \\n              ∫\\n              \\n                a\\n              \\n              \\n                b\\n              \\n            \\n            (\\n            x\\n            −\\n            a\\n            )\\n            \\n              f\\n              ′\\n            \\n            (\\n            \\n              ξ\\n              \\n                x\\n              \\n            \\n            )\\n            \\n            d\\n            x\\n          \\n          |\\n        \\n        .\\n      \\n    \\n    {\\\\displaystyle \\\\left|\\\\int _{a}^{b}f(x)\\\\,dx-(b-a)f(a)\\\\right|=\\\\left|\\\\int _{a}^{b}(x-a)f\\'(\\\\xi _{x})\\\\,dx\\\\right|.}\\n  We can further approximate the integral on the right-hand side by bringing the absolute value into the integrand, and replacing the term in \\n  \\n    \\n      \\n        \\n          f\\n          ′\\n        \\n      \\n    \\n    {\\\\displaystyle f\\'}\\n   by an upper bound\\n\\nwhere the supremum was used to approximate.\\nHence, if we approximate the integral \\n  \\n    \\n      \\n        \\n          ∫\\n          \\n            a\\n          \\n          \\n            b\\n          \\n        \\n        f\\n        (\\n        x\\n        )\\n        \\n        d\\n        x\\n      \\n    \\n    {\\\\textstyle \\\\int _{a}^{b}f(x)\\\\,dx}\\n   by the quadrature rule \\n  \\n    \\n      \\n        (\\n        b\\n        −\\n        a\\n        )\\n        f\\n        (\\n        a\\n        )\\n      \\n    \\n    {\\\\displaystyle (b-a)f(a)}\\n   our error is no greater than the right hand side of 1. We can convert this into an error analysis for the Riemann sum, giving an upper bound of\\n\\n  \\n    \\n      \\n        \\n          \\n            \\n              n\\n              \\n                −\\n                1\\n              \\n            \\n            2\\n          \\n        \\n        \\n          sup\\n          \\n            0\\n            ≤\\n            x\\n            ≤\\n            1\\n          \\n        \\n        \\n          |\\n          \\n            \\n              f\\n              ′\\n            \\n            (\\n            x\\n            )\\n          \\n          |\\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\frac {n^{-1}}{2}}\\\\sup _{0\\\\leq x\\\\leq 1}\\\\left|f\\'(x)\\\\right|}\\n  for the error term of that particular approximation. (Note that this is precisely the error we calculated for the example \\n  \\n    \\n      \\n        f\\n        (\\n        x\\n        )\\n        =\\n        x\\n      \\n    \\n    {\\\\displaystyle f(x)=x}\\n  .) Using more derivatives, and by tweaking the quadrature, we can do a similar error analysis using a Taylor series (using a partial sum with remainder term) for f. This error analysis gives a strict upper bound on the error, if the derivatives of f are available.\\nThis integration method can be combined with interval arithmetic to produce computer proofs and verified calculations.\\n\\n\\n=== Integrals over infinite intervals ===\\nSeveral methods exist for approximate integration over unbounded intervals. The standard technique involves specially derived quadrature rules, such as Gauss-Hermite quadrature for integrals on the whole real line and Gauss-Laguerre quadrature for integrals on the positive reals. Monte Carlo methods can also be used, or a change of variables to a finite interval; e.g., for the whole line one could use\\n\\n  \\n    \\n      \\n        \\n          ∫\\n          \\n            −\\n            ∞\\n          \\n          \\n            ∞\\n          \\n        \\n        f\\n        (\\n        x\\n        )\\n        \\n        d\\n        x\\n        =\\n        \\n          ∫\\n          \\n            −\\n            1\\n          \\n          \\n            +\\n            1\\n          \\n        \\n        f\\n        \\n          (\\n          \\n            \\n              t\\n              \\n                1\\n                −\\n                \\n                  t\\n                  \\n                    2\\n                  \\n                \\n              \\n            \\n          \\n          )\\n        \\n        \\n          \\n            \\n              1\\n              +\\n              \\n                t\\n                \\n                  2\\n                \\n              \\n            \\n            \\n              \\n                (\\n                \\n                  1\\n                  −\\n                  \\n                    t\\n                    \\n                      2\\n                    \\n                  \\n                \\n                )\\n              \\n              \\n                2\\n              \\n            \\n          \\n        \\n        \\n        d\\n        t\\n        ,\\n      \\n    \\n    {\\\\displaystyle \\\\int _{-\\\\infty }^{\\\\infty }f(x)\\\\,dx=\\\\int _{-1}^{+1}f\\\\left({\\\\frac {t}{1-t^{2}}}\\\\right){\\\\frac {1+t^{2}}{\\\\left(1-t^{2}\\\\right)^{2}}}\\\\,dt,}\\n  and for semi-infinite intervals one could use\\n\\n  \\n    \\n      \\n        \\n          \\n            \\n              \\n                \\n                  ∫\\n                  \\n                    a\\n                  \\n                  \\n                    ∞\\n                  \\n                \\n                f\\n                (\\n                x\\n                )\\n                \\n                d\\n                x\\n              \\n              \\n                \\n                =\\n                \\n                  ∫\\n                  \\n                    0\\n                  \\n                  \\n                    1\\n                  \\n                \\n                f\\n                \\n                  (\\n                  \\n                    a\\n                    +\\n                    \\n                      \\n                        t\\n                        \\n                          1\\n                          −\\n                          t\\n                        \\n                      \\n                    \\n                  \\n                  )\\n                \\n                \\n                  \\n                    \\n                      d\\n                      t\\n                    \\n                    \\n                      (\\n                      1\\n                      −\\n                      t\\n                      \\n                        )\\n                        \\n                          2\\n                        \\n                      \\n                    \\n                  \\n                \\n                ,\\n              \\n            \\n            \\n              \\n                \\n                  ∫\\n                  \\n                    −\\n                    ∞\\n                  \\n                  \\n                    a\\n                  \\n                \\n                f\\n                (\\n                x\\n                )\\n                \\n                d\\n                x\\n              \\n              \\n                \\n                =\\n                \\n                  ∫\\n                  \\n                    0\\n                  \\n                  \\n                    1\\n                  \\n                \\n                f\\n                \\n                  (\\n                  \\n                    a\\n                    −\\n                    \\n                      \\n                        \\n                          1\\n                          −\\n                          t\\n                        \\n                        t\\n                      \\n                    \\n                  \\n                  )\\n                \\n                \\n                  \\n                    \\n                      d\\n                      t\\n                    \\n                    \\n                      t\\n                      \\n                        2\\n                      \\n                    \\n                  \\n                \\n                ,\\n              \\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\begin{aligned}\\\\int _{a}^{\\\\infty }f(x)\\\\,dx&=\\\\int _{0}^{1}f\\\\left(a+{\\\\frac {t}{1-t}}\\\\right){\\\\frac {dt}{(1-t)^{2}}},\\\\\\\\\\\\int _{-\\\\infty }^{a}f(x)\\\\,dx&=\\\\int _{0}^{1}f\\\\left(a-{\\\\frac {1-t}{t}}\\\\right){\\\\frac {dt}{t^{2}}},\\\\end{aligned}}}\\n  as possible transformations.\\n\\n\\n== Multidimensional integrals ==\\nThe quadrature rules discussed so far are all designed to compute one-dimensional integrals. To compute integrals in multiple dimensions, one approach is to phrase the multiple integral as repeated one-dimensional integrals by applying Fubini\\'s theorem (the tensor product rule). This approach requires the function evaluations to grow exponentially as the number of dimensions increases. Three methods are known to overcome this so-called curse of dimensionality.\\nA great many additional techniques for forming multidimensional cubature integration rules for a variety of weighting functions are given in the monograph by Stroud.\\nIntegration on the sphere has been reviewed by Hesse et al. (2015).\\n\\n\\n=== Monte Carlo ===\\n\\nMonte Carlo methods and quasi-Monte Carlo methods are easy to apply to multi-dimensional integrals. They may yield greater accuracy for the same number of function evaluations than repeated integrations using one-dimensional methods.A large class of useful Monte Carlo methods are the so-called Markov chain Monte Carlo algorithms, which include the Metropolis–Hastings algorithm and Gibbs sampling.\\n\\n\\n=== Sparse grids ===\\nSparse grids were originally developed by Smolyak for the quadrature of high-dimensional functions. The method is always based on a one-dimensional quadrature rule, but performs a more sophisticated combination of univariate results. However, whereas the tensor product rule guarantees that the weights of all of the cubature points will be positive if the weights of the quadrature points were positive, Smolyak\\'s rule does not guarantee that the  weights will all be positive.\\n\\n\\n=== Bayesian Quadrature ===\\nBayesian quadrature is a statistical approach to the numerical problem of computing integrals and falls under the field of probabilistic numerics. It can provide a full handling of the uncertainty over the solution of the integral expressed as a Gaussian process posterior variance.\\n\\n\\n== Connection with differential equations ==\\nThe problem of evaluating the integral\\n\\n  \\n    \\n      \\n        F\\n        (\\n        x\\n        )\\n        =\\n        \\n          ∫\\n          \\n            a\\n          \\n          \\n            x\\n          \\n        \\n        f\\n        (\\n        u\\n        )\\n        \\n        d\\n        u\\n      \\n    \\n    {\\\\displaystyle F(x)=\\\\int _{a}^{x}f(u)\\\\,du}\\n  can be reduced to an initial value problem for an ordinary differential equation by applying the first part of the fundamental theorem of calculus. By differentiating both sides of the above with respect to the argument x, it is seen that the function F satisfies\\n\\n  \\n    \\n      \\n        \\n          \\n            \\n              d\\n              F\\n              (\\n              x\\n              )\\n            \\n            \\n              d\\n              x\\n            \\n          \\n        \\n        =\\n        f\\n        (\\n        x\\n        )\\n        ,\\n        \\n        F\\n        (\\n        a\\n        )\\n        =\\n        0.\\n      \\n    \\n    {\\\\displaystyle {\\\\frac {dF(x)}{dx}}=f(x),\\\\quad F(a)=0.}\\n  Methods developed for ordinary differential equations, such as Runge–Kutta methods, can be applied to the restated problem and thus be used to evaluate the integral. For instance, the standard fourth-order Runge–Kutta method applied to the differential equation yields Simpson\\'s rule from above.\\nThe differential equation \\n  \\n    \\n      \\n        \\n          F\\n          ′\\n        \\n        (\\n        x\\n        )\\n        =\\n        f\\n        (\\n        x\\n        )\\n      \\n    \\n    {\\\\displaystyle F\\'(x)=f(x)}\\n   has a special form: the right-hand side contains only the independent variable (here \\n  \\n    \\n      \\n        x\\n      \\n    \\n    {\\\\displaystyle x}\\n  ) and not the dependent variable (here \\n  \\n    \\n      \\n        F\\n      \\n    \\n    {\\\\displaystyle F}\\n  ). This simplifies the theory and algorithms considerably. The problem of evaluating integrals is thus best studied in its own right.\\n\\n\\n== See also ==\\nNumerical methods for ordinary differential equations\\nTruncation error (numerical integration)\\nClenshaw–Curtis quadrature\\nGauss-Kronrod quadrature\\nRiemann Sum or Riemann Integral\\nTrapezoidal rule\\nRomberg\\'s method\\nTanh-sinh quadrature\\nNonelementary Integral\\n\\n\\n== References ==\\n\\nPhilip J. Davis and Philip Rabinowitz, Methods of Numerical Integration.\\nGeorge E. Forsythe, Michael A. Malcolm, and Cleve B. Moler, Computer Methods for Mathematical Computations. Englewood Cliffs, NJ: Prentice-Hall, 1977. (See Chapter  5.)\\nPress, W.H.; Teukolsky, S.A.; Vetterling, W.T.; Flannery, B.P. (2007), \"Chapter 4. Integration of Functions\", Numerical Recipes: The Art of Scientific Computing (3rd ed.), New York: Cambridge University Press, ISBN 978-0-521-88068-8\\nJosef Stoer and Roland Bulirsch, Introduction to Numerical Analysis. New York: Springer-Verlag, 1980. (See Chapter 3.)\\nBoyer, C. B., A History of Mathematics, 2nd ed. rev. by Uta C. Merzbach, New York: Wiley, 1989 ISBN 0-471-09763-2 (1991 pbk ed. ISBN 0-471-54397-7).\\nEves, Howard, An Introduction to the History of Mathematics, Saunders, 1990, ISBN 0-03-029558-0,\\n\\n\\n== External links ==\\nIntegration: Background, Simulations, etc. at Holistic Numerical Methods Institute\\nLobatto Quadrature from Wolfram Mathworld\\nLobatto quadrature formula from Encyclopedia of Mathematics\\nImplementations of many quadrature and cubature formulae within the free Tracker Component Library.', 'Numerical methods for ordinary differential equations are methods used to find numerical approximations to the solutions of ordinary differential equations (ODEs). Their use is also known as \"numerical integration\", although this term can also refer to the computation of integrals.\\nMany differential equations cannot be solved using symbolic computation (\"analysis\"). For practical purposes, however – such as in engineering – a numeric approximation to the solution is often sufficient. The algorithms studied here can be used to compute such an approximation. An alternative method is to use techniques from calculus to obtain a series expansion of the solution.\\nOrdinary differential equations occur in many scientific disciplines, including physics, chemistry, biology, and economics. In addition, some methods in numerical partial differential equations convert the partial differential equation into an ordinary differential equation, which must then be solved.\\n\\n\\n== The problem ==\\nA first-order differential equation is an Initial value problem (IVP) of the form,\\n\\nwhere \\n  \\n    \\n      \\n        f\\n      \\n    \\n    {\\\\displaystyle f}\\n   is a function \\n  \\n    \\n      \\n        f\\n        :\\n        [\\n        \\n          t\\n          \\n            0\\n          \\n        \\n        ,\\n        ∞\\n        )\\n        ×\\n        \\n          \\n            R\\n          \\n          \\n            d\\n          \\n        \\n        →\\n        \\n          \\n            R\\n          \\n          \\n            d\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle f:[t_{0},\\\\infty )\\\\times \\\\mathbb {R} ^{d}\\\\to \\\\mathbb {R} ^{d}}\\n  , and the initial condition \\n  \\n    \\n      \\n        \\n          y\\n          \\n            0\\n          \\n        \\n        ∈\\n        \\n          \\n            R\\n          \\n          \\n            d\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle y_{0}\\\\in \\\\mathbb {R} ^{d}}\\n   is a given vector.  First-order means that only the first derivative of y appears in the equation, and higher derivatives are absent.\\nWithout loss of generality to higher-order systems, we restrict ourselves to first-order differential equations, because a higher-order ODE can be converted into a larger system of first-order equations by introducing extra variables.  For example, the second-order equation y′′ = −y can be rewritten as two first-order equations: y′ = z and z′ = −y.\\nIn this section, we describe numerical methods for IVPs, and remark that boundary value problems (BVPs) require a different set of tools.  In a BVP, one defines values, or components of the solution y at more than one point.  Because of this, different methods need to be used to solve BVPs.  For example, the shooting method (and its variants) or global methods like finite differences, Galerkin methods, or collocation methods are appropriate for that class of problems.\\nThe Picard–Lindelöf theorem states that there is a unique solution, provided f is Lipschitz-continuous.\\n\\n\\n== Methods ==\\nNumerical methods for solving first-order IVPs often fall into one of two large categories: linear multistep methods, or Runge–Kutta methods. A further division can be realized by dividing methods into those that are explicit and those that are implicit. For example,  implicit linear multistep methods include Adams-Moulton methods, and backward differentiation methods (BDF), whereas implicit Runge–Kutta methods include diagonally implicit Runge–Kutta (DIRK), singly diagonally implicit Runge–Kutta (SDIRK), and Gauss–Radau (based on Gaussian quadrature) numerical methods. Explicit examples from the linear multistep family include the Adams–Bashforth methods, and any Runge–Kutta method with a lower diagonal Butcher tableau is explicit. A loose rule of thumb dictates that stiff differential equations require the use of implicit schemes, whereas non-stiff problems can be solved more efficiently with explicit schemes.\\nThe so-called general linear methods (GLMs) are a generalization of the above two large classes of methods.\\n\\n\\n=== Euler method ===\\n\\nFrom any point on a curve, you can find an approximation of a nearby point on the curve by moving a short distance along a line tangent to the curve.\\nStarting with the differential equation (1), we replace the derivative y′ by the finite difference approximation\\n\\nwhich when re-arranged yields the following formula\\n\\n  \\n    \\n      \\n        y\\n        (\\n        t\\n        +\\n        h\\n        )\\n        ≈\\n        y\\n        (\\n        t\\n        )\\n        +\\n        h\\n        \\n          y\\n          ′\\n        \\n        (\\n        t\\n        )\\n      \\n    \\n    {\\\\displaystyle y(t+h)\\\\approx y(t)+hy\\'(t)}\\n  and using (1) gives:\\n\\nThis formula is usually applied in the following way. We choose a step size h, and we construct the sequence \\n  \\n    \\n      \\n        \\n          t\\n          \\n            0\\n          \\n        \\n        ,\\n        \\n          t\\n          \\n            1\\n          \\n        \\n        =\\n        \\n          t\\n          \\n            0\\n          \\n        \\n        +\\n        h\\n        ,\\n        \\n          t\\n          \\n            2\\n          \\n        \\n        =\\n        \\n          t\\n          \\n            0\\n          \\n        \\n        +\\n        2\\n        h\\n        ,\\n        .\\n        .\\n        .\\n      \\n    \\n    {\\\\displaystyle t_{0},t_{1}=t_{0}+h,t_{2}=t_{0}+2h,...}\\n     We denote by \\n  \\n    \\n      \\n        \\n          y\\n          \\n            n\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle y_{n}}\\n   a numerical estimate of the exact solution \\n  \\n    \\n      \\n        y\\n        (\\n        \\n          t\\n          \\n            n\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle y(t_{n})}\\n  . Motivated by (3), we compute these estimates by the following recursive scheme \\n\\nThis is the Euler method (or forward Euler method, in contrast with the backward Euler method, to be described below). The method is named after Leonhard Euler who described it in 1768.\\nThe Euler method is an example of an explicit method. This means that the new value yn+1 is defined in terms of things that are already known, like yn.\\n\\n\\n=== Backward Euler method ===\\n\\nIf, instead of (2), we use the approximation\\n\\nwe get the backward Euler method:\\n\\nThe backward Euler method is an implicit method, meaning that we have to solve an equation to find yn+1. One often uses fixed-point iteration or (some modification of) the Newton–Raphson method to achieve this.\\nIt costs more time to solve this equation than explicit methods; this cost must be taken into consideration when one selects the method to use. The advantage of implicit methods such as (6) is that they are usually more stable for solving a stiff equation, meaning that a larger step size h can be used.\\n\\n\\n=== First-order exponential integrator method ===\\n\\nExponential integrators describe a large class of integrators that have recently seen a lot of development.  They date back to at least the 1960s.\\nIn place of (1), we assume the differential equation is either of the form\\n\\nor it has been locally linearized about a background state to produce a linear term \\n  \\n    \\n      \\n        −\\n        A\\n        y\\n      \\n    \\n    {\\\\displaystyle -Ay}\\n   and a nonlinear term \\n  \\n    \\n      \\n        \\n          \\n            N\\n          \\n        \\n        (\\n        y\\n        )\\n      \\n    \\n    {\\\\displaystyle {\\\\mathcal {N}}(y)}\\n  .\\nExponential integrators are constructed by multiplying (7) by \\n  \\n    \\n      \\n        \\n          e\\n          \\n            A\\n            t\\n          \\n        \\n      \\n    \\n    {\\\\textstyle e^{At}}\\n  , and exactly integrating the result over\\na time interval \\n  \\n    \\n      \\n        [\\n        \\n          t\\n          \\n            n\\n          \\n        \\n        ,\\n        \\n          t\\n          \\n            n\\n            +\\n            1\\n          \\n        \\n        =\\n        \\n          t\\n          \\n            n\\n          \\n        \\n        +\\n        h\\n        ]\\n      \\n    \\n    {\\\\displaystyle [t_{n},t_{n+1}=t_{n}+h]}\\n  :\\n\\n  \\n    \\n      \\n        \\n          y\\n          \\n            n\\n            +\\n            1\\n          \\n        \\n        =\\n        \\n          e\\n          \\n            −\\n            A\\n            h\\n          \\n        \\n        \\n          y\\n          \\n            n\\n          \\n        \\n        +\\n        \\n          ∫\\n          \\n            0\\n          \\n          \\n            h\\n          \\n        \\n        \\n          e\\n          \\n            −\\n            (\\n            h\\n            −\\n            τ\\n            )\\n            A\\n          \\n        \\n        \\n          \\n            N\\n          \\n        \\n        \\n          (\\n          \\n            y\\n            \\n              (\\n              \\n                \\n                  t\\n                  \\n                    n\\n                  \\n                \\n                +\\n                τ\\n              \\n              )\\n            \\n          \\n          )\\n        \\n        \\n        d\\n        τ\\n        .\\n      \\n    \\n    {\\\\displaystyle y_{n+1}=e^{-Ah}y_{n}+\\\\int _{0}^{h}e^{-(h-\\\\tau )A}{\\\\mathcal {N}}\\\\left(y\\\\left(t_{n}+\\\\tau \\\\right)\\\\right)\\\\,d\\\\tau .}\\n  This integral equation is exact, but it doesn\\'t define the integral.\\nThe first-order exponential integrator can be realized by holding \\n  \\n    \\n      \\n        \\n          \\n            N\\n          \\n        \\n        (\\n        y\\n        (\\n        \\n          t\\n          \\n            n\\n          \\n        \\n        +\\n        τ\\n        )\\n        )\\n      \\n    \\n    {\\\\displaystyle {\\\\mathcal {N}}(y(t_{n}+\\\\tau ))}\\n   constant over the full interval:\\n\\n\\n=== Generalizations ===\\nThe Euler method is often not accurate enough. In more precise terms, it only has order one (the concept of order is explained below). This caused mathematicians to look for higher-order methods.\\nOne possibility is to use not only the previously computed value yn to determine yn+1, but to make the solution depend on more past values. This yields a so-called multistep method. Perhaps the simplest is the leapfrog method which is second order and (roughly speaking) relies on two time values.\\nAlmost all practical multistep methods fall within the family of linear multistep methods, which have the form\\n\\n  \\n    \\n      \\n        \\n          \\n            \\n              \\n              \\n                \\n\\n                \\n                \\n                  α\\n                  \\n                    k\\n                  \\n                \\n                \\n                  y\\n                  \\n                    n\\n                    +\\n                    k\\n                  \\n                \\n                +\\n                \\n                  α\\n                  \\n                    k\\n                    −\\n                    1\\n                  \\n                \\n                \\n                  y\\n                  \\n                    n\\n                    +\\n                    k\\n                    −\\n                    1\\n                  \\n                \\n                +\\n                ⋯\\n                +\\n                \\n                  α\\n                  \\n                    0\\n                  \\n                \\n                \\n                  y\\n                  \\n                    n\\n                  \\n                \\n              \\n            \\n            \\n              \\n              \\n                \\n                \\n\\n                \\n                \\n                =\\n                h\\n                \\n                  [\\n                  \\n                    \\n                      β\\n                      \\n                        k\\n                      \\n                    \\n                    f\\n                    (\\n                    \\n                      t\\n                      \\n                        n\\n                        +\\n                        k\\n                      \\n                    \\n                    ,\\n                    \\n                      y\\n                      \\n                        n\\n                        +\\n                        k\\n                      \\n                    \\n                    )\\n                    +\\n                    \\n                      β\\n                      \\n                        k\\n                        −\\n                        1\\n                      \\n                    \\n                    f\\n                    (\\n                    \\n                      t\\n                      \\n                        n\\n                        +\\n                        k\\n                        −\\n                        1\\n                      \\n                    \\n                    ,\\n                    \\n                      y\\n                      \\n                        n\\n                        +\\n                        k\\n                        −\\n                        1\\n                      \\n                    \\n                    )\\n                    +\\n                    ⋯\\n                    +\\n                    \\n                      β\\n                      \\n                        0\\n                      \\n                    \\n                    f\\n                    (\\n                    \\n                      t\\n                      \\n                        n\\n                      \\n                    \\n                    ,\\n                    \\n                      y\\n                      \\n                        n\\n                      \\n                    \\n                    )\\n                  \\n                  ]\\n                \\n                .\\n              \\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\begin{aligned}&{}\\\\alpha _{k}y_{n+k}+\\\\alpha _{k-1}y_{n+k-1}+\\\\cdots +\\\\alpha _{0}y_{n}\\\\\\\\&{}\\\\quad =h\\\\left[\\\\beta _{k}f(t_{n+k},y_{n+k})+\\\\beta _{k-1}f(t_{n+k-1},y_{n+k-1})+\\\\cdots +\\\\beta _{0}f(t_{n},y_{n})\\\\right].\\\\end{aligned}}}\\n  Another possibility is to use more points in the interval \\n  \\n    \\n      \\n        [\\n        \\n          t\\n          \\n            n\\n          \\n        \\n        ,\\n        \\n          t\\n          \\n            n\\n            +\\n            1\\n          \\n        \\n        ]\\n      \\n    \\n    {\\\\displaystyle [t_{n},t_{n+1}]}\\n  . This leads to the family of Runge–Kutta methods, named after Carl Runge and Martin Kutta. One of their fourth-order methods is especially popular.\\n\\n\\n=== Advanced features ===\\nA good implementation of one of these methods for solving an ODE entails more than the time-stepping formula.\\nIt is often inefficient to use the same step size all the time, so variable step-size methods have been developed. Usually, the step size is chosen such that the (local) error per step is below some tolerance level. This means that the methods must also compute an error indicator, an estimate of the local error.\\nAn extension of this idea is to choose dynamically between different methods of different orders (this is called a variable order method). Methods based on Richardson extrapolation, such as the Bulirsch–Stoer algorithm, are often used to construct various methods of different orders.\\nOther desirable features include: \\n\\ndense output: cheap numerical approximations for the whole integration interval, and not only at the points t0, t1, t2, ...\\nevent location: finding the times where, say, a particular function vanishes. This typically requires the use of a root-finding algorithm.\\nsupport for parallel computing.\\nwhen used for integrating with respect to time, time reversibility\\n\\n\\n=== Alternative methods ===\\nMany methods do not fall within the framework discussed here. Some classes of alternative methods are:\\n\\nmultiderivative methods, which use not only the function f but also its derivatives. This class includes Hermite–Obreschkoff methods and Fehlberg methods, as well as methods like the Parker–Sochacki method or Bychkov–Scherbakov method, which compute the coefficients of the Taylor series of the solution y recursively.\\nmethods for second order ODEs. We said that all higher-order ODEs can be transformed to first-order ODEs of the form (1). While this is certainly true, it may not be the best way to proceed. In particular, Nyström methods work directly with second-order equations.\\ngeometric integration methods are especially designed for special classes of ODEs (for example, symplectic integrators for the solution of Hamiltonian equations). They take care that the numerical solution respects the underlying structure or geometry of these classes.\\nQuantized state systems methods are a family of ODE integration methods based on the idea of state quantization. They are efficient when simulating sparse systems with frequent discontinuities.\\n\\n\\n=== Parallel-in-time methods ===\\nFor applications that require parallel computing on supercomputers, the degree of concurrency offered by a numerical method becomes relevant. \\nIn view of the challenges from exascale computing systems, numerical methods for initial value problems which can provide concurrency in temporal direction are being studied.Parareal is a relatively well known example of such a parallel-in-time integration method, but early ideas go back into the 1960s.\\n\\n\\n== Analysis ==\\nNumerical analysis is not only the design of numerical methods, but also their analysis. Three central concepts in this analysis are:\\n\\nconvergence: whether the method approximates the solution,\\norder: how well it approximates the solution, and\\nstability: whether errors are damped out.\\n\\n\\n=== Convergence ===\\n\\nA numerical method is said to be convergent if the numerical solution approaches the exact solution as the step size h goes to 0. More precisely, we require that for every ODE (1) with a Lipschitz function f and every t* > 0,\\n\\n  \\n    \\n      \\n        \\n          lim\\n          \\n            h\\n            →\\n            \\n              0\\n              \\n                +\\n              \\n            \\n          \\n        \\n        \\n          max\\n          \\n            n\\n            =\\n            0\\n            ,\\n            1\\n            ,\\n            …\\n            ,\\n            ⌊\\n            \\n              t\\n              \\n                ∗\\n              \\n            \\n            \\n              /\\n            \\n            h\\n            ⌋\\n          \\n        \\n        \\n          ‖\\n          \\n            \\n              y\\n              \\n                n\\n                ,\\n                h\\n              \\n            \\n            −\\n            y\\n            (\\n            \\n              t\\n              \\n                n\\n              \\n            \\n            )\\n          \\n          ‖\\n        \\n        =\\n        0.\\n      \\n    \\n    {\\\\displaystyle \\\\lim _{h\\\\to 0^{+}}\\\\max _{n=0,1,\\\\dots ,\\\\lfloor t^{*}/h\\\\rfloor }\\\\left\\\\|y_{n,h}-y(t_{n})\\\\right\\\\|=0.}\\n  All the methods mentioned above are convergent.\\n\\n\\n=== Consistency and order ===\\n\\nSuppose the numerical method is\\n\\n  \\n    \\n      \\n        \\n          y\\n          \\n            n\\n            +\\n            k\\n          \\n        \\n        =\\n        Ψ\\n        (\\n        \\n          t\\n          \\n            n\\n            +\\n            k\\n          \\n        \\n        ;\\n        \\n          y\\n          \\n            n\\n          \\n        \\n        ,\\n        \\n          y\\n          \\n            n\\n            +\\n            1\\n          \\n        \\n        ,\\n        …\\n        ,\\n        \\n          y\\n          \\n            n\\n            +\\n            k\\n            −\\n            1\\n          \\n        \\n        ;\\n        h\\n        )\\n        .\\n        \\n      \\n    \\n    {\\\\displaystyle y_{n+k}=\\\\Psi (t_{n+k};y_{n},y_{n+1},\\\\dots ,y_{n+k-1};h).\\\\,}\\n  The local (truncation) error of the method is the error committed by one step of the method. That is, it is the difference between the result given by the method, assuming that no error was made in earlier steps, and the exact solution:\\n\\n  \\n    \\n      \\n        \\n          δ\\n          \\n            n\\n            +\\n            k\\n          \\n          \\n            h\\n          \\n        \\n        =\\n        Ψ\\n        \\n          (\\n          \\n            \\n              t\\n              \\n                n\\n                +\\n                k\\n              \\n            \\n            ;\\n            y\\n            (\\n            \\n              t\\n              \\n                n\\n              \\n            \\n            )\\n            ,\\n            y\\n            (\\n            \\n              t\\n              \\n                n\\n                +\\n                1\\n              \\n            \\n            )\\n            ,\\n            …\\n            ,\\n            y\\n            (\\n            \\n              t\\n              \\n                n\\n                +\\n                k\\n                −\\n                1\\n              \\n            \\n            )\\n            ;\\n            h\\n          \\n          )\\n        \\n        −\\n        y\\n        (\\n        \\n          t\\n          \\n            n\\n            +\\n            k\\n          \\n        \\n        )\\n        .\\n      \\n    \\n    {\\\\displaystyle \\\\delta _{n+k}^{h}=\\\\Psi \\\\left(t_{n+k};y(t_{n}),y(t_{n+1}),\\\\dots ,y(t_{n+k-1});h\\\\right)-y(t_{n+k}).}\\n  The method is said to be consistent if \\n\\n  \\n    \\n      \\n        \\n          lim\\n          \\n            h\\n            →\\n            0\\n          \\n        \\n        \\n          \\n            \\n              δ\\n              \\n                n\\n                +\\n                k\\n              \\n              \\n                h\\n              \\n            \\n            h\\n          \\n        \\n        =\\n        0.\\n      \\n    \\n    {\\\\displaystyle \\\\lim _{h\\\\to 0}{\\\\frac {\\\\delta _{n+k}^{h}}{h}}=0.}\\n  The method has order \\n  \\n    \\n      \\n        p\\n      \\n    \\n    {\\\\displaystyle p}\\n   if\\n\\n  \\n    \\n      \\n        \\n          δ\\n          \\n            n\\n            +\\n            k\\n          \\n          \\n            h\\n          \\n        \\n        =\\n        O\\n        (\\n        \\n          h\\n          \\n            p\\n            +\\n            1\\n          \\n        \\n        )\\n        \\n        \\n          \\n            as \\n          \\n        \\n        h\\n        →\\n        0.\\n      \\n    \\n    {\\\\displaystyle \\\\delta _{n+k}^{h}=O(h^{p+1})\\\\quad {\\\\mbox{as }}h\\\\to 0.}\\n  Hence a method is consistent if it has an order greater than 0. The (forward) Euler method (4) and the backward Euler method (6) introduced above both have order 1, so they are consistent. Most methods being used in practice attain higher order. Consistency is a necessary condition for convergence, but not sufficient; for a method to be convergent, it must be both consistent and zero-stable.\\nA related concept is the global (truncation) error, the error sustained in all the steps one needs to reach a fixed time \\n  \\n    \\n      \\n        t\\n      \\n    \\n    {\\\\displaystyle t}\\n  . Explicitly, the global error at time \\n  \\n    \\n      \\n        t\\n      \\n    \\n    {\\\\displaystyle t}\\n   is \\n  \\n    \\n      \\n        \\n          y\\n          \\n            N\\n          \\n        \\n        −\\n        y\\n        (\\n        t\\n        )\\n      \\n    \\n    {\\\\displaystyle y_{N}-y(t)}\\n   where \\n  \\n    \\n      \\n        N\\n        =\\n        (\\n        t\\n        −\\n        \\n          t\\n          \\n            0\\n          \\n        \\n        )\\n        \\n          /\\n        \\n        h\\n      \\n    \\n    {\\\\displaystyle N=(t-t_{0})/h}\\n  . The global error of a \\n  \\n    \\n      \\n        p\\n      \\n    \\n    {\\\\displaystyle p}\\n  th order one-step method is \\n  \\n    \\n      \\n        O\\n        (\\n        \\n          h\\n          \\n            p\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle O(h^{p})}\\n  ; in particular, such a method is convergent.  This statement is not necessarily true for multi-step methods.\\n\\n\\n=== Stability and stiffness ===\\n\\nFor some differential equations, application of standard methods—such as the Euler method, explicit Runge–Kutta methods, or multistep methods (for example, Adams–Bashforth methods)—exhibit instability in the solutions, though other methods may produce stable solutions. This \"difficult behaviour\" in the equation (which may not necessarily be complex itself) is described as stiffness, and is often caused by the presence of different time scales in the underlying problem. For example, a collision in a mechanical system like in an impact oscillator typically occurs at much smaller time scale than the time for the motion of objects; this discrepancy makes for very \"sharp turns\" in the curves of the state parameters.\\nStiff problems are ubiquitous in chemical kinetics, control theory, solid mechanics, weather forecasting, biology, plasma physics, and electronics. One way to overcome stiffness is to extend the notion of differential equation to that of differential inclusion, which allows for and models non-smoothness.\\n\\n\\n== History ==\\nBelow is a timeline of some important developments in this field.\\n1768 - Leonhard Euler publishes his method.\\n1824 - Augustin Louis Cauchy proves convergence of the Euler method. In this proof, Cauchy uses the implicit Euler method.\\n1855 - First mention of the multistep methods of John Couch Adams in a letter written by Francis Bashforth.\\n1895 - Carl Runge publishes the first Runge–Kutta method.\\n1901 - Martin Kutta describes the popular fourth-order Runge–Kutta method.\\n1910 - Lewis Fry Richardson announces his extrapolation method, Richardson extrapolation.\\n1952 - Charles F. Curtiss and Joseph Oakland Hirschfelder coin the term stiff equations.\\n1963 - Germund Dahlquist introduces A-stability of integration methods.\\n\\n\\n== Numerical solutions to second-order one-dimensional boundary value problems ==\\nBoundary value problems (BVPs) are usually solved numerically by solving an approximately equivalent matrix problem obtained by discretizing the original BVP. The most commonly used method for numerically solving BVPs in one dimension is called the Finite Difference Method.  This method takes advantage of linear combinations of point values to construct finite difference coefficients that describe derivatives of the function. For example, the second-order central difference approximation to the first derivative is given by:\\n\\n  \\n    \\n      \\n        \\n          \\n            \\n              \\n                u\\n                \\n                  i\\n                  +\\n                  1\\n                \\n              \\n              −\\n              \\n                u\\n                \\n                  i\\n                  −\\n                  1\\n                \\n              \\n            \\n            \\n              2\\n              h\\n            \\n          \\n        \\n        =\\n        \\n          u\\n          ′\\n        \\n        (\\n        \\n          x\\n          \\n            i\\n          \\n        \\n        )\\n        +\\n        \\n          \\n            O\\n          \\n        \\n        (\\n        \\n          h\\n          \\n            2\\n          \\n        \\n        )\\n        ,\\n      \\n    \\n    {\\\\displaystyle {\\\\frac {u_{i+1}-u_{i-1}}{2h}}=u\\'(x_{i})+{\\\\mathcal {O}}(h^{2}),}\\n  and the second-order central difference for the second derivative is given by:\\n\\n  \\n    \\n      \\n        \\n          \\n            \\n              \\n                u\\n                \\n                  i\\n                  +\\n                  1\\n                \\n              \\n              −\\n              2\\n              \\n                u\\n                \\n                  i\\n                \\n              \\n              +\\n              \\n                u\\n                \\n                  i\\n                  −\\n                  1\\n                \\n              \\n            \\n            \\n              h\\n              \\n                2\\n              \\n            \\n          \\n        \\n        =\\n        \\n          u\\n          ″\\n        \\n        (\\n        \\n          x\\n          \\n            i\\n          \\n        \\n        )\\n        +\\n        \\n          \\n            O\\n          \\n        \\n        (\\n        \\n          h\\n          \\n            2\\n          \\n        \\n        )\\n        .\\n      \\n    \\n    {\\\\displaystyle {\\\\frac {u_{i+1}-2u_{i}+u_{i-1}}{h^{2}}}=u\\'\\'(x_{i})+{\\\\mathcal {O}}(h^{2}).}\\n  In both of these formulae, \\n  \\n    \\n      \\n        h\\n        =\\n        \\n          x\\n          \\n            i\\n          \\n        \\n        −\\n        \\n          x\\n          \\n            i\\n            −\\n            1\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle h=x_{i}-x_{i-1}}\\n   is the distance between neighbouring x values on the discretized domain. One then constructs a linear system that can then be solved by standard matrix methods. For example, suppose the equation to be solved is:\\n\\n  \\n    \\n      \\n        \\n          \\n            \\n              \\n              \\n                \\n\\n                \\n                \\n                  \\n                    \\n                      \\n                        d\\n                        \\n                          2\\n                        \\n                      \\n                      u\\n                    \\n                    \\n                      d\\n                      \\n                        x\\n                        \\n                          2\\n                        \\n                      \\n                    \\n                  \\n                \\n                −\\n                u\\n                =\\n                0\\n                ,\\n              \\n            \\n            \\n              \\n              \\n                \\n\\n                \\n                u\\n                (\\n                0\\n                )\\n                =\\n                0\\n                ,\\n              \\n            \\n            \\n              \\n              \\n                \\n\\n                \\n                u\\n                (\\n                1\\n                )\\n                =\\n                1.\\n              \\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\begin{aligned}&{}{\\\\frac {d^{2}u}{dx^{2}}}-u=0,\\\\\\\\&{}u(0)=0,\\\\\\\\&{}u(1)=1.\\\\end{aligned}}}\\n  The next step would be to discretize the problem and use linear derivative approximations such as\\n\\n  \\n    \\n      \\n        \\n          u\\n          \\n            i\\n          \\n          ″\\n        \\n        =\\n        \\n          \\n            \\n              \\n                u\\n                \\n                  i\\n                  +\\n                  1\\n                \\n              \\n              −\\n              2\\n              \\n                u\\n                \\n                  i\\n                \\n              \\n              +\\n              \\n                u\\n                \\n                  i\\n                  −\\n                  1\\n                \\n              \\n            \\n            \\n              h\\n              \\n                2\\n              \\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle u\\'\\'_{i}={\\\\frac {u_{i+1}-2u_{i}+u_{i-1}}{h^{2}}}}\\n  and solve the resulting system of linear equations. This would lead to equations such as:\\n\\n  \\n    \\n      \\n        \\n          \\n            \\n              \\n                u\\n                \\n                  i\\n                  +\\n                  1\\n                \\n              \\n              −\\n              2\\n              \\n                u\\n                \\n                  i\\n                \\n              \\n              +\\n              \\n                u\\n                \\n                  i\\n                  −\\n                  1\\n                \\n              \\n            \\n            \\n              h\\n              \\n                2\\n              \\n            \\n          \\n        \\n        −\\n        \\n          u\\n          \\n            i\\n          \\n        \\n        =\\n        0\\n        ,\\n        \\n        ∀\\n        i\\n        =\\n        \\n          1\\n          ,\\n          2\\n          ,\\n          3\\n          ,\\n          .\\n          .\\n          .\\n          ,\\n          n\\n          −\\n          1\\n        \\n        .\\n      \\n    \\n    {\\\\displaystyle {\\\\frac {u_{i+1}-2u_{i}+u_{i-1}}{h^{2}}}-u_{i}=0,\\\\quad \\\\forall i={1,2,3,...,n-1}.}\\n  On first viewing, this system of equations appears to have difficulty associated with the fact that the equation involves no terms that are not multiplied by variables, but in fact this is false. At i = 1 and n − 1 there is a term involving the boundary values \\n  \\n    \\n      \\n        u\\n        (\\n        0\\n        )\\n        =\\n        \\n          u\\n          \\n            0\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle u(0)=u_{0}}\\n   and \\n  \\n    \\n      \\n        u\\n        (\\n        1\\n        )\\n        =\\n        \\n          u\\n          \\n            n\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle u(1)=u_{n}}\\n   and since these two values are known, one can simply substitute them into this equation and as a result have a non-homogeneous linear system of equations that has non-trivial solutions.\\n\\n\\n== See also ==\\nCourant–Friedrichs–Lewy condition\\nEnergy drift\\nGeneral linear methods\\nList of numerical analysis topics#Numerical methods for ordinary differential equations\\nReversible reference system propagation algorithm\\nModelica Language and OpenModelica software\\n\\n\\n== Notes ==\\n\\n\\n== References ==\\nBradie, Brian (2006). A Friendly Introduction to Numerical Analysis. Upper Saddle River, New Jersey: Pearson Prentice Hall. ISBN 978-0-13-013054-9.\\nJ. C. Butcher, Numerical methods for ordinary differential equations, ISBN 0-471-96758-0\\nErnst Hairer, Syvert Paul Nørsett and Gerhard Wanner, Solving ordinary differential equations I: Nonstiff problems, second edition, Springer Verlag, Berlin, 1993. ISBN 3-540-56670-8.\\nErnst Hairer and Gerhard Wanner, Solving ordinary differential equations II: Stiff and differential-algebraic problems, second edition, Springer Verlag, Berlin, 1996. ISBN 3-540-60452-9.  (This two-volume monograph systematically covers all aspects of the field.)\\nHochbruck, Marlis; Ostermann, Alexander (May 2010). \"Exponential integrators\". Acta Numerica. 19: 209–286. Bibcode:2010AcNum..19..209H. CiteSeerX 10.1.1.187.6794. doi:10.1017/S0962492910000048.\\nArieh Iserles, A First Course in the Numerical Analysis of Differential Equations, Cambridge University Press, 1996. ISBN 0-521-55376-8 (hardback), ISBN 0-521-55655-4 (paperback).  (Textbook, targeting advanced undergraduate and postgraduate students in mathematics, which also discusses numerical partial differential equations.)\\nJohn Denholm Lambert, Numerical Methods for Ordinary Differential Systems, John Wiley & Sons, Chichester, 1991. ISBN 0-471-92990-5.  (Textbook, slightly more demanding than the book by Iserles.)\\n\\n\\n== External links ==\\nJoseph W. Rudmin, Application of the Parker–Sochacki Method to Celestial Mechanics, 1998.\\nDominique Tournès, L\\'intégration approchée des équations différentielles ordinaires (1671-1914), thèse de doctorat de l\\'université Paris 7 - Denis Diderot, juin 1996. Réimp. Villeneuve d\\'Ascq : Presses universitaires du Septentrion, 1997, 468 p. (Extensive online material on ODE numerical analysis history, for English-language material on the history of ODE numerical analysis, see, for example, the paper books by Chabert and Goldstine quoted by him.)\\nPchelintsev, A.N. (2020). \"An accurate numerical method and algorithm for constructing solutions of chaotic systems\" (PDF). Journal of Applied Nonlinear Dynamics. 9 (2): 207–221. doi:10.5890/JAND.2020.06.004.\\nkv on GitHub (C++ library with rigorous ODE solvers)\\nINTLAB (A library made by MATLAB/GNU Octave which includes rigorous ODE solvers)', 'Special functions are particular mathematical functions that have more or less established names and notations due to their importance in mathematical analysis, functional analysis, geometry, physics, or other applications.\\nThe term is defined by consensus, and thus lacks a general formal definition, but the List of mathematical functions contains functions that are commonly accepted as special.\\n\\n\\n== Tables of special functions ==\\nMany special functions appear as solutions of differential equations or integrals of elementary functions. Therefore, tables of integrals usually include descriptions of special functions, and tables of special functions include most important integrals; at least, the integral representation of special functions. Because symmetries of differential equations are essential to both physics and mathematics, the theory of special functions is closely related to the theory of Lie groups and Lie algebras, as well as certain topics in mathematical physics.\\nSymbolic computation engines usually recognize the majority of special functions.\\n\\n\\n=== Notations used for special functions ===\\nFunctions with established international notations are the sine (\\n  \\n    \\n      \\n        sin\\n      \\n    \\n    {\\\\displaystyle \\\\sin }\\n  ), cosine (\\n  \\n    \\n      \\n        cos\\n      \\n    \\n    {\\\\displaystyle \\\\cos }\\n  ), exponential function (\\n  \\n    \\n      \\n        exp\\n      \\n    \\n    {\\\\displaystyle \\\\exp }\\n  ), and error function (\\n  \\n    \\n      \\n        erf\\n      \\n    \\n    {\\\\displaystyle \\\\operatorname {erf} }\\n   or \\n  \\n    \\n      \\n        erfc\\n      \\n    \\n    {\\\\displaystyle \\\\operatorname {erfc} }\\n  ).\\nSome special functions have several notations:\\n\\nThe natural logarithm may be denoted \\n  \\n    \\n      \\n        ln\\n      \\n    \\n    {\\\\displaystyle \\\\ln }\\n  , \\n  \\n    \\n      \\n        log\\n      \\n    \\n    {\\\\displaystyle \\\\log }\\n  , \\n  \\n    \\n      \\n        \\n          log\\n          \\n            e\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle \\\\log _{e}}\\n  , or \\n  \\n    \\n      \\n        Log\\n      \\n    \\n    {\\\\displaystyle \\\\operatorname {Log} }\\n   depending on the context.\\nThe tangent function may be denoted \\n  \\n    \\n      \\n        tan\\n      \\n    \\n    {\\\\displaystyle \\\\tan }\\n  , \\n  \\n    \\n      \\n        Tan\\n      \\n    \\n    {\\\\displaystyle \\\\operatorname {Tan} }\\n  , or \\n  \\n    \\n      \\n        tg\\n      \\n    \\n    {\\\\displaystyle \\\\operatorname {tg} }\\n   (\\n  \\n    \\n      \\n        tg\\n      \\n    \\n    {\\\\displaystyle \\\\operatorname {tg} }\\n   is used mainly in Russian and Bulgarian literature).\\nThe arctangent may be denoted \\n  \\n    \\n      \\n        arctan\\n      \\n    \\n    {\\\\displaystyle \\\\arctan }\\n  , \\n  \\n    \\n      \\n        atan\\n      \\n    \\n    {\\\\displaystyle \\\\operatorname {atan} }\\n  , \\n  \\n    \\n      \\n        arctg\\n      \\n    \\n    {\\\\displaystyle \\\\operatorname {arctg} }\\n  , or \\n  \\n    \\n      \\n        \\n          tan\\n          \\n            −\\n            1\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle \\\\tan ^{-1}}\\n  .\\nThe Bessel functions may be denoted\\n\\n  \\n    \\n      \\n        \\n          J\\n          \\n            n\\n          \\n        \\n        (\\n        x\\n        )\\n        ,\\n      \\n    \\n    {\\\\displaystyle J_{n}(x),}\\n  \\n\\n  \\n    \\n      \\n        besselj\\n        \\u2061\\n        (\\n        n\\n        ,\\n        x\\n        )\\n        ,\\n      \\n    \\n    {\\\\displaystyle \\\\operatorname {besselj} (n,x),}\\n  \\n\\n  \\n    \\n      \\n        \\n          \\n            B\\n            e\\n            s\\n            s\\n            e\\n            l\\n            J\\n          \\n        \\n        [\\n        n\\n        ,\\n        x\\n        ]\\n        .\\n      \\n    \\n    {\\\\displaystyle {\\\\rm {BesselJ}}[n,x].}\\n  Subscripts are often used to indicate arguments, typically integers. In a few cases, the semicolon (;) or even backslash (\\\\) is used as a separator. In this case, the translation to algorithmic languages admits ambiguity and may lead to confusion.\\nSuperscripts may indicate not only exponentiation, but modification of a function. Examples (particularly with trigonometric functions and hyperbolic functions) include:\\n\\n  \\n    \\n      \\n        \\n          cos\\n          \\n            3\\n          \\n        \\n        \\u2061\\n        (\\n        x\\n        )\\n      \\n    \\n    {\\\\displaystyle \\\\cos ^{3}(x)}\\n   usually indicates \\n  \\n    \\n      \\n        (\\n        cos\\n        \\u2061\\n        (\\n        x\\n        )\\n        \\n          )\\n          \\n            3\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle (\\\\cos(x))^{3}}\\n  \\n\\n  \\n    \\n      \\n        \\n          cos\\n          \\n            2\\n          \\n        \\n        \\u2061\\n        (\\n        x\\n        )\\n      \\n    \\n    {\\\\displaystyle \\\\cos ^{2}(x)}\\n   is typically \\n  \\n    \\n      \\n        (\\n        cos\\n        \\u2061\\n        (\\n        x\\n        )\\n        \\n          )\\n          \\n            2\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle (\\\\cos(x))^{2}}\\n  , but never \\n  \\n    \\n      \\n        cos\\n        \\u2061\\n        (\\n        cos\\n        \\u2061\\n        (\\n        x\\n        )\\n        )\\n      \\n    \\n    {\\\\displaystyle \\\\cos(\\\\cos(x))}\\n  \\n\\n  \\n    \\n      \\n        \\n          cos\\n          \\n            −\\n            1\\n          \\n        \\n        \\u2061\\n        (\\n        x\\n        )\\n      \\n    \\n    {\\\\displaystyle \\\\cos ^{-1}(x)}\\n   usually means \\n  \\n    \\n      \\n        arccos\\n        \\u2061\\n        (\\n        x\\n        )\\n      \\n    \\n    {\\\\displaystyle \\\\arccos(x)}\\n  , and not \\n  \\n    \\n      \\n        (\\n        cos\\n        \\u2061\\n        (\\n        x\\n        )\\n        \\n          )\\n          \\n            −\\n            1\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle (\\\\cos(x))^{-1}}\\n  ; this one typically causes the most confusion, since interpretation with this exponent value is inconsistent with the others.\\n\\n\\n=== Evaluation of special functions ===\\nMost special functions are considered as a function of a complex variable. They are analytic; the singularities and cuts are described; the differential and integral representations are known and the expansion to the Taylor series or asymptotic series are available. In addition, sometimes there exist relations with other special functions; a complicated special function can be expressed in terms of simpler functions. Various representations can be used for the evaluation;  the simplest way to evaluate a function is to expand it into a Taylor series. However, such representation may converge slowly or not at all. In algorithmic languages, rational approximations are typically used, although they may behave badly in the case of complex argument(s).\\n\\n\\n== History of special functions ==\\n\\n\\n=== Classical theory ===\\nWhile trigonometry can be codified—as was clear already to expert mathematicians of the eighteenth century (if not before)—the search for a complete and unified theory of special functions has continued since the nineteenth century. The high point of special function theory in the period 1800–1900 was the theory of elliptic functions; treatises that were essentially complete, such as that of Tannery and Molk, could be written as handbooks to all the basic identities of the theory. They were based on techniques from complex analysis.\\nFrom that time onwards it would be assumed that analytic function theory, which had already unified the trigonometric and exponential functions, was a fundamental tool. The end of the century also saw a very detailed discussion of spherical harmonics.\\n\\n\\n=== Changing and fixed motivations ===\\nOf course the wish for a broad theory including as many as possible of the known special functions has its intellectual appeal, but it is worth noting other motivations.  For a long time, the special functions were in the particular province of applied mathematics; applications to the physical sciences and engineering determined the relative importance of functions. In the days before the electronic computer, the ultimate compliment to a special function was the computation, by hand, of extended tables of its values. This was a capital-intensive process, intended to make the function available by look-up, as for the familiar logarithm tables. The aspects of the theory that then mattered might then be two:\\n\\nfor numerical analysis, discovery of infinite series or other analytical expression allowing rapid calculation; and\\nreduction of as many functions as possible to the given function.In contrast, one might say, there are approaches typical of the interests of pure mathematics: asymptotic analysis, analytic continuation and monodromy in the complex plane, and the discovery of symmetry principles and other structure behind the façade of endless formulae in rows.  There is not a real conflict between these approaches, in fact.\\n\\n\\n=== Twentieth century ===\\nThe twentieth century saw several waves of interest in special function theory.  The classic Whittaker and Watson (1902) textbook sought to unify the theory by using complex variables; the G. N. Watson tome A Treatise on the Theory of Bessel Functions pushed the techniques as far as possible for one important type that particularly admitted asymptotics to be studied.\\nThe later Bateman Manuscript Project, under the editorship of Arthur Erdélyi, attempted to be encyclopedic, and came around the time when electronic computation was coming to the fore and tabulation ceased to be the main issue.\\n\\n\\n=== Contemporary theories ===\\nThe modern theory of orthogonal polynomials is of a definite but limited scope. Hypergeometric series became an intricate theory, in need of later conceptual arrangement. Lie groups, and in particular their representation theory, explain what a spherical function can be in general; from 1950 onwards substantial parts of classical theory could be recast in terms of Lie groups. Further,  work on algebraic combinatorics also revived interest in older parts of the theory. Conjectures of Ian G. Macdonald helped to open up large and active new fields with the typical special function flavour. Difference equations have begun to take their place besides differential equations as a source for special functions.\\n\\n\\n== Special functions in number theory ==\\nIn number theory, certain special functions have traditionally been studied, such as particular Dirichlet series and modular forms. Almost all aspects of special function theory are reflected there, as well as some new ones, such as came out of the monstrous moonshine theory.\\n\\n\\n== Researchers ==\\n\\n\\n== See also ==\\nList of mathematical functions\\nList of special functions and eponyms\\nElementary function\\n\\n\\n== References ==\\n\\nAndrews, George E.; Askey, Richard; Roy, Ranjan (1999). Special functions. Encyclopedia of Mathematics and its Applications. 71. Cambridge University Press. ISBN 978-0-521-62321-6. MR 1688958.\\n\\n\\n== External links ==\\nNational Institute of Standards and Technology, United States Department of Commerce. NIST Digital Library of Mathematical Functions. Archived from the original on December 13, 2018.\\nWeisstein, Eric W. \"Special Function\". MathWorld.\\nOnline calculator, Online scientific calculator with over 100 functions (>=32 digits, many complex) (German language)\\nSpecial functions at EqWorld: The World of Mathematical Equations\\nSpecial functions and polynomials by Gerard \\'t Hooft and Stefan Nobbenhuis (April 8, 2013)\\nNumerical Methods for Special Functions, by A. Gil, J. Segura, N.M. Temme (2007).\\nR. Jagannathan, (P,Q)-Special Functions\\nSpecialfunctionswiki', 'In mathematics and computer science, computer algebra, also called symbolic computation or algebraic computation, is a scientific area that refers to the study and development of algorithms and software for manipulating mathematical expressions and other mathematical objects. Although computer algebra could be considered a subfield of scientific computing, they are generally considered as distinct fields because scientific computing is usually based on numerical computation with approximate floating point numbers, while symbolic computation emphasizes exact computation with expressions containing variables that have no given value and are manipulated as symbols.\\nSoftware applications that perform symbolic calculations are called computer algebra systems, with the term system alluding to the complexity of the main applications  that include, at least, a method to represent mathematical data in a computer, a user programming language (usually different from the language used for the implementation), a dedicated memory manager, a user interface for the input/output of mathematical expressions, a large set of routines to perform usual operations, like simplification of expressions, differentiation using chain rule, polynomial factorization, indefinite integration, etc.\\nComputer algebra is widely used to experiment in mathematics and to design the formulas that are used in numerical programs. It is also used for complete scientific computations, when purely numerical methods fail, as in public key cryptography, or for some non-linear problems.\\n\\n\\n== Terminology ==\\nSome authors distinguish computer algebra from symbolic computation using the latter name to refer to kinds of symbolic computation other than the computation with mathematical formulas. Some authors use symbolic computation for the computer science aspect of the subject and \"computer algebra\" for the mathematical aspect. In some languages the name of the field is not a direct translation of its English name. Typically, it is called calcul formel in French, which means \"formal computation\". This name reflects the ties this field has with formal methods.\\nSymbolic computation has also been referred to, in the past, as symbolic manipulation, algebraic manipulation, symbolic processing, symbolic mathematics, or symbolic algebra, but these terms, which also refer to non-computational manipulation, are no longer used in reference to computer algebra.\\n\\n\\n== Scientific community ==\\nThere is no learned society that is specific to computer algebra, but this function is assumed by the special interest group of the Association for Computing Machinery named SIGSAM (Special Interest Group\\non Symbolic and Algebraic Manipulation).There are several annual conferences on computer algebra, the premier being ISSAC (International Symposium on Symbolic and Algebraic Computation), which is regularly sponsored by SIGSAM.There are several journals specializing in computer algebra, the top one being Journal of Symbolic Computation founded in 1985 by Bruno Buchberger. There are also several other journals that regularly publish articles in computer algebra.\\n\\n\\n== Computer science aspects ==\\n\\n\\n=== Data representation ===\\nAs numerical software is highly efficient for approximate numerical computation, it is common, in computer algebra, to emphasize exact computation with exactly represented data. Such an exact representation implies that, even when the size of the output is small, the intermediate data generated during a computation may grow in an unpredictable way. This behavior is called expression swell. To obviate this problem, various methods are used in the representation of the data, as well as in the algorithms that manipulate them.\\n\\n\\n==== Numbers ====\\nThe usual numbers systems used in numerical computation are floating point numbers and integers of a fixed bounded size. None of these is convenient for computer algebra, due to expression swell.Therefore, the basic numbers used in computer algebra are the integers of the mathematicians, commonly represented by an unbounded signed sequence of digits in some base of numeration, usually the largest base allowed by the machine word. These integers allow to define the rational numbers, which are irreducible fractions of two integers.\\nProgramming an efficient implementation of the arithmetic operations is a hard task. Therefore, most free computer algebra systems and some commercial ones such as Mathematica and Maple (software), use the GMP library, which is thus a de facto standard.\\n\\n\\n==== Expressions ====\\n\\nExcept for numbers and variables, every mathematical expression may be viewed as the symbol of an operator followed by a sequence of operands. In computer algebra software, the expressions are usually represented in this way. This representation is very flexible, and many things that seem not to be mathematical expressions at first glance, may be represented and manipulated as such. For example, an equation is an expression with “=” as an operator, a matrix may be represented as an expression with “matrix” as an operator and its rows as operands.\\nEven programs may be considered and represented as expressions with operator “procedure” and, at least, two operands, the list of parameters and the body, which is itself an expression with “body” as an operator and a sequence of instructions as operands. Conversely, any mathematical expression may be viewed as a program. For example, the expression a + b may be viewed as a program for the addition, with a and b as parameters. Executing this program consists in evaluating the expression for given values of a and b; if they do not have any value—that is they are indeterminates—, the result of the evaluation is simply its input.\\nThis process of delayed evaluation is fundamental in computer algebra. For example, the operator “=” of the equations is also, in most computer algebra systems, the name of the program of the equality test: normally, the evaluation of an equation results in an equation, but, when an equality test is needed,—either explicitly asked by the user through an “evaluation to a Boolean” command, or automatically started by the system in the case of a test inside a program—then the evaluation to a boolean 0 or 1 is executed.\\nAs the size of the operands of an expression is unpredictable and may change during a working session, the sequence of the operands is usually represented as a sequence of either pointers (like in Macsyma) or entries in a hash table (like in Maple).\\n\\n\\n=== Simplification ===\\nThe raw application of the basic rules of differentiation with respect to x on the expression \\n  \\n    \\n      \\n        \\n          a\\n          \\n            x\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle a^{x}}\\n   gives the result \\n\\n  \\n    \\n      \\n        x\\n        ⋅\\n        \\n          a\\n          \\n            x\\n            −\\n            1\\n          \\n        \\n        ⋅\\n        0\\n        +\\n        \\n          a\\n          \\n            x\\n          \\n        \\n        ⋅\\n        \\n          (\\n          \\n            1\\n            ⋅\\n            log\\n            \\u2061\\n            a\\n            +\\n            x\\n            ⋅\\n            \\n              \\n                0\\n                a\\n              \\n            \\n          \\n          )\\n        \\n        .\\n      \\n    \\n    {\\\\displaystyle x\\\\cdot a^{x-1}\\\\cdot 0+a^{x}\\\\cdot \\\\left(1\\\\cdot \\\\log a+x\\\\cdot {\\\\frac {0}{a}}\\\\right).}\\n  Such a complicated expression is clearly not acceptable, and a procedure of simplification is needed as soon as one works with general expressions.\\nThis simplification is normally done through rewriting rules. There are several classes of rewriting rules that have to be considered. The simplest consists in the rewriting rules that always reduce the size of the expression, like E − E → 0 or sin(0) → 0. They are systematically applied in computer algebra systems.\\nThe first difficulty occurs with associative operations like addition and multiplication. The standard way to deal with associativity is to consider that addition and multiplication have an arbitrary number of operands, that is that a + b + c is represented as \"+\"(a, b, c). Thus a + (b + c) and (a + b) + c are both simplified to \"+\"(a, b, c), which is displayed a + b + c. What about a − b + c? To deal with this problem, the simplest way is to rewrite systematically −E, E − F, E/F as, respectively, (−1)⋅E, E + (−1)⋅F, E⋅F−1. In other words, in the internal representation of the expressions, there is no subtraction nor division nor unary minus, outside the representation of the numbers.\\nA second difficulty occurs with the commutativity of addition and multiplication. The problem is to recognize quickly the like terms in order to combine or cancel them. In fact, the method for finding like terms, consisting of testing every pair of terms, is too costly for being practicable with very long sums and products. For solving this problem, Macsyma sorts the operands of sums and products with a function of comparison that is designed in order that like terms are in consecutive places, and thus easily detected. In Maple, the hash function is designed for generating collisions when like terms are entered, allowing to combine them as soon as they are introduced. This design of the hash function allows also to recognize immediately the expressions or subexpressions that appear several times in a computation and to store them only once. This allows not only to save some memory space but also to speed up computation, by avoiding repetition of the same operations on several identical expressions.\\nSome rewriting rules sometimes increase and sometimes decrease the size of the expressions to which they are applied. This is the case of distributivity or trigonometric identities. For example, the distributivity law allows rewriting \\n  \\n    \\n      \\n        (\\n        x\\n        +\\n        1\\n        \\n          )\\n          \\n            4\\n          \\n        \\n        →\\n        \\n          x\\n          \\n            4\\n          \\n        \\n        +\\n        4\\n        \\n          x\\n          \\n            3\\n          \\n        \\n        +\\n        6\\n        \\n          x\\n          \\n            2\\n          \\n        \\n        +\\n        4\\n        x\\n        +\\n        1\\n      \\n    \\n    {\\\\displaystyle (x+1)^{4}\\\\rightarrow x^{4}+4x^{3}+6x^{2}+4x+1}\\n   and \\n  \\n    \\n      \\n        (\\n        x\\n        −\\n        1\\n        )\\n        (\\n        \\n          x\\n          \\n            4\\n          \\n        \\n        +\\n        \\n          x\\n          \\n            3\\n          \\n        \\n        +\\n        \\n          x\\n          \\n            2\\n          \\n        \\n        +\\n        x\\n        +\\n        1\\n        )\\n        →\\n        \\n          x\\n          \\n            5\\n          \\n        \\n        −\\n        1.\\n      \\n    \\n    {\\\\displaystyle (x-1)(x^{4}+x^{3}+x^{2}+x+1)\\\\rightarrow x^{5}-1.}\\n   As there is no way to make a good general choice of applying or not such a rewriting rule, such rewritings are done only when explicitly asked for by the user. For the distributivity, the computer function that applies this rewriting rule is generally called \"expand\". The reverse rewriting rule, called \"factor\", requires a non-trivial algorithm, which is thus a key function in computer algebra systems (see Polynomial factorization).\\n\\n\\n== Mathematical aspects ==\\nIn this section we consider some fundamental mathematical questions that arise as soon as one wants to manipulate mathematical expressions in a computer. We consider mainly the case of the multivariate rational fractions. This is not a real restriction, because, as soon as the irrational functions appearing in an expression are simplified, they are usually considered as new indeterminates. For example, \\n\\n  \\n    \\n      \\n        (\\n        sin\\n        \\u2061\\n        (\\n        x\\n        +\\n        y\\n        \\n          )\\n          \\n            2\\n          \\n        \\n        +\\n        log\\n        \\u2061\\n        (\\n        \\n          z\\n          \\n            2\\n          \\n        \\n        −\\n        5\\n        )\\n        \\n          )\\n          \\n            3\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle (\\\\sin(x+y)^{2}+\\\\log(z^{2}-5))^{3}}\\n  is viewed as a polynomial in \\n  \\n    \\n      \\n        sin\\n        \\u2061\\n        (\\n        x\\n        +\\n        y\\n        )\\n      \\n    \\n    {\\\\displaystyle \\\\sin(x+y)}\\n   and \\n  \\n    \\n      \\n        log\\n        \\u2061\\n        (\\n        \\n          z\\n          \\n            2\\n          \\n        \\n        −\\n        5\\n        )\\n      \\n    \\n    {\\\\displaystyle \\\\log(z^{2}-5)}\\n  \\n\\n\\n=== Equality ===\\nThere are two notions of equality for mathematical expressions. The syntactic equality is the equality of the expressions which means that they are written (or represented in a computer) in the same way. Being trivial, the syntactic equality is rarely considered by mathematicians, although it is the only equality that is easy to test with a program. The semantic equality is when two expressions represent the same mathematical object, like in\\n\\n  \\n    \\n      \\n        (\\n        x\\n        +\\n        y\\n        \\n          )\\n          \\n            2\\n          \\n        \\n        =\\n        \\n          x\\n          \\n            2\\n          \\n        \\n        +\\n        2\\n        x\\n        y\\n        +\\n        \\n          y\\n          \\n            2\\n          \\n        \\n        .\\n      \\n    \\n    {\\\\displaystyle (x+y)^{2}=x^{2}+2xy+y^{2}.}\\n  It is known from Richardson\\'s theorem that there may not exist an algorithm that decides if two expressions representing numbers are semantically equal, if exponentials and logarithms are allowed in the expressions. Therefore, (semantical) equality may be tested only on some classes of expressions such as the polynomials and rational fractions.\\nTo test the equality of two expressions, instead of designing specific algorithms, it is usual to put expressions in some canonical form or to put their difference in a normal form, and to test the syntactic equality of the result.\\nUnlike in usual mathematics, \"canonical form\" and \"normal form\" are not synonymous in computer algebra. A canonical form is such that two expressions in canonical form are semantically equal if and only if they are syntactically equal, while a normal form is such that an expression in normal form is semantically zero only if it is syntactically zero. In other words, zero has a unique representation by expressions in normal form.\\nNormal forms are usually preferred in computer algebra for several reasons. Firstly, canonical forms may be more costly to compute than normal forms. For example, to put a polynomial in canonical form, one has to expand by distributivity every product, while it is not necessary with a normal form (see below). Secondly, it may be the case, like for expressions involving radicals, that a canonical form, if it exists, depends on some arbitrary choices and that these choices may be different for two expressions that have been computed independently. This may make impracticable the use of a canonical form.\\n\\n\\n== History ==\\nAt the beginning of computer algebra, circa 1970, when the long-known algorithms were first put on computers, they turned out to be highly inefficient. Therefore, a large part of the work of the researchers in the field consisted in revisiting classical algebra in order to make it effective and to discover efficient algorithms to implement this effectiveness. A typical example of this kind of work is the computation of polynomial greatest common divisors, which is required to simplify fractions. Surprisingly, the classical Euclid\\'s algorithm turned out to be inefficient for polynomials over infinite fields, and thus new algorithms needed to be developed. The same was also true for the classical algorithms from linear algebra.\\n\\n\\n== See also ==\\nAutomated theorem prover\\nComputer-assisted proof\\nComputational algebraic geometry\\nComputer algebra system\\nProof checker\\nModel checker\\nSymbolic-numeric computation\\nSymbolic simulation\\nSymbolic artificial intelligence\\n\\n\\n== References ==\\n\\n\\n== Further reading ==\\nFor a detailed definition of the subject: \\n\\nSymbolic Computation (An Editorial), Bruno Buchberger, Journal of Symbolic Computation (1985) 1, pp. 1–6.For textbooks devoted to the subject:\\n\\nDavenport, James H.; Siret, Yvon; Tournier, Èvelyne (1988). Computer algebra: systems and algorithms for algebraic computation. Translated from the French by A. Davenport and J.H. Davenport. Academic Press. ISBN 978-0-12-204230-0.\\nvon zur Gathen, Joachim; Gerhard, Jürgen (2003). Modern computer algebra (second ed.). Cambridge University Press. ISBN 0-521-82646-2.\\nGeddes, K. O.; Czapor, S. R.; Labahn, G. (1992). \"Algorithms for Computer Algebra\". doi:10.1007/b102438. ISBN 978-0-7923-9259-0. \\nBuchberger, Bruno; Collins, George Edwin; Loos, Rüdiger; Albrecht, Rudolf, eds. (1983). \"Computer Algebra\". Computing Supplementa. 4. doi:10.1007/978-3-7091-7551-4. ISBN 978-3-211-81776-6.', 'In mathematics and computer science, computer algebra, also called symbolic computation or algebraic computation, is a scientific area that refers to the study and development of algorithms and software for manipulating mathematical expressions and other mathematical objects. Although computer algebra could be considered a subfield of scientific computing, they are generally considered as distinct fields because scientific computing is usually based on numerical computation with approximate floating point numbers, while symbolic computation emphasizes exact computation with expressions containing variables that have no given value and are manipulated as symbols.\\nSoftware applications that perform symbolic calculations are called computer algebra systems, with the term system alluding to the complexity of the main applications  that include, at least, a method to represent mathematical data in a computer, a user programming language (usually different from the language used for the implementation), a dedicated memory manager, a user interface for the input/output of mathematical expressions, a large set of routines to perform usual operations, like simplification of expressions, differentiation using chain rule, polynomial factorization, indefinite integration, etc.\\nComputer algebra is widely used to experiment in mathematics and to design the formulas that are used in numerical programs. It is also used for complete scientific computations, when purely numerical methods fail, as in public key cryptography, or for some non-linear problems.\\n\\n\\n== Terminology ==\\nSome authors distinguish computer algebra from symbolic computation using the latter name to refer to kinds of symbolic computation other than the computation with mathematical formulas. Some authors use symbolic computation for the computer science aspect of the subject and \"computer algebra\" for the mathematical aspect. In some languages the name of the field is not a direct translation of its English name. Typically, it is called calcul formel in French, which means \"formal computation\". This name reflects the ties this field has with formal methods.\\nSymbolic computation has also been referred to, in the past, as symbolic manipulation, algebraic manipulation, symbolic processing, symbolic mathematics, or symbolic algebra, but these terms, which also refer to non-computational manipulation, are no longer used in reference to computer algebra.\\n\\n\\n== Scientific community ==\\nThere is no learned society that is specific to computer algebra, but this function is assumed by the special interest group of the Association for Computing Machinery named SIGSAM (Special Interest Group\\non Symbolic and Algebraic Manipulation).There are several annual conferences on computer algebra, the premier being ISSAC (International Symposium on Symbolic and Algebraic Computation), which is regularly sponsored by SIGSAM.There are several journals specializing in computer algebra, the top one being Journal of Symbolic Computation founded in 1985 by Bruno Buchberger. There are also several other journals that regularly publish articles in computer algebra.\\n\\n\\n== Computer science aspects ==\\n\\n\\n=== Data representation ===\\nAs numerical software is highly efficient for approximate numerical computation, it is common, in computer algebra, to emphasize exact computation with exactly represented data. Such an exact representation implies that, even when the size of the output is small, the intermediate data generated during a computation may grow in an unpredictable way. This behavior is called expression swell. To obviate this problem, various methods are used in the representation of the data, as well as in the algorithms that manipulate them.\\n\\n\\n==== Numbers ====\\nThe usual numbers systems used in numerical computation are floating point numbers and integers of a fixed bounded size. None of these is convenient for computer algebra, due to expression swell.Therefore, the basic numbers used in computer algebra are the integers of the mathematicians, commonly represented by an unbounded signed sequence of digits in some base of numeration, usually the largest base allowed by the machine word. These integers allow to define the rational numbers, which are irreducible fractions of two integers.\\nProgramming an efficient implementation of the arithmetic operations is a hard task. Therefore, most free computer algebra systems and some commercial ones such as Mathematica and Maple (software), use the GMP library, which is thus a de facto standard.\\n\\n\\n==== Expressions ====\\n\\nExcept for numbers and variables, every mathematical expression may be viewed as the symbol of an operator followed by a sequence of operands. In computer algebra software, the expressions are usually represented in this way. This representation is very flexible, and many things that seem not to be mathematical expressions at first glance, may be represented and manipulated as such. For example, an equation is an expression with “=” as an operator, a matrix may be represented as an expression with “matrix” as an operator and its rows as operands.\\nEven programs may be considered and represented as expressions with operator “procedure” and, at least, two operands, the list of parameters and the body, which is itself an expression with “body” as an operator and a sequence of instructions as operands. Conversely, any mathematical expression may be viewed as a program. For example, the expression a + b may be viewed as a program for the addition, with a and b as parameters. Executing this program consists in evaluating the expression for given values of a and b; if they do not have any value—that is they are indeterminates—, the result of the evaluation is simply its input.\\nThis process of delayed evaluation is fundamental in computer algebra. For example, the operator “=” of the equations is also, in most computer algebra systems, the name of the program of the equality test: normally, the evaluation of an equation results in an equation, but, when an equality test is needed,—either explicitly asked by the user through an “evaluation to a Boolean” command, or automatically started by the system in the case of a test inside a program—then the evaluation to a boolean 0 or 1 is executed.\\nAs the size of the operands of an expression is unpredictable and may change during a working session, the sequence of the operands is usually represented as a sequence of either pointers (like in Macsyma) or entries in a hash table (like in Maple).\\n\\n\\n=== Simplification ===\\nThe raw application of the basic rules of differentiation with respect to x on the expression \\n  \\n    \\n      \\n        \\n          a\\n          \\n            x\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle a^{x}}\\n   gives the result \\n\\n  \\n    \\n      \\n        x\\n        ⋅\\n        \\n          a\\n          \\n            x\\n            −\\n            1\\n          \\n        \\n        ⋅\\n        0\\n        +\\n        \\n          a\\n          \\n            x\\n          \\n        \\n        ⋅\\n        \\n          (\\n          \\n            1\\n            ⋅\\n            log\\n            \\u2061\\n            a\\n            +\\n            x\\n            ⋅\\n            \\n              \\n                0\\n                a\\n              \\n            \\n          \\n          )\\n        \\n        .\\n      \\n    \\n    {\\\\displaystyle x\\\\cdot a^{x-1}\\\\cdot 0+a^{x}\\\\cdot \\\\left(1\\\\cdot \\\\log a+x\\\\cdot {\\\\frac {0}{a}}\\\\right).}\\n  Such a complicated expression is clearly not acceptable, and a procedure of simplification is needed as soon as one works with general expressions.\\nThis simplification is normally done through rewriting rules. There are several classes of rewriting rules that have to be considered. The simplest consists in the rewriting rules that always reduce the size of the expression, like E − E → 0 or sin(0) → 0. They are systematically applied in computer algebra systems.\\nThe first difficulty occurs with associative operations like addition and multiplication. The standard way to deal with associativity is to consider that addition and multiplication have an arbitrary number of operands, that is that a + b + c is represented as \"+\"(a, b, c). Thus a + (b + c) and (a + b) + c are both simplified to \"+\"(a, b, c), which is displayed a + b + c. What about a − b + c? To deal with this problem, the simplest way is to rewrite systematically −E, E − F, E/F as, respectively, (−1)⋅E, E + (−1)⋅F, E⋅F−1. In other words, in the internal representation of the expressions, there is no subtraction nor division nor unary minus, outside the representation of the numbers.\\nA second difficulty occurs with the commutativity of addition and multiplication. The problem is to recognize quickly the like terms in order to combine or cancel them. In fact, the method for finding like terms, consisting of testing every pair of terms, is too costly for being practicable with very long sums and products. For solving this problem, Macsyma sorts the operands of sums and products with a function of comparison that is designed in order that like terms are in consecutive places, and thus easily detected. In Maple, the hash function is designed for generating collisions when like terms are entered, allowing to combine them as soon as they are introduced. This design of the hash function allows also to recognize immediately the expressions or subexpressions that appear several times in a computation and to store them only once. This allows not only to save some memory space but also to speed up computation, by avoiding repetition of the same operations on several identical expressions.\\nSome rewriting rules sometimes increase and sometimes decrease the size of the expressions to which they are applied. This is the case of distributivity or trigonometric identities. For example, the distributivity law allows rewriting \\n  \\n    \\n      \\n        (\\n        x\\n        +\\n        1\\n        \\n          )\\n          \\n            4\\n          \\n        \\n        →\\n        \\n          x\\n          \\n            4\\n          \\n        \\n        +\\n        4\\n        \\n          x\\n          \\n            3\\n          \\n        \\n        +\\n        6\\n        \\n          x\\n          \\n            2\\n          \\n        \\n        +\\n        4\\n        x\\n        +\\n        1\\n      \\n    \\n    {\\\\displaystyle (x+1)^{4}\\\\rightarrow x^{4}+4x^{3}+6x^{2}+4x+1}\\n   and \\n  \\n    \\n      \\n        (\\n        x\\n        −\\n        1\\n        )\\n        (\\n        \\n          x\\n          \\n            4\\n          \\n        \\n        +\\n        \\n          x\\n          \\n            3\\n          \\n        \\n        +\\n        \\n          x\\n          \\n            2\\n          \\n        \\n        +\\n        x\\n        +\\n        1\\n        )\\n        →\\n        \\n          x\\n          \\n            5\\n          \\n        \\n        −\\n        1.\\n      \\n    \\n    {\\\\displaystyle (x-1)(x^{4}+x^{3}+x^{2}+x+1)\\\\rightarrow x^{5}-1.}\\n   As there is no way to make a good general choice of applying or not such a rewriting rule, such rewritings are done only when explicitly asked for by the user. For the distributivity, the computer function that applies this rewriting rule is generally called \"expand\". The reverse rewriting rule, called \"factor\", requires a non-trivial algorithm, which is thus a key function in computer algebra systems (see Polynomial factorization).\\n\\n\\n== Mathematical aspects ==\\nIn this section we consider some fundamental mathematical questions that arise as soon as one wants to manipulate mathematical expressions in a computer. We consider mainly the case of the multivariate rational fractions. This is not a real restriction, because, as soon as the irrational functions appearing in an expression are simplified, they are usually considered as new indeterminates. For example, \\n\\n  \\n    \\n      \\n        (\\n        sin\\n        \\u2061\\n        (\\n        x\\n        +\\n        y\\n        \\n          )\\n          \\n            2\\n          \\n        \\n        +\\n        log\\n        \\u2061\\n        (\\n        \\n          z\\n          \\n            2\\n          \\n        \\n        −\\n        5\\n        )\\n        \\n          )\\n          \\n            3\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle (\\\\sin(x+y)^{2}+\\\\log(z^{2}-5))^{3}}\\n  is viewed as a polynomial in \\n  \\n    \\n      \\n        sin\\n        \\u2061\\n        (\\n        x\\n        +\\n        y\\n        )\\n      \\n    \\n    {\\\\displaystyle \\\\sin(x+y)}\\n   and \\n  \\n    \\n      \\n        log\\n        \\u2061\\n        (\\n        \\n          z\\n          \\n            2\\n          \\n        \\n        −\\n        5\\n        )\\n      \\n    \\n    {\\\\displaystyle \\\\log(z^{2}-5)}\\n  \\n\\n\\n=== Equality ===\\nThere are two notions of equality for mathematical expressions. The syntactic equality is the equality of the expressions which means that they are written (or represented in a computer) in the same way. Being trivial, the syntactic equality is rarely considered by mathematicians, although it is the only equality that is easy to test with a program. The semantic equality is when two expressions represent the same mathematical object, like in\\n\\n  \\n    \\n      \\n        (\\n        x\\n        +\\n        y\\n        \\n          )\\n          \\n            2\\n          \\n        \\n        =\\n        \\n          x\\n          \\n            2\\n          \\n        \\n        +\\n        2\\n        x\\n        y\\n        +\\n        \\n          y\\n          \\n            2\\n          \\n        \\n        .\\n      \\n    \\n    {\\\\displaystyle (x+y)^{2}=x^{2}+2xy+y^{2}.}\\n  It is known from Richardson\\'s theorem that there may not exist an algorithm that decides if two expressions representing numbers are semantically equal, if exponentials and logarithms are allowed in the expressions. Therefore, (semantical) equality may be tested only on some classes of expressions such as the polynomials and rational fractions.\\nTo test the equality of two expressions, instead of designing specific algorithms, it is usual to put expressions in some canonical form or to put their difference in a normal form, and to test the syntactic equality of the result.\\nUnlike in usual mathematics, \"canonical form\" and \"normal form\" are not synonymous in computer algebra. A canonical form is such that two expressions in canonical form are semantically equal if and only if they are syntactically equal, while a normal form is such that an expression in normal form is semantically zero only if it is syntactically zero. In other words, zero has a unique representation by expressions in normal form.\\nNormal forms are usually preferred in computer algebra for several reasons. Firstly, canonical forms may be more costly to compute than normal forms. For example, to put a polynomial in canonical form, one has to expand by distributivity every product, while it is not necessary with a normal form (see below). Secondly, it may be the case, like for expressions involving radicals, that a canonical form, if it exists, depends on some arbitrary choices and that these choices may be different for two expressions that have been computed independently. This may make impracticable the use of a canonical form.\\n\\n\\n== History ==\\nAt the beginning of computer algebra, circa 1970, when the long-known algorithms were first put on computers, they turned out to be highly inefficient. Therefore, a large part of the work of the researchers in the field consisted in revisiting classical algebra in order to make it effective and to discover efficient algorithms to implement this effectiveness. A typical example of this kind of work is the computation of polynomial greatest common divisors, which is required to simplify fractions. Surprisingly, the classical Euclid\\'s algorithm turned out to be inefficient for polynomials over infinite fields, and thus new algorithms needed to be developed. The same was also true for the classical algorithms from linear algebra.\\n\\n\\n== See also ==\\nAutomated theorem prover\\nComputer-assisted proof\\nComputational algebraic geometry\\nComputer algebra system\\nProof checker\\nModel checker\\nSymbolic-numeric computation\\nSymbolic simulation\\nSymbolic artificial intelligence\\n\\n\\n== References ==\\n\\n\\n== Further reading ==\\nFor a detailed definition of the subject: \\n\\nSymbolic Computation (An Editorial), Bruno Buchberger, Journal of Symbolic Computation (1985) 1, pp. 1–6.For textbooks devoted to the subject:\\n\\nDavenport, James H.; Siret, Yvon; Tournier, Èvelyne (1988). Computer algebra: systems and algorithms for algebraic computation. Translated from the French by A. Davenport and J.H. Davenport. Academic Press. ISBN 978-0-12-204230-0.\\nvon zur Gathen, Joachim; Gerhard, Jürgen (2003). Modern computer algebra (second ed.). Cambridge University Press. ISBN 0-521-82646-2.\\nGeddes, K. O.; Czapor, S. R.; Labahn, G. (1992). \"Algorithms for Computer Algebra\". doi:10.1007/b102438. ISBN 978-0-7923-9259-0. \\nBuchberger, Bruno; Collins, George Edwin; Loos, Rüdiger; Albrecht, Rudolf, eds. (1983). \"Computer Algebra\". Computing Supplementa. 4. doi:10.1007/978-3-7091-7551-4. ISBN 978-3-211-81776-6.', 'Computational physics is the study and implementation of numerical analysis to solve problems in physics for which a quantitative theory already exists. Historically, computational physics was the first application of modern computers in science, and is now a subset of computational science. It is sometimes regarded as a subdiscipline (or offshoot) of theoretical physics, but others consider it an intermediate branch between theoretical and experimental physics - an area of study which supplements both theory and experiment.\\n\\n\\n== Overview ==\\n\\nIn physics, different theories based on mathematical models provide very precise predictions on how systems behave. Unfortunately, it is often the case that solving the mathematical model for a particular system in order to produce a useful prediction is not feasible. This can occur, for instance, when the solution does not have a closed-form expression, or is too complicated. In such cases, numerical approximations are required. Computational physics is the subject that deals with these numerical approximations: the approximation of the solution is written as a finite (and typically large) number of simple mathematical operations (algorithm), and a computer is used to perform these operations and compute an approximated solution and respective error.\\n\\n\\n=== Status in physics ===\\nThere is a debate about the status of computation within the scientific method. Sometimes it is regarded as more akin to theoretical physics; some others regard computer simulation as \"computer experiments\", yet still others consider it an intermediate or different branch between theoretical and experimental physics, a third way that supplements theory and experiment. While computers can be used in experiments for the measurement and recording (and storage) of data, this clearly does not constitute a computational approach.\\n\\n\\n== Challenges in computational physics ==\\nComputational physics problems are in general very difficult to solve exactly. This is due to several (mathematical) reasons: lack of algebraic and/or analytic solvability, complexity, and chaos. For example, - even apparently simple problems, such as calculating the wavefunction of an electron orbiting an atom in a strong electric field (Stark effect), may require great effort to formulate a practical algorithm (if one can be found); other cruder or brute-force techniques, such as graphical methods or root finding, may be required. On the more advanced side, mathematical perturbation theory is also sometimes used (a working is shown for this particular example here). In addition, the computational cost and computational complexity for many-body problems (and their classical counterparts) tend to grow quickly. A macroscopic system typically has a size of the order of \\n  \\n    \\n      \\n        \\n          10\\n          \\n            23\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle 10^{23}}\\n   constituent particles, so it is somewhat of a problem. Solving quantum mechanical problems is generally of exponential order in the size of the system and for classical N-body it is of order N-squared. Finally, many physical systems are inherently nonlinear at best, and at worst chaotic: this means it can be difficult to ensure any numerical errors do not grow to the point of rendering the \\'solution\\' useless.\\n\\n\\n== Methods and algorithms ==\\nBecause computational physics uses a broad class of problems, it is generally divided amongst the different mathematical problems it numerically solves, or the methods it applies. Between them, one can consider:\\n\\nroot finding (using e.g. Newton-Raphson method)\\nsystem of linear equations (using e.g. LU decomposition)\\nordinary differential equations (using e.g. Runge–Kutta methods)\\nintegration (using e.g. Romberg method and Monte Carlo integration)\\npartial differential equations (using e.g. finite difference method and relaxation method)\\nmatrix eigenvalue problem (using e.g. Jacobi eigenvalue algorithm and power iteration)All these methods (and several others) are used to calculate physical properties of the modeled systems.\\nComputational physics also borrows a number of ideas from computational chemistry - for example, the density functional theory used by computational solid state physicists to calculate properties of solids is basically the same as that used by chemists to calculate the properties of molecules.\\nFurthermore, computational physics encompasses the tuning of the software/hardware structure to solve the problems (as the problems usually can be very large, in processing power need or in memory requests).\\n\\n\\n== Divisions ==\\nIt is possible to find a corresponding computational branch for every major field in physics, for example computational mechanics and computational electrodynamics. Computational mechanics consists of computational fluid dynamics (CFD), computational solid mechanics and computational contact mechanics. One subfield at the confluence between CFD and electromagnetic modelling is computational magnetohydrodynamics. The quantum many-body problem leads naturally to the large and rapidly growing field of computational chemistry.\\nComputational solid state physics is a very important division of computational physics dealing directly with material science.\\nA field related to computational condensed matter is computational statistical mechanics, which deals with the simulation of models and theories (such as percolation and spin models) that are difficult to solve otherwise. Computational statistical physics makes heavy use of Monte Carlo-like methods. More broadly, (particularly through the use of agent based modeling and cellular automata) it also concerns itself with  (and finds application in, through the use of its techniques) in the social sciences, network theory, and mathematical models for the propagation of disease (most notably, the SIR Model) and the spread of forest fires.\\nNumerical relativity is a (relatively) new field interested in finding numerical solutions to the field equations of general (and special) relativity, and computational particle physics deals with problems motivated by particle physics.\\nComputational astrophysics is the application of these techniques and methods to astrophysical problems and phenomena.\\nComputational biophysics is a branch of biophysics and computational biology itself, applying methods of computer science and physics to large complex biological problems.\\n\\n\\n== Applications ==\\nDue to the broad class of problems computational physics deals, it is an essential component of modern research in different areas of physics, namely: accelerator physics, astrophysics, fluid mechanics (computational fluid dynamics), lattice field theory/lattice gauge theory (especially lattice quantum chromodynamics), plasma physics (see plasma modeling), simulating physical systems (using e.g. molecular dynamics), nuclear engineering computer codes, protein structure prediction, weather prediction, solid state physics, soft condensed matter physics, hypervelocity impact physics etc.\\nComputational solid state physics, for example, uses density functional theory to calculate properties of solids, a method similar to that used by chemists to study molecules.  Other quantities of interest in solid state physics, such as the electronic band structure, magnetic properties and charge densities can be calculated by this and several methods, including the Luttinger-Kohn/k.p method and ab-initio methods.\\n\\n\\n== See also ==\\nAdvanced Simulation Library\\nCECAM - Centre européen de calcul atomique et moléculaire\\nDivision of Computational Physics (DCOMP) of the American Physical Society\\nImportant publications in computational physics\\nMathematical and theoretical physics\\nOpen Source Physics, computational physics libraries and pedagogical tools\\nTimeline of computational physics\\nCar–Parrinello molecular dynamics\\n\\n\\n== References ==\\n\\n\\n== Further reading ==\\nA.K. Hartmann, Practical Guide to Computer Simulations, World Scientific (2009)\\nInternational Journal of Modern Physics C (IJMPC): Physics and Computers, World Scientific\\nSteven E. Koonin, Computational Physics, Addison-Wesley (1986)\\nT. Pang, An Introduction to Computational Physics, Cambridge University Press (2010)\\nB. Stickler, E. Schachinger, Basic concepts in computational physics, Springer Verlag (2013). ISBN 9783319024349.\\nE. Winsberg, Science in the Age of Computer Simulation. Chicago: University of Chicago Press, 2010.\\n\\n\\n== External links ==\\nC20 IUPAP Commission on Computational Physics\\nAmerican Physical Society: Division of Computational Physics\\nInstitute of Physics: Computational Physics Group\\nSciDAC: Scientific Discovery through Advanced Computing\\nOpen Source Physics\\nSCINET Scientific Software Framework\\nComputational Physics Course with youtube videos', 'Computational chemistry is a branch of chemistry that uses computer simulation to assist in solving chemical problems. It uses methods of theoretical chemistry, incorporated into computer programs, to calculate the structures and properties of molecules, groups of molecules, and solids. It is necessary because, apart from relatively recent results concerning the hydrogen molecular ion (dihydrogen cation, see references therein for more details), the quantum many-body problem cannot be solved analytically, much less in closed form.  While computational results normally complement the information obtained by chemical experiments, it can in some cases predict hitherto unobserved chemical phenomena. It is widely used in the design of new drugs and materials.\\nExamples of such properties are structure (i.e., the expected positions of the constituent atoms), absolute and relative (interaction) energies, electronic charge density distributions, dipoles and higher multipole moments, vibrational frequencies, reactivity, or other spectroscopic quantities, and cross sections for collision with other particles.\\nThe methods used cover both static and dynamic situations. In all cases, the computer time and other resources (such as memory and disk space) increase rapidly with the size of the system being studied.  That system can be one molecule, a group of molecules, or a solid.  Computational chemistry methods range from very approximate to highly accurate; the latter are usually feasible for small systems only. Ab initio methods are based entirely on quantum mechanics and basic physical constants. Other methods are called empirical or semi-empirical because they use additional empirical parameters.\\nBoth ab initio and semi-empirical approaches involve approximations.  These range from simplified forms of the first-principles equations that are easier or faster to solve, to approximations limiting the size of the system (for example, periodic boundary conditions), to fundamental approximations to the underlying equations that are required to achieve any solution to them at all.  For example, most ab initio calculations make the Born–Oppenheimer approximation, which greatly simplifies the underlying Schrödinger equation by assuming that the nuclei remain in place during the calculation.  In principle, ab initio methods eventually converge to the exact solution of the underlying equations as the number of approximations is reduced.  In practice, however, it is impossible to eliminate all approximations, and residual error inevitably remains.  The goal of computational chemistry is to minimize this residual error while keeping the calculations tractable.\\nIn some cases, the details of electronic structure are less important than the long-time phase space behavior of molecules. This is the case in conformational studies of proteins and protein-ligand binding thermodynamics. Classical approximations to the potential energy surface are used, typically with molecular mechanics force fields, as they are computationally less intensive than electronic calculations, to enable longer simulations of molecular dynamics. Furthermore, cheminformatics uses even more empirical (and computationally cheaper) methods like machine learning based on physicochemical properties. One typical problem in cheminformatics is to predict the binding affinity of drug molecules to a given target. Other problems include predicting binding specificity, off-target effects, toxicity, and pharmacokinetic properties.\\n\\n\\n== History ==\\nBuilding on the founding discoveries and theories in the history of quantum mechanics, the first theoretical calculations in chemistry were those of Walter Heitler and Fritz London in 1927, using valence bond theory. The books that were influential in the early development of computational quantum chemistry include Linus Pauling and E. Bright Wilson\\'s 1935 Introduction to Quantum Mechanics – with Applications to Chemistry, Eyring, Walter and Kimball\\'s 1944 Quantum Chemistry, Heitler\\'s 1945 Elementary Wave Mechanics – with Applications to Quantum Chemistry, and later Coulson\\'s 1952 textbook Valence, each of which served as primary references for chemists in the decades to follow.\\nWith the development of efficient computer technology in the 1940s, the solutions of elaborate wave equations for complex atomic systems began to be a realizable objective.  In the early 1950s, the first semi-empirical atomic orbital calculations were performed. Theoretical chemists became extensive users of the early digital computers. One major advance came with the 1951 paper in Reviews of Modern Physics by Clemens C. J. Roothaan in 1951, largely on the \"LCAO MO\" approach (Linear Combination of Atomic Orbitals Molecular Orbitals), for many years the second-most cited paper in that journal. A very detailed account of such use in the United Kingdom is given by Smith and Sutcliffe. The first ab initio Hartree–Fock method calculations on diatomic molecules were performed in 1956 at MIT, using a basis set of Slater orbitals. For diatomic molecules, a systematic study using a minimum basis set and the first calculation with a larger basis set were published by Ransil and Nesbet respectively in 1960. The first polyatomic calculations using Gaussian orbitals were performed in the late 1950s. The first configuration interaction calculations were performed in Cambridge on the EDSAC computer in the 1950s using Gaussian orbitals by Boys and coworkers. By 1971, when a bibliography of ab initio calculations was published, the largest molecules included were naphthalene and azulene. Abstracts of many earlier developments in ab initio theory have been published by Schaefer.In 1964, Hückel method calculations (using a simple linear combination of atomic orbitals (LCAO) method to determine electron energies of molecular orbitals of π electrons in conjugated hydrocarbon systems) of molecules, ranging in complexity from butadiene and benzene to ovalene, were generated on computers at Berkeley and Oxford. These empirical methods were replaced in the 1960s by semi-empirical methods such as CNDO.In the early 1970s, efficient ab initio computer programs such as ATMOL, Gaussian, IBMOL, and POLYAYTOM, began to be used to speed ab initio calculations of molecular orbitals. Of these four programs, only Gaussian, now vastly expanded, is still in use, but many other programs are now in use. At the same time, the methods of molecular mechanics, such as MM2 force field, were developed, primarily by Norman Allinger.One of the first mentions of the term computational chemistry can be found in the 1970 book Computers and Their Role in the Physical Sciences by Sidney Fernbach and Abraham Haskell Taub, where they state \"It seems, therefore, that \\'computational chemistry\\' can finally be more and more of a reality.\" During the 1970s, widely different methods began to be seen as part of a new emerging discipline of computational chemistry. The Journal of Computational Chemistry was first published in 1980.\\nComputational chemistry has featured in several Nobel Prize awards, most notably in 1998 and 2013. Walter Kohn, \"for his development of the density-functional theory\", and John Pople, \"for his development of computational methods in quantum chemistry\", received the 1998 Nobel Prize in Chemistry. Martin Karplus, Michael Levitt and Arieh Warshel received the 2013 Nobel Prize in Chemistry for \"the development of multiscale models for complex chemical systems\".\\n\\n\\n== Fields of application ==\\nThe term theoretical chemistry may be defined as a mathematical description of chemistry, whereas computational chemistry is usually used when a mathematical method is sufficiently well developed that it can be automated for implementation on a computer. In theoretical chemistry, chemists, physicists, and mathematicians develop algorithms and computer programs to predict atomic and molecular properties and reaction paths for chemical reactions. Computational chemists, in contrast, may simply apply existing computer programs and methodologies to specific chemical questions.\\nComputational chemistry has two different aspects:\\n\\nComputational studies, used to find a starting point for a laboratory synthesis, or to assist in understanding experimental data, such as the position and source of spectroscopic peaks.\\nComputational studies, used to predict the possibility of so far entirely unknown molecules or to explore reaction mechanisms not readily studied via experiments.Thus, computational chemistry can assist the experimental chemist or it can challenge the experimental chemist to find entirely new chemical objects.\\nSeveral major areas may be distinguished within computational chemistry:\\n\\nThe prediction of the molecular structure of molecules by the use of the simulation of forces, or more accurate quantum chemical methods, to find stationary points on the energy surface as the position of the nuclei is varied.\\nStoring and searching for data on chemical entities (see chemical databases).\\nIdentifying correlations between chemical structures and properties (see quantitative structure–property relationship (QSPR) and quantitative structure–activity relationship (QSAR)).\\nComputational approaches to help in the efficient synthesis of compounds.\\nComputational approaches to design molecules that interact in specific ways with other molecules (e.g. drug design and catalysis).\\n\\n\\n== Accuracy ==\\nComputational chemistry is not an exact description of real-life chemistry, as our mathematical models of the physical laws of nature can only provide us with an approximation. However, the majority of chemical phenomena can be described to a certain degree in a qualitative or approximate quantitative computational scheme. \\nMolecules consist of nuclei and electrons, so the methods of quantum mechanics apply. Computational chemists often attempt to solve the non-relativistic Schrödinger equation, with relativistic corrections added, although some progress has been made in solving the fully relativistic Dirac equation. In principle, it is possible to solve the Schrödinger equation in either its time-dependent or time-independent form, as appropriate for the problem in hand; in practice, this is not possible except for very small systems. Therefore, a great number of approximate methods strive to achieve the best trade-off between accuracy and computational cost.\\nAccuracy can always be improved with greater computational cost. Significant errors can present themselves in ab initio models comprising many electrons, due to the computational cost of full relativistic-inclusive methods. This complicates the study of molecules interacting with high atomic mass unit atoms, such as transitional metals and their catalytic properties. Present algorithms in computational chemistry can routinely calculate the properties of small molecules that contain up to about 40 electrons with errors for energies less than a few kJ/mol. For geometries, bond lengths can be predicted within a few picometers and bond angles within 0.5 degrees. The treatment of larger molecules that contain a few dozen atoms is computationally tractable by more approximate methods such as density functional theory (DFT).\\nThere is some dispute within the field whether or not the latter methods are sufficient to describe complex chemical reactions, such as those in biochemistry. Large molecules can be studied by semi-empirical approximate methods. Even larger molecules are treated by classical mechanics methods that use what are called molecular mechanics (MM).  In QM-MM methods, small parts of large complexes are treated quantum mechanically (QM), and the remainder is treated approximately (MM).\\n\\n\\n== Methods ==\\nOne molecular formula can represent more than one molecular isomer: a set of isomers. Each isomer is a local minimum on the energy surface (called the potential energy surface) created from the total energy (i.e., the electronic energy, plus the repulsion energy between the nuclei) as a function of the coordinates of all the nuclei. A stationary point is a geometry such that the derivative of the energy with respect to all displacements of the nuclei is zero. A local (energy) minimum is a stationary point where all such displacements lead to an increase in energy. The local minimum that is lowest is called the global minimum and corresponds to the most stable isomer. If there is one particular coordinate change that leads to a decrease in the total energy in both directions, the stationary point is a transition structure and the coordinate is the reaction coordinate. This process of determining stationary points is called geometry optimization.\\nThe determination of molecular structure by geometry optimization became routine only after efficient methods for calculating the first derivatives of the energy with respect to all atomic coordinates became available. Evaluation of the related second derivatives allows the prediction of vibrational frequencies if harmonic motion is estimated. More importantly, it allows for the characterization of stationary points. The frequencies are related to the eigenvalues of the Hessian matrix, which contains second derivatives. If the eigenvalues are all positive, then the frequencies are all real and the stationary point is a local minimum. If one eigenvalue is negative (i.e., an imaginary frequency), then the stationary point is a transition structure. If more than one eigenvalue is negative, then the stationary point is a more complex one, and is usually of little interest. When one of these is found, it is necessary to move the search away from it if the experimenter is looking solely for local minima and transition structures.\\nThe total energy is determined by approximate solutions of the time-dependent Schrödinger equation, usually with no relativistic terms included, and by making use of the Born–Oppenheimer approximation, which allows for the separation of electronic and nuclear motions, thereby simplifying the Schrödinger equation. This leads to the evaluation of the total energy as a sum of the electronic energy at fixed nuclei positions and the repulsion energy of the nuclei. A notable exception are certain approaches called direct quantum chemistry, which treat electrons and nuclei on a common footing. Density functional methods and semi-empirical methods are variants on the major theme. For very large systems, the relative total energies can be compared using molecular mechanics. The ways of determining the total energy to predict molecular structures are:\\n\\n\\n=== Ab initio methods ===\\n\\nThe programs used in computational chemistry are based on many different quantum-chemical methods that solve the molecular Schrödinger equation associated with the molecular Hamiltonian. Methods that do not include any empirical or semi-empirical parameters in their equations – being derived directly from theoretical principles, with no inclusion of experimental data – are called ab initio methods. This does not imply that the solution is an exact one; they are all approximate quantum mechanical calculations. It means that a particular approximation is rigorously defined on first principles (quantum theory) and then solved within an error margin that is qualitatively known beforehand. If numerical iterative methods must be used, the aim is to iterate until full machine accuracy is obtained (the best that is possible with a finite word length on the computer, and within the mathematical and/or physical approximations made).\\n\\nThe simplest type of ab initio electronic structure calculation is the Hartree–Fock method (HF), an extension of molecular orbital theory, in which the correlated electron-electron repulsion is not specifically taken into account; only its average effect is included in the calculation. As the basis set size is increased, the energy and wave function tend towards a limit called the Hartree–Fock limit. Many types of calculations (termed post-Hartree–Fock methods) begin with a Hartree–Fock calculation and subsequently correct for electron-electron repulsion, referred to also as electronic correlation. As these methods are pushed to the limit, they approach the exact solution of the non-relativistic Schrödinger equation. To obtain exact agreement with experiment, it is necessary to include relativistic and spin orbit terms, both of which are far more important for heavy atoms. In all of these approaches, along with choice of method, it is necessary to choose a basis set. This is a set of functions, usually centered on the different atoms in the molecule, which are used to expand the molecular orbitals with the linear combination of atomic orbitals (LCAO) molecular orbital method ansatz. Ab initio methods need to define a level of theory (the method) and a basis set.\\nThe Hartree–Fock wave function is a single configuration or determinant. In some cases, particularly for bond breaking processes, this is inadequate, and several configurations must be used. Here, the coefficients of the configurations, and of the basis functions, are optimized together.\\nThe total molecular energy can be evaluated as a function of the molecular geometry; in other words, the potential energy surface. Such a surface can be used for reaction dynamics. The stationary points of the surface lead to predictions of different isomers and the transition structures for conversion between isomers, but these can be determined without a full knowledge of the complete surface.\\nA particularly important objective, called computational thermochemistry, is to calculate thermochemical quantities such as the enthalpy of formation to chemical accuracy. Chemical accuracy is the accuracy required to make realistic chemical predictions and is generally considered to be 1 kcal/mol or 4 kJ/mol. To reach that accuracy in an economic way it is necessary to use a series of post-Hartree–Fock methods and combine the results. These methods are called quantum chemistry composite methods.\\n\\n\\n=== Density functional methods ===\\n\\nDensity functional theory (DFT) methods are often considered to be ab initio methods for determining the molecular electronic structure, even though many of the most common functionals use parameters derived from empirical data, or from more complex calculations. In DFT, the total energy is expressed in terms of the total one-electron density rather than the wave function. In this type of calculation, there is an approximate Hamiltonian and an approximate expression for the total electron density. DFT methods can be very accurate for little computational cost. Some methods combine the density functional exchange functional with the Hartree–Fock exchange term and are termed hybrid functional methods.\\n\\n\\n=== Semi-empirical methods ===\\n\\nSemi-empirical quantum chemistry methods are based on the Hartree–Fock method formalism, but make many approximations and obtain some parameters from empirical data. They were very important in computational chemistry from the 60s to the 90s, especially for treating large molecules where the full Hartree–Fock method without the approximations were too costly. The use of empirical parameters appears to allow some inclusion of correlation effects into the methods.\\nPrimitive semi-empirical methods were designed even before, where the two-electron part of the Hamiltonian is not explicitly included. For π-electron systems, this was the Hückel method proposed by Erich Hückel, and for all valence electron systems, the extended Hückel method proposed by Roald Hoffmann. Sometimes, Hückel methods are referred to as \"completely emprirical\" because they do not derive from a Hamiltonian.Yet, the term \"empirical methods\", or \"empirical force fields\" is usually used to describe Molecular Mechanics.\\n\\n\\n=== Molecular mechanics ===\\n\\nIn many cases, large molecular systems can be modeled successfully while avoiding quantum mechanical calculations entirely. Molecular mechanics simulations, for example, use one classical expression for the energy of a compound, for instance the harmonic oscillator. All constants appearing in the equations must be obtained beforehand from experimental data or ab initio calculations.\\nThe database of compounds used for parameterization, i.e., the resulting set of parameters and functions is called the force field, is crucial to the success of molecular mechanics calculations. A force field parameterized against a specific class of molecules, for instance proteins, would be expected to only have any relevance when describing other molecules of the same class.\\nThese methods can be applied to proteins and other large biological molecules, and allow studies of the approach and interaction (docking) of potential drug molecules.\\n\\n\\n=== Methods for solids ===\\n\\nComputational chemical methods can be applied to solid state physics problems. The electronic structure of a crystal is in general described by a band structure, which defines the energies of electron orbitals for each point in the Brillouin zone. Ab initio and semi-empirical calculations yield orbital energies; therefore, they can be applied to band structure calculations. Since it is time-consuming to calculate the energy for a molecule, it is even more time-consuming to calculate them for the entire list of points in the Brillouin zone.\\n\\n\\n=== Chemical dynamics ===\\nOnce the electronic and nuclear variables are separated (within the Born–Oppenheimer representation), in the time-dependent approach, the wave packet corresponding to the nuclear degrees of freedom is propagated via the time evolution operator (physics) associated to the time-dependent Schrödinger equation (for the full molecular Hamiltonian). In the complementary energy-dependent approach, the time-independent Schrödinger equation is solved using the scattering theory formalism. The potential representing the interatomic interaction is given by the potential energy surfaces. In general, the potential energy surfaces are coupled via the vibronic coupling terms.\\nThe most popular methods for propagating the wave packet associated to the molecular geometry are:\\n\\nthe split operator technique,\\nthe Chebyshev (real) polynomial,\\nthe multi-configuration time-dependent Hartree method (MCTDH),\\nthe semiclassical method.\\n\\n\\n=== Molecular dynamics ===\\n\\nMolecular dynamics (MD) use either quantum mechanics, molecular mechanics or a mixture of both to calculate forces which are then used to solve Newton\\'s laws of motion to examine the time-dependent behaviour of systems. The result of a molecular dynamics simulation is a trajectory that describes how the position and velocity of particles varies with time. The phase point of a system described by the positions and momenta of all its particles on a previous time point, will determine the next phase point in time by integrating over Newton\\'s laws of motion. \\n\\n\\n=== Monte Carlo ===\\nMonte Carlo (MC) generates configurations of a system by making random changes to the positions of its particles, together with their orientations and conformations where appropriate. It is a random sampling method, which makes use of the so-called importance sampling. Importance sampling methods are able to generate low energy states, as this enables properties to be calculated accurately. The potential energy of each configuration of the system can be calculated, together with the values of other properties, from the positions of the atoms.\\n\\n\\n=== Quantum mechanics/Molecular mechanics (QM/MM) ===\\n\\nQM/MM is a hybrid method that attempts to combine the accuracy of quantum mechanics with the speed of molecular mechanics. It is useful for simulating very large molecules such as enzymes.\\n\\n\\n== Interpreting molecular wave functions ==\\nThe atoms in molecules (QTAIM) model of Richard Bader was developed to effectively link the quantum mechanical model of a molecule, as an electronic wavefunction, to chemically useful concepts such as atoms in molecules, functional groups, bonding, the theory of Lewis pairs, and the valence bond model. Bader has demonstrated that these empirically useful chemistry concepts can be related to the topology of the observable charge density distribution, whether measured or calculated from a quantum mechanical wavefunction. QTAIM analysis of molecular wavefunctions is implemented, for example, in the AIMAll software package.\\n\\n\\n== Software packages ==\\nMany self-sufficient computational chemistry software packages exist. Some include many methods covering a wide range, while others concentrate on a very specific range or even on one method. Details of most of them can be found in:\\n\\nBiomolecular modelling programs: proteins, nucleic acid.\\nMolecular mechanics programs.\\nQuantum chemistry and solid state physics software supporting several methods.\\nMolecular design software\\nSemi-empirical programs.\\nValence bond programs.\\n\\n\\n== See also ==\\n\\n\\n== Citations ==\\n\\n\\n== General bibliography ==\\nC. J. Cramer Essentials of Computational Chemistry, John Wiley & Sons (2002).\\nT. Clark A Handbook of Computational Chemistry, Wiley, New York (1985).\\nDronskowski, Richard (2005). Computational Chemistry of Solid State Materials: A Guide for Materials Scientists, Chemists, Physicists and others. doi:10.1002/9783527612277. ISBN 9783527314102. S2CID 99908474.\\nA.K. Hartmann, Practical Guide to Computer Simulations, World Scientific (2009)\\nF. Jensen Introduction to Computational Chemistry, John Wiley & Sons (1999).\\nK.I. Ramachandran, G Deepa and Krishnan Namboori. P.K. Computational Chemistry and Molecular Modeling Principles and applications Springer-Verlag GmbH ISBN 978-3-540-77302-3.\\nRogers, Donald W. (2003). Computational Chemistry Using the PC. doi:10.1002/0471474908. ISBN 0471428000.\\nP. v. R. Schleyer (Editor-in-Chief). Encyclopedia of Computational Chemistry. Wiley, 1998. ISBN 0-471-96588-X.\\nD. Sherrill. Notes on Quantum Mechanics and Computational Chemistry.\\nJ. Simons An introduction to Theoretical Chemistry, Cambridge (2003) ISBN 978-0-521-53047-7.\\nA. Szabo, N.S. Ostlund, Modern Quantum Chemistry, McGraw-Hill (1982).\\nD. Young Computational Chemistry: A Practical Guide for Applying Techniques to Real World Problems, John Wiley & Sons (2001).\\nD. Young\\'s Introduction to Computational Chemistry.\\nLewars, Errol G. (2011). Computational Chemistry. Heidelberg: Springer. Bibcode:2011coch.book.....L. doi:10.1007/978-90-481-3862-3. ISBN 978-90-481-3860-9.\\n\\n\\n== Specialized journals on computational chemistry ==\\nAnnual Reports in Computational Chemistry\\nComputational and Theoretical Chemistry\\nComputational and Theoretical Polymer Science\\nComputers & Chemical Engineering\\nJournal of Chemical Information and Modeling\\nJournal of Chemical Information and Modeling\\nJournal of Chemical Software\\nJournal of Chemical Theory and Computation\\nJournal of Cheminformatics\\nJournal of Computational Chemistry\\nJournal of Computer Aided Chemistry\\nJournal of Computer Chemistry Japan\\nJournal of Computer-aided Molecular Design\\nJournal of Theoretical and Computational Chemistry\\nMolecular Informatics\\nTheoretical Chemistry Accounts\\n\\n\\n== External links ==\\nNIST Computational Chemistry Comparison and Benchmark DataBase – Contains a database of thousands of computational and experimental results for hundreds of systems\\nAmerican Chemical Society Division of Computers in Chemistry – American Chemical Society Computers in Chemistry Division, resources for grants, awards, contacts and meetings.\\nCSTB report Mathematical Research in Materials Science: Opportunities and Perspectives – CSTB Report\\n3.320 Atomistic Computer Modeling of Materials (SMA 5107)  Free MIT Course\\nChem 4021/8021 Computational Chemistry Free University of Minnesota Course\\nTechnology Roadmap for Computational Chemistry\\nApplications of molecular and materials modelling.\\nImpact of Advances in Computing and Communications Technologies on Chemical Science and Technology CSTB Report\\nMD and Computational Chemistry applications on GPUs', 'Bioinformatics ( (listen)) is an interdisciplinary field that develops methods and software tools for understanding biological data, in particular when the data sets are large and complex. As an interdisciplinary field of science, bioinformatics combines biology, computer science, information engineering, mathematics and statistics to analyze and interpret the biological data. Bioinformatics has been used for in silico analyses of biological queries using mathematical and statistical techniques.Bioinformatics includes biological studies that use computer programming as part of their methodology, as well as a specific analysis \"pipelines\" that are repeatedly used, particularly in the field of genomics. Common uses of bioinformatics include the identification of candidates genes and single nucleotide polymorphisms (SNPs). Often, such identification is made with the aim to better understand the genetic basis of disease, unique adaptations, desirable properties (esp. in agricultural species), or differences between populations. In a less formal way, bioinformatics also tries to understand the organizational principles within nucleic acid and protein sequences, called proteomics.\\n\\n\\n== Overview ==\\nBioinformatics has become an important part of many areas of biology. In experimental molecular biology, bioinformatics techniques such as image and signal processing allow extraction of useful results from large amounts of raw data. In the field of genetics, it aids in sequencing and annotating genomes and their observed mutations. It plays a role in the text mining of biological literature and the development of biological and gene ontologies to organize and query biological data. It also plays a role in the analysis of gene and protein expression and regulation. Bioinformatics tools aid in comparing, analyzing and interpreting genetic and genomic data and more generally in the understanding of evolutionary aspects of molecular biology. At a more integrative level, it helps analyze and catalogue the biological pathways and networks that are an important part of systems biology. In structural biology, it aids in the simulation and modeling of DNA, RNA, proteins as well as biomolecular interactions.\\n\\n\\n=== History ===\\nHistorically, the term bioinformatics did not mean what it means today. Paulien Hogeweg and Ben Hesper coined it in 1970 to refer to the study of information processes in biotic systems. This definition placed bioinformatics as a field parallel to biochemistry (the study of chemical processes in biological systems).\\n\\n\\n==== Sequences ====\\n\\nComputers became essential in molecular biology when protein sequences became available after Frederick Sanger determined the sequence of insulin in the early 1950s. Comparing multiple sequences manually turned out to be impractical. A pioneer in the field was Margaret Oakley Dayhoff. She compiled one of the first protein sequence databases, initially published as books and pioneered methods of sequence alignment and molecular evolution. Another early contributor to bioinformatics was Elvin A. Kabat, who pioneered biological sequence analysis in 1970 with his comprehensive volumes of antibody sequences released with Tai Te Wu between 1980 and 1991.\\nIn the 1970s, new techniques for sequencing DNA were applied to bacteriophage MS2 and øX174, and the extended nucleotide sequences were then parsed with informational and statistical algorithms.  These studies illustrated that well known features, such as the coding segments and the triplet code, are revealed in straightforward statistical analyses and were thus proof of the concept that bioinformatics would be insightful.\\n\\n\\n=== Goals ===\\nTo study how normal cellular activities are altered in different disease states, the biological data must be combined to form a comprehensive picture of these activities. Therefore, the field of bioinformatics has evolved such that the most pressing task now involves the analysis and interpretation of various types of data. This also includes nucleotide and amino acid sequences, protein domains, and protein structures. The actual process of analyzing and interpreting data is referred to as computational biology. Important sub-disciplines within bioinformatics and computational biology include:\\n\\nDevelopment and implementation of computer programs that enable efficient access to, management and use of, various types of information.\\nDevelopment of new algorithms (mathematical formulas) and statistical measures that assess relationships among members of large data sets. For example, there are methods to locate a gene within a sequence, to predict protein structure and/or function, and to cluster protein sequences into families of related sequences.The primary goal of bioinformatics is to increase the understanding of biological processes. What sets it apart from other approaches, however, is its focus on developing and applying computationally intensive techniques to achieve this goal. Examples include: pattern recognition, data mining, machine learning algorithms, and visualization. Major research efforts in the field include sequence alignment, gene finding, genome assembly, drug design, drug discovery, protein structure alignment, protein structure prediction, prediction of gene expression and protein–protein interactions, genome-wide association studies, the modeling of evolution and cell division/mitosis.\\nBioinformatics now entails the creation and advancement of databases, algorithms, computational and statistical techniques, and theory to solve formal and practical problems arising from the management and analysis of biological data.\\nOver the past few decades, rapid developments in genomic and other molecular research technologies and developments in information technologies have combined to produce a tremendous amount of information related to molecular biology. Bioinformatics is the name given to these mathematical and computing approaches used to glean understanding of biological processes.\\nCommon activities in bioinformatics include mapping and analyzing DNA and protein sequences, aligning DNA and protein sequences to compare them, and creating and viewing 3-D models of protein structures.\\n\\n\\n=== Relation to other fields ===\\nBioinformatics is a science field that is similar to but distinct from biological computation, while it is often considered synonymous to computational biology. Biological computation uses bioengineering and biology to build biological computers, whereas bioinformatics uses computation to better understand biology. Bioinformatics and computational biology involve the analysis of biological data, particularly DNA, RNA, and protein sequences. The field of bioinformatics experienced explosive growth starting in the mid-1990s, driven largely by the Human Genome Project and by rapid advances in DNA sequencing technology.\\nAnalyzing biological data to produce meaningful information involves writing and running software programs that use algorithms from graph theory, artificial intelligence, soft computing, data mining, image processing, and computer simulation. The algorithms in turn depend on theoretical foundations such as discrete mathematics, control theory, system theory, information theory, and statistics.\\n\\n\\n== Sequence analysis ==\\n\\nSince the Phage Φ-X174 was sequenced in 1977, the DNA sequences of thousands of organisms have been decoded and stored in databases. This sequence information is analyzed to determine genes that encode proteins, RNA genes, regulatory sequences, structural motifs, and repetitive sequences. A comparison of genes within a species or between different species can show similarities between protein functions, or relations between species (the use of molecular systematics to construct phylogenetic trees). With the growing amount of data, it long ago became impractical to analyze DNA sequences manually. Computer programs such as BLAST are used routinely to search sequences—as of 2008, from more than 260,000 organisms, containing over 190 billion nucleotides.\\n\\n\\n=== DNA sequencing ===\\n\\nBefore sequences can be analyzed they have to be obtained from the data storage bank example the Genbank. DNA sequencing is still a non-trivial problem as the raw data may be noisy or afflicted by weak signals. Algorithms have been developed for base calling for the various experimental approaches to DNA sequencing.\\n\\n\\n=== Sequence assembly ===\\n\\nMost DNA sequencing techniques produce short fragments of sequence that need to be assembled to obtain complete gene or genome sequences. The so-called shotgun sequencing technique (which was used, for example, by The Institute for Genomic Research (TIGR) to sequence the first bacterial genome, Haemophilus influenzae) generates the sequences of many thousands of small DNA fragments (ranging from 35 to 900 nucleotides long, depending on the sequencing technology). The ends of these fragments overlap and, when aligned properly by a genome assembly program, can be used to reconstruct the complete genome. Shotgun sequencing yields sequence data quickly, but the task of assembling the fragments can be quite complicated for larger genomes. For a genome as large as the human genome, it may take many days of CPU time on large-memory, multiprocessor computers to assemble the fragments, and the resulting assembly usually contains numerous gaps that must be filled in later. Shotgun sequencing is the method of choice for virtually all genomes sequenced today, and genome assembly algorithms are a critical area of bioinformatics research.\\n\\n\\n=== Genome annotation ===\\n\\nIn the context of genomics, annotation is the process of marking the genes and other biological features in a DNA sequence. This process needs to be automated because most genomes are too large to annotate by hand, not to mention the desire to annotate as many genomes as possible, as the rate of sequencing has ceased to pose a bottleneck. Annotation is made possible by the fact that genes have recognisable start and stop regions, although the exact sequence found in these regions can vary between genes.\\nThe first description of a comprehensive genome annotation system was published in 1995 by the team at The Institute for Genomic Research that performed the first complete sequencing and analysis of the genome of a free-living organism, the bacterium Haemophilus influenzae. Owen White designed and built a software system to identify the genes encoding all proteins, transfer RNAs, ribosomal RNAs (and other sites) and to make initial functional assignments. Most current genome annotation systems work similarly, but the programs available for analysis of genomic DNA, such as the GeneMark program trained and used to find protein-coding genes in Haemophilus influenzae, are constantly changing and improving.\\nFollowing the goals that the Human Genome Project left to achieve after its closure in 2003, a new project developed by the National Human Genome Research Institute in the U.S appeared. The so-called ENCODE project is a collaborative data collection of the functional elements of the human genome that uses next-generation DNA-sequencing technologies and genomic tiling arrays, technologies able to automatically generate large amounts of data at a dramatically reduced per-base cost but with the same accuracy (base call error) and fidelity (assembly error).\\n\\n\\n==== Gene function prediction ====\\nWhile genome annotation is primarily based on sequence similarity (and thus homology), other properties of sequences can be used to predict the function of genes. In fact, most gene function prediction methods focus on protein sequences as they are more informative and more feature-rich. For instance, the distribution of hydrophobic amino acids predicts transmembrane segments in proteins. However, protein function prediction can also use external information such as gene (or protein) expression data, protein structure, or protein-protein interactions.\\n\\n\\n=== Computational evolutionary biology ===\\n\\nEvolutionary biology is the study of the origin and descent of species, as well as their change over time. Informatics has assisted evolutionary biologists by enabling researchers to:\\n\\ntrace the evolution of a large number of organisms by measuring changes in their DNA, rather than through physical taxonomy or physiological observations alone,\\ncompare entire genomes, which permits the study of more complex evolutionary events, such as gene duplication, horizontal gene transfer, and the prediction of factors important in bacterial speciation,\\nbuild complex computational population genetics models to predict the outcome of the system over time\\ntrack and share information on an increasingly large number of species and organismsFuture work endeavours to reconstruct the now more complex tree of life.The area of research within computer science that uses genetic algorithms is sometimes confused with computational evolutionary biology, but the two areas are not necessarily related.\\n\\n\\n=== Comparative genomics ===\\n\\nThe core of comparative genome analysis is the establishment of the correspondence between genes (orthology analysis) or other genomic features in different organisms. It is these intergenomic maps that make it possible to trace the evolutionary processes responsible for the divergence of two genomes. A multitude of evolutionary events acting at various organizational levels shape genome evolution. At the lowest level, point mutations affect individual nucleotides. At a higher level, large chromosomal segments undergo duplication, lateral transfer, inversion, transposition, deletion and insertion. Ultimately, whole genomes are involved in processes of hybridization, polyploidization and endosymbiosis, often leading to rapid speciation. The complexity of genome evolution poses many exciting challenges to developers of mathematical models and algorithms, who have recourse to a spectrum of algorithmic, statistical and mathematical techniques, ranging from exact, heuristics, fixed parameter and approximation algorithms for problems based on parsimony models to Markov chain Monte Carlo algorithms for Bayesian analysis of problems based on probabilistic models.\\nMany of these studies are based on the detection of sequence homology to assign sequences to protein families.\\n\\n\\n=== Pan genomics ===\\n\\nPan genomics is a concept introduced in 2005 by Tettelin and Medini which eventually took root in bioinformatics. Pan genome is the complete gene repertoire of a particular taxonomic group: although initially applied to closely related strains of a species, it can be applied to a larger context like genus, phylum etc. It is divided in two parts- The Core genome: Set of genes common to all the genomes under study (These are often housekeeping genes vital for survival) and The Dispensable/Flexible Genome: Set of genes not present in all but one or some genomes under study. A bioinformatics tool BPGA can be used to characterize the Pan Genome of bacterial species.\\n\\n\\n=== Genetics of disease ===\\n\\nWith the advent of next-generation sequencing we are obtaining enough sequence data to map the genes of complex diseases infertility, breast cancer or Alzheimer\\'s disease. Genome-wide association studies are a useful approach to pinpoint the mutations responsible for such complex diseases. Through these studies, thousands of DNA variants have been identified that are associated with similar diseases and traits. Furthermore, the possibility for genes to be used at prognosis, diagnosis or treatment is one of the most essential applications. Many studies are discussing both the promising ways to choose the genes to be used and the problems and pitfalls of using genes to predict disease presence or prognosis.\\n\\n\\n=== Analysis of mutations in cancer ===\\n\\nIn cancer, the genomes of affected cells are rearranged in complex or even unpredictable ways. Massive sequencing efforts are used to identify previously unknown point mutations in a variety of genes in cancer. Bioinformaticians continue to produce specialized automated systems to manage the sheer volume of sequence data produced, and they create new algorithms and software to compare the sequencing results to the growing collection of human genome sequences and germline polymorphisms. New physical detection technologies are employed, such as oligonucleotide microarrays to identify chromosomal gains and losses (called comparative genomic hybridization), and single-nucleotide polymorphism arrays to detect known point mutations. These detection methods simultaneously measure several hundred thousand sites throughout the genome, and when used in high-throughput to measure thousands of samples, generate terabytes of data per experiment. Again the massive amounts and new types of data generate new opportunities for bioinformaticians. The data is often found to contain considerable variability, or noise, and thus Hidden Markov model and change-point analysis methods are being developed to infer real copy number changes.\\nTwo important principles can be used in the analysis of cancer genomes bioinformatically pertaining to the identification of mutations in the exome. First, cancer is a disease of accumulated somatic mutations in genes. Second cancer contains driver mutations which need to be distinguished from passengers.With the breakthroughs that this next-generation sequencing technology is providing to the field of Bioinformatics, cancer genomics could drastically change. These new methods and software allow bioinformaticians to sequence many cancer genomes quickly and affordably. This could create a more flexible process for classifying types of cancer by analysis of cancer driven mutations in the genome. Furthermore, tracking of patients while the disease progresses may be possible in the future with the sequence of cancer samples.Another type of data that requires novel informatics development is the analysis of lesions found to be recurrent among many tumors.\\n\\n\\n== Gene and protein expression ==\\n\\n\\n=== Analysis of gene expression ===\\nThe expression of many genes can be determined by measuring mRNA levels with multiple techniques including microarrays, expressed cDNA sequence tag (EST) sequencing, serial analysis of gene expression (SAGE) tag sequencing, massively parallel signature sequencing (MPSS), RNA-Seq, also known as \"Whole Transcriptome Shotgun Sequencing\" (WTSS), or various applications of multiplexed in-situ hybridization. All of these techniques are extremely noise-prone and/or subject to bias in the biological measurement, and a major research area in computational biology involves developing statistical tools to separate signal from noise in high-throughput gene expression studies. Such studies are often used to determine the genes implicated in a disorder: one might compare microarray data from cancerous epithelial cells to data from non-cancerous cells to determine the transcripts that are up-regulated and down-regulated in a particular population of cancer cells.\\n\\n\\n=== Analysis of protein expression ===\\nProtein microarrays and high throughput (HT) mass spectrometry (MS) can provide a snapshot of the proteins present in a biological sample. Bioinformatics is very much involved in making sense of protein microarray and HT MS data; the former approach faces similar problems as with microarrays targeted at mRNA, the latter involves the problem of matching large amounts of mass data against predicted masses from protein sequence databases, and the complicated statistical analysis of samples where multiple, but incomplete peptides from each protein are detected. Cellular protein localization in a tissue context can be achieved through affinity proteomics displayed as spatial data based on immunohistochemistry and tissue microarrays.\\n\\n\\n=== Analysis of regulation ===\\nGene regulation is the complex orchestration of events by which a signal, potentially an extracellular signal such as a hormone, eventually leads to an increase or decrease in the activity of one or more proteins. Bioinformatics techniques have been applied to explore various steps in this process.\\nFor example, gene expression can be regulated by nearby elements in the genome. Promoter analysis involves the identification and study of sequence motifs in the DNA surrounding the coding region of a gene. These motifs influence the extent to which that region is transcribed into mRNA. Enhancer elements far away from the promoter can also regulate gene expression, through three-dimensional looping interactions. These interactions can be determined by bioinformatic analysis of chromosome conformation capture experiments.\\nExpression data can be used to infer gene regulation: one might compare microarray data from a wide variety of states of an organism to form hypotheses about the genes involved in each state. In a single-cell organism, one might compare stages of the cell cycle, along with various stress conditions (heat shock, starvation, etc.). One can then apply clustering algorithms to that expression data to determine which genes are co-expressed. For example, the upstream regions (promoters) of co-expressed genes can be searched for over-represented regulatory elements. Examples of clustering algorithms applied in gene clustering are k-means clustering, self-organizing maps (SOMs), hierarchical clustering, and consensus clustering methods.\\n\\n\\n== Analysis of cellular organization ==\\nSeveral approaches have been developed to analyze the location of organelles, genes, proteins, and other components within cells. This is relevant as the location of these components affects the events within a cell and thus helps us to predict the behavior of biological systems. A gene ontology category, cellular component, has been devised to capture subcellular localization in many biological databases.\\n\\n\\n=== Microscopy and image analysis ===\\nMicroscopic pictures allow us to locate both organelles as well as molecules. It may also help us to distinguish between normal and abnormal cells, e.g. in cancer.\\n\\n\\n=== Protein localization ===\\nThe localization of proteins helps us to evaluate the role of a protein. For instance, if a protein is found in the nucleus it may be involved in gene regulation or splicing. By contrast, if a protein is found in mitochondria, it may be involved in respiration or other metabolic processes. Protein localization is thus an important component of protein function prediction. There are well developed protein subcellular localization prediction resources available, including protein subcellular location databases, and prediction tools.\\n\\n\\n=== Nuclear organization of chromatin ===\\n\\nData from high-throughput chromosome conformation capture experiments, such as Hi-C (experiment) and ChIA-PET, can provide information on the spatial proximity of DNA loci. Analysis of these experiments can determine the three-dimensional structure and nuclear organization of chromatin. Bioinformatic challenges in this field include partitioning the genome into domains, such as Topologically Associating Domains (TADs), that are organised together in three-dimensional space.\\n\\n\\n== Structural bioinformatics ==\\n\\nProtein structure prediction is another important application of bioinformatics. The amino acid sequence of a protein, the so-called primary structure, can be easily determined from the sequence on the gene that codes for it. In the vast majority of cases, this primary structure uniquely determines a structure in its native environment. (Of course, there are exceptions, such as the bovine spongiform encephalopathy (mad cow disease) prion.) Knowledge of this structure is vital in understanding the function of the protein. Structural information is usually classified as one of secondary, tertiary and quaternary structure. A viable general solution to such predictions remains an open problem. Most efforts have so far been directed towards heuristics that work most of the time.One of the key ideas in bioinformatics is the notion of homology. In the genomic branch of bioinformatics, homology is used to predict the function of a gene: if the sequence of gene A, whose function is known, is homologous to the sequence of gene B, whose function is unknown, one could infer that B may share A\\'s function. In the structural branch of bioinformatics, homology is used to determine which parts of a protein are important in structure formation and interaction with other proteins. In a technique called homology modeling, this information is used to predict the structure of a protein once the structure of a homologous protein is known. This currently remains the only way to predict protein structures reliably.\\nOne example of this is hemoglobin in humans and the hemoglobin in legumes (leghemoglobin), which are distant relatives from the same protein superfamily. Both serve the same purpose of transporting oxygen in the organism. Although both of these proteins have completely different amino acid sequences, their protein structures are virtually identical, which reflects their near identical purposes and shared ancestor.Other techniques for predicting protein structure include protein threading and de novo (from scratch) physics-based modeling.\\nAnother aspect of structural bioinformatics include the use of protein structures for Virtual Screening models such as Quantitative Structure-Activity Relationship models and proteochemometric models (PCM). Furthermore, a protein\\'s crystal structure can be used in simulation of for example ligand-binding studies and in silico mutagenesis studies.\\n\\n\\n== Network and systems biology ==\\n\\nNetwork analysis seeks to understand the relationships within biological networks such as metabolic or protein–protein interaction networks. Although biological networks can be constructed from a single type of molecule or entity (such as genes), network biology often attempts to integrate many different data types, such as proteins, small molecules, gene expression data, and others, which are all connected physically, functionally, or both.\\nSystems biology involves the use of computer simulations of cellular subsystems (such as the networks of metabolites and enzymes that comprise metabolism, signal transduction pathways and gene regulatory networks) to both analyze and visualize the complex connections of these cellular processes. Artificial life or virtual evolution attempts to understand evolutionary processes via the computer simulation of simple (artificial) life forms.\\n\\n\\n=== Molecular interaction networks ===\\n\\nTens of thousands of three-dimensional protein structures have been determined by X-ray crystallography and protein nuclear magnetic resonance spectroscopy (protein NMR) and a central question in structural bioinformatics is whether it is practical to predict possible protein–protein interactions only based on these 3D shapes, without performing protein–protein interaction experiments. A variety of methods have been developed to tackle the protein–protein docking problem, though it seems that there is still much work to be done in this field.\\nOther interactions encountered in the field include Protein–ligand (including drug) and protein–peptide. Molecular dynamic simulation of movement of atoms about rotatable bonds is the fundamental principle behind computational algorithms, termed docking algorithms, for studying molecular interactions.\\n\\n\\n== Others ==\\n\\n\\n=== Literature analysis ===\\n\\nThe growth in the number of published literature makes it virtually impossible to read every paper, resulting in disjointed sub-fields of research. Literature analysis aims to employ computational and statistical linguistics to mine this growing library of text resources. For example:\\n\\nAbbreviation recognition – identify the long-form and abbreviation of biological terms\\nNamed-entity recognition – recognizing biological terms such as gene names\\nProtein–protein interaction – identify which proteins interact with which proteins from textThe area of research draws from statistics and computational linguistics.\\n\\n\\n=== High-throughput image analysis ===\\nComputational technologies are used to accelerate or fully automate the processing, quantification and analysis of large amounts of high-information-content biomedical imagery. Modern image analysis systems augment an observer\\'s ability to make measurements from a large or complex set of images, by improving accuracy, objectivity, or speed. A fully developed analysis system may completely replace the observer. Although these systems are not unique to biomedical imagery, biomedical imaging is becoming more important for both diagnostics and research. Some examples are:\\n\\nhigh-throughput and high-fidelity quantification and sub-cellular localization (high-content screening, cytohistopathology, Bioimage informatics)\\nmorphometrics\\nclinical image analysis and visualization\\ndetermining the real-time air-flow patterns in breathing lungs of living animals\\nquantifying occlusion size in real-time imagery from the development of and recovery during arterial injury\\nmaking behavioral observations from extended video recordings of laboratory animals\\ninfrared measurements for metabolic activity determination\\ninferring clone overlaps in DNA mapping, e.g. the Sulston score\\n\\n\\n=== High-throughput single cell data analysis ===\\n\\nComputational techniques are used to analyse high-throughput, low-measurement single cell data, such as that obtained from flow cytometry. These methods typically involve finding populations of cells that are relevant to a particular disease state or experimental condition.\\n\\n\\n=== Biodiversity informatics ===\\n\\nBiodiversity informatics deals with the collection and analysis of biodiversity data, such as taxonomic databases, or microbiome data. Examples of such analyses include phylogenetics, niche modelling, species richness mapping, DNA barcoding, or species identification tools.\\n\\n\\n=== Ontologies and data integration ===\\nBiological ontologies are directed acyclic graphs of controlled vocabularies. They are designed to capture biological concepts and descriptions in a way that can be easily categorised and analysed with computers. When categorised in this way, it is possible to gain added value from holistic and integrated analysis.\\nThe OBO Foundry was an effort to standardise certain ontologies. One of the most widespread is the Gene ontology which describes gene function. There are also ontologies which describe phenotypes.\\n\\n\\n== Databases ==\\n\\nDatabases are essential for bioinformatics research and applications. Many databases exist, covering various information types: for example, DNA and protein sequences, molecular structures, phenotypes and biodiversity. Databases may contain empirical data (obtained directly from experiments), predicted data (obtained from analysis), or, most commonly, both. They may be specific to a particular organism, pathway or molecule of interest. Alternatively, they can incorporate data compiled from multiple other databases. These databases vary in their format, access mechanism, and whether they are public or not.\\nSome of the most commonly used databases are listed below. For a more comprehensive list, please check the link at the beginning of the subsection.\\n\\nUsed in biological sequence analysis: Genbank, UniProt\\nUsed in structure analysis: Protein Data Bank (PDB)\\nUsed in finding Protein Families and Motif Finding: InterPro, Pfam\\nUsed for Next Generation Sequencing: Sequence Read Archive\\nUsed in Network Analysis: Metabolic Pathway Databases (KEGG, BioCyc), Interaction Analysis Databases, Functional Networks\\nUsed in design of synthetic genetic circuits: GenoCAD\\n\\n\\n== Software and tools ==\\nSoftware tools for bioinformatics range from simple command-line tools, to more complex graphical programs and standalone web-services available from various bioinformatics companies or public institutions.\\n\\n\\n=== Open-source bioinformatics software ===\\nMany free and open-source software tools have existed and continued to grow since the 1980s. The combination of a continued need for new algorithms for the analysis of emerging types of biological readouts, the potential for innovative in silico experiments, and freely available open code bases have helped to create opportunities for all research groups to contribute to both bioinformatics and the range of open-source software available, regardless of their funding arrangements. The open source tools often act as incubators of ideas, or community-supported plug-ins in commercial applications. They may also provide de facto standards and shared object models for assisting with the challenge of bioinformation integration.\\nThe range of open-source software packages includes titles such as Bioconductor, BioPerl, Biopython, BioJava, BioJS, BioRuby, Bioclipse, EMBOSS, .NET Bio, Orange with its bioinformatics add-on, Apache Taverna, UGENE and GenoCAD. To maintain this tradition and create further opportunities, the non-profit Open Bioinformatics Foundation have supported the annual Bioinformatics Open Source Conference (BOSC) since 2000.An alternative method to build public bioinformatics databases is to use the MediaWiki engine with the WikiOpener extension. This system allows the database to be accessed and updated by all experts in the field.\\n\\n\\n=== Web services in bioinformatics ===\\nSOAP- and REST-based interfaces have been developed for a wide variety of bioinformatics applications allowing an application running on one computer in one part of the world to use algorithms, data and computing resources on servers in other parts of the world. The main advantages derive from the fact that end users do not have to deal with software and database maintenance overheads.\\nBasic bioinformatics services are classified by the EBI into three categories: SSS (Sequence Search Services), MSA (Multiple Sequence Alignment), and BSA (Biological Sequence Analysis). The availability of these service-oriented bioinformatics resources demonstrate the applicability of web-based bioinformatics solutions, and range from a collection of standalone tools with a common data format under a single, standalone or web-based interface, to integrative, distributed and extensible bioinformatics workflow management systems.\\n\\n\\n=== Bioinformatics workflow management systems ===\\n\\nA bioinformatics workflow management system is a specialized form of a workflow management system designed specifically to compose and execute a series of computational or data manipulation steps, or a workflow, in a Bioinformatics application. Such systems are designed to\\n\\nprovide an easy-to-use environment for individual application scientists themselves to create their own workflows,\\nprovide interactive tools for the scientists enabling them to execute their workflows and view their results in real-time,\\nsimplify the process of sharing and reusing workflows between the scientists, and\\nenable scientists to track the provenance of the workflow execution results and the workflow creation steps.Some of the platforms giving this service: Galaxy, Kepler, Taverna, UGENE, Anduril, HIVE.\\n\\n\\n=== BioCompute and BioCompute Objects ===\\nIn 2014, the US Food and Drug Administration sponsored a conference held at the National Institutes of Health Bethesda Campus to discuss reproducibility in bioinformatics. Over the next three years, a consortium of stakeholders met regularly to discuss what would become BioCompute paradigm. These stakeholders included representatives from government, industry, and academic entities. Session leaders represented numerous branches of the FDA and NIH Institutes and Centers, non-profit entities including the Human Variome Project and the European Federation for Medical Informatics, and research institutions including Stanford, the New York Genome Center, and the George Washington University.\\nIt was decided that the BioCompute paradigm would be in the form of digital \\'lab notebooks\\' which allow for the reproducibility, replication, review, and reuse, of bioinformatics protocols. This was proposed to enable greater continuity within a research group over the course of normal personnel flux while furthering the exchange of ideas between groups. The US FDA funded this work so that information on pipelines would be more transparent and accessible to their regulatory staff.In 2016, the group reconvened at the NIH in Bethesda and discussed the potential for a BioCompute Object, an instance of the BioCompute paradigm. This work was copied as both a \"standard trial use\" document and a preprint paper uploaded to bioRxiv. The BioCompute object allows for the JSON-ized record to be shared among employees, collaborators, and regulators.\\n\\n\\n== Education platforms ==\\nSoftware platforms designed to teach bioinformatics concepts and methods include Rosalind and online courses offered through the Swiss Institute of Bioinformatics Training Portal. The Canadian Bioinformatics Workshops provides videos and slides from training workshops on their website under a Creative Commons license. The 4273π project or 4273pi project also offers open source educational materials for free. The course runs on low cost Raspberry Pi computers and has been used to teach adults and school pupils. 4273π is actively developed by a consortium of academics and research staff who have run research level bioinformatics using Raspberry Pi computers and the 4273π operating system.MOOC platforms also provide online certifications in bioinformatics and related disciplines, including Coursera\\'s Bioinformatics Specialization (UC San Diego) and Genomic Data Science Specialization (Johns Hopkins) as well as EdX\\'s Data Analysis for Life Sciences XSeries (Harvard).  University of Southern California offers a Masters In Translational Bioinformatics focusing on biomedical applications.\\n\\n\\n== Conferences ==\\nThere are several large conferences that are concerned with bioinformatics. Some of the most notable examples are Intelligent Systems for Molecular Biology (ISMB), European Conference on Computational Biology (ECCB), and Research in Computational Molecular Biology (RECOMB).\\n\\n\\n== See also ==\\n\\n\\n== References ==\\n\\n\\n== Further reading ==\\n\\n\\n== External links ==\\n\\n The dictionary definition of bioinformatics at Wiktionary\\n Learning materials related to Bioinformatics at Wikiversity\\n Media related to Bioinformatics at Wikimedia Commons\\nBioinformatics Resource Portal (SIB)', \"Computational biology involves the development and application of data-analytical and theoretical methods, mathematical modelling and computational simulation techniques to the study of biological, ecological, behavioural, and social systems. The field is broadly defined and includes foundations in biology, applied mathematics, statistics, biochemistry, chemistry, biophysics, molecular biology, genetics, genomics, computer science, ecology, and evolution.Computational biology is different from biological computing, which is a subfield of computer engineering using bioengineering and biology to build computers.\\n\\n\\n== Introduction ==\\nComputational biology, which includes many aspects of bioinformatics and much more, is the science of using biological data to develop algorithms or models in order to understand biological systems and relationships. \\nUntil recently, biologists did not have access to very large amounts of data. This data has now become commonplace, particularly in molecular biology and genomics. Researchers were able to develop analytical methods for interpreting biological information, but were unable to share them quickly among colleagues.Bioinformatics began to develop in the early 1970s. It was considered the science of analyzing informatics processes of various biological systems. At this time, research in artificial intelligence was using network models of the human brain in order to generate new algorithms. This use of biological data to develop other fields pushed biological researchers to revisit the idea of using computers to evaluate and compare large data sets. By 1982, information was being shared among researchers through the use of punch cards. The amount of data being shared began to grow exponentially by the end of the 1980s. This required the development of new computational methods in order to quickly analyze and interpret relevant information.Since the late 1990s, computational biology has become an important part of developing emerging technologies for the field of biology.\\nThe terms computational biology and evolutionary computation have a similar name, but are not to be confused. Unlike computational biology, evolutionary computation is not concerned with modeling and analyzing biological data. It instead creates algorithms based on the ideas of evolution across species. Sometimes referred to as genetic algorithms, the research of this field can be applied to computational biology. While evolutionary computation is not inherently a part of computational biology, computational evolutionary biology is a subfield of it.Computational biology has been used to help sequence the human genome, create accurate models of the human brain, and assist in modeling biological systems.\\n\\n\\n== Subfields ==\\n\\n\\n=== Computational anatomy ===\\n\\nComputational anatomy is a discipline focusing on the study of anatomical shape and form at the visible or gross anatomical \\n  \\n    \\n      \\n        50\\n        −\\n        100\\n        μ\\n      \\n    \\n    {\\\\displaystyle 50-100\\\\mu }\\n   \\nscale of morphology. It involves the development and application of computational, mathematical and data-analytical methods for modeling and simulation of biological structures. It focuses on the anatomical structures being imaged, rather than the medical imaging devices. Due to the availability of dense 3D measurements via technologies such as magnetic resonance imaging (MRI), computational anatomy has emerged as a subfield of medical imaging and bioengineering for extracting anatomical coordinate systems at the morphome scale in 3D.\\nThe original formulation of computational anatomy is as a generative model of shape and form from exemplars acted upon via transformations. The diffeomorphism group is used to study different coordinate systems via coordinate transformations as generated via the Lagrangian and Eulerian velocities of flow from one anatomical configuration in \\n  \\n    \\n      \\n        \\n          \\n            \\n              R\\n            \\n          \\n          \\n            3\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\mathbb {R} }^{3}}\\n   to another. It relates with shape statistics and morphometrics, with the distinction that diffeomorphisms are used to map coordinate systems, whose study is known as diffeomorphometry.\\n\\n\\n=== Computational biomodeling ===\\n\\nComputational biomodeling is a field concerned with building computer models of biological systems. Computational biomodeling aims to develop and use visual simulations in order to assess the complexity of biological systems. This is accomplished through the use of specialized algorithms, and visualization software. These models allow for prediction of how systems will react under different environments. This is useful for determining if a system is robust. A robust biological system is one that “maintain their state and functions against external and internal perturbations”, which is essential for a biological system to survive. Computational biomodeling generates a large archive of such data, allowing for analysis from multiple users. While current techniques focus on small biological systems, researchers are working on approaches that will allow for larger networks to be analyzed and modeled. A majority of researchers believe that this will be essential in developing modern medical approaches to creating new drugs and gene therapy.\\nA useful modelling approach is to use Petri nets via tools such as esyN.\\n\\n\\n=== Computational ecology ===\\nComputational methods in ecology have seen increasing interest. Until recent decades, theoretical ecology has largely dealt with analytic models that were largely detached from the statistical models used by empirical ecologists. However, computational methods have aided in developing ecological theory via simulation of ecological systems, in addition to increasing application of methods from computational statistics in ecological analyses.\\n\\n\\n=== Computational evolutionary biology ===\\nComputational biology has assisted the field of evolutionary biology in many capacities. This includes:\\n\\nUsing DNA data to reconstruct the tree of life with computational phylogenetics\\nFitting population genetics models (either forward time or backward time) to DNA data to make inferences about demographic or selective history\\nBuilding population genetics models of evolutionary systems from first principles in order to predict what is likely to evolve\\n\\n\\n=== Computational genomics ===\\n\\nComputational genomics is a field within genomics which studies the genomes of cells and organisms. It is sometimes referred to as Computational and Statistical Genetics and encompasses much of Bioinformatics. The Human Genome Project is one example of computational genomics. This project looks to sequence the entire human genome into a set of data. Once fully implemented, this could allow for doctors to analyze the genome of an individual patient.  This opens the possibility of personalized medicine, prescribing treatments based on an individual's pre-existing genetic patterns. This project has created many similar programs. Researchers are looking to sequence the genomes of animals, plants, bacteria, and all other types of life.One of the main ways that genomes are compared is by sequence homology. Homology is the study of biological structures and nucleotide sequences in different organisms that come from a common ancestor. Research suggests that between 80 and 90% of genes in newly sequenced prokaryotic genomes can be identified this way.This field is still in development. An untouched project in the development of computational genomics is the analysis of intergenic regions. Studies show that roughly 97% of the human genome consists of these regions. Researchers in computational genomics are working on understanding the functions of non-coding regions of the human genome through the development of computational and statistical methods and via large consortia projects such as ENCODE (The Encyclopedia of DNA Elements) and the Roadmap Epigenomics Project.\\n\\n\\n=== Computational neuropsychiatry ===\\nComputational neuropsychiatry is the emerging field that uses mathematical and computer-assisted modeling of brain mechanisms involved in mental disorders. It was already demonstrated by several initiatives that computational modeling is an important contribution to understand neuronal circuits that could generate mental functions and dysfunctions.\\n\\n\\n=== Computational neuroscience ===\\n\\nComputational neuroscience is the study of brain function in terms of the information processing properties of the structures that make up the nervous system. It is a subset of the field of neuroscience, and looks to analyze brain data to create practical applications. It looks to model the brain in order to examine specific aspects of the neurological system. Various types of models of the brain include:\\n\\nRealistic Brain Models: These models look to represent every aspect of the brain, including as much detail at the cellular level as possible. Realistic models provide the most information about the brain, but also have the largest margin for error. More variables in a brain model create the possibility for more error to occur. These models do not account for parts of the cellular structure that scientists do not know about. Realistic brain models are the most computationally heavy and the most expensive to implement.\\nSimplifying Brain Models:  These models look to limit the scope of a model in order to assess a specific physical property of the neurological system. This allows for the intensive computational problems to be solved, and reduces the amount of potential error from a realistic brain model.It is the work of computational neuroscientists to improve the algorithms and data structures currently used to increase the speed of such calculations.\\n\\n\\n=== Computational oncology ===\\nComputational oncology, sometimes also called cancer computational biology, is a field that aims to determine the future mutations in cancer through an algorithmic approach to analyzing data. Research in this field has led to the use of high-throughput measurement. High throughput measurement allows for the gathering of millions of data points using robotics and other sensing devices. This data is collected from DNA, RNA, and other biological structures. Areas of focus include determining the characteristics of tumors, analyzing molecules that are deterministic in causing cancer, and understanding how the human genome relates to the causation of tumors and cancer.\\n\\n\\n=== Computational pharmacology ===\\nComputational pharmacology (from a computational biology perspective) is “the study of the effects of genomic data to find links between specific genotypes and diseases and then screening drug data”. The pharmaceutical industry requires a shift in methods to analyze drug data. Pharmacologists were able to use Microsoft Excel to compare chemical and genomic data related to the effectiveness of drugs. However, the industry has reached what is referred to as the Excel barricade. This arises from the limited number of cells accessible on a spreadsheet. This development led to the need for computational pharmacology. Scientists and researchers develop computational methods to analyze these massive data sets. This allows for an efficient comparison between the notable data points and allows for more accurate drugs to be developed.Analysts project that if major medications fail due to patents, that computational biology will be necessary to replace current drugs on the market. Doctoral students in computational biology are being encouraged to pursue careers in industry rather than take Post-Doctoral positions. This is a direct result of major pharmaceutical companies needing more qualified analysts of the large data sets required for producing new drugs.\\n\\n\\n== Software and tools ==\\nComputational Biologists use a wide range of software. These range from command line programs to graphical and web-based programs.\\n\\n\\n=== Open source software ===\\nOpen source software provides a platform to develop computational biological methods. Specifically, open source means that every person and/or entity can access and benefit from software developed in research. PLOS cites four main reasons for the use of open source software including:\\n\\nReproducibility: This allows for researchers to use the exact methods used to calculate the relations between biological data.\\nFaster Development: developers and researchers do not have to reinvent existing code for minor tasks. Instead they can use pre-existing programs to save time on the development and implementation of larger projects.\\nIncreased quality: Having input from multiple researchers studying the same topic provides a layer of assurance that errors will not be in the code.\\nLong-term availability: Open source programs are not tied to any businesses or patents. This allows for them to be posted to multiple web pages and ensure that they are available in the future.\\n\\n\\n== Conferences ==\\nThere are several large conferences that are concerned with computational biology. Some notable examples are Intelligent Systems for Molecular Biology (ISMB), European Conference on Computational Biology (ECCB) and Research in Computational Molecular Biology (RECOMB).\\n\\n\\n== Journals ==\\nThere are numerous journals dedicated to computational biology. Some notable examples include Journal of Computational Biology and PLOS Computational Biology. The PLOS computational biology journal is a peer-reviewed journal that has many notable research projects in the field of computational biology. They provide reviews on software, tutorials for open source software, and display information on upcoming computational biology conferences. PLOS Computational Biology is an open access journal. The publication may be openly used provided the author is cited.\\n\\n\\n== Related fields ==\\nComputational biology, bioinformatics and mathematical biology are all interdisciplinary approaches to the life sciences that draw from quantitative disciplines such as mathematics and information science. The NIH describes computational/mathematical biology as the use of computational/mathematical approaches to address theoretical and experimental questions in biology and, by contrast, bioinformatics as the application of information science to understand complex life-sciences data.Specifically, the NIH defines\\n\\nComputational biology: The development and application of data-analytical and theoretical methods, mathematical modeling and computational simulation techniques to the study of biological, behavioral, and social systems.\\nBioinformatics: Research, development, or application of computational tools and approaches for expanding the use of biological, medical, behavioral or health data, including those to acquire, store, organize, archive, analyze, or visualize such data.\\nWhile each field is distinct, there may be significant overlap at their interface.\\n\\n\\n== See also ==\\n\\n\\n== References ==\\n\\n\\n== External links ==\\nbioinformatics.org\", 'Biological data refers to a compound or information derived from living organisms and their products. A medicinal compound made from living organisms, such as a serum or a vaccine, could be characterized as biological data. Biological data is highly complex when compared with other forms of data. There are many forms of biological data, including text, sequence data, protein structure, genomic data and amino acids, and links among others.\\n\\n\\n== Biological Data and Bioinformatics ==\\nBiological data works closely with Bioinformatics, which is a recent discipline focusing on addressing the need to analyze and interpret vast amounts of genomic data.\\nIn the past few decades, leaps in genomic research have led to massive amounts of biological data. As a result, bioinformatics was created as the convergence of genomics, biotechnology, and information technology, while concentrating on biological data.\\nBiological Data has also been difficult to define, as bioinformatics is a wide-encompassing field. Further, the question of what constitutes as being a living organism has been contentious, as \"alive\" represents a nebulous term that encompasses molecular evolution, biological modeling, biophysics, and systems biology. From the past decade onwards, bioinformatics and the analysis of biological data have been thriving as a result of leaps in technology required to manage and interpret data. It is currently a thriving field, as society has become more concentrated on the acquisition, transfer, and exploitation of bioinformatics and biological data.\\n\\n\\n== Types of Biological Data ==\\nBiological Data can be extracted for use in the domains of omics, bio-imaging, and medical imaging. Life scientists value biological data to provide molecular details in living organisms. Tools for DNA sequencing, gene expression (GE), bio-imaging, neuro-imaging, and brain-machine interfaces are all domains that utilize biological data, and model biological systems with high dimensionality.Moreover, raw biological sequence data usually refers to DNA, RNA, and amino acids.Biological Data can also be described as data on biological entities. For instance, characteristics such as: sequences, graphs, geometric information, scalar and vector fields, patterns, constraints, images, and spatial information may all be characterized as biological data, as they describe features of biological beings. In many instances, biological data are associated with several of these categories. For instance, as described in the National Institute of Health\\'s report on Catalyzing Inquiry at the Interface of Computing and Biology, a protein structure may be associated with a one-dimensional sequence, a two-dimensional image, and a three dimensional structure, and so on.\\n\\n\\n=== Biomedical Databases ===\\nBiomedical Databases have often been referred to as the databases of Electronic Health Records (EHRs), genomic data in decentralized federal database systems, and biological data, including genomic data, collected from large-scale clinical studies.\\n\\n\\n== Bio-hacking and Privacy Threats ==\\n\\n\\n=== Bio-hacking ===\\nBio-computing attacks have become more common as recent studies have shown that common tools may allow an assailant to synthesize biological information which can be used to hijack information from DNA-analyses. The threat of biohacking has become more apparent as DNA-analysis increases in commonality in fields such as forensic science, clinical research, and genomics.\\nBiohacking can be carried out by synthesizing malicious DNA and inserted into biological samples. Researchers have established scenarios that demonstrate the threat of biohacking, such as a hacker reaching a biological sample by hiding malicious DNA on common surfaces, such as lab coats, benches, or rubber gloves, which would then contaminate the genetic data.However, the threat of biohacking may be mitigated by using similar techniques that are used to prevent conventional injection attacks. Clinicians and researchers may mitigate a bio-hack by extracting genetic information from biological samples, and comparing the samples to identify material unknown materials. Studies have shown that comparing genetic information with biological samples, to identify bio-hacking code, has been up to 95% effective in detecting malicious DNA inserts in bio-hacking attacks.\\n\\n\\n=== Genetic Samples as Personal Data ===\\nPrivacy concerns in genomic research have arises around the notion of whether or not genomic samples contain personal data, or should be regarded as physical matter. Moreover, concerns arise as some countries recognize genomic data as personal data (and apply data protection rules) while other countries regard the samples in terms of physical matter and do not apply the same data protection laws to genomic samples. The forthcoming General Data Protection Regulation (GDPR) has been cited as a potential legal instrument that may better enforce privacy regulations in bio-banking and genomic research.However, ambiguity surrounding the definition of \"personal data\" in the text of the GDPR, especially regarding biological data, has led to doubts on whether regulation will be enforced for genetic samples. Article 4(1) states that personal data is defined as \"Any information relating to an identified or identifiable natural person (\\'data subject\\')\"\\n\\n\\n== Applications of Deep Learning to Biological Data ==\\nAs a result of rapid advances in data science and computational power, life scientists have been able to apply data-intensive machine learning methods to biological data, such as deep learning (DL), reinforcement learning (RL), and their combination (deep RL). These methods, alongside increases in data storage and computing, have allowed life scientists to mine biological data and analyze data sets that were previously too large or complex. Deep Learning (DL) and reinforcement learning (RL) have been used in the field of omics research (which includes genomics, proteomics, or metabolomics.) Typically, raw biological sequence data (such as DNA, RNA, and amino acids) is extracted and used to analyze features, functions, structures, and molecular dynamics from the biological data. From that point onwards, different analyses may be performed, such as GE profiling splicing junction prediction, and protein-protein interaction evaluation may all be performed.Reinforcement learning, a term stemming from behavioral psychology, is a method of problem solving by learning things through trial and error. Reinforcement learning can be applied to biological data, in the field of omics, by using RL to predict bacterial genomes.Other studies have shown that reinforcement learning can be used to accurately predict biological sequence annotation.Deep Learning (DL) architectures are also useful in training biological data. For instance, DL architectures that target pixel levels of biological images have been used to identify the process of mitosis in histological images of the breast. DL architectures have also been used to identify nuclei in images of breast cancer cells.\\n\\n\\n== Challenges to Data Mining in Biomedical Informatics ==\\n\\n\\n=== Complexity ===\\nThe primary problem facing biomedical data models has typically been complexity, as life scientists in clinical settings and biomedical research face the possibility of information overload. However, information overload has often been a debated phenomenon in medical fields. Computational advances have allowed for separate communities to form under different philosophies. For instance, data mining and machine learning researchers search for relevant patterns in biological data, and the architecture does not rely on human intervention. However, there are risks involved when modeling artifacts when human intervention, such as end user comprehension and control, are lessened.Researchers have pointed out that with increasing health care costs and tremendous amounts of underutilized data, health information technologies may be the key to improving the efficiency and quality of healthcare.\\n\\n\\n=== Database Errors and Abuses ===\\nElectronic health records (EHR) can contain genomic data from millions of patients, and the creation of these databases has resulted in both praise and concern.Legal scholars have pointed towards three primary concerns for increasing litigation pertaining to biomedical databases. First, data contained in biomedical databases may be incorrect or incomplete. Second, systemic biases, which may arise from researcher biases or the nature of the biological data, may threaten the validity of research results. Third, the presence of data mining in biological databases can make it easier for individuals with political, social, or economic agendas to manipulate research findings to sway public opinion.An example of database misuse occurred in 2009 when the Journal of Psychiatric Research published a study that associated abortion to psychiatric disorders. The purpose of the study was to analyze associations between abortion history and psychiatric disorders, such as anxiety disorders (including panic disorder, PTSD, and agoraphobia) alongside substance abuse disorders and mood disorders.\\nHowever, the study was discredited in 2012 when scientists scrutinized the methodology of the study and found it severely faulty. The researchers had used \"national data sets with reproductive history and mental health variables\" to produce their findings. However, the researchers had failed to compare women (who had unplanned pregnancies and had abortions) to the group of women who did not have abortions, while focusing on psychiatric problems that occurred after the terminated pregnancies. As a result, the findings which appeared to give scientific credibility, gave rise to several states enacting legislation that required women to seek counseling before abortions, due to the potential of long-term mental health consequences.\\nAnother article, published in the New York Times, demonstrated how Electronic Health Records (EHR) systems could be manipulated by doctors to exaggerate the amount of care they provided for purposes of Medicare reimbursement.\\n\\n\\n== Biomedical Data Sharing ==\\nSharing biomedical data has been touted as an effective way to enhance research reproducibility and scientific discovery.While researchers struggle with technological issues in sharing data, social issues are also a barrier to sharing biological data. For instance, clinicians and researchers face unique challenges to sharing biological or health data within their medical communities, such as privacy concerns and patient privacy laws such as HIPAA.\\n\\n\\n=== Attitudes Towards Data Sharing ===\\nAccording to a 2015 study focusing on the attitudes of practices of clinicians and scientific research staff, a majority of the respondents reported data sharing as important to their work, but signified that their expertise in the subject was low. Of the 190 respondents to the survey, 135 identified themselves as clinical or basic research scientists, and the population of the survey included clinical and basic research scientists in the Intramural Research Program at the National Institute of Health. The study also found that, among the respondents, sharing data directly with other clinicians was a common practice, but the subjects of the study had little practice uploading data to a repository.\\nWithin the field of biomedical research, data sharing has been promoted as an important way for researchers to share and reuse data in order to fully capture the benefits towards personalized and precision medicine.\\n\\n\\n=== Challenges to Data Sharing ===\\nData sharing in healthcare has remained a challenge for several reasons. Despite research advances in data sharing in healthcare, many healthcare organizations remain reluctant or unwilling to release medical data on account of privacy laws such as the Health Insurance Portability and Accountability Act (HIPAA). Moreover, sharing biological data between institutions requires protecting confidentiality for data that may span several organizations. Achieving data syntax and semantic heterogeneity while meeting diverse privacy requirements are all factors that pose barriers to data sharing.\\n\\n\\n== References ==', 'Protein folding is the physical process by which a protein chain is translated to its native three-dimensional structure, typically a \"folded\" conformation by which the protein becomes biologically functional. Via an expeditious and reproducible process, a polypeptide folds into its characteristic three-dimensional structure from a random coil. Each protein exists first as an unfolded polypeptide or random coil after being translated from a sequence of mRNA to a linear chain of amino acids. At this stage the polypeptide lacks any stable (long-lasting) three-dimensional structure (the left hand side of the first figure). As the polypeptide chain is being synthesized by a ribosome, the linear chain begins to fold into its three-dimensional structure.\\nFolding of many proteins begins even during translation of the polypeptide chain. Amino acids interact with each other to produce a well-defined three-dimensional structure, the folded protein (the right hand side of the figure), known as the native state. The resulting three-dimensional structure is determined by the amino acid sequence or primary structure (Anfinsen\\'s dogma).The correct three-dimensional structure is essential to function, although some parts of functional proteins may remain unfolded, so that protein dynamics is important. Failure to fold into native structure generally produces inactive proteins, but in some instances misfolded proteins have modified or toxic functionality. Several neurodegenerative and other diseases are believed to result from the accumulation of amyloid fibrils formed by misfolded proteins. Many allergies are caused by incorrect folding of some proteins, because the immune system does not produce antibodies for certain protein structures.Denaturation of proteins is a process of transition from the folded to the unfolded state. It happens in cooking, in burns, in proteinopathies, and in other contexts.\\nThe duration of the folding process varies dramatically depending on the protein of interest. When studied outside the cell, the slowest folding proteins require many minutes or hours to fold primarily due to proline isomerization, and must pass through a number of intermediate states, like checkpoints, before the process is complete. On the other hand, very small single-domain proteins with lengths of up to a hundred amino acids typically fold in a single step. Time scales of milliseconds are the norm and the very fastest known protein folding reactions are complete within a few microseconds.Understanding and simulating the protein folding process has been an important challenge for computational biology since the late 1960s.\\n\\n\\n== Process of protein folding ==\\n\\n\\n=== Primary structure ===\\nThe primary structure of a protein, its linear amino-acid sequence, determines its native conformation. The specific amino acid residues and their position in the polypeptide chain are the determining factors for which portions of the protein fold closely together and form its three-dimensional conformation. The amino acid composition is not as important as the sequence. The essential fact of folding, however, remains that the amino acid sequence of each protein contains the information that specifies both the native structure and the pathway to attain that state. This is not to say that nearly identical amino acid sequences always fold similarly. Conformations differ based on environmental factors as well; similar proteins fold differently based on where they are found.\\n\\n\\n=== Secondary structure ===\\n\\nFormation of a secondary structure is the first step in the folding process that a protein takes to assume its native structure. Characteristic of secondary structure are the structures known as alpha helices and beta sheets that fold rapidly because they are stabilized by intramolecular hydrogen bonds, as was first characterized by Linus Pauling. Formation of intramolecular hydrogen bonds provides another important contribution to protein stability. α-helices are formed by hydrogen bonding of the backbone to form a spiral shape (refer to figure on the right). The β pleated sheet is a structure that forms with the backbone bending over itself to form the hydrogen bonds (as displayed in the figure to the left). The hydrogen bonds are between the amide hydrogen and carbonyl oxygen of the peptide bond. There exists anti-parallel β pleated sheets and parallel β pleated sheets where the stability of the hydrogen bonds is stronger in the anti-parallel β sheet as it hydrogen bonds with the ideal 180 degree angle compared to the slanted hydrogen bonds formed by parallel sheets.\\n\\n\\n=== Tertiary structure ===\\nThe alpha helices and beta pleated sheets can be amphipathic in nature, or contain a hydrophilic portion and a hydrophobic portion. This property of secondary structures aids in the tertiary structure of a protein in which the folding occurs so that the hydrophilic sides are facing the aqueous environment surrounding the protein and the hydrophobic sides are facing the hydrophobic core of the protein. Secondary structure hierarchically gives way to tertiary structure formation. Once the protein\\'s tertiary structure is formed and stabilized by the hydrophobic interactions, there may also be covalent bonding in the form of disulfide bridges formed between two cysteine residues. Tertiary structure of a protein involves a single polypeptide chain; however, additional interactions of folded polypeptide chains give rise to quaternary structure formation.\\n\\n\\n=== Quaternary structure ===\\nTertiary structure may give way to the formation of quaternary structure in some proteins, which usually involves the \"assembly\" or \"coassembly\" of subunits that have already folded; in other words, multiple polypeptide chains could interact to form a fully functional quaternary protein.\\n\\n\\n=== Driving forces of protein folding ===\\n\\nFolding is a spontaneous process that is mainly guided by hydrophobic interactions, formation of intramolecular hydrogen bonds, van der Waals forces, and it is opposed by conformational entropy. The process of folding often begins co-translationally, so that the N-terminus of the protein begins to fold while the C-terminal portion of the protein is still being synthesized by the ribosome; however, a protein molecule may fold spontaneously during or after biosynthesis. While these macromolecules may be regarded as \"folding themselves\", the process also depends on the solvent (water or lipid bilayer), the concentration of salts, the pH, the temperature, the possible presence of cofactors and of molecular chaperones.\\nProteins will have limitations on their folding abilities by the restricted bending angles or conformations that are possible. These allowable angles of protein folding are described with a two-dimensional plot known as the Ramachandran plot, depicted with psi and phi angles of allowable rotation.\\n\\n\\n==== Hydrophobic effect ====\\nProtein folding must be thermodynamically favorable within a cell in order for it to be a spontaneous reaction. Since it is known that protein folding is a spontaneous reaction, then it must assume a negative Gibbs free energy value. Gibbs free energy in protein folding is directly related to enthalpy and entropy. For a negative delta G to arise and for protein folding to become thermodynamically favorable, then either enthalpy, entropy, or both terms must be favorable.\\n\\nMinimizing the number of hydrophobic side-chains exposed to water is an important driving force behind the folding process. The hydrophobic effect is the phenomenon in which the hydrophobic chains of a protein collapse into the core of the protein (away from the hydrophilic environment). In an aqueous environment, the water molecules tend to aggregate around the hydrophobic regions or side chains of the protein, creating water shells of ordered water molecules. An ordering of water molecules around a hydrophobic region increases order in a system and therefore contributes a negative change in entropy (less entropy in the system). The water molecules are fixed in these water cages which drives the hydrophobic collapse, or the inward folding of the hydrophobic groups. The hydrophobic collapse introduces entropy back to the system via the breaking of the water cages which frees the ordered water molecules. The multitude of hydrophobic groups interacting within the core of the globular folded protein contributes a significant amount to protein stability after folding, because of the vastly accumulated van der Waals forces (specifically London Dispersion forces). The hydrophobic effect exists as a driving force in thermodynamics only if there is the presence of an aqueous medium with an amphiphilic molecule containing a large hydrophobic region. The strength of hydrogen bonds depends on their environment; thus, H-bonds enveloped in a hydrophobic core contribute more than H-bonds exposed to the aqueous environment to the stability of the native state.In proteins with globular folds, hydrophobic amino acids tend to be interspersed along the primary sequence, rather than randomly distributed or clustered together. However, proteins that have recently been born de novo, which tend to be intrinsically disordered, show the opposite pattern of hydrophobic amino acid clustering along the primary sequence.\\n\\n\\n==== Chaperones ====\\n\\nMolecular chaperones are a class of proteins that aid in the correct folding of other proteins in vivo. Chaperones exist in all cellular compartments and interact with the polypeptide chain in order to allow the native three-dimensional conformation of the protein to form; however, chaperones themselves are not included in the final structure of the protein they are assisting in. Chaperones may assist in folding even when the nascent polypeptide is being synthesized by the ribosome. Molecular chaperones operate by binding to stabilize an otherwise unstable structure of a protein in its folding pathway, but chaperones do not contain the necessary information to know the correct native structure of the protein they are aiding; rather, chaperones work by preventing incorrect folding conformations. In this way, chaperones do not actually increase the rate of individual steps involved in the folding pathway toward the native structure; instead, they work by reducing possible unwanted aggregations of the polypeptide chain that might otherwise slow down the search for the proper intermediate and they provide a more efficient pathway for the polypeptide chain to assume the correct conformations. Chaperones are not to be confused with folding catalyst proteins, which catalyze chemical reactions responsible for slow steps in folding pathways. Examples of folding catalysts are protein disulfide isomerases and peptidyl-prolyl isomerases that may be involved in formation of disulfide bonds or interconversion between cis and trans stereoisomers of peptide group. Chaperones are shown to be critical in the process of protein folding in vivo because they provide the protein with the aid needed to assume its proper alignments and conformations efficiently enough to become \"biologically relevant\". This means that the polypeptide chain could theoretically fold into its native structure without the aid of chaperones, as demonstrated by protein folding experiments conducted in vitro; however, this process proves to be too inefficient or too slow to exist in biological systems; therefore, chaperones are necessary for protein folding in vivo. Along with its role in aiding native structure formation, chaperones are shown to be involved in various roles such as protein transport, degradation, and even allow denatured proteins exposed to certain external denaturant factors an opportunity to refold into their correct native structures.A fully denatured protein lacks both tertiary and secondary structure, and exists as a so-called random coil. Under certain conditions some proteins can refold; however, in many cases, denaturation is irreversible. Cells sometimes protect their proteins against the denaturing influence of heat with enzymes known as heat shock proteins (a type of chaperone), which assist other proteins both in folding and in remaining folded.  Heat shock proteins have been found in all species examined, from bacteria to humans, suggesting that they evolved very early and have an important function.  Some proteins never fold in cells at all except with the assistance of chaperones which either isolate individual proteins so that their folding is not interrupted by interactions with other proteins or help to unfold misfolded proteins, allowing them to refold into the correct native structure. This function is crucial to prevent the risk of precipitation into insoluble amorphous aggregates. The external factors involved in protein denaturation or disruption of the native state include temperature, external fields (electric, magnetic), molecular crowding, and even the limitation of space (i.e. confinement), which can have a big influence on the folding of proteins. High concentrations of solutes, extremes of pH, mechanical forces, and the presence of chemical denaturants can contribute to protein denaturation, as well. These individual factors are categorized together as stresses. Chaperones are shown to exist in increasing concentrations during times of cellular stress and help the proper folding of emerging proteins as well as denatured or misfolded ones.Under some conditions proteins will not fold into their biochemically functional forms. Temperatures above or below the range that cells tend to live in will cause thermally unstable proteins to unfold or denature (this is why boiling makes an egg white turn opaque). Protein thermal stability is far from constant, however; for example, hyperthermophilic bacteria have been found that grow at temperatures as high as 122 °C, which of course requires that their full complement of vital proteins and protein assemblies be stable at that temperature or above.\\nThe bacterium E. coli is the host for bacteriophage T4, and the phage encoded gp31 protein (P17313) appears to be structurally and functionally homologous to E. coli chaperone protein GroES and able to substitute for it in the assembly of bacteriophage T4 virus particles during infection.  Like GroES, gp31 forms a stable complex with GroEL chaperonin that is absolutely necessary for the folding and assembly in vivo of the bacteriophage T4 major capsid protein gp23.\\n\\n\\n=== Fold switching ===\\nSome proteins have multiple native structures, and change their fold based on some external factors. For example, the KaiB protein switches fold throughout the day, acting as a clock for cyanobacteria. It has been estimated that around 0.5–4% of PDB (Protein Data Bank) proteins switch folds.\\n\\n\\n== Protein misfolding and neurodegenerative disease ==\\n\\nA protein is considered to be misfolded if it cannot achieve its normal native state. This can be due to mutations in the amino acid sequence or a disruption of the normal folding process by external factors. The misfolded protein typically contains β-sheets that are organized in a supramolecular arrangement known as a cross-β structure. These β-sheet-rich assemblies are very stable, very insoluble, and generally resistant to proteolysis. The structural stability of these fibrillar assemblies is caused by extensive interactions between the protein monomers, formed by backbone hydrogen bonds between their β-strands. The misfolding of proteins can trigger the further misfolding and accumulation of other proteins into aggregates or oligomers. The increased levels of aggregated proteins in the cell leads to formation of amyloid-like structures which can cause degenerative disorders and cell death. The amyloids are fibrillary structures that contain intermolecular hydrogen bonds which are highly insoluble and made from converted protein aggregates. Therefore, the proteasome pathway may not be efficient enough to degrade the misfolded proteins prior to aggregation. Misfolded proteins can interact with one another and form structured aggregates and gain toxicity through intermolecular interactions.Aggregated proteins are associated with prion-related illnesses such as Creutzfeldt–Jakob disease, bovine spongiform encephalopathy (mad cow disease), amyloid-related illnesses such as Alzheimer\\'s disease and familial amyloid cardiomyopathy or polyneuropathy, as well as intracellular aggregation diseases such as Huntington\\'s and Parkinson\\'s disease. These age onset degenerative diseases are associated with the aggregation of misfolded proteins into insoluble, extracellular aggregates and/or intracellular inclusions including cross-β amyloid fibrils. It is not completely clear whether the aggregates are the cause or merely a reflection of the loss of protein homeostasis, the balance between synthesis, folding, aggregation and protein turnover. Recently the European Medicines Agency approved the use of Tafamidis or Vyndaqel (a kinetic stabilizer of tetrameric transthyretin) for the treatment of transthyretin amyloid diseases. This suggests that the process of amyloid fibril formation (and not the fibrils themselves) causes the degeneration of post-mitotic tissue in human amyloid diseases. Misfolding and excessive degradation instead of folding and function leads to a number of proteopathy diseases such as antitrypsin-associated emphysema, cystic fibrosis and the lysosomal storage diseases, where loss of function is the origin of the disorder. While protein replacement therapy has historically been used to correct the latter disorders, an emerging approach is to use pharmaceutical chaperones to fold mutated proteins to render them functional.\\n\\n\\n== Experimental techniques for studying protein folding ==\\nWhile inferences about protein folding can be made through mutation studies, typically, experimental techniques for studying protein folding rely on the gradual unfolding or folding of proteins and observing conformational changes using standard non-crystallographic techniques.\\n\\n\\n=== X-ray crystallography ===\\n\\nX-ray crystallography is one of the more efficient and important methods for attempting to decipher the three dimensional configuration of a folded protein. To be able to conduct X-ray crystallography, the protein under investigation must be located inside a crystal lattice. To place a protein inside a crystal lattice, one must have a suitable solvent for crystallization, obtain a pure protein at supersaturated levels in solution, and precipitate the crystals in solution. Once a protein is crystallized, X-ray beams can be concentrated through the crystal lattice which would diffract the beams or shoot them outwards in various directions. These exiting beams are correlated to the specific three-dimensional configuration of the protein enclosed within. The X-rays specifically interact with the electron clouds surrounding the individual atoms within the protein crystal lattice and produce a discernible diffraction pattern. Only by relating the electron density clouds with the amplitude of the X-rays can this pattern be read and lead to assumptions of the phases or phase angles involved that complicate this method. Without the relation established through a mathematical basis known as Fourier transform, the \"phase problem\" would render predicting the diffraction patterns very difficult. Emerging methods like multiple isomorphous replacement use the presence of a heavy metal ion to diffract the X-rays into a more predictable manner, reducing the number of variables involved and resolving the phase problem.\\n\\n\\n=== Fluorescence spectroscopy ===\\nFluorescence spectroscopy is a highly sensitive method for studying the folding state of proteins. Three amino acids, phenylalanine (Phe), tyrosine (Tyr) and tryptophan (Trp), have intrinsic fluorescence properties, but only Tyr and Trp are used experimentally because their quantum yields are high enough to give good fluorescence signals. Both Trp and Tyr are excited by a wavelength of 280 nm, whereas only Trp is excited by a wavelength of 295 nm. Because of their aromatic character, Trp and Tyr residues are often found fully or partially buried in the hydrophobic core of proteins, at the interface between two protein domains, or at the interface between subunits of oligomeric proteins. In this apolar environment, they have high quantum yields and therefore high fluorescence intensities. Upon disruption of the protein\\'s tertiary or quaternary structure, these side chains become more exposed to the hydrophilic environment of the solvent, and their quantum yields decrease, leading to low fluorescence intensities. For Trp residues, the wavelength of their maximal fluorescence emission also depend on their environment.\\nFluorescence spectroscopy can be used to characterize the equilibrium unfolding of proteins by measuring the variation in the intensity of fluorescence emission or in the wavelength of maximal emission as functions of a denaturant value. The denaturant can be a chemical molecule (urea, guanidinium hydrochloride), temperature, pH, pressure, etc. The equilibrium between the different but discrete protein states, i.e. native state, intermediate states, unfolded state, depends on the denaturant value; therefore, the global fluorescence signal of their equilibrium mixture also depends on this value. One thus obtains a profile relating the global protein signal to the denaturant value. The profile of equilibrium unfolding may enable one to detect and identify intermediates of unfolding. General equations have been developed by Hugues Bedouelle to obtain the thermodynamic parameters that characterize the unfolding equilibria for homomeric or heteromeric proteins, up to trimers and potentially tetramers, from such profiles. Fluorescence spectroscopy can be combined with fast-mixing devices such as stopped flow, to measure protein folding kinetics, generate a chevron plot and derive a Phi value analysis.\\n\\n\\n=== Circular dichroism ===\\n\\nCircular dichroism is one of the most general and basic tools to study protein folding. Circular dichroism spectroscopy measures the absorption of circularly polarized light. In proteins, structures such as alpha helices and beta sheets are chiral, and thus absorb such light. The absorption of this light acts as a marker of the degree of foldedness of the protein ensemble. This technique has been used to measure equilibrium unfolding of the protein by measuring the change in this absorption as a function of denaturant concentration or temperature. A denaturant melt measures the free energy of unfolding as well as the protein\\'s m value, or denaturant dependence. A temperature melt measures the denaturation temperature (Tm) of the protein. As for fluorescence spectroscopy, circular-dichroism spectroscopy can be combined with fast-mixing devices such as stopped flow to measure protein folding kinetics and to generate chevron plots.\\n\\n\\n=== Vibrational circular dichroism of proteins ===\\nThe more recent developments of vibrational circular dichroism (VCD) techniques for proteins, currently involving Fourier transform (FT) instruments, provide powerful means for determining protein conformations in solution even for very large protein molecules. Such VCD studies of proteins can be combined with X-ray diffraction data for protein crystals, FT-IR data for protein solutions in heavy water (D2O), or quantum computations.\\n\\n\\n=== Protein nuclear magnetic resonance spectroscopy ===\\n\\nProtein nuclear magnetic resonance (NMR) is able to collect protein structural data by inducing a magnet field through samples of concentrated protein. In NMR, depending on the chemical environment, certain nuclei will absorb specific radio-frequencies. Because protein structural changes operate on a time scale from ns to ms, NMR is especially equipped to study intermediate structures in timescales of ps to s. Some of the main techniques for studying proteins structure and non-folding protein structural changes include COSY, TOCSY, HSQC, time relaxation (T1 & T2), and NOE. NOE is especially useful because magnetization transfers can be observed between spatially proximal hydrogens are observed. Different NMR experiments have varying degrees of timescale sensitivity that are appropriate for different protein structural changes. NOE can pick up bond vibrations or side chain rotations, however, NOE is too sensitive to pick up protein folding because it occurs at larger timescale.\\nBecause protein folding takes place in about 50 to 3000 s−1 CPMG Relaxation dispersion and chemical exchange saturation transfer have become some of the primary techniques for NMR analysis of folding. In addition, both techniques are used to uncover excited intermediate states in the protein folding landscape. To do this, CPMG Relaxation dispersion takes advantage of the spin echo phenomenon. This technique exposes the target nuclei to a 90 pulse followed by one or more 180 pulses. As the nuclei refocus, a broad distribution indicates the target nuclei is involved in an intermediate excited state. By looking at Relaxation dispersion plots the data collect information on the thermodynamics and kinetics between the excited and ground. Saturation Transfer measures changes in signal from the ground state as excited states become perturbed. It uses weak radio frequency irradiation to saturate the excited state of a particular nuclei which transfers its saturation to the ground state. This signal is amplified by decreasing the magnetization (and the signal) of the ground state.The main limitations in NMR is that its resolution decreases with proteins that are larger than 25 kDa and is not as detailed as X-ray crystallography. Additionally, protein NMR analysis is quite difficult and can propose multiple solutions from the same NMR spectrum.In a study focused on the folding of an amyotrophic lateral sclerosis involved protein SOD1, excited intermediates were studied with relaxation dispersion and Saturation transfer. SOD1 had been previously tied to many disease causing mutants which were assumed to be involved in protein aggregation, however the mechanism was still unknown. By using Relaxation Dispersion and Saturation Transfer experiments many excited intermediate states were uncovered misfolding in the SOD1 mutants.\\n\\n\\n=== Dual-polarization interferometry ===\\n\\nDual polarisation interferometry is a surface-based technique for measuring the optical properties of molecular layers. When used to characterize protein folding, it measures the conformation by determining the overall size of a monolayer of the protein and its density in real time at sub-Angstrom resolution, although real-time measurement of the kinetics of protein folding are limited to processes that occur slower than ~10 Hz. Similar to circular dichroism, the stimulus for folding can be a denaturant or temperature.\\n\\n\\n=== Studies of folding with high time resolution ===\\nThe study of protein folding has been greatly advanced in recent years by the development of fast, time-resolved techniques. Experimenters rapidly trigger the folding of a sample of unfolded protein and observe the resulting dynamics. Fast techniques in use include neutron scattering, ultrafast mixing of solutions, photochemical methods, and laser temperature jump spectroscopy. Among the many scientists who have contributed to the development of these techniques are Jeremy Cook, Heinrich Roder, Harry Gray, Martin Gruebele, Brian Dyer, William Eaton, Sheena Radford, Chris Dobson, Alan Fersht, Bengt Nölting and Lars Konermann.\\n\\n\\n=== Proteolysis ===\\nProteolysis is routinely used to probe the fraction unfolded under a wide range of solution conditions (e.g. fast parallel proteolysis (FASTpp).\\n\\n\\n=== Single-molecule force spectroscopy ===\\nSingle molecule techniques such as optical tweezers and AFM have been used to understand protein folding mechanisms of isolated proteins as well as proteins with chaperones. Optical tweezers have been used to stretch single protein molecules from their C- and N-termini and unfold them to allow study of the subsequent refolding. The technique allows one to measure folding rates at single-molecule level; for example, optical tweezers have been recently applied to study folding and unfolding of proteins involved in blood coagulation. von Willebrand factor (vWF) is a protein with an essential role in blood clot formation process. It discovered – using single molecule optical tweezers measurement – that calcium-bound vWF acts as a shear force sensor in the blood. Shear force leads to unfolding of the A2 domain of vWF, whose refolding rate is dramatically enhanced in the presence of calcium. Recently, it was also shown that the simple src SH3 domain accesses multiple unfolding pathways under force.\\n\\n\\n=== Biotin painting ===\\nBiotin painting enables condition-specific cellular snapshots of (un)folded proteins. Biotin \\'painting\\' shows a bias towards predicted Intrinsically disordered proteins.\\n\\n\\n== Computational studies of protein folding ==\\nComputational studies of protein folding includes three main aspects related to the prediction of protein stability, kinetics, and structure. A recent review summarizes the available computational methods for protein folding. \\n\\n\\n=== Levinthal\\'s paradox ===\\nIn 1969, Cyrus Levinthal noted that, because of the very large number of degrees of freedom in an unfolded polypeptide chain, the molecule has an astronomical number of possible conformations. An estimate of 3300 or 10143 was made in one of his papers. Levinthal\\'s paradox is a thought experiment based on the observation that if a protein were folded by sequential sampling of all possible conformations, it would take an astronomical amount of time to do so, even if the conformations were sampled at a rapid rate (on the nanosecond or picosecond scale). Based upon the observation that proteins fold much faster than this, Levinthal then proposed that a random conformational search does not occur, and the protein must, therefore, fold through a series of meta-stable intermediate states.\\n\\n\\n=== Energy landscape of protein folding ===\\n\\nThe configuration space of a protein during folding can be visualized as an energy landscape. According to Joseph Bryngelson and Peter Wolynes, proteins follow the principle of minimal frustration meaning that naturally evolved proteins have optimized their folding energy landscapes, and that nature has chosen amino acid sequences so that the folded state of the protein is sufficiently stable. In addition, the acquisition of the folded state had to become a sufficiently fast process. Even though nature has reduced the level of frustration in proteins, some degree of it remains up to now as can be observed in the presence of local minima in the energy landscape of proteins.\\nA consequence of these evolutionarily selected sequences is that proteins are generally thought to have globally \"funneled energy landscapes\" (coined by José Onuchic) that are largely directed toward the native state. This \"folding funnel\" landscape allows the protein to fold to the native state through any of a large number of pathways and intermediates, rather than being restricted to a single mechanism. The theory is supported by both computational simulations of model proteins and experimental studies, and it has been used to improve methods for protein structure prediction and design. The description of protein folding by the leveling free-energy landscape is also consistent with the 2nd law of thermodynamics. Physically, thinking of landscapes in terms of visualizable potential or total energy surfaces simply with maxima, saddle points, minima, and funnels, rather like geographic landscapes, is perhaps a little misleading. The relevant description is really a high-dimensional phase space in which manifolds might take a variety of more complicated topological forms.The unfolded polypeptide chain begins at the top of the funnel where it may assume the largest number of unfolded variations and is in its highest energy state. Energy landscapes such as these indicate that there are a large number of initial possibilities, but only a single native state is possible; however, it does not reveal the numerous folding pathways that are possible. A different molecule of the same exact protein may be able to follow marginally different folding pathways, seeking different lower energy intermediates, as long as the same native structure is reached. Different pathways may have different frequencies of utilization depending on the thermodynamic favorability of each pathway. This means that if one pathway is found to be more thermodynamically favorable than another, it is likely to be used more frequently in the pursuit of the native structure. As the protein begins to fold and assume its various conformations, it always seeks a more thermodynamically favorable structure than before and thus continues through the energy funnel. Formation of secondary structures is a strong indication of increased stability within the protein, and only one combination of secondary structures assumed by the polypeptide backbone will have the lowest energy and therefore be present in the native state of the protein. Among the first structures to form once the polypeptide begins to fold are alpha helices and beta turns, where alpha helices can form in as little as 100 nanoseconds and beta turns in 1 microsecond.There exists a saddle point in the energy funnel landscape where the transition state for a particular protein is found. The transition state in the energy funnel diagram is the conformation that must be assumed by every molecule of that protein if the protein wishes to finally assume the native structure. No protein may assume the native structure without first passing through the transition state. The transition state can be referred to as a variant or premature form of the native state rather than just another intermediary step. The folding of the transition state is shown to be rate-determining, and even though it exists in a higher energy state than the native fold, it greatly resembles the native structure. Within the transition state, there exists a nucleus around which the protein is able to fold, formed by a process referred to as \"nucleation condensation\" where the structure begins to collapse onto the nucleus.\\n\\n\\n=== Modeling of protein folding ===\\n \\nDe novo or ab initio techniques for computational protein structure prediction can be used for simulating various aspects of protein folding. Molecular Dynamics (MD) was used in simulations of protein folding and dynamics in silico. First equilibrium folding simulations were done using implicit solvent model and umbrella sampling. Because of computational cost, ab initio MD folding simulations with explicit water are limited to peptides and very small proteins. MD simulations of larger proteins remain restricted to dynamics of the experimental structure or its high-temperature unfolding. Long-time folding processes (beyond about 1 millisecond), like folding of small-size proteins (about 50 residues) or larger, can be accessed using coarse-grained models.Several large-scale computational projects, such as Rosetta@home, Folding@home and Foldit, target protein folding.\\nLong continuous-trajectory simulations have been performed on Anton, a massively parallel supercomputer designed and built around custom ASICs and interconnects by D. E. Shaw Research. The longest published result of a simulation performed using Anton is a 2.936 millisecond simulation of NTL9 at 355 K.\\n\\n\\n== See also ==\\n\\n\\n== References ==\\n\\n\\n== External links ==\\nHuman Proteome Folding Project', 'A phylogenetic tree (also phylogeny or evolutionary tree ) is a branching diagram or a tree showing the evolutionary relationships among various biological species or other entities based upon similarities and differences in their physical or genetic characteristics. All life on Earth is part of a single phylogenetic tree, indicating common ancestry.\\nIn a rooted phylogenetic tree, each node with descendants represents the inferred most recent common ancestor of those descendants, and the edge lengths in some trees may be interpreted as time estimates. Each node is called a taxonomic unit. Internal nodes are generally called hypothetical taxonomic units, as they cannot be directly observed. Trees are useful in fields of biology such as bioinformatics, systematics, and phylogenetics. Unrooted trees illustrate only the relatedness of the leaf nodes and do not require the ancestral root to be known or inferred.\\n\\n\\n== History ==\\nThe idea of a \"tree of life\" arose from ancient notions of a ladder-like progression from lower into higher forms of life (such as in the Great Chain of Being). Early representations of \"branching\" phylogenetic trees include a \"paleontological chart\" showing the geological relationships among plants and animals in the book Elementary Geology, by Edward Hitchcock (first edition: 1840).\\nCharles Darwin (1859) also produced one of the first illustrations and crucially popularized the notion of an evolutionary \"tree\" in his seminal book The Origin of Species. Over a century later, evolutionary biologists still use tree diagrams to depict evolution because such diagrams effectively convey the concept that speciation occurs through the adaptive and semirandom splitting of lineages. Over time, species classification has become less static and more dynamic.\\nThe term phylogenetic, or phylogeny, derives from the two ancient greek words φῦλον (phûlon), meaning \"race, lineage\", and γένεσις (génesis), meaning \"origin, source\".\\n\\n\\n== Properties ==\\n\\n\\n=== Rooted tree ===\\n\\nA rooted phylogenetic tree (see two graphics at top) is a directed tree with a unique node — the root — corresponding to the (usually imputed) most recent common ancestor of all the entities at the leaves of the tree. The root node does not have a parent node, but serves as the parent of all other nodes in the tree. The root is therefore a node of degree 2, while other internal nodes have a minimum degree of 3 (where \"degree\" here refers to the total number of incoming and outgoing edges).\\nThe most common method for rooting trees is the use of an uncontroversial outgroup—close enough to allow inference from trait data or molecular sequencing, but far enough to be a clear outgroup.\\n\\n\\n=== Unrooted tree ===\\n\\nUnrooted trees illustrate the relatedness of the leaf nodes without making assumptions about ancestry. They do not require the ancestral root to be known or inferred. Unrooted trees can always be generated from rooted ones by simply omitting the root. By contrast, inferring the root of an unrooted tree requires some means of identifying ancestry. This is normally done by including an outgroup in the input data so that the root is necessarily between the outgroup and the rest of the taxa in the tree, or by introducing additional assumptions about the relative rates of evolution on each branch, such as an application of the molecular clock hypothesis.\\n\\n\\n=== Bifurcating versus multifurcating ===\\nBoth rooted and unrooted trees can be either bifurcating or multifurcating. A rooted bifurcating tree has exactly two descendants arising from each interior node (that is, it forms a binary tree), and an unrooted bifurcating tree takes the form of an unrooted binary tree, a free tree with exactly three neighbors at each internal node. In contrast, a rooted multifurcating tree may have more than two children at some nodes and an unrooted multifurcating tree may have more than three neighbors at some nodes.\\n\\n\\n=== Labeled versus unlabeled ===\\nBoth rooted and unrooted trees can be either labeled or unlabeled. A labeled tree has specific values assigned to its leaves, while an unlabeled tree, sometimes called a tree shape, defines a topology only. Some sequence-based trees built from a small genomic locus, such as Phylotree, feature internal nodes labeled with inferred ancestral haplotypes.\\n\\n\\n=== Enumerating trees ===\\n\\nThe number of possible trees for a given number of leaf nodes depends on the specific type of tree, but there are always more labeled than unlabeled trees, more multifurcating than bifurcating trees, and more rooted than unrooted trees. The last distinction is the most biologically relevant; it arises because there are many places on an unrooted tree to put the root. For bifurcating labeled trees, the total number of rooted trees is:\\n\\n  \\n    \\n      \\n        (\\n        2\\n        n\\n        −\\n        3\\n        )\\n        !\\n        !\\n        =\\n        \\n          \\n            \\n              (\\n              2\\n              n\\n              −\\n              3\\n              )\\n              !\\n            \\n            \\n              \\n                2\\n                \\n                  n\\n                  −\\n                  2\\n                \\n              \\n              (\\n              n\\n              −\\n              2\\n              )\\n              !\\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle (2n-3)!!={\\\\frac {(2n-3)!}{2^{n-2}(n-2)!}}}\\n   for \\n  \\n    \\n      \\n        n\\n        ≥\\n        2\\n      \\n    \\n    {\\\\displaystyle n\\\\geq 2}\\n  , \\n  \\n    \\n      \\n        n\\n      \\n    \\n    {\\\\displaystyle n}\\n   represents the number of leaf nodes.For bifurcating labeled trees, the total number of unrooted trees is:\\n\\n  \\n    \\n      \\n        (\\n        2\\n        n\\n        −\\n        5\\n        )\\n        !\\n        !\\n        =\\n        \\n          \\n            \\n              (\\n              2\\n              n\\n              −\\n              5\\n              )\\n              !\\n            \\n            \\n              \\n                2\\n                \\n                  n\\n                  −\\n                  3\\n                \\n              \\n              (\\n              n\\n              −\\n              3\\n              )\\n              !\\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle (2n-5)!!={\\\\frac {(2n-5)!}{2^{n-3}(n-3)!}}}\\n   for \\n  \\n    \\n      \\n        n\\n        ≥\\n        3\\n      \\n    \\n    {\\\\displaystyle n\\\\geq 3}\\n  .Among labeled bifurcating trees, the number of unrooted trees with \\n  \\n    \\n      \\n        n\\n      \\n    \\n    {\\\\displaystyle n}\\n   leaves is equal to the number of rooted trees with \\n  \\n    \\n      \\n        n\\n        −\\n        1\\n      \\n    \\n    {\\\\displaystyle n-1}\\n   leaves.The number of rooted trees grows quickly as a function of the number of tips. For 10 tips, there are more than \\n  \\n    \\n      \\n        34\\n        ×\\n        \\n          10\\n          \\n            6\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle 34\\\\times 10^{6}}\\n   possible bifurcating trees, and the number of multifurcating trees rises faster, with ca. 7 times as many of the latter as of the former.\\n\\n\\n== Special tree types ==\\n\\n\\n=== Dendrogram ===\\nA dendrogram is a general name for a tree, whether phylogenetic or not, and hence also for the diagrammatic representation of a phylogenetic tree.\\n\\n\\n=== Cladogram ===\\nA cladogram only represents a branching pattern; i.e., its branch lengths do not represent time or relative amount of character change, and its internal nodes do not represent ancestors.\\n\\n\\n=== Phylogram ===\\nA phylogram is a phylogenetic tree that has branch lengths proportional to the amount of character change.A chronogram is a phylogenetic tree that explicitly represents time through its branch lengths.\\n\\n\\n=== Dahlgrenogram ===\\nA Dahlgrenogram is a diagram representing a cross section of a phylogenetic tree\\n\\n\\n=== Phylogenetic network ===\\nA phylogenetic network is not strictly speaking a tree, but rather a more general graph, or a directed acyclic graph in the case of rooted networks. They are used to overcome some of the limitations inherent to trees.\\n\\n\\n=== Spindle diagram ===\\n\\nA spindle diagram, or bubble diagram, is often called a romerogram, after its popularisation by the American palaeontologist Alfred Romer.\\nIt represents taxonomic diversity (horizontal width) against geological time (vertical axis) in order to reflect the variation of abundance of various taxa through time.\\nHowever, a spindle diagram is not an evolutionary tree: the taxonomic spindles obscure the actual relationships of the parent taxon to the daughter taxon and have the disadvantage of involving the paraphyly of the parental group.\\nThis type of diagram is no longer used in the form originally proposed.\\n\\n\\n=== Coral of life ===\\n Darwin also mentioned that the coral may be a more suitable metaphor than the tree. Indeed, phylogenetic corals are useful for portraying past and present life, and they have some advantages over trees (anastomoses allowed, etc.).\\n\\n\\n== Construction ==\\n\\nPhylogenetic trees composed with a nontrivial number of input sequences are constructed using computational phylogenetics methods. Distance-matrix methods such as neighbor-joining or UPGMA, which calculate genetic distance from multiple sequence alignments, are simplest to implement, but do not invoke an evolutionary model. Many sequence alignment methods such as ClustalW also create trees by using the simpler algorithms (i.e. those based on distance) of tree construction. Maximum parsimony is another simple method of estimating phylogenetic trees, but implies an implicit model of evolution (i.e. parsimony). More advanced methods use the optimality criterion of maximum likelihood, often within a Bayesian framework, and apply an explicit model of evolution to phylogenetic tree estimation. Identifying the optimal tree using many of these techniques is NP-hard, so heuristic search and optimization methods are used in combination with tree-scoring functions to identify a reasonably good tree that fits the data.\\nTree-building methods can be assessed on the basis of several criteria:\\nefficiency (how long does it take to compute the answer, how much memory does it need?)\\npower (does it make good use of the data, or is information being wasted?)\\nconsistency (will it converge on the same answer repeatedly, if each time given different data for the same model problem?)\\nrobustness (does it cope well with violations of the assumptions of the underlying model?)\\nfalsifiability (does it alert us when it is not good to use, i.e. when assumptions are violated?)Tree-building techniques have also gained the attention of mathematicians. Trees can also be built using T-theory.\\n\\n\\n=== File formats ===\\nTrees can be encoded in a number of different formats, all of which must represent the nested structure of a tree. They may or may not encode branch lengths and other features. Standardized formats are critical for distributing and sharing trees without relying on graphics output that is hard to import into existing software. Commonly used formats are\\n\\nNexus file format\\nNewick format\\n\\n\\n== Limitations of phylogenetic analysis ==\\nAlthough phylogenetic trees produced on the basis of sequenced genes or genomic data in different species can provide evolutionary insight, these analyses have important limitations. Most importantly, the trees that they generate are not necessarily correct – they do not necessarily accurately represent the evolutionary history of the included taxa. As with any scientific result, they are subject to falsification by further study (e.g., gathering of additional data, analyzing the existing data with improved methods). The data on which they are based may be noisy; the analysis can be confounded by genetic recombination, horizontal gene transfer, hybridisation between species that were not nearest neighbors on the tree before hybridisation takes place, convergent evolution, and conserved sequences.\\nAlso, there are problems in basing an analysis on a single type of character, such as a single gene or protein or only on morphological analysis, because such trees constructed from another unrelated data source often differ from the first, and therefore great care is needed in inferring phylogenetic relationships among species. This is most true of genetic material that is subject to lateral gene transfer and recombination, where different haplotype blocks can have different histories. In these types of analysis, the output tree of a phylogenetic analysis of a single gene is an estimate of the gene\\'s phylogeny (i.e. a gene tree) and not the phylogeny of the taxa (i.e. species tree) from which these characters were sampled, though ideally, both should be very close. For this reason, serious phylogenetic studies generally use a combination of genes that come from different genomic sources (e.g., from mitochondrial or plastid vs. nuclear genomes), or genes that would be expected to evolve under different selective regimes, so that homoplasy (false homology) would be unlikely to result from natural selection.\\nWhen extinct species are included as terminal nodes in an analysis (rather than, for example, to constrain internal nodes), they are considered not to represent direct ancestors of any extant species. Extinct species do not typically contain high-quality DNA.\\nThe range of useful DNA materials has expanded with advances in extraction and sequencing technologies. Development of technologies able to infer sequences from smaller fragments, or from spatial patterns of DNA degradation products, would further expand the range of DNA considered useful.\\nPhylogenetic trees can also be inferred from a range of other data types, including morphology, the presence or absence of particular types of genes, insertion and deletion events – and any other observation thought to contain an evolutionary signal.\\nPhylogenetic networks are used when bifurcating trees are not suitable, due to these complications which suggest a more reticulate evolutionary history of the organisms sampled.\\n\\n\\n== See also ==\\n\\n\\n== References ==\\n\\n\\n== Further reading ==\\nSchuh, R. T. and A. V. Z. Brower. 2009. Biological Systematics: principles and applications (2nd edn.) ISBN 978-0-8014-4799-0\\nManuel Lima, The Book of Trees: Visualizing Branches of Knowledge, 2014, Princeton Architectural Press, New York.\\nMEGA, a free software to draw phylogenetic trees.\\nGontier, N. 2011. \"Depicting the Tree of Life: the Philosophical and Historical Roots of Evolutionary Tree Diagrams.\"  Evolution, Education, Outreach 4: 515–538.\\n\\n\\n== External links ==\\n\\n\\n=== Images ===\\nHuman Y-Chromosome 2002 Phylogenetic Tree\\niTOL: Interactive Tree Of Life\\nPhylogenetic Tree of Artificial Organisms Evolved on Computers\\nMiyamoto and Goodman\\'s Phylogram of Eutherian Mammals\\n\\n\\n=== General ===\\nAn overview of different methods of tree visualization is available at Page, R. D. M. (2011). \"Space, time, form: Viewing the Tree of Life\". Trends in Ecology & Evolution. 27 (2): 113–120. doi:10.1016/j.tree.2011.12.002. PMID 22209094.\\nOneZoom: Tree of Life – all living species as intuitive and zoomable fractal explorer (responsive design)\\nDiscover Life An interactive tree based on the U.S. National Science Foundation\\'s Assembling the Tree of Life Project\\nPhyloCode\\nA Multiple Alignment of 139 Myosin Sequences and a Phylogenetic Tree\\nTree of Life Web Project\\nPhylogenetic inferring on the T-REX server\\nNCBI\\'s Taxonomy Database[1]\\nETE: A Python Environment for Tree Exploration This is a programming library to analyze, manipulate and visualize phylogenetic trees. Ref.\\nA daily-updated tree of (sequenced) life Fang, H.; Oates, M. E.; Pethica, R. B.; Greenwood, J. M.; Sardar, A. J.; Rackham, O. J. L.; Donoghue, P. C. J.; Stamatakis, A.; De Lima Morais, D. A.; Gough, J. (2013). \"A daily-updated tree of (sequenced) life as a reference for genome research\". Scientific Reports. 3: 2015. Bibcode:2013NatSR...3E2015F. doi:10.1038/srep02015. PMC 6504836. PMID 23778980.', 'Computational neuroscience (also known as theoretical neuroscience or mathematical neuroscience) is a branch of neuroscience which employs mathematical models, theoretical analysis and abstractions of the brain to understand the principles that govern the development, structure, physiology and cognitive abilities of the nervous system.Computational neuroscience employs computational simulations to validate and solve mathematical models, and so can be seen as a sub-field of theoretical neuroscience; however, the two fields are often synonymous. The term mathematical neuroscience is also used sometimes, to stress the quantitative nature of the field.Computational neuroscience focuses on the description of biologically plausible neurons (and neural systems) and their physiology and dynamics, and it is therefore not directly concerned with biologically unrealistic models used in connectionism, control theory, cybernetics, quantitative psychology, machine learning, artificial neural networks, artificial intelligence and computational learning theory; although mutual inspiration exists and sometimes there is no strict limit between fields, with model abstraction in computational neuroscience depending on research scope and the granularity at which biological entities are analyzed.\\nModels in theoretical neuroscience are aimed at capturing the essential features of the biological system at multiple spatial-temporal scales, from membrane currents, and chemical coupling via network oscillations, columnar and topographic architecture, nuclei, all the way up to psychological faculties like memory, learning and behavior. These computational models frame hypotheses that can be directly tested by biological or psychological experiments.\\n\\n\\n== History ==\\nThe term \\'computational neuroscience\\' was introduced by Eric L. Schwartz, who organized a conference, held in 1985 in Carmel, California, at the request of the Systems Development Foundation to provide a summary of the current status of a field which until that point was referred to by a variety of names, such as neural modeling, brain theory and neural networks. The proceedings of this definitional meeting were published in 1990 as the book Computational Neuroscience. The first of the annual open international meetings focused on Computational Neuroscience was organized by James M. Bower  and John Miller in San Francisco, California in 1989. The first graduate educational program in computational neuroscience was organized as the Computational and Neural Systems Ph.D. program at the California Institute of Technology in 1985.\\nThe early historical roots of the field can be traced to the work of people including Louis Lapicque, Hodgkin & Huxley, Hubel and Wiesel, and David Marr. Lapicque introduced the integrate and fire model of the neuron in a seminal article published in 1907, a model still popular for artificial neural networks studies because of its simplicity (see a recent review).\\nAbout 40 years later, Hodgkin & Huxley developed the voltage clamp and created the first biophysical model of the action potential. Hubel & Wiesel discovered that neurons in the primary visual cortex, the first cortical area to process information coming from the retina, have oriented receptive fields and are organized in columns. David Marr\\'s work focused on the interactions between neurons, suggesting computational approaches to the study of how functional groups of neurons within the hippocampus and neocortex interact, store, process, and transmit information. Computational modeling of biophysically realistic neurons and dendrites began with the work of Wilfrid Rall, with the first multicompartmental model using cable theory.\\n\\n\\n== Major topics ==\\nResearch in computational neuroscience can be roughly categorized into several lines of inquiry. Most computational neuroscientists collaborate closely with experimentalists in analyzing novel data and synthesizing new models of biological phenomena.\\n\\n\\n=== Single-neuron modeling ===\\n\\nEven a single neuron has complex biophysical characteristics and can perform computations (e.g.). Hodgkin and Huxley\\'s original model only employed two voltage-sensitive currents (Voltage sensitive ion channels are glycoprotein molecules which extend through the lipid bilayer, allowing ions to traverse under certain conditions through the axolemma), the fast-acting sodium and the inward-rectifying potassium. Though successful in predicting the timing and qualitative features of the action potential, it nevertheless failed to predict a number of important features such as adaptation and shunting. Scientists now believe that there are a wide variety of voltage-sensitive currents, and the implications of the differing dynamics, modulations, and sensitivity of these currents is an important topic of computational neuroscience.The computational functions of complex dendrites are also under intense investigation. There is a large body of literature regarding how different currents interact with geometric properties of neurons.Some models are also tracking biochemical pathways at very small scales such as spines or synaptic clefts.\\nThere are many software packages, such as  GENESIS and NEURON, that allow rapid and systematic in silico modeling of realistic neurons. Blue Brain, a project founded by Henry Markram from the École Polytechnique Fédérale de Lausanne, aims to construct a biophysically detailed simulation of a cortical column on the Blue Gene supercomputer.\\nModeling the richness of biophysical properties on the single-neuron scale can supply mechanisms that serve as the building blocks for network dynamics. However, detailed neuron descriptions are computationally expensive and this can handicap the pursuit of realistic network investigations, where many neurons need to be simulated. As a result, researchers that study large neural circuits typically represent each neuron and synapse with an artificially simple model, ignoring much of the biological detail. Hence there is a drive to produce simplified neuron models that can retain significant biological fidelity at a low computational overhead. Algorithms have been developed to produce faithful, faster running, simplified surrogate neuron models from computationally expensive, detailed neuron models.\\n\\n\\n=== Development, axonal patterning, and guidance ===\\nComputational neuroscience aims to address a wide array of questions. How do axons and dendrites form during development? How do axons know where to target and how to reach these targets? How do neurons migrate to the proper position in the central and peripheral systems? How do synapses form? We know from molecular biology that distinct parts of the nervous system release distinct chemical cues, from growth factors to hormones that modulate and influence the growth and development of functional connections between neurons.\\nTheoretical investigations into the formation and patterning of synaptic connection and morphology are still nascent. One hypothesis that has recently garnered some attention is the minimal wiring hypothesis, which postulates that the formation of axons and dendrites effectively minimizes resource allocation while maintaining maximal information storage.\\n\\n\\n=== Sensory processing ===\\nEarly models on sensory processing understood within a theoretical framework are credited to Horace Barlow. Somewhat similar to the minimal wiring hypothesis described in the preceding section, Barlow understood the processing of the early sensory systems to be a form of  efficient coding, where the neurons encoded information which minimized the number of spikes. Experimental and computational work have since supported this hypothesis in one form or another.  For the example of visual processing, efficient coding is manifested in the\\nforms of efficient spatial coding, color coding, temporal/motion coding,  stereo coding, and combinations of them.Further along the visual pathway, even the efficiently coded visual information is too much for the capacity of the information bottleneck, the visual attentional bottleneck. A subsequent theory, V1 Saliency Hypothesis (V1SH), has been developed on exogenous attentional selection of a fraction of visual input for further processing, guided by a bottom-up saliency map in the primary visual cortex.Current research in sensory processing is divided among a biophysical modelling of different subsystems and a more theoretical modelling of perception. Current models of perception have suggested that the brain performs some form of Bayesian inference and integration of different sensory information in generating our perception of the physical world.\\n\\n\\n=== Motor control ===\\nMany models of the way the brain controls movement have been developed. This includes models of  processing in the brain such as the cerebellum\\'s role for error correction, skill learning in motor cortex and the basal ganglia, or the control of the vestibulo ocular reflex. This also includes many normative models, such as those of the Bayesian or optimal control flavor which are built on the idea that the brain efficiently solves its problems.\\n\\n\\n=== Memory and synaptic plasticity ===\\n\\nEarlier models of memory are primarily based on the postulates of Hebbian learning. Biologically relevant models such as  Hopfield net have been developed to address the properties of associative (also known as \"content-addressable\") style of memory that occur in biological systems. These attempts are primarily focusing on the formation of medium- and long-term memory, localizing in the hippocampus. Models of working memory, relying on theories of network oscillations and persistent activity, have been built to capture some features of the prefrontal cortex in context-related memory. Additional models look at the close relationship between the basal ganglia and the prefrontal cortex and how that contributes to working memory.One of the major problems in neurophysiological memory is how it is maintained and changed through multiple time scales. Unstable synapses are easy to train but also prone to stochastic disruption. Stable  synapses forget less easily, but they are also harder to consolidate. One recent computational hypothesis involves cascades of plasticity that allow synapses to function at multiple time scales. Stereochemically detailed models of the acetylcholine receptor-based synapse with the Monte Carlo method, working at the time scale of microseconds, have been built. It is likely that computational tools will contribute greatly to our understanding of how synapses function and change in relation to external stimulus in the coming decades.\\n\\n\\n=== Behaviors of networks ===\\nBiological neurons are connected to each other in a complex, recurrent fashion. These connections are, unlike most artificial neural networks, sparse and usually specific.  It is not known how information is transmitted through such sparsely connected networks, although specific areas of the brain, such as the visual cortex, are understood in some detail. It is also unknown what the computational functions of these specific connectivity patterns are, if any.\\nThe interactions of neurons in a small network can be often reduced to simple models such as the Ising model. The statistical mechanics of such simple systems are well-characterized theoretically. Some recent evidence suggests that dynamics of arbitrary neuronal networks can be reduced to pairwise interactions. It is not known, however, whether such descriptive dynamics impart any important computational function. With the emergence of two-photon microscopy and calcium imaging, we now have powerful experimental methods with which to test the new theories regarding neuronal networks.\\nIn some cases the complex interactions between inhibitory and excitatory neurons can be simplified using mean-field theory, which gives rise to the population model of neural networks. While many neurotheorists prefer such models with reduced complexity, others argue that uncovering structural-functional relations depends on including as much neuronal and network structure as possible. Models of this type are typically built in large simulation platforms like GENESIS or NEURON. There have been some attempts to provide unified methods that bridge and integrate these levels of complexity.\\n\\n\\n=== Visual attention, identification, and categorization ===\\nVisual attention can be described as a set of mechanisms that limit some processing to a subset of incoming stimuli. Attentional mechanisms shape what we see and what we can act upon. They allow for concurrent selection of some (preferably, relevant) information and inhibition of other information. In order to have a more concrete specification of the mechanism underlying visual attention and the binding of features, a number of computational models have been proposed aiming to explain psychophysical findings. In general, all models postulate the existence of a saliency or priority map for registering the potentially interesting areas of the retinal input, and a gating mechanism for reducing the amount of incoming visual information, so that the limited computational resources of the brain can handle it.\\nAn example theory that is being extensively tested behaviorally and physiologically is the  V1 Saliency Hypothesis  that a bottom-up saliency map is created in the primary visual cortex to guide attention exogenously. Computational neuroscience provides a mathematical framework for studying the mechanisms involved in brain function and allows complete simulation and prediction of neuropsychological syndromes.\\n\\n\\n=== Cognition, discrimination, and learning ===\\nComputational modeling of higher cognitive functions has only recently begun. Experimental data comes primarily from single-unit recording in primates.  The frontal lobe and parietal lobe function as integrators of information from multiple sensory modalities. There are some tentative ideas regarding how simple mutually inhibitory functional circuits in these areas may carry out biologically relevant computation.The brain seems to be able to discriminate and adapt particularly well in certain contexts. For instance, human beings seem to have an enormous capacity for memorizing and recognizing faces. One of the key goals of computational neuroscience is to dissect how biological systems carry out these complex computations efficiently and potentially replicate these processes in building intelligent machines.\\nThe brain\\'s large-scale organizational principles are illuminated by many fields, including biology, psychology, and clinical practice. Integrative neuroscience attempts to consolidate these observations through unified descriptive models and databases of behavioral measures and recordings. These are the bases for some quantitative modeling of large-scale brain activity.The Computational Representational Understanding of Mind (CRUM) is another attempt at modeling human cognition through simulated processes like acquired rule-based systems in decision making and the manipulation of visual representations in decision making.\\n\\n\\n=== Consciousness ===\\nOne of the ultimate goals of psychology/neuroscience is to be able to explain the everyday experience of conscious life. Francis Crick, Giulio Tononi and Christof Koch made some attempts to formulate consistent frameworks for future work in neural correlates of consciousness (NCC), though much of the work in this field remains speculative. Specifically, Crick cautioned the field of neuroscience to not approach topics that are traditionally left to philosophy and religion.\\n\\n\\n=== Computational clinical neuroscience ===\\nComputational clinical neuroscience is a field that brings together experts in neuroscience, neurology, psychiatry, decision sciences and computational modeling to quantitatively define and investigate problems in neurological and psychiatric diseases, and to train scientists and clinicians that wish to apply these models to diagnosis and treatment.\\n\\n\\n== Technology ==\\n\\n\\n=== Neuromorphic computing ===\\nA neuromorphic computer/chip is any device that uses physical artificial neurons (made from silicon) to do computations (See: neuromorphic computing, physical neural network). One of the advantages of using a physical model computer such as this is that it takes the computational load of the processor (in the sense that the structural and some of the functional elements don\\'t have to be programmed since they are in hardware). In recent times, neuromorphic technology has been used to build supercomputers which are used in international neuroscience collaborations. Examples include the Human Brain Project SpiNNaker supercomputer and the BrainScaleS computer.\\n\\n\\n== See also ==\\n\\n\\n== Notes and references ==\\n\\n\\n== Bibliography ==\\nChklovskii DB (2004). \"Synaptic connectivity and neuronal morphology: two sides of the same coin\". Neuron. 43 (5): 609–17. doi:10.1016/j.neuron.2004.08.012. PMID 15339643. S2CID 16217065.\\nSejnowski, Terrence J.; Churchland, Patricia Smith (1992). The computational brain. Cambridge, Mass: MIT Press. ISBN 978-0-262-03188-2.\\nGerstner, W.; Kistler, W.; Naud, R.; Paninski, L. (2014). Neuronal Dynamics. Cambridge, UK: Cambridge University Press. ISBN 9781107447615.\\nDayan P.; Abbott, L. F. (2001). Theoretical neuroscience: computational and mathematical modeling of neural systems. Cambridge, Mass: MIT Press. ISBN 978-0-262-04199-7.\\nEliasmith, Chris; Anderson, Charles H. (2003). Neural engineering: Representation, computation, and dynamics in neurobiological systems. Cambridge, Mass: MIT Press. ISBN 978-0-262-05071-5.\\nHodgkin AL, Huxley AF (28 August 1952). \"A quantitative description of membrane current and its application to conduction and excitation in nerve\". J. Physiol. 117 (4): 500–44. doi:10.1113/jphysiol.1952.sp004764. PMC 1392413. PMID 12991237.\\nWilliam Bialek; Rieke, Fred; David Warland; Rob de Ruyter van Steveninck (1999). Spikes: exploring the neural code. Cambridge, Mass: MIT. ISBN 978-0-262-68108-7.\\nSchutter, Erik de (2001). Computational neuroscience: realistic modeling for experimentalists. Boca Raton: CRC. ISBN 978-0-8493-2068-2.\\nSejnowski, Terrence J.; Hemmen, J. L. van (2006). 23 problems in systems neuroscience. Oxford [Oxfordshire]: Oxford University Press. ISBN 978-0-19-514822-0.\\nMichael A. Arbib; Shun-ichi Amari; Prudence H. Arbib (2002). The Handbook of Brain Theory and Neural Networks. Cambridge, Massachusetts: The MIT Press. ISBN 978-0-262-01197-6.\\nZhaoping, Li (2014). Understanding vision: theory, models, and data. Oxford, UK: Oxford University Press. ISBN 978-0199564668.\\n\\n\\n== See also ==\\n\\n\\n=== Software ===\\nBRIAN, a Python based simulator\\nBudapest Reference Connectome, web based 3D visualization tool to browse connections in the human brain\\nEmergent, neural simulation software.\\nGENESIS, a general neural simulation system.\\nNEST is a simulator for spiking neural network models that focuses on the dynamics, size and structure of neural systems rather than on the exact morphology of individual neurons.\\n\\n\\n== External links ==\\n\\n\\n=== Journals ===\\nJournal of Mathematical Neuroscience\\nJournal of Computational Neuroscience\\nNeural Computation\\nCognitive Neurodynamics\\nFrontiers in Computational Neuroscience\\nPLoS Computational Biology\\nFrontiers in Neuroinformatics\\n\\n\\n=== Conferences ===\\nComputational and Systems Neuroscience (COSYNE) – a computational neuroscience meeting with a systems neuroscience focus.\\nAnnual Computational Neuroscience Meeting (CNS) – a yearly computational neuroscience meeting.\\nComputational Cognitive Neuroscience - a yearly computational neuroscience meeting with a focus on cognitive phenomena.\\nNeural Information Processing Systems (NIPS)– a leading annual conference covering mostly machine learning.\\nInternational Conference on Cognitive Neurodynamics (ICCN) – a yearly conference.\\nUK Mathematical Neurosciences Meeting– a yearly conference, focused on mathematical aspects.\\nBernstein Conference on Computational Neuroscience (BCCN)– a yearly computational neuroscience conference ].\\nAREADNE Conferences– a biennial meeting that includes theoretical and experimental results.\\n\\n\\n=== Websites ===\\nEncyclopedia of Computational Neuroscience, part of Scholarpedia, an online expert curated encyclopedia on computational neuroscience and dynamical systems', 'Neurophysiology is the study of nerve cells (neurones) as they receive and transmit information. It is a branch of physiology and neuroscience that focuses on the functioning of the nervous system. The word originates from the Greek word νεῦρον meaning \"nerve\" and physiology meaning knowledge about the function of living systems (φύσις meaning \"nature\" and -λογία meaning \"knowledge\"). Studies of neurophysiology emerged as early as 4000 BCE. During this time period, the focus was to better understand the nervous system through the brain and spinal cord and the connection it has with mental health. Currently methods used to utilise the research of neurophysiology include electrophysiological recordings, such as patch clamp, voltage clamp, extracellular single-unit recording and recording of local field potentials.\\n\\n\\n== History ==\\nNeurophysiology has been a subject of study since as early as 4,000 B.C.\\nIn the early B.C. years, most studies were of different natural sedatives like alcohol and poppy plants. In 1700 B.C., the Edwin Smith surgical papyrus was written. This papyrus was crucial in understanding how the ancient Egyptians understood the nervous system.  This papyrus looked at different case studies about injuries to different parts of the body, most notably the head. Beginning around 460 B.C., Hippocrates began to study epilepsy, and theorized that it had its origins in the brain. Hippocrates also theorized that the brain was involved in sensation, and that it was where intelligence was derived from.  Hippocrates, as well as most ancient Greeks, believed that relaxation and a stress free environment was crucial in helping treat neurological disorders. In 280 B.C., Erasistratus of Chios theorized that there were divisions in the vestibular processing the brain, as well as deducing from observation that sensation was located there.\\nIn 177 Galen theorized that human thought occurred in the brain, as opposed to the heart as Aristotle had theorized. The optic chiasm, which is crucial to the visual system, was discovered around 100 C.E. by Marinus. Circa 1000, Al-Zahrawi, living in Iberia, began to write about different surgical treatments for neurological disorders. In 1216, the first anatomy textbook in Europe, which included a description of the brain, was written by Mondino de Luzzi. In 1402, St Mary of Bethlehem Hospital (later known as Bedlam in Britain) was the first hospital used exclusively for the mentally ill.\\nIn 1504, Leonardo da Vinci continued his study of the human body with a wax cast of the human ventricle system. In 1536, Nicolo Massa described the effects of different diseases, such as syphilis on the nervous system. He also noticed that the ventricular cavities were filled with cerebrospinal fluid. In 1542, the term physiology was used for the first time by a French physician named Jean Fernel, to explain bodily function in relation to the brain. In 1543, Andreas Vesalius wrote De humani corporis fabrica, which revolutionized the study of anatomy. In this book, he described the pineal gland and what he believed the function was, and was able to draw the corpus striatum which is made up of the basal ganglia and the internal capsule. In 1549, Jason Pratensis published De Cerebri Morbis. This book was devoted to neurological diseases, and discussed symptoms, as well as ideas from Galen and other Greek, Roman and Arabic authors. It also looked into the anatomy and specific functions of different areas. In 1550, Andreas Vesalius worked on a case of hydrocephalus, or fluid filling the brain. In the same year, Bartolomeo Eustachi studied the optic nerve, mainly focusing on its origin in the brain. In 1564, Giulio Cesare Aranzio discovered the hippocampus, naming it such due to its shape resemblance to a sea horse.\\nIn 1621, Robert Burton published The Anatomy of Melancholy, which looked at the loss of important characters in one\\'s life as leading to depression. In 1649, René Descartes studied the pineal gland. He mistakenly believed that it was the \"soul\" of the brain, and believed it was where thoughts formed. In 1658, Johann Jakob Wepfer studied a patient in which he believed that a broken blood vessel had caused apoplexy, or a stroke.\\nIn 1749, David Hartley published Observations on Man, which focused on frame (neurology), duty (moral psychology) and expectations (spirituality) and how these integrated within one another. This text was also the first to use the English term psychology. In 1752, the Society of Friends created an asylum in Philadelphia, Pennsylvania. The asylum intended to give not only medical treatment to those mentally ill, but also provide with caretakers and comfortable living conditions. In 1755, Jean-Baptiste Le Roy began using electroconvulsive therapy for the mentally ill, a treatment still used today in specific cases. In 1760, Arne-Charles studied how different lesions in the cerebellum could affect motor movements. In 1776, Vincenzo Malacarne studied the cerebellum intensely, and published a book solely based on its function and appearance.\\nIn 1784, Félix Vicq-d\\'Azyr, discovered a black colored structure in the midbrain. In 1791 Samuel Thomas von Sömmerring alluded to this structure, calling it the substantia nigra.  In the same year, Luigi Galvani described the role of electricity in nerves of dissected frogs.  In 1808, Franz Joseph Gall studied and published work on phrenology. Phrenology was the faulty science of looking at head shape to determine different aspects of personality and brain function. In 1811, Julien Jean César Legallois studied respiration in animal dissection and lesions and found the center of respiration in the medulla oblongata. In the same year, Charles Bell finished work on what would later become known as the Bell-Magendie law, which compared functional differences between dorsal and ventral roots of the spinal cord. In 1822, Karl Friedrich Burdach distinguished between the lateral and medial geniculate bodies, as well as named the cingulate gyrus. In 1824, F. Magendie studied and produced the first evidence of the cerebellum\\'s role in equilibration to complete the Bell-Magendie law. In 1838, Theodor Schwann began studying white and grey matter in the brain, and discovered the myelin sheath. These cells, which cover the axons of the neurons in the brain, are named Schwann cells after him. In 1848, Phineas Gage, the classical neurophysiology patient, had his brain pierced by an iron tamping rod in a blasting accident. He became an excellent case study in the connection between the prefrontal cortex and behavior, decision making and consequences. In 1849, Hermann von Helmholtz studied the speed of frog nerve impulses while studying electricity in the body.\\nWhile these are not all the developments in neurophysiology before 1849, these developments were significant to the study of the brain and body.\\n\\n\\n== See also ==\\nBrain\\nNeuroscience\\nNeural coding\\nNeurology\\n\\n\\n== References ==\\n\\n\\n== Sources ==\\nFye WB (October 1995). \"Julien Jean César Legallois\". Clinical Cardiology. 18 (10): 599–600. doi:10.1002/clc.4960181015. PMID 8785909.\\n\"NEUROSURGERY://ON-CALL®\". Cyber Museum of Neurosurgery. Retrieved 30 April 2012.\\nGallistel, C. R. (1981). \"Bell, Magendie and the Proposals to Restrict the Use of Animals in Neurobehavioral Research\" (PDF). The American Psychologist. Ruccs.rutgers.edu. 36 (4): 357–60. doi:10.1037//0003-066x.36.4.357. PMID 7023302. Archived from the original (PDF) on 1 April 2011. Retrieved 30 April 2012.\\n\"History of Biology 1800-1849\". Retrieved 30 April 2012.\\n\"History of Neuroscience\". University of Washington. Retrieved 30 April 2012.\\nDuque-Parra JE (September 2004). \"Perspective on the vestibular cortex throughout history\". The Anatomical Record Part B: The New Anatomist. 280 (1): 15–9. doi:10.1002/ar.b.20031. PMID 15382110.\\n\"Article Number: EONS : 0736 : Cerebellum\" (PDF). Retrieved 30 April 2012.\\n\"David Hartley\". Stanford Encyclopedia of Philosophy. Retrieved 30 April 2012.\\nFrank, Leonard R. (2006). \"The Electroshock Quotationary\" (PDF). Retrieved 30 April 2012.\\nPearce JM (April 1997). \"Johann Jakob Wepfer (1620-95) and cerebral haemorrhage\". Journal of Neurology, Neurosurgery, and Psychiatry. 62 (4): 387. doi:10.1136/jnnp.62.4.387. PMC 1074098. PMID 9120455.\\nFinger, Stanley (2001). Origins of Neuroscience. ISBN 9780195146943. Retrieved 30 April 2012.\\nWaln, Robert. \"An Account of the Asylum for the Insane, Established by the Society of Friends, near Frankford, in the Vicinity of Philadelphia\". Retrieved 30 April 2012.\\n\"Anatomy Words\". Retrieved 30 April 2012.\\n\"Andreas Vesalius and Modern Human Anatomy\". Archived from the original on 15 September 2012. Retrieved 30 April 2012.\\nO\\'Malley, Charles Donald (1964). Andreas Vesalius of Brussels, 1514-1564. University of California Press. Retrieved 30 April 2012.\\nPestronk A (March 1988). \"The first neurology book. De Cerebri Morbis...(1549) by Jason Pratensis\". Archives of Neurology. 45 (3): 341–4. doi:10.1001/archneur.1988.00520270123032. PMID 3277602.\\n\"Descartes and the Pineal Gland\". Stanford Encyclopedia of Philosophy. Retrieved 30 April 2012.\\nMcCaffrey, Patrick. \"Chapter 5. The Corpus Striatum, Rhinencephalon, Connecting Fibers, and Diencephalon\". CMSD 620 Neuroanatomy of Speech, Swallowing and Language. The Neuroscience on the Web Series. CSU. Archived from the original on 7 January 2018. Retrieved 30 April 2012.\\nBrink A (December 1979). \"Depression and loss: a theme in Robert Burton\\'s \\'Anatomy of melancholy\\' (1621)\". Canadian Journal of Psychiatry. 24 (8): 767–72. doi:10.1177/070674377902400811. PMID 391384. S2CID 35532320.\\n\"Al-Zahrawi - Father of Surgery\". Retrieved 30 April 2012.\\n\"Andreas Vesalius\". Encyclopedia.com. Retrieved 30 April 2012.\\nJeffery G (October 2001). \"Architecture of the optic chiasm and the mechanisms that sculpt its development\". Physiological Reviews. 81 (4): 1393–414. doi:10.1152/physrev.2001.81.4.1393. PMID 11581492. S2CID 203231.\\n\"Mondino De\\' Luzzi\". Encyclopedia.com. Retrieved 30 April 2012.\\n\"A History of the Brain\". Stanford University. Retrieved 30 April 2012.', 'In computer science, specifically software engineering and hardware engineering, formal methods are a particular kind of mathematically rigorous techniques for the specification, development and verification of software and hardware systems. The use of formal methods for software and hardware design is motivated by the expectation that, as in other engineering disciplines, performing appropriate mathematical analysis can contribute to the reliability and robustness of a design.Formal methods are best described as the application of a fairly broad variety of theoretical computer science fundamentals, in particular logic calculi, formal languages, automata theory, discrete event dynamic system and program semantics, but also type systems and algebraic data types to problems in software and hardware specification and verification.\\n\\n\\n== Background ==\\nSemi-Formal Methods are formalisms and languages that are not considered fully “formal”. It defers the task of completing the semantics to a later stage, which is then done either by human interpretation or by interpretation through software like code or test case generators.\\n\\n\\n== Taxonomy ==\\nFormal methods can be used at a number of levels:\\nLevel 0: Formal specification may be undertaken and then a program developed from this informally. This has been dubbed formal methods lite. This may be the most cost-effective option in many cases.\\nLevel 1: Formal development and formal verification may be used to produce a program in a more formal manner. For example, proofs of properties or refinement from the specification to a program may be undertaken. This may be most appropriate in high-integrity systems involving safety or security.\\nLevel 2: Theorem provers may be used to undertake fully formal machine-checked proofs. Despite improving tools and declining costs, this can be very expensive and is only practically worthwhile if the cost of mistakes is very high (e.g., in critical parts of operating system or microprocessor design).\\nFurther information on this is expanded below.\\nAs with programming language semantics, styles of formal methods may be roughly classified as follows:\\n\\nDenotational semantics, in which the meaning of a system is expressed in the mathematical theory of domains.  Proponents of such methods rely on the well-understood nature of domains to give meaning to the system; critics point out that not every system may be intuitively or naturally viewed as a function.\\nOperational semantics, in which the meaning of a system is expressed as a sequence of actions of a (presumably) simpler computational model. Proponents of such methods point to the simplicity of their models as a means to expressive clarity; critics counter that the problem of semantics has just been delayed (who defines the semantics of the simpler model?).\\nAxiomatic semantics, in which the meaning of the system is expressed in terms of preconditions and postconditions which are true before and after the system performs a task, respectively. Proponents note the connection to classical logic; critics note that such semantics never really describe what a system does (merely what is true before and afterwards).\\n\\n\\n=== Lightweight formal methods ===\\nSome practitioners believe that the formal methods community has overemphasized full formalization of a specification or design. They contend that the expressiveness of the languages involved, as well as the complexity of the systems being modelled, make full formalization a difficult and expensive task. As an alternative, various lightweight formal methods, which emphasize partial specification and focused application, have been proposed. Examples of this lightweight approach to formal methods include the Alloy object modelling notation, Denney\\'s synthesis of some aspects of the Z notation with use case driven development, and the CSK VDM Tools.\\n\\n\\n== Uses ==\\nFormal methods can be applied at various points through the development process.\\n\\n\\n=== Specification ===\\nFormal methods may be used to give a description of the system to be developed, at whatever level(s) of detail desired. This formal description can be used to guide further development activities (see following sections); additionally, it can be used to verify that the requirements for the system being developed have been completely and accurately specified, or formalising system requirements by expressing them in a formal language with a precise and unambiguously defined syntax and semantics.\\nThe need for formal specification systems has been noted for years. In the ALGOL 58 report, John Backus presented a formal notation for describing programming language syntax, later named Backus normal form then renamed Backus–Naur form (BNF). Backus also wrote that a formal description of the meaning of syntactically valid ALGOL programs wasn\\'t completed in time for inclusion in the report. \"Therefore the formal treatment of the semantics of legal programs will be included in a subsequent paper.\" It never appeared.\\n\\n\\n=== Development ===\\nFormal development is the use of formal methods as an integrated part of a tool-supported system development process.\\nOnce a formal specification has been produced, the specification may be used as a guide while the concrete system is developed during the design process (i.e., realized typically in software, but also potentially in hardware). For example:\\n\\nIf the formal specification is in operational semantics, the observed behavior of the concrete system can be compared with the behavior of the specification (which itself should be executable or simulatable). Additionally, the operational commands of the specification may be amenable to direct translation into executable code.\\nIf the formal specification is in axiomatic semantics, the preconditions and postconditions of the specification may become assertions in the executable code.\\n\\n\\n=== Verification ===\\nFormal verification is the use of software tools to prove properties of a formal specification, or to prove that a formal model of a system implementation satisfies its specification.\\nOnce a formal specification has been developed, the specification may be used as the basis for proving properties of the specification (and hopefully by inference the developed system).\\n\\n\\n==== Sign-off verification ====\\nSign-off verification is the use of a formal verification tool that is highly trusted.  Such a tool can replace traditional verification methods (the tool may even be certified).\\n\\n\\n==== Human-directed proof ====\\nSometimes, the motivation for proving the correctness of a system is not the obvious need for reassurance of the correctness of the system, but a desire to understand the system better. Consequently, some proofs of correctness are produced in the style of mathematical proof: handwritten (or typeset) using natural language, using a level of informality common to such proofs. A \"good\" proof is one that is readable and understandable by other human readers.\\nCritics of such approaches point out that the ambiguity inherent in natural language allows errors to be undetected in such proofs; often, subtle errors can be present in the low-level details typically overlooked by such proofs. Additionally, the work involved in producing such a good proof requires a high level of mathematical sophistication and expertise.\\n\\n\\n==== Automated proof ====\\nIn contrast, there is increasing interest in producing proofs of correctness of such systems by automated means. Automated techniques fall into three general categories:\\n\\nAutomated theorem proving, in which a system attempts to produce a formal proof from scratch, given a description of the system, a set of logical axioms, and a set of inference rules.\\nModel checking, in which a system verifies certain properties by means of an exhaustive search of all possible states that a system could enter during its execution.\\nAbstract interpretation, in which a system verifies an over-approximation of a behavioural property of the program, using a fixpoint computation over a (possibly complete) lattice representing it.Some automated theorem provers require guidance as to which properties are \"interesting\" enough to pursue, while others work without human intervention. Model checkers can quickly get bogged down in checking millions of uninteresting states if not given a sufficiently abstract model.\\nProponents of such systems argue that the results have greater mathematical certainty than human-produced proofs, since all the tedious details have been algorithmically verified. The training required to use such systems is also less than that required to produce good mathematical proofs by hand, making the techniques accessible to a wider variety of practitioners.\\nCritics note that some of those systems are like oracles: they make a pronouncement of truth, yet give no explanation of that truth. There is also the problem of \"verifying the verifier\"; if the program which aids in the verification is itself unproven, there may be reason to doubt the soundness of the produced results. Some modern model checking tools produce a \"proof log\" detailing each step in their proof, making it possible to perform, given suitable tools, independent verification.\\nThe main feature of the abstract interpretation approach is that it provides a sound analysis, i.e. no false negatives are returned. Moreover, it is efficiently scalable, by tuning the abstract domain representing the property to be analyzed, and by applying widening operators to get fast convergence.\\n\\n\\n== Applications ==\\nFormal methods are applied in different areas of hardware and software, including routers, Ethernet switches, routing protocols, security applications, and operating system microkernels such as seL4. There are several examples in which they have been used to verify the functionality of the hardware and software used in DCs. IBM used ACL2, a theorem prover, in the AMD x86 processor development process. Intel uses such methods to verify its hardware and firmware (permanent software programmed into a read-only memory). Dansk Datamatik Center used formal methods in the 1980s to develop a compiler system for the Ada programming language that went on to become a long-lived commercial product.There are several other projects of NASA in which formal methods are applied, such as Next Generation Air Transportation System, Unmanned Aircraft System integration in National Airspace System, and Airborne Coordinated Conflict Resolution and Detection (ACCoRD).B-Method with Atelier B, is used to develop safety automatisms for the various subways installed throughout the world by Alstom and Siemens, and also for Common Criteria certification and the development of system models by ATMEL and STMicroelectronics.\\nFormal verification has been frequently used in hardware by most of the well-known hardware vendors, such as IBM, Intel, and AMD. There are many areas of hardware, where Intel have used FMs to verify the working of the products, such as parameterized verification of cache-coherent protocol, Intel Core i7 processor execution engine validation  (using theorem proving, BDDs, and symbolic evaluation), optimization for Intel IA-64 architecture using HOL light theorem prover, and verification of high-performance dual-port gigabit Ethernet controller with support for PCI express protocol and Intel advance management technology using Cadence. Similarly, IBM has used formal methods in the verification of power gates, registers, and functional verification of the IBM Power7 microprocessor.\\n\\n\\n== In software development ==\\nIn software development, formal methods are mathematical approaches to solving software (and hardware) problems at the requirements, specification, and design levels. Formal methods are most likely to be applied to safety-critical or security-critical software and systems, such as avionics software. Software safety assurance standards, such as DO-178C allows the usage of formal methods through supplementation, and Common Criteria mandates formal methods at the highest levels of categorization.\\nFor sequential software, examples of formal methods include the B-Method, the specification languages used in automated theorem proving, RAISE, and the Z notation.\\nIn functional programming, property-based testing has allowed the mathematical specification and testing (if not exhaustive testing) of the expected behaviour of individual functions.\\nThe Object Constraint Language (and specializations such as Java Modeling Language) has allowed object-oriented systems to be formally specified, if not necessarily formally verified.\\nFor concurrent software and systems, Petri nets, process algebra, and finite state machines (which are based on automata theory - see also virtual finite state machine or event driven finite state machine) allow executable software specification and can be used to build up and validate application behaviour.\\nAnother approach to formal methods in software development is to write a specification in some form of logic—usually a variation of first-order logic (FOL)—and then to directly execute the logic as though it were a program. The OWL language, based on Description Logic (DL), is an example. There is also work on mapping some version of English (or another natural language) automatically to and from logic, as well as executing the logic directly. Examples are Attempto Controlled English, and Internet Business Logic, which do not seek to control the vocabulary or syntax. A feature of systems that support bidirectional English-logic mapping and direct execution of the logic is that they can be made to explain their results, in English, at the business or scientific level.\\n\\n\\n== Formal methods and notations ==\\nThere are a variety of formal methods and notations available.\\n\\n\\n=== Specification languages ===\\nAbstract State Machines (ASMs)\\nA Computational Logic for Applicative Common Lisp (ACL2)\\nActor model\\nAlloy\\nANSI/ISO C Specification Language (ACSL)\\nAutonomic System Specification Language (ASSL)\\nB-Method\\nCADP\\nCommon Algebraic Specification Language (CASL)\\nEsterel\\nJava Modeling Language (JML)\\nKnowledge Based Software Assistant (KBSA)\\nLustre\\nmCRL2\\nPerfect Developer\\nPetri nets\\nPredicative programming\\nProcess calculi\\nCSP\\nLOTOS\\nπ-calculus\\nRAISE\\nRebeca Modeling Language\\nSPARK Ada\\nSpecification and Description Language\\nTLA+\\nUSL\\nVDM\\nVDM-SL\\nVDM++\\nZ notation\\n\\n\\n=== Model checkers ===\\n\\nESBMC\\nMALPAS Software Static Analysis Toolset – an industrial-strength model checker used for formal proof of safety-critical systems\\nPAT – a free model checker, simulator and refinement checker for concurrent systems and CSP extensions (e.g., shared variables, arrays, fairness)\\nSPIN\\nUPPAAL\\n\\n\\n== Organizations ==\\n\\nAPCB\\nBCS-FACS\\nFormal Methods Europe\\nZ User Group\\n\\n\\n== See also ==\\nAbstract interpretation\\nAutomated theorem proving\\nDesign by contract\\nFormal methods people\\nFormal specification\\nFormal verification\\nFormal system\\nModel checking\\nSoftware engineering\\nSpecification language\\n\\n\\n== References ==\\n\\n\\n== Further reading ==\\n\\n\\n== External links ==\\nFormal Methods Europe (FME)\\nFormal Methods Wiki\\nFormal methods from FoldocArchival materialFormal method keyword on Microsoft Academic Search via Archive.org\\nEvidence on Formal Methods uses and impact on Industry supported by the DEPLOY project (EU FP7) in Archive.org', 'Software engineering is the systematic application of engineering approaches to the development of software.A software engineer is a person who applies the principles of software engineering to design, develop, maintain, test, and evaluate computer software. The term programmer is sometimes used as a synonym, but may also lack connotations of engineering education or skills.\\nEngineering techniques are used to inform the software development process which involves the definition, implementation, assessment, measurement, management, change, and improvement of the software life cycle process itself. It heavily uses software configuration management which is about systematically controlling changes to the configuration, and maintaining the integrity and traceability of the configuration and code throughout the system life cycle. Modern processes use software versioning.\\n\\n\\n== History ==\\n\\nBeginning in the 1960s, software engineering was seen as its own type of engineering. Additionally, the development of software engineering was seen as a struggle. It was difficult to keep up with the hardware which caused many problems for software engineers. Problems included software that was over budget, exceeded deadlines, required extensive de-bugging and maintenance, and unsuccessfully met the needs of consumers or was never even completed. In 1968 NATO held the first Software Engineering conference where issues related to software were addressed: guidelines and best practices for the development of software were established. The origins of the term \"software engineering\" have been attributed to various sources. The term \"software engineering\" appeared in a list of services offered by companies in the June 1965 issue of COMPUTERS and AUTOMATION and was used more formally in the August 1966 issue of Communications of the ACM (Volume 9, number 8) “letter to the ACM membership” by the ACM President Anthony A. Oettinger,  it is also associated with the title of a NATO conference in 1968 by Professor Friedrich L. Bauer, the first conference on software engineering. Independently, Margaret Hamilton named the discipline \"software engineering\" during the Apollo missions to give what they were doing legitimacy.  At the time there was perceived to be a \"software crisis\". The 40th International Conference on Software Engineering (ICSE 2018) celebrates 50 years of \"Software Engineering\" with the Plenary Sessions\\' keynotes of Frederick Brooks and Margaret Hamilton.In 1984, the Software Engineering Institute (SEI) was established as a federally funded research and development center headquartered on the campus of Carnegie Mellon University in Pittsburgh, Pennsylvania, United States. Watts Humphrey founded the SEI Software Process Program, aimed at understanding and managing the software engineering process.  The Process Maturity Levels introduced would become the Capability Maturity Model Integration for Development(CMMI-DEV), which has defined how the US Government evaluates the abilities of a software development team.\\nModern, generally accepted best-practices for software engineering have been collected by the ISO/IEC JTC 1/SC 7 subcommittee and published as the Software Engineering Body of Knowledge (SWEBOK). Software engineering is considered one of major computing disciplines.\\n\\n\\n== Definitions and terminology controversies ==\\nNotable definitions of software engineering include:\\n\\n\"The systematic application of scientific and technological knowledge, methods, and experience to the design, implementation, testing, and documentation of software\"—The Bureau of Labor Statistics—IEEE Systems and software engineering – Vocabulary\\n\"The application of a systematic, disciplined, quantifiable approach to the development, operation, and maintenance of software\"—IEEE Standard Glossary of Software Engineering Terminology\\n\"an engineering discipline that is concerned with all aspects of software production\"—Ian Sommerville\\n\"the establishment and use of sound engineering principles in order to economically obtain software that is reliable and works efficiently on real machines\"—Fritz Bauer\\n\"a branch of computer science that deals with the design, implementation, and maintenance of complex computer programs\"—Merriam-Webster\\n\"\\'software engineering\\' encompasses not just the act of writing code, but all of the tools and processes an organization uses to build and maintain that code over time. [...] Software engineering can be thought of as \\'programming integrated over time.\\'\"—Software Engineering at GoogleThe term has also been used less formally:\\n\\nas the informal contemporary term for the broad range of activities that were formerly called computer programming and systems analysis;\\nas the broad term for all aspects of the practice of computer programming, as opposed to the theory of computer programming, which is formally studied as a sub-discipline of computer science;\\nas the term embodying the advocacy of a specific approach to computer programming, one that urges that it be treated as an engineering discipline rather than an art or a craft, and advocates the codification of recommended practices.\\n\\n\\n=== Etymology of \"software engineer\" ===\\nMargaret Hamilton promoted the term \"software engineering\" during her work on the Apollo program. The term \"engineering\" was used to acknowledge that the work should be taken just as seriously as other contributions toward the advancement of technology. Hamilton details her use of the term:When I first came up with the term, no one had heard of it before, at least in our world. It was an ongoing joke for a long time. They liked to kid me about my radical ideas. It was a memorable day when one of the most respected hardware gurus explained to everyone in a meeting that he agreed with me that the process of building software should also be considered an engineering discipline, just like with hardware. Not because of his acceptance of the new \"term\" per se, but because we had earned his and the acceptance of the others in the room as being in an engineering field in its own right.\\n\\n\\n=== Suitability of the term ===\\nIndividual commentators have disagreed sharply on how to define software engineering or its legitimacy as an engineering discipline. David Parnas has said that software engineering is, in fact, a form of engineering. Steve McConnell has said that it is not, but that it should be. Donald Knuth has said that programming is an art and a science. Edsger W. Dijkstra claimed that the terms software engineering and software engineer have been misused  and should be considered harmful, particularly in the United States.\\n\\n\\n== Tasks in large scale projects ==\\n\\n\\n=== Software requirements ===\\n\\nRequirements engineering is about the elicitation, analysis, specification, and validation of requirements for software. Software requirements can be of three different types. There are functional requirements, non-functional requirements, and domain requirements. The operation of the software should be performed and the proper output should be expected for the user to use. Non-functional requirements deal with issues like portability, security, maintainability, reliability, scalability, performance, reusability, and flexibility. They are classified into the following types: interference constraints, performance constraints (such as response time, security, storage space, etc.), operating constraints, life cycle constraints (maintainability, portability, etc.), and economic constraints. Knowledge of how the system or software works is needed when it comes to specifying non-functional requirements. Domain requirements have to do with the characteristic of a certain category or domain of projects.\\n\\n\\n=== Software design ===\\n\\nSoftware design is about the process of defining the architecture, components, interfaces, and other characteristics of a system or component. This is also called software architecture. Software design is divided into three different levels of design. The three levels are interface design, architectural design, and detailed design. Interface design is the interaction between a system and its environment. This happens at a high level of abstraction along with the inner workings of the system. Architectural design has to do with the major components of a system and their responsibilities, properties, interfaces, and their relationships and interactions that occur between them. Detailed design is the internal elements of all the major system components, their properties, relationships, processing, and usually their algorithms and the data structures. \\n\\n\\n=== Software construction ===\\n\\nSoftware construction, the main activity of software development, is the combination of programming, unit testing, integration testing, and debugging. Testing during this phase is generally performed by the programmer while the software is under construction, to verify what was just written and decide when the code is ready to be sent to the next step.\\n\\n\\n=== Software testing ===\\n\\nSoftware testing is an empirical, technical investigation conducted to provide stakeholders with information about the quality of the product or service under test, with different approaches such as unit testing and integration testing. It is one aspect of software quality. As a separate phase in software development, it is typically performed by quality assurance staff or a developer other than the one who wrote the code.\\n\\n\\n=== Software maintenance ===\\n\\nSoftware maintenance refers to the activities required to provide cost-effective support after shipping the software product. Software maintenance is modifying and updating software applications after distribution to correct faults and to improve its performance. Software has a lot to do with the real world and when the real world changes, software maintenance is required. Software maintenance includes: error correction, optimization, deletion of unused and discarded features, and enhancement of features that already exist. Usually, maintenance takes up about 40% to 80% of the project cost therefore, focusing on maintenance keeps the costs down.\\n\\n\\n== Education ==\\nKnowledge of computer programming is a prerequisite for becoming a software engineer. In 2004 the IEEE Computer Society produced the SWEBOK, which has been published as ISO/IEC Technical Report 1979:2005, describing the body of knowledge that they recommend to be mastered by a graduate software engineer with four years of experience.\\nMany software engineers enter the profession by obtaining a university degree or training at a vocational school. One standard international curriculum for undergraduate software engineering degrees was defined by the Joint Task Force on Computing Curricula of the IEEE Computer Society and the Association for Computing Machinery, and updated in 2014. A number of universities have Software Engineering degree programs; as of 2010, there were 244 Campus Bachelor of Software Engineering programs, 70 Online programs, 230 Masters-level programs, 41 Doctorate-level programs, and 69 Certificate-level programs in the United States.\\nIn addition to university education, many companies sponsor internships for students wishing to pursue careers in information technology. These internships can introduce the student to interesting real-world tasks that typical software engineers encounter every day. Similar experience can be gained through military service in software engineering.\\n\\n\\n=== Software engineering degree programs ===\\nHalf of all practitioners today have degrees in computer science, information systems, or information technology. A small, but growing, number of practitioners have software engineering degrees. In 1987, the Department of Computing at Imperial College London introduced the first three-year software engineering Bachelor\\'s degree in the UK and the world; in the following year, the University of Sheffield established a similar program.  In 1996, the Rochester Institute of Technology established the first software engineering bachelor\\'s degree program in the United States, however, it did not obtain ABET accreditation until 2003, the same time as Rice University, Clarkson University, Milwaukee School of Engineering and Mississippi State University obtained theirs. In 1997, PSG College of Technology in Coimbatore, India was the first to start a five-year integrated Master of Science degree in Software Engineering.Since then, software engineering undergraduate degrees have been established at many universities. A standard international curriculum for undergraduate software engineering degrees, SE2004, was defined by a steering committee between 2001 and 2004 with funding from the Association for Computing Machinery and the IEEE Computer Society. As of 2004, in the U.S., about 50 universities offer software engineering degrees, which teach both computer science and engineering principles and practices. The first software engineering Master\\'s degree was established at Seattle University in 1979. Since then graduate software engineering degrees have been made available from many more universities.  Likewise in Canada, the Canadian Engineering Accreditation Board (CEAB) of the Canadian Council of Professional Engineers has recognized several software engineering programs.\\nIn 1998, the US Naval Postgraduate School (NPS) established the first doctorate program in Software Engineering in the world. Additionally, many online advanced degrees in Software Engineering have appeared such as the Master of Science in Software Engineering (MSE) degree offered through the Computer Science and Engineering Department at California State University, Fullerton. Steve McConnell opines that because most universities teach computer science rather than software engineering, there is a shortage of true software engineers. ETS (École de technologie supérieure) University and UQAM (Université du Québec à Montréal) were mandated by IEEE to develop the Software Engineering Body of Knowledge (SWEBOK), which has become an ISO standard describing the body of knowledge covered by a software engineer.\\n\\n\\n== Profession ==\\n\\nLegal requirements for the licensing or certification of professional software engineers vary around the world. In the UK, there is no licensing or legal requirement to assume or use the job title Software Engineer.  In some areas of Canada, such as Alberta, British Columbia, Ontario, and Quebec, software engineers can hold the Professional Engineer (P.Eng) designation and/or the Information Systems Professional (I.S.P.) designation. In Europe, Software Engineers can obtain the European Engineer (EUR ING) professional title.\\nThe United States, since 2013, has offered an NCEES Professional Engineer exam for Software Engineering, thereby allowing Software Engineers to be licensed and recognized. NCEES will end the exam after April 2019 due to lack of participation. Mandatory licensing is currently still largely debated, and perceived as controversial. In some parts of the US such as Texas, the use of the term Engineer is regulated by law and reserved only for use by individuals who have a Professional Engineer license.The IEEE Computer Society and the ACM, the two main US-based professional organizations of software engineering, publish guides to the profession of software engineering. The IEEE\\'s Guide to the Software Engineering Body of Knowledge – 2004 Version, or SWEBOK, defines the field and describes the knowledge the IEEE expects a practicing software engineer to have. The most current SWEBOK v3 is an updated version and was released in 2014. The IEEE also promulgates a \"Software Engineering Code of Ethics\".\\n\\n\\n=== Employment ===\\n\\nThe U. S. Bureau of Labor Statistics (BLS) counted 1,365,500 software developers holding jobs in the U.S. in 2018. Due to its relative newness as a field of study, formal education in software engineering is often taught as part of a computer science curriculum, and many software engineers hold computer science degrees. The BLS estimates from 2014 to 2024 that computer software engineering would increase by 17% . This is down from the 2012 to 2022 BLS estimate of 22% for software engineering. And, is further down from their 30% 2010 to 2020 BLS estimate. Due to this trend, job growth may not be as fast as during the last decade, as jobs that would have gone to computer software engineers in the United States would instead be outsourced to computer software engineers in countries such as India and other foreign countries. In addition, the BLS Job Outlook for Computer Programmers, 2014–24 predicts an −8% (a decline, in their words), then a decline in the Job Outlook, 2019-29 of -9%, and a 10% decline for 2020-2030 for those who program computers. Furthermore, women in many software fields has also been declining over the years as compared to other engineering fields. However, this trend may change or slow in the future as many current software engineers in the U.S. market leave the profession or  age out of the market in the next few decades.\\nMany software engineers work as employees or contractors. Software engineers work with businesses, government agencies (civilian or military), and non-profit organizations. Some software engineers work for themselves as freelancers. Some organizations have specialists to perform each of the tasks in the software development process. Other organizations require software engineers to do many or all of them. In large projects, people may specialize in only one role. In small projects, people may fill several or all roles at the same time. Many companies hire interns, often university or college students during a summer break, or externships. Specializations include analysts, architects, developers, testers, technical support, middleware analysts, project managers, educators, and researchers.\\nMost software engineers and programmers work 40 hours a week, but about 15 percent of software engineers and 11 percent of programmers worked more than 50 hours a week in 2008. Potential injuries in these occupations are possible because like other workers who spend long periods sitting in front of a computer terminal typing at a keyboard, engineers and programmers are susceptible to eyestrain, back discomfort, and hand and wrist problems such as carpal tunnel syndrome.\\n\\n\\n=== Certification ===\\nThe Software Engineering Institute offers certifications on specific topics like security, process improvement and software architecture. IBM, Microsoft and other companies also sponsor their own certification examinations. Many IT certification programs are oriented toward specific technologies, and managed by the vendors of these technologies. These certification programs are tailored to the institutions that would employ people who use these technologies.\\nBroader certification of general software engineering skills is available through various professional societies. As of 2006, the IEEE had certified over 575 software professionals as a Certified Software Development Professional (CSDP). In 2008 they added an entry-level certification known as the Certified Software Development Associate (CSDA). The ACM had a professional certification program in the early 1980s, which was discontinued due to lack of interest. The ACM examined the possibility of professional certification of software engineers in the late 1990s, but eventually decided that such certification was inappropriate for the professional industrial practice of software engineering.In the U.K. the British Computer Society has developed a legally recognized professional certification called Chartered IT Professional (CITP), available to fully qualified members (MBCS). Software engineers may be eligible for membership of the Institution of Engineering and Technology and so qualify for Chartered Engineer status. In Canada the Canadian Information Processing Society has developed a legally recognized professional certification called Information Systems Professional (ISP). In Ontario, Canada, Software Engineers who graduate from a Canadian Engineering Accreditation Board (CEAB) accredited program, successfully complete PEO\\'s (Professional Engineers Ontario) Professional Practice Examination (PPE) and have at least 48 months of acceptable engineering experience are eligible to be licensed through the Professional Engineers Ontario and can become Professional Engineers P.Eng. The PEO does not recognize any online or distance education however; and does not consider Computer Science programs to be equivalent to software engineering programs despite the tremendous overlap between the two. This has sparked controversy and a certification war. It has also held the number of P.Eng holders for the profession exceptionally low. The vast majority of working professionals in the field hold a degree in CS, not SE. Given the difficult certification path for holders of non-SE degrees, most never bother to pursue the license.\\n\\n\\n=== Impact of globalization ===\\nThe initial impact of outsourcing, and the relatively lower cost of international human resources in developing third world countries led to a massive migration of software development activities from corporations in North America and Europe to India and later: China, Russia, and other developing countries. This approach had some flaws, mainly the distance / time zone difference that prevented human interaction between clients and developers and the massive job transfer. This had a negative impact on many aspects of the software engineering profession. For example, some students in the developed world avoid education related to software engineering because of the fear of offshore outsourcing (importing software products or services from other countries) and of being displaced by foreign visa workers. Although statistics do not currently show a threat to software engineering itself; a related career, computer programming does appear to have been affected. Nevertheless, the ability to smartly leverage offshore and near-shore resources via the follow-the-sun workflow has improved the overall operational capability of many organizations. When North Americans are leaving work, Asians are just arriving to work. When Asians are leaving work, Europeans are arriving to work. This provides a continuous ability to have human oversight on business-critical processes 24 hours per day, without paying overtime compensation or disrupting a key human resource, sleep patterns.\\nWhile global outsourcing has several advantages, global – and generally distributed – development can run into serious difficulties resulting from the distance between developers. This is due to the key elements of this type of distance that have been identified as geographical, temporal, cultural and communication (that includes the use of different languages and dialects of English in different locations). Research has been carried out in the area of global software development over the last 15 years and an extensive body of relevant work published that highlights the benefits and problems associated with the complex activity. As with other aspects of software engineering research is ongoing in this and related areas.\\n\\n\\n=== Prizes ===\\nThere are several prizes in the field of software engineering:\\nThe Codie awards is a yearly award issued by the Software and Information Industry Association for excellence in software development within the software industry.\\nJolt Awards are awards in the software industry.\\nStevens Award is a software engineering award given in memory of Wayne Stevens.\\n\\n\\n== Criticism ==\\nSoftware engineering sees its practitioners as individuals who follow well-defined engineering approaches to problem-solving. These approaches are specified in various software engineering books and research papers, always with the connotations of predictability, precision, mitigated risk and professionalism. This perspective has led to calls for licensing, certification and codified bodies of knowledge as mechanisms for spreading the engineering knowledge and maturing the field.\\nSoftware engineering extends engineering and draws on the engineering model, i.e. engineering process, engineering project management, engineering requirements, engineering design, engineering construction, and engineering validation. The concept is so new that it is rarely understood, and it is widely misinterpreted, including in software engineering textbooks, papers, and among the communities of programmers and crafters.\\nOne of the core issues in software engineering is that its approaches are not empirical enough because a real-world validation of approaches is usually absent, or very limited and hence software engineering is often misinterpreted as feasible only in a \"theoretical environment.\"\\nEdsger Dijkstra, the founder of many of the concepts used within software development today, rejected the idea of \"software engineering\" up until his death in 2002, arguing that those terms were poor analogies for what\\nhe called the \"radical novelty\" of computer science:\\n\\nA number of these phenomena have been bundled under the name \"Software Engineering\". As economics is known as \"The Miserable Science\", software engineering should be known as \"The Doomed Discipline\", doomed because it cannot even approach its goal since its goal is self-contradictory. Software engineering, of course, presents itself as another worthy cause, but that is eyewash: if you carefully read its literature and analyse what its devotees actually do, you will discover that software engineering has accepted as its charter \"How to program if you cannot.\"\\n\\n\\n== See also ==\\n\\n\\n=== Study and practice ===\\nComputer science\\nInformation engineering\\nSoftware craftsmanship\\nSoftware development\\nRelease engineering\\n\\n\\n=== Roles ===\\nProgrammer\\nSystems analyst\\nSystems architect\\n\\n\\n=== Professional aspects ===\\nBachelor of Science in Information Technology\\nBachelor of Software Engineering\\nList of software engineering conferences\\nList of computer science journals (including software engineering journals)\\nSoftware Engineering Institute\\n\\n\\n== References ==\\n\\n\\n=== Citations ===\\n\\n\\n=== Sources ===\\n\\n\\n== Further reading ==\\nGuide to the Software Engineering Body of Knowledge (SWEBOK Guide): Version 3.0. Pierre Bourque, Richard E. Fairley (eds.). IEEE Computer Society. 2014. ISBN 978-0-7695-5166-1.CS1 maint: others (link)\\nPressman, Roger S (2009). Software Engineering: A Practitioner\\'s Approach (7th ed.). Boston, Mass: McGraw-Hill. ISBN 978-0-07-337597-7.\\nSommerville, Ian (2010) [2010]. Software Engineering (9th ed.). Harlow, England: Pearson Education. ISBN 978-0-13-703515-1.\\nJalote, Pankaj (2005) [1991]. An Integrated Approach to Software Engineering (3rd ed.). Springer. ISBN 978-0-387-20881-7.\\nBruegge, Bernd; Dutoit, Allen (2009). Object-oriented software engineering : using UML, patterns, and Java (3rd ed.). Prentice Hall. ISBN 978-0-13-606125-0.\\nOshana, Robert (2019-06-21). Software engineering for embedded systems : methods, practical techniques, and applications (Second ed.). Kidlington, Oxford, United Kingdom. ISBN 978-0-12-809433-4.\\n\\n\\n== External links ==\\nGuide to the Software Engineering Body of Knowledge\\nThe Open Systems Engineering and Software Development Life Cycle Framework OpenSDLC.org the integrated Creative Commons SDLC\\nSoftware Engineering Institute Carnegie Mellon', 'In mathematics and computer science, an algorithm ( (listen)) is a finite sequence of well-defined instructions, typically used to solve a class of specific problems or to perform a computation. Algorithms are used as specifications for performing calculations, data processing, automated reasoning, automated decision-making and other tasks. In contrast, a heuristic is a  technique used in problem solving that uses practical methods and/or various estimates in order to produce solutions that may not be optimal but are sufficient given the circumstances.As an effective method, an algorithm can be expressed within a finite amount of space and time, and in a well-defined formal language for calculating a function. Starting from an initial state and initial input (perhaps empty), the instructions describe a computation that, when executed, proceeds through a finite number of well-defined successive states, eventually producing \"output\" and terminating at a final ending state. The transition from one state to the next is not necessarily deterministic; some algorithms, known as randomized algorithms, incorporate random input.\\n\\n\\n== History ==\\nThe concept of algorithm has existed since antiquity. Arithmetic algorithms, such as a division algorithm, were used by ancient Babylonian mathematicians c. 2500 BC and Egyptian mathematicians c. 1550 BC. Greek mathematicians later used algorithms in 240 BC in the sieve of Eratosthenes for finding prime numbers, and the Euclidean algorithm for finding the greatest common divisor of two numbers. Arabic mathematicians such as al-Kindi in the 9th century used cryptographic algorithms for code-breaking, based on frequency analysis.The word algorithm is derived from the name of the 9th-century Persian mathematician Muḥammad ibn Mūsā al-Khwārizmī, whose nisba (identifying him as from Khwarazm) was Latinized as Algoritmi (Arabized Persian الخوارزمی c. 780–850).Muḥammad ibn Mūsā al-Khwārizmī was a mathematician, astronomer, geographer, and scholar in the House of Wisdom in Baghdad, whose name means \\'the native of Khwarazm\\', a region that was part of Greater Iran and is now in Uzbekistan. About 825, al-Khwarizmi wrote an Arabic language treatise on the Hindu–Arabic numeral system, which was translated into Latin during the 12th century. The manuscript starts with the phrase Dixit Algorizmi (\\'Thus spake Al-Khwarizmi\\'), where \"Algorizmi\" was the translator\\'s Latinization of Al-Khwarizmi\\'s name. Al-Khwarizmi was the most widely read mathematician in Europe in the late Middle Ages, primarily through another of his books, the Algebra. In late medieval Latin, algorismus, English \\'algorism\\', the corruption of his name, simply meant the \"decimal number system\". In the 15th century, under the influence of the Greek word ἀριθμός (arithmos), \\'number\\' (cf. \\'arithmetic\\'), the Latin word was altered to algorithmus, and the corresponding English term \\'algorithm\\' is first attested in the 17th century; the modern sense was introduced in the 19th century.In English, it was first used in about 1230 and then by Chaucer in 1391. English adopted the French term, but it was not until the late 19th century that \"algorithm\" took on the meaning that it has in modern English.Another early use of the word is from 1240, in a manual titled Carmen de Algorismo composed by Alexandre de Villedieu. It begins with:\\n\\nHaec algorismus ars praesens dicitur, in qua / Talibus Indorum fruimur bis quinque figuris.\\nwhich translates to:\\n\\nAlgorism is the art by which at present we use those Indian figures, which number two times five.\\nThe poem is a few hundred lines long and summarizes the art of calculating with the new styled Indian dice (Tali Indorum), or Hindu numerals.A partial formalization of the modern concept of algorithm began with attempts to solve the Entscheidungsproblem  (decision problem) posed by David Hilbert in 1928. Later formalizations were framed as attempts to define \"effective calculability\" or \"effective method\". Those formalizations included the Gödel–Herbrand–Kleene recursive functions of 1930, 1934 and 1935, Alonzo Church\\'s lambda calculus of 1936, Emil Post\\'s Formulation 1 of 1936, and Alan Turing\\'s Turing machines of 1936–37 and 1939.\\n\\n\\n== Informal definition ==\\n\\nAn informal definition could be \"a set of rules that precisely defines a sequence of operations\", which would include all computer programs (including programs that do not perform numeric calculations), and (for example) any prescribed bureaucratic procedure\\nor cook-book recipe.In general, a program is only an algorithm if it stops eventually—even though infinite loops may sometimes prove desirable.\\nA prototypical example of an algorithm is the Euclidean algorithm, which is used to determine the maximum common divisor of two integers; an example (there are others) is described by the flowchart above and as an example in a later section.\\nBoolos, Jeffrey & 1974, 1999 offer an informal meaning of the word \"algorithm\" in the following quotation:\\n\\nNo human being can write fast enough, or long enough, or small enough† ( †\"smaller and smaller without limit … you\\'d be trying to write on molecules, on atoms, on electrons\") to list all members of an enumerably infinite set by writing out their names, one after another, in some notation. But humans can do something equally useful, in the case of certain enumerably infinite sets: They can give explicit instructions for determining the nth member of the set, for arbitrary finite n. Such instructions are to be given quite explicitly, in a form in which they could be followed by a computing machine, or by a human who is capable of carrying out only very elementary operations on symbols.\\nAn \"enumerably infinite set\" is one whose elements can be put into one-to-one correspondence with the integers. Thus Boolos and Jeffrey are saying that an algorithm implies instructions for a process that \"creates\" output integers from an arbitrary \"input\" integer or integers that, in theory, can be arbitrarily large. For example, an algorithm can be an algebraic equation such as y = m + n (i.e., two arbitrary \"input variables\" m and n that produce an output y), but various authors\\' attempts to define the notion indicate that the word implies much more than this, something on the order of (for the addition example):\\n\\nPrecise instructions (in a language understood by \"the computer\") for a fast, efficient, \"good\" process that specifies the \"moves\" of \"the computer\" (machine or human, equipped with the necessary internally contained information and capabilities) to find, decode, and then process arbitrary input integers/symbols m and n, symbols + and = … and \"effectively\" produce, in a \"reasonable\" time, output-integer y at a specified place and in a specified format.The concept of algorithm is also used to define the notion of decidability—a notion that is central for explaining how formal systems come into being starting from a small set of axioms and rules. In logic, the time that an algorithm requires to complete cannot be measured, as it is not apparently related to the customary physical dimension. From such uncertainties, that characterize ongoing work, stems the unavailability of a definition of algorithm that suits both concrete (in some sense) and abstract usage of the term.\\n\\n\\n== Formalization ==\\nAlgorithms are essential to the way computers process data. Many computer programs contain algorithms that detail the specific instructions a computer should perform—in a specific order—to carry out a specified task, such as calculating employees\\' paychecks or printing students\\' report cards. Thus, an algorithm can be considered to be any sequence of operations that can be simulated by a Turing-complete system. Authors who assert this thesis include Minsky (1967), Savage (1987) and Gurevich (2000):\\n\\n Minsky: \"But we will also maintain, with Turing … that any procedure which could \"naturally\" be called effective, can, in fact, be realized by a (simple) machine. Although this may seem extreme, the arguments … in its favor are hard to refute\".\\n Gurevich: \"… Turing\\'s informal argument in favor of his thesis justifies a stronger thesis: every algorithm can be simulated by a Turing machine … according to Savage [1987], an algorithm is a computational process defined by a Turing machine\".Turing machines can define computational processes that do not terminate. The informal definitions of algorithms generally require that the algorithm always terminates. This requirement renders the task of deciding whether a formal procedure is an algorithm impossible in the general case—due to a major theorem of computability theory known as the halting problem.\\nTypically, when an algorithm is associated with processing information, data can be read from an input source, written to an output device and stored for further processing. Stored data are regarded as part of the internal state of the entity performing the algorithm. In practice, the state is stored in one or more data structures.\\nFor some of these computational processes, the algorithm must be rigorously defined: specified in the way it applies in all possible circumstances that could arise. This means that any conditional steps must be systematically dealt with, case-by-case; the criteria for each case must be clear (and computable).\\nBecause an algorithm is a precise list of precise steps, the order of computation is always crucial to the functioning of the algorithm. Instructions are usually assumed to be listed explicitly, and are described as starting \"from the top\" and going \"down to the bottom\"—an idea that is described more formally by flow of control.\\nSo far, the discussion on the formalization of an algorithm has assumed the premises of imperative programming. This is the most common conception—one which attempts to describe a task in discrete, \"mechanical\" means. Unique to this conception of formalized algorithms is the assignment operation, which sets the value of a variable. It derives from the intuition of \"memory\" as a scratchpad. An example of such an assignment can be found below.\\nFor some alternate conceptions of what constitutes an algorithm, see functional programming and logic programming.\\n\\n\\n== Expressing algorithms ==\\nAlgorithms can be expressed in many kinds of notation, including natural languages, pseudocode, flowcharts, drakon-charts, programming languages or control tables (processed by interpreters). Natural language expressions of algorithms tend to be verbose and ambiguous, and are rarely used for complex or technical algorithms. Pseudocode, flowcharts, drakon-charts and control tables are structured ways to express algorithms that avoid many of the ambiguities common in the statements based on natural language. Programming languages are primarily intended for expressing algorithms in a form that can be executed by a computer, but are also often used as a way to define or document algorithms.\\nThere is a wide variety of representations possible and one can express a given Turing machine program as a sequence of machine tables (see finite-state machine, state transition table and control table for more), as flowcharts and drakon-charts (see state diagram for more), or as a form of rudimentary machine code or assembly code called \"sets of quadruples\" (see Turing machine for more).\\nRepresentations of algorithms can be classed into three accepted levels of Turing machine description, as follows:\\n1 High-level description\\n\"…prose to describe an algorithm, ignoring the implementation details. At this level, we do not need to mention how the machine manages its tape or head.\"\\n2 Implementation description\\n\"…prose used to define the way the Turing machine uses its head and the way that it stores data on its tape. At this level, we do not give details of states or transition function.\"\\n3 Formal description\\nMost detailed, \"lowest level\", gives the Turing machine\\'s \"state table\".For an example of the simple algorithm \"Add m+n\" described in all three levels, see Algorithm#Examples.\\n\\n\\n== Design ==\\n\\nAlgorithm design refers to a method or a mathematical process for problem-solving and engineering algorithms. The design of algorithms is part of many solution theories of operation research, such as dynamic programming and divide-and-conquer. Techniques for designing and implementing algorithm designs are also called algorithm design patterns, with examples including the template method pattern and the decorator pattern.\\nOne of the most important aspects of algorithm design is resource (run-time, memory usage) efficiency; the big O notation is used to describe e.g. an algorithm\\'s run-time growth as the size of its input increases.\\nTypical steps in the development of algorithms:\\n\\nProblem definition\\nDevelopment of a model\\nSpecification of the algorithm\\nDesigning an algorithm\\nChecking the correctness of the algorithm\\nAnalysis of algorithm\\nImplementation of algorithm\\nProgram testing\\nDocumentation preparation\\n\\n\\n== Implementation ==\\n\\nMost algorithms are intended to be implemented as computer programs. However, algorithms are also implemented by other means, such as in a biological neural network (for example, the human brain implementing arithmetic or an insect looking for food), in an electrical circuit, or in a mechanical device.\\n\\n\\n== Computer algorithms ==\\n\\nIn computer systems, an algorithm is basically an instance of logic written in software by software developers, to be effective for the intended \"target\" computer(s) to produce output from given (perhaps null) input. An optimal algorithm, even running in old hardware, would produce faster results than a non-optimal (higher time complexity) algorithm for the same purpose, running in more efficient hardware; that is why algorithms, like computer hardware, are considered technology.\\n\"Elegant\" (compact) programs, \"good\" (fast) programs : The notion of \"simplicity and elegance\" appears informally in Knuth and precisely in Chaitin:\\n\\nKnuth: \" … we want good algorithms in some loosely defined aesthetic sense. One criterion … is the length of time taken to perform the algorithm …. Other criteria are adaptability of the algorithm to computers, its simplicity and elegance, etc\"Chaitin: \" … a program is \\'elegant,\\' by which I mean that it\\'s the smallest possible program for producing the output that it does\"Chaitin prefaces his definition with: \"I\\'ll show you can\\'t prove that a program is \\'elegant\\'\"—such a proof would solve the Halting problem (ibid).\\nAlgorithm versus function computable by an algorithm: For a given function multiple algorithms may exist. This is true, even without expanding the available instruction set available to the programmer. Rogers observes that \"It is ... important to distinguish between the notion of algorithm, i.e. procedure and the notion of function computable by algorithm, i.e. mapping yielded by procedure. The same function may have several different algorithms\".Unfortunately, there may be a tradeoff between goodness (speed) and elegance (compactness)—an elegant program may take more steps to complete a computation than one less elegant. An example that uses Euclid\\'s algorithm appears below.\\nComputers (and computors), models of computation: A computer (or human \"computor\") is a restricted type of machine, a \"discrete deterministic mechanical device\" that blindly follows its instructions. Melzak\\'s and Lambek\\'s primitive models reduced this notion to four elements: (i) discrete, distinguishable locations, (ii) discrete, indistinguishable counters (iii) an agent, and (iv) a list of instructions that are effective relative to the capability of the agent.Minsky describes a more congenial variation of Lambek\\'s \"abacus\" model in his \"Very Simple Bases for Computability\". Minsky\\'s machine proceeds sequentially through its five (or six, depending on how one counts) instructions unless either a conditional IF-THEN GOTO or an unconditional GOTO changes program flow out of sequence. Besides HALT, Minsky\\'s machine includes three assignment (replacement, substitution) operations: ZERO (e.g. the contents of location replaced by 0: L ← 0), SUCCESSOR (e.g. L ← L+1), and DECREMENT (e.g. L ← L − 1). Rarely must a programmer write \"code\" with such a limited instruction set. But Minsky shows (as do Melzak and Lambek) that his machine is Turing complete with only four general types of instructions: conditional GOTO, unconditional GOTO, assignment/replacement/substitution, and HALT.  However, a few different assignment instructions (e.g. DECREMENT, INCREMENT, and ZERO/CLEAR/EMPTY for a Minsky machine) are also required for Turing-completeness; their exact specification is somewhat up to the designer. The unconditional GOTO is a convenience; it can be constructed by initializing a dedicated location to zero e.g. the instruction \" Z ← 0 \"; thereafter the instruction IF Z=0 THEN GOTO xxx is unconditional.\\nSimulation of an algorithm: computer (computor) language: Knuth advises the reader that \"the best way to learn an algorithm is to try it . . . immediately take pen and paper and work through an example\". But what about a simulation or execution of the real thing? The programmer must translate the algorithm into a language that the simulator/computer/computor can effectively execute. Stone gives an example of this: when computing the roots of a quadratic equation the computor must know how to take a square root. If they don\\'t, then the algorithm, to be effective, must provide a set of rules for extracting a square root.This means that the programmer must know a \"language\" that is effective relative to the target computing agent (computer/computor).\\nBut what model should be used for the simulation? Van Emde Boas observes \"even if we base complexity theory on abstract instead of concrete machines, arbitrariness of the choice of a model remains. It is at this point that the notion of simulation enters\". When speed is being measured, the instruction set matters. For example, the subprogram in Euclid\\'s algorithm to compute the remainder would execute much faster if the programmer had a \"modulus\" instruction available rather than just subtraction (or worse: just Minsky\\'s \"decrement\").\\nStructured programming, canonical structures: Per the Church–Turing thesis, any algorithm can be computed by a model known to be Turing complete, and per Minsky\\'s demonstrations, Turing completeness requires only four instruction types—conditional GOTO, unconditional GOTO, assignment, HALT. Kemeny and Kurtz observe that, while \"undisciplined\" use of unconditional GOTOs and conditional IF-THEN GOTOs can result in \"spaghetti code\", a programmer can write structured programs using only these instructions; on the other hand \"it is also possible, and not too hard, to write badly structured programs in a structured language\". Tausworthe augments the three Böhm-Jacopini canonical structures: SEQUENCE, IF-THEN-ELSE, and WHILE-DO, with two more: DO-WHILE and CASE. An additional benefit of a structured program is that it lends itself to proofs of correctness using mathematical induction.Canonical flowchart symbols: The graphical aide called a flowchart, offers a way to describe and document an algorithm (and a computer program of one). Like the program flow of a Minsky machine, a flowchart always starts at the top of a page and proceeds down. Its primary symbols are only four: the directed arrow showing program flow, the rectangle (SEQUENCE, GOTO), the diamond (IF-THEN-ELSE), and the dot (OR-tie). The Böhm–Jacopini canonical structures are made of these primitive shapes. Sub-structures can \"nest\" in rectangles, but only if a single exit occurs from the superstructure. The symbols, and their use to build the canonical structures are shown in the diagram.\\n\\n\\n== Examples ==\\n\\n\\n=== Algorithm example ===\\nOne of the simplest algorithms is to find the largest number in a list of numbers of random order. Finding the solution requires looking at every number in the list. From this follows a simple algorithm, which can be stated in a high-level description in English prose, as:\\nHigh-level description:\\n\\nIf there are no numbers in the set then there is no highest number.\\nAssume the first number in the set is the largest number in the set.\\nFor each remaining number in the set: if this number is larger than the current largest number, consider this number to be the largest number in the set.\\nWhen there are no numbers left in the set to iterate over, consider the current largest number to be the largest number of the set.(Quasi-)formal description:\\nWritten in prose but much closer to the high-level language of a computer program, the following is the more formal coding of the algorithm in pseudocode or pidgin code:\\n\\n\\n=== Euclid\\'s algorithm ===\\nIn mathematics, the Euclidean algorithm, or Euclid\\'s algorithm, is an efficient method for computing the greatest common divisor (GCD) of two integers (numbers), the largest number that divides them both without a remainder. It is named after the ancient Greek mathematician Euclid, who first described it in his Elements (c. 300 BC). It is one of the oldest algorithms in common use. It can be used to reduce fractions to their simplest form, and is a part of many other number-theoretic and cryptographic calculations. \\n\\nEuclid poses the problem thus: \"Given two numbers not prime to one another, to find their greatest common measure\". He defines \"A number [to be] a multitude composed of units\": a counting number, a positive integer not including zero. To \"measure\" is to place a shorter measuring length s successively (q times) along longer length l until the remaining portion r is less than the shorter length s. In modern words, remainder r = l − q×s, q being the quotient, or remainder r is the \"modulus\", the integer-fractional part left over after the division.For Euclid\\'s method to succeed, the starting lengths must satisfy two requirements: (i) the lengths must not be zero, AND (ii) the subtraction must be \"proper\"; i.e., a test must guarantee that the smaller of the two numbers is subtracted from the larger (or the two can be equal so their subtraction yields zero).\\nEuclid\\'s original proof adds a third requirement: the two lengths must not be prime to one another. Euclid stipulated this so that he could construct a reductio ad absurdum proof that the two numbers\\' common measure is in fact the greatest. While Nicomachus\\' algorithm is the same as Euclid\\'s, when the numbers are prime to one another, it yields the number \"1\" for their common measure. So, to be precise, the following is really Nicomachus\\' algorithm.\\n\\n\\n==== Computer language for Euclid\\'s algorithm ====\\nOnly a few instruction types are required to execute Euclid\\'s algorithm—some logical tests (conditional GOTO), unconditional GOTO, assignment (replacement), and subtraction.\\n\\nA location is symbolized by upper case letter(s), e.g. S, A, etc.\\nThe varying quantity (number) in a location is written in lower case letter(s) and (usually) associated with the location\\'s name. For example, location L at the start might contain the number l = 3009.\\n\\n\\n==== An inelegant program for Euclid\\'s algorithm ====\\n\\nThe following algorithm is framed as Knuth\\'s four-step version of Euclid\\'s and Nicomachus\\', but, rather than using division to find the remainder, it uses successive subtractions of the shorter length s from the remaining length r until r is less than s. The high-level description, shown in boldface, is adapted from Knuth 1973:2–4:\\nINPUT:\\n\\n1 [Into two locations L and S put the numbers l and s that represent the two lengths]:\\n  INPUT L, S\\n2 [Initialize R: make the remaining length r equal to the starting/initial/input length l]:\\n  R ← L\\n\\nE0: [Ensure r ≥ s.]\\n\\n3 [Ensure the smaller of the two numbers is in S and the larger in R]:\\n  IF R > S THEN\\n    the contents of L is the larger number so skip over the exchange-steps 4, 5 and 6:\\n    GOTO step 7\\n  ELSE\\n    swap the contents of R and S.\\n4   L ← R (this first step is redundant, but is useful for later discussion).\\n5   R ← S\\n6   S ← L\\n\\nE1: [Find remainder]: Until the remaining length r in R is less than the shorter length s in S, repeatedly subtract the measuring number s in S from the remaining length r in R.\\n\\n7 IF S > R THEN\\n    done measuring so\\n    GOTO 10\\n  ELSE\\n    measure again,\\n8   R ← R − S\\n9   [Remainder-loop]:\\n    GOTO 7.\\n\\nE2: [Is the remainder zero?]: EITHER (i) the last measure was exact, the remainder in R is zero, and the program can halt, OR (ii) the algorithm must continue: the last measure left a remainder in R less than measuring number in S.\\n\\n10 IF R = 0 THEN\\n     done so\\n     GOTO step 15\\n   ELSE\\n     CONTINUE TO step 11,\\n\\nE3: [Interchange s and r]: The nut of Euclid\\'s algorithm. Use remainder r to measure what was previously smaller number s; L serves as a temporary location.\\n\\n11  L ← R\\n12  R ← S\\n13  S ← L\\n14  [Repeat the measuring process]:\\n    GOTO 7\\n\\nOUTPUT:\\n\\n15 [Done. S contains the greatest common divisor]:\\n   PRINT S\\n\\nDONE:\\n\\n16 HALT, END, STOP.\\n\\n\\n==== An elegant program for Euclid\\'s algorithm ====\\nThe following version of Euclid\\'s algorithm requires only six core instructions to do what thirteen are required to do by \"Inelegant\"; worse, \"Inelegant\" requires more types of instructions. The flowchart of \"Elegant\" can be found at the top of this article. In the (unstructured) Basic language, the steps are numbered, and the instruction LET [] = [] is the assignment instruction symbolized by ←.\\n\\nHow \"Elegant\" works: In place of an outer \"Euclid loop\", \"Elegant\" shifts back and forth between two \"co-loops\", an A > B loop that computes A ← A − B, and a B ≤ A loop that computes B ← B − A. This works because, when at last the minuend M is less than or equal to the subtrahend S (Difference = Minuend − Subtrahend), the minuend can become s (the new measuring length) and the subtrahend can become the new r (the length to be measured); in other words the \"sense\" of the subtraction reverses.\\nThe following version can be used with programming languages from the C-family:\\n\\n\\n=== Testing the Euclid algorithms ===\\nDoes an algorithm do what its author wants it to do? A few test cases usually give some confidence in the core functionality. But tests are not enough. For test cases, one source uses 3009 and 884. Knuth suggested 40902, 24140. Another interesting case is the two relatively prime numbers 14157 and 5950.\\nBut \"exceptional cases\" must be identified and tested. Will \"Inelegant\" perform properly when R > S, S > R, R = S? Ditto for \"Elegant\": B > A, A > B, A = B? (Yes to all). What happens when one number is zero, both numbers are zero? (\"Inelegant\" computes forever in all cases; \"Elegant\" computes forever when A = 0.) What happens if negative numbers are entered? Fractional numbers? If the input numbers, i.e. the domain of the function computed by the algorithm/program, is to include only positive integers including zero, then the failures at zero indicate that the algorithm (and the program that instantiates it) is a partial function rather than a total function. A notable failure due to exceptions is the Ariane 5 Flight 501 rocket failure (June 4, 1996).\\nProof of program correctness by use of mathematical induction: Knuth demonstrates the application of mathematical induction to an \"extended\" version of Euclid\\'s algorithm, and he proposes \"a general method applicable to proving the validity of any algorithm\". Tausworthe proposes that a measure of the complexity of a program be the length of its correctness proof.\\n\\n\\n=== Measuring and improving the Euclid algorithms ===\\nElegance (compactness) versus goodness (speed): With only six core instructions, \"Elegant\" is the clear winner, compared to \"Inelegant\" at thirteen instructions. However, \"Inelegant\" is faster (it arrives at HALT in fewer steps). Algorithm analysis indicates why this is the case: \"Elegant\" does two conditional tests in every subtraction loop, whereas \"Inelegant\" only does one. As the algorithm (usually) requires many loop-throughs, on average much time is wasted doing a \"B = 0?\" test that is needed only after the remainder is computed.\\nCan the algorithms be improved?: Once the programmer judges a program \"fit\" and \"effective\"—that is, it computes the function intended by its author—then the question becomes, can it be improved?\\nThe compactness of \"Inelegant\" can be improved by the elimination of five steps. But Chaitin proved that compacting an algorithm cannot be automated by a generalized algorithm; rather, it can only be done heuristically; i.e., by exhaustive search (examples to be found at Busy beaver), trial and error, cleverness, insight, application of inductive reasoning, etc. Observe that steps 4, 5 and 6 are repeated in steps 11, 12 and 13. Comparison with \"Elegant\" provides a hint that these steps, together with steps 2 and 3, can be eliminated. This reduces the number of core instructions from thirteen to eight, which makes it \"more elegant\" than \"Elegant\", at nine steps.\\nThe speed of \"Elegant\" can be improved by moving the \"B=0?\" test outside of the two subtraction loops. This change calls for the addition of three instructions (B = 0?, A = 0?, GOTO). Now \"Elegant\" computes the example-numbers faster; whether this is always the case for any given A, B, and R, S would require a detailed analysis.\\n\\n\\n== Algorithmic analysis ==\\n\\nIt is frequently important to know how much of a particular resource (such as time or storage) is theoretically required for a given algorithm. Methods have been developed for the analysis of algorithms to obtain such quantitative answers (estimates); for example, the sorting algorithm above has a time requirement of O(n), using the big O notation with n as the length of the list. At all times the algorithm only needs to remember two values: the largest number found so far, and its current position in the input list. Therefore, it is said to have a space requirement of O(1), if the space required to store the input numbers is not counted, or O(n) if it is counted.\\nDifferent algorithms may complete the same task with a different set of instructions in less or more time, space, or \\'effort\\' than others. For example, a binary search algorithm (with cost O(log n) ) outperforms a sequential search (cost O(n) ) when used for table lookups on sorted lists or arrays.\\n\\n\\n=== Formal versus empirical ===\\n\\nThe analysis, and study of algorithms is a discipline of computer science, and is often practiced abstractly without the use of a specific programming language or implementation. In this sense, algorithm analysis resembles other mathematical disciplines in that it focuses on the underlying properties of the algorithm and not on the specifics of any particular implementation. Usually pseudocode is used for analysis as it is the simplest and most general representation. However, ultimately, most algorithms are usually implemented on particular hardware/software platforms and their algorithmic efficiency is eventually put to the test using real code. For the solution of a \"one off\" problem, the efficiency of a particular algorithm may not have significant consequences (unless n is extremely large) but for algorithms designed for fast interactive, commercial or long life scientific usage it may be critical. Scaling from small n to large n frequently exposes inefficient algorithms that are otherwise benign.\\nEmpirical testing is useful because it may uncover unexpected interactions that affect performance. Benchmarks may be used to compare before/after potential improvements to an algorithm after program optimization.\\nEmpirical tests cannot replace formal analysis, though, and are not trivial to perform in a fair manner.\\n\\n\\n=== Execution efficiency ===\\n\\nTo illustrate the potential improvements possible even in well-established algorithms, a recent significant innovation, relating to FFT algorithms (used heavily in the field of image processing), can decrease processing time up to 1,000 times for applications like medical imaging. In general, speed improvements depend on special properties of the problem, which are very common in practical applications. Speedups of this magnitude enable computing devices that make extensive use of image processing (like digital cameras and medical equipment) to consume less power.\\n\\n\\n== Classification ==\\nThere are various ways to classify algorithms, each with its own merits.\\n\\n\\n=== By implementation ===\\nOne way to classify algorithms is by implementation means.\\n\\nRecursion\\nA recursive algorithm is one that invokes (makes reference to) itself repeatedly until a certain condition (also known as termination condition) matches, which is a method common to functional programming. Iterative algorithms use repetitive constructs like loops and sometimes additional data structures like stacks to solve the given problems. Some problems are naturally suited for one implementation or the other. For example, towers of Hanoi is well understood using recursive implementation. Every recursive version has an equivalent (but possibly more or less complex) iterative version, and vice versa.\\nLogical\\nAn algorithm may be viewed as controlled logical deduction. This notion may be expressed as: Algorithm = logic + control. The logic component expresses the axioms that may be used in the computation and the control component determines the way in which deduction is applied to the axioms. This is the basis for the logic programming paradigm. In pure logic programming languages, the control component is fixed and algorithms are specified by supplying only the logic component. The appeal of this approach is the elegant semantics: a change in the axioms produces a well-defined change in the algorithm.\\nSerial, parallel or distributed\\nAlgorithms are usually discussed with the assumption that computers execute one instruction of an algorithm at a time. Those computers are sometimes called serial computers. An algorithm designed for such an environment is called a serial algorithm, as opposed to parallel algorithms or distributed algorithms. Parallel algorithms take advantage of computer architectures where several processors can work on a problem at the same time, whereas distributed algorithms utilize multiple machines connected with a computer network. Parallel or distributed algorithms divide the problem into more symmetrical or asymmetrical subproblems and collect the results back together. The resource consumption in such algorithms is not only processor cycles on each processor but also the communication overhead between the processors. Some sorting algorithms can be parallelized efficiently, but their communication overhead is expensive. Iterative algorithms are generally parallelizable. Some problems have no parallel algorithms and are called inherently serial problems.\\nDeterministic or non-deterministic\\nDeterministic algorithms solve the problem with exact decision at every step of the algorithm whereas non-deterministic algorithms solve problems via guessing although typical guesses are made more accurate through the use of heuristics.\\nExact or approximate\\nWhile many algorithms reach an exact solution, approximation algorithms seek an approximation that is closer to the true solution. The approximation can be reached by either using a deterministic or a random strategy. Such algorithms have practical value for many hard problems. One of the examples of an approximate algorithm is the Knapsack problem, where there is a set of given items. Its goal is to pack the knapsack to get the maximum total value. Each item has some weight and some value. Total weight that can be carried is no more than some fixed number X. So, the solution must consider weights of items as well as their value.\\nQuantum algorithm\\nThey run on a realistic model of quantum computation. The term is usually used for those algorithms which seem inherently quantum, or use some essential feature of Quantum computing such as quantum superposition or quantum entanglement.\\n\\n\\n=== By design paradigm ===\\nAnother way of classifying algorithms is by their design methodology or paradigm. There is a certain number of paradigms, each different from the other. Furthermore, each of these categories includes many different types of algorithms. Some common paradigms are:\\n\\nBrute-force or exhaustive search\\nThis is the naive method of trying every possible solution to see which is best.\\nDivide and conquer\\nA divide and conquer algorithm repeatedly reduces an instance of a problem to one or more smaller instances of the same problem (usually recursively) until the instances are small enough to solve easily. One such example of divide and conquer is merge sorting. Sorting can be done on each segment of data after dividing data into segments and sorting of entire data can be obtained in the conquer phase by merging the segments. A simpler variant of divide and conquer is called a decrease and conquer algorithm, which solves an identical subproblem and uses the solution of this subproblem to solve the bigger problem. Divide and conquer divides the problem into multiple subproblems and so the conquer stage is more complex than decrease and conquer algorithms. An example of a decrease and conquer algorithm is the binary search algorithm.\\nSearch and enumeration\\nMany problems (such as playing chess) can be modeled as problems on graphs. A graph exploration algorithm specifies rules for moving around a graph and is useful for such problems. This category also includes search algorithms, branch and bound enumeration and backtracking.\\nRandomized algorithm\\nSuch algorithms make some choices randomly (or pseudo-randomly). They can be very useful in finding approximate solutions for problems where finding exact solutions can be impractical (see heuristic method below). For some of these problems, it is known that the fastest approximations must involve some randomness. Whether randomized algorithms with polynomial time complexity can be the fastest algorithms for some problems is an open question known as the P versus NP problem. There are two large classes of such algorithms:Monte Carlo algorithms return a correct answer with high-probability. E.g. RP is the subclass of these that run in polynomial time.\\nLas Vegas algorithms always return the correct answer, but their running time is only probabilistically bound, e.g. ZPP.Reduction of complexity\\nThis technique involves solving a difficult problem by transforming it into a better-known problem for which we have (hopefully) asymptotically optimal algorithms. The goal is to find a reducing algorithm whose complexity is not dominated by the resulting reduced algorithm\\'s. For example, one selection algorithm for finding the median in an unsorted list involves first sorting the list (the expensive portion) and then pulling out the middle element in the sorted list (the cheap portion). This technique is also known as transform and conquer.\\nBack tracking\\nIn this approach, multiple solutions are built incrementally and abandoned when it is determined that they cannot lead to a valid full solution.\\n\\n\\n=== Optimization problems ===\\nFor optimization problems there is a more specific classification of algorithms; an algorithm for such problems may fall into one or more of the general categories described above as well as into one of the following:\\n\\nLinear programming\\nWhen searching for optimal solutions to a linear function bound to linear equality and inequality constraints, the constraints of the problem can be used directly in producing the optimal solutions. There are algorithms that can solve any problem in this category, such as the popular simplex algorithm. Problems that can be solved with linear programming include the maximum flow problem for directed graphs. If a problem additionally requires that one or more of the unknowns must be an integer then it is classified in integer programming. A linear programming algorithm can solve such a problem if it can be proved that all restrictions for integer values are superficial, i.e., the solutions satisfy these restrictions anyway. In the general case, a specialized algorithm or an algorithm that finds approximate solutions is used, depending on the difficulty of the problem.\\nDynamic programming\\nWhen a problem shows optimal substructures—meaning the optimal solution to a problem can be constructed from optimal solutions to subproblems—and overlapping subproblems, meaning the same subproblems are used to solve many different problem instances, a quicker approach called dynamic programming avoids recomputing solutions that have already been computed. For example, Floyd–Warshall algorithm, the shortest path to a goal from a vertex in a weighted graph can be found by using the shortest path to the goal from all adjacent vertices. Dynamic programming and memoization go together. The main difference between dynamic programming and divide and conquer is that subproblems are more or less independent in divide and conquer, whereas subproblems overlap in dynamic programming. The difference between dynamic programming and straightforward recursion is in caching or memoization of recursive calls. When subproblems are independent and there is no repetition, memoization does not help; hence dynamic programming is not a solution for all complex problems. By using memoization or maintaining a table of subproblems already solved, dynamic programming reduces the exponential nature of many problems to polynomial complexity.\\nThe greedy method\\nA greedy algorithm is similar to a dynamic programming algorithm in that it works by examining substructures, in this case not of the problem but of a given solution. Such algorithms start with some solution, which may be given or have been constructed in some way, and improve it by making small modifications. For some problems they can find the optimal solution while for others they stop at local optima, that is, at solutions that cannot be improved by the algorithm but are not optimum. The most popular use of greedy algorithms is for finding the minimal spanning tree where finding the optimal solution is possible with this method. Huffman Tree, Kruskal, Prim, Sollin are greedy algorithms that can solve this optimization problem.\\nThe heuristic method\\nIn optimization problems, heuristic algorithms can be used to find a solution close to the optimal solution in cases where finding the optimal solution is impractical. These algorithms work by getting closer and closer to the optimal solution as they progress. In principle, if run for an infinite amount of time, they will find the optimal solution. Their merit is that they can find a solution very close to the optimal solution in a relatively short time. Such algorithms include local search, tabu search, simulated annealing, and genetic algorithms. Some of them, like simulated annealing, are non-deterministic algorithms while others, like tabu search, are deterministic. When a bound on the error of the non-optimal solution is known, the algorithm is further categorized as an approximation algorithm.\\n\\n\\n=== By field of study ===\\n\\nEvery field of science has its own problems and needs efficient algorithms. Related problems in one field are often studied together. Some example classes are search algorithms, sorting algorithms, merge algorithms, numerical algorithms, graph algorithms, string algorithms, computational geometric algorithms, combinatorial algorithms, medical algorithms, machine learning, cryptography, data compression algorithms and parsing techniques.\\nFields tend to overlap with each other, and algorithm advances in one field may improve those of other, sometimes completely unrelated, fields. For example, dynamic programming was invented for optimization of resource consumption in industry but is now used in solving a broad range of problems in many fields.\\n\\n\\n=== By complexity ===\\n\\nAlgorithms can be classified by the amount of time they need to complete compared to their input size:\\n\\nConstant time: if the time needed by the algorithm is the same, regardless of the input size. E.g. an access to an array element.\\nLogarithmic time: if the time is a logarithmic function of the input size. E.g. binary search algorithm.\\nLinear time: if the time is proportional to the input size. E.g. the traverse of a list.\\nPolynomial time: if the time is a power of the input size. E.g. the bubble sort algorithm has quadratic time complexity.\\nExponential time: if the time is an exponential function of the input size. E.g. Brute-force search.Some problems may have multiple algorithms of differing complexity, while other problems might have no algorithms or no known efficient algorithms. There are also mappings from some problems to other problems. Owing to this, it was found to be more suitable to classify the problems themselves instead of the algorithms into equivalence classes based on the complexity of the best possible algorithms for them.\\n\\n\\n== Continuous algorithms ==\\nThe adjective \"continuous\" when applied to the word \"algorithm\" can mean:\\n\\nAn algorithm operating on data that represents continuous quantities, even though this data is represented by discrete approximations—such algorithms are studied in numerical analysis; or\\nAn algorithm in the form of a differential equation that operates continuously on the data, running on an analog computer.\\n\\n\\n== Legal issues ==\\n\\nAlgorithms, by themselves, are not usually patentable. In the United States, a claim consisting solely of simple manipulations of abstract concepts, numbers, or signals does not constitute \"processes\" (USPTO 2006), and hence algorithms are not patentable (as in Gottschalk v. Benson). However practical applications of algorithms are sometimes patentable. For example, in Diamond v. Diehr, the application of a simple feedback algorithm to aid in the curing of synthetic rubber was deemed patentable. The patenting of software is highly controversial, and there are highly criticized patents involving algorithms, especially data compression algorithms, such as Unisys\\' LZW patent.\\nAdditionally, some cryptographic algorithms have export restrictions (see export of cryptography).\\n\\n\\n== History: Development of the notion of \"algorithm\" ==\\n\\n\\n=== Ancient Near East ===\\nThe earliest  evidence of algorithms is found in the Babylonian mathematics of ancient Mesopotamia (modern Iraq). A Sumerian clay tablet found in Shuruppak near Baghdad and dated to circa 2500 BC described the earliest division algorithm. During the Hammurabi dynasty circa 1800-1600 BC, Babylonian clay tablets described algorithms for computing formulas. Algorithms were also used in Babylonian astronomy. Babylonian clay tablets describe and employ algorithmic procedures to compute the time and place of significant astronomical events.Algorithms for arithmetic are also found in ancient Egyptian mathematics, dating back to the Rhind Mathematical Papyrus circa 1550 BC. Algorithms were later used in ancient Hellenistic mathematics. Two examples are the Sieve of Eratosthenes, which was described in the Introduction to Arithmetic by Nicomachus,:\\u200aCh 9.2\\u200a and the Euclidean algorithm, which was first described in Euclid\\'s Elements (c. 300 BC).:\\u200aCh 9.1\\u200a\\n\\n\\n=== Discrete and distinguishable symbols ===\\nTally-marks: To keep track of their flocks, their sacks of grain and their money the ancients used tallying: accumulating stones or marks scratched on sticks or making discrete symbols in clay. Through the Babylonian and Egyptian use of marks and symbols, eventually Roman numerals and the abacus evolved (Dilson, p. 16–41). Tally marks appear prominently in unary numeral system arithmetic used in Turing machine and Post–Turing machine computations.\\n\\n\\n=== Manipulation of symbols as \"place holders\" for numbers: algebra ===\\nMuhammad ibn Mūsā al-Khwārizmī, a Persian mathematician, wrote the Al-jabr in the 9th century. The terms \"algorism\" and \"algorithm\" are derived from the name al-Khwārizmī, while the term \"algebra\" is derived from the book Al-jabr. In Europe, the word \"algorithm\" was originally used to refer to the sets of rules and techniques used by Al-Khwarizmi to solve algebraic equations, before later being generalized to refer to any set of rules or techniques. This eventually culminated in Leibniz\\'s notion of the calculus ratiocinator (ca 1680):\\n\\nA good century and a half ahead of his time, Leibniz proposed an algebra of logic, an algebra that would specify the rules for manipulating logical concepts in the manner that ordinary algebra specifies the rules for manipulating numbers.\\n\\n\\n=== Cryptographic algorithms ===\\nThe first cryptographic algorithm for deciphering encrypted code was developed by Al-Kindi, a 9th-century Arab mathematician, in A Manuscript On Deciphering Cryptographic Messages. He gave the first description of cryptanalysis by frequency analysis, the earliest codebreaking algorithm.\\n\\n\\n=== Mechanical contrivances with discrete states ===\\nThe clock: Bolter credits the invention of the weight-driven clock as \"The key invention [of Europe in the Middle Ages]\", in particular, the verge escapement that provides us with the tick and tock of a mechanical clock. \"The accurate automatic machine\" led immediately to \"mechanical automata\" beginning in the 13th century and finally to \"computational machines\"—the difference engine and analytical engines of Charles Babbage and Countess Ada Lovelace, mid-19th century. Lovelace is credited with the first creation of an algorithm intended for processing on a computer—Babbage\\'s analytical engine, the first device considered a real Turing-complete computer instead of just a calculator—and is sometimes called \"history\\'s first programmer\" as a result, though a full implementation of Babbage\\'s second device would not be realized until decades after her lifetime.\\nLogical machines 1870 – Stanley Jevons\\' \"logical abacus\" and \"logical machine\": The technical problem was to reduce Boolean equations when presented in a form similar to what is now known as Karnaugh maps. Jevons (1880) describes first a simple \"abacus\" of \"slips of wood furnished with pins, contrived so that any part or class of the [logical] combinations can be picked out mechanically ... More recently, however, I have reduced the system to a completely mechanical form, and have thus embodied the whole of the indirect process of inference in what may be called a Logical Machine\" His machine came equipped with \"certain moveable wooden rods\" and \"at the foot are 21 keys like those of a piano [etc] ...\". With this machine he could analyze a \"syllogism or any other simple logical argument\".This machine he displayed in 1870 before the Fellows of the Royal Society. Another logician John Venn, however, in his 1881 Symbolic Logic, turned a jaundiced eye to this effort: \"I have no high estimate myself of the interest or importance of what are sometimes called logical machines ... it does not seem to me that any contrivances at present known or likely to be discovered really deserve the name of logical machines\"; see more at Algorithm characterizations. But not to be outdone he too presented \"a plan somewhat analogous, I apprehend, to Prof. Jevon\\'s abacus ... [And] [a]gain, corresponding to Prof. Jevons\\'s logical machine, the following contrivance may be described. I prefer to call it merely a logical-diagram machine ... but I suppose that it could do very completely all that can be rationally expected of any logical machine\".Jacquard loom, Hollerith punch cards, telegraphy and telephony – the electromechanical relay: Bell and Newell (1971) indicate that the Jacquard loom (1801), precursor to Hollerith cards (punch cards, 1887), and \"telephone switching technologies\" were the roots of a tree leading to the development of the first computers. By the mid-19th century the telegraph, the precursor of the telephone, was in use throughout the world, its discrete and distinguishable encoding of letters as \"dots and dashes\" a common sound. By the late 19th century the ticker tape (ca 1870s) was in use, as was the use of Hollerith cards in the 1890 U.S. census. Then came the teleprinter (ca. 1910) with its punched-paper use of Baudot code on tape.\\nTelephone-switching networks of electromechanical relays (invented 1835) was behind the work of George Stibitz (1937), the inventor of the digital adding device. As he worked in Bell Laboratories, he observed the \"burdensome\\' use of mechanical calculators with gears. \"He went home one evening in 1937 intending to test his idea... When the tinkering was over, Stibitz had constructed a binary adding device\".Davis (2000) observes the particular importance of the electromechanical relay (with its two \"binary states\" open and closed):\\n\\nIt was only with the development, beginning in the 1930s, of electromechanical calculators using electrical relays, that machines were built having the scope Babbage had envisioned.\"\\n\\n\\n=== Mathematics during the 19th century up to the mid-20th century ===\\nSymbols and rules: In rapid succession, the mathematics of George Boole (1847, 1854), Gottlob Frege (1879), and Giuseppe Peano (1888–1889) reduced arithmetic to a sequence of symbols manipulated by rules. Peano\\'s The principles of arithmetic, presented by a new method (1888) was \"the first attempt at an axiomatization of mathematics in a symbolic language\".But Heijenoort gives Frege (1879) this kudos: Frege\\'s is \"perhaps the most important single work ever written in logic. ... in which we see a \" \\'formula language\\', that is a lingua characterica, a language written with special symbols, \"for pure thought\", that is, free from rhetorical embellishments ... constructed from specific symbols that are manipulated according to definite rules\". The work of Frege was further simplified and amplified by Alfred North Whitehead and Bertrand Russell in their Principia Mathematica (1910–1913).\\nThe paradoxes: At the same time a number of disturbing paradoxes appeared in the literature, in particular, the Burali-Forti paradox (1897), the Russell paradox (1902–03), and the Richard Paradox. The resultant considerations led to Kurt Gödel\\'s paper (1931)—he specifically cites the paradox of the liar—that completely reduces rules of recursion to numbers.\\nEffective calculability: In an effort to solve the Entscheidungsproblem defined precisely by Hilbert in 1928, mathematicians first set about to define what was meant by an \"effective method\" or \"effective calculation\" or \"effective calculability\" (i.e., a calculation that would succeed). In rapid succession the following appeared: Alonzo Church, Stephen Kleene and J.B. Rosser\\'s λ-calculus a finely honed definition of \"general recursion\" from the work of Gödel acting on suggestions of Jacques Herbrand (cf. Gödel\\'s Princeton lectures of 1934) and subsequent simplifications by Kleene. Church\\'s proof that the Entscheidungsproblem was unsolvable, Emil Post\\'s definition of effective calculability as a worker mindlessly following a list of instructions to move left or right through a sequence of rooms and while there either mark or erase a paper or observe the paper and make a yes-no decision about the next instruction. Alan Turing\\'s proof of that the Entscheidungsproblem was unsolvable by use of his \"a- [automatic-] machine\"—in effect almost identical to Post\\'s \"formulation\", J. Barkley Rosser\\'s definition of \"effective method\" in terms of \"a machine\". Kleene\\'s proposal of a precursor to \"Church thesis\" that he called \"Thesis I\", and a few years later Kleene\\'s renaming his Thesis \"Church\\'s Thesis\" and proposing \"Turing\\'s Thesis\".\\n\\n\\n=== Emil Post (1936) and Alan Turing (1936–37, 1939) ===\\nEmil Post (1936) described the actions of a \"computer\" (human being) as follows:\\n\\n\"...two concepts are involved: that of a symbol space in which the work leading from problem to answer is to be carried out, and a fixed unalterable set of directions.His symbol space would be\\n\\n\"a two-way infinite sequence of spaces or boxes... The problem solver or worker is to move and work in this symbol space, being capable of being in, and operating in but one box at a time.... a box is to admit of but two possible conditions, i.e., being empty or unmarked, and having a single mark in it, say a vertical stroke.\"One box is to be singled out and called the starting point. ...a specific problem is to be given in symbolic form by a finite number of boxes [i.e., INPUT] being marked with a stroke. Likewise, the answer [i.e., OUTPUT] is to be given in symbolic form by such a configuration of marked boxes...\"A set of directions applicable to a general problem sets up a deterministic process when applied to each specific problem. This process terminates only when it comes to the direction of type (C ) [i.e., STOP]\". See more at Post–Turing machine\\nAlan Turing\\'s work preceded that of Stibitz (1937); it is unknown whether Stibitz knew of the work of Turing. Turing\\'s biographer believed that Turing\\'s use of a typewriter-like model derived from a youthful interest: \"Alan had dreamt of inventing typewriters as a boy; Mrs. Turing had a typewriter, and he could well have begun by asking himself what was meant by calling a typewriter \\'mechanical\\'\". Given the prevalence of Morse code and telegraphy, ticker tape machines, and teletypewriters we might conjecture that all were influences.\\nTuring—his model of computation is now called a Turing machine—begins, as did Post, with an analysis of a human computer that he whittles down to a simple set of basic motions and \"states of mind\". But he continues a step further and creates a machine as a model of computation of numbers.\\n\"Computing is normally done by writing certain symbols on paper. We may suppose this paper is divided into squares like a child\\'s arithmetic book...I assume then that the computation is carried out on one-dimensional paper, i.e., on a tape divided into squares. I shall also suppose that the number of symbols which may be printed is finite...\"The behavior of the computer at any moment is determined by the symbols which he is observing, and his \"state of mind\" at that moment. We may suppose that there is a bound B to the number of symbols or squares which the computer can observe at one moment. If he wishes to observe more, he must use successive observations. We will also suppose that the number of states of mind which need be taken into account is finite...\"Let us imagine that the operations performed by the computer to be split up into \\'simple operations\\' which are so elementary that it is not easy to imagine them further divided.\"Turing\\'s reduction yields the following:\\n\\n\"The simple operations must therefore include:\\n\"(a) Changes of the symbol on one of the observed squares\\n\"(b) Changes of one of the squares observed to another square within L squares of one of the previously observed squares.\"It may be that some of these change necessarily invoke a change of state of mind. The most general single operation must, therefore, be taken to be one of the following:\\n\\n\"(A) A possible change (a) of symbol together with a possible change of state of mind.\\n\"(B) A possible change (b) of observed squares, together with a possible change of state of mind\"\"We may now construct a machine to do the work of this computer.\"A few years later, Turing expanded his analysis (thesis, definition) with this forceful expression of it:\\n\\n\"A function is said to be \"effectively calculable\" if its values can be found by some purely mechanical process. Though it is fairly easy to get an intuitive grasp of this idea, it is nevertheless desirable to have some more definite, mathematical expressible definition ... [he discusses the history of the definition pretty much as presented above with respect to Gödel, Herbrand, Kleene, Church, Turing, and Post] ... We may take this statement literally, understanding by a purely mechanical process one which could be carried out by a machine. It is possible to give a mathematical description, in a certain normal form, of the structures of these machines. The development of these ideas leads to the author\\'s definition of a computable function, and to an identification of computability † with effective calculability ... .\\n\"† We shall use the expression \"computable function\" to mean a function calculable by a machine, and we let \"effectively calculable\" refer to the intuitive idea without particular identification with any one of these definitions\".\\n\\n\\n=== J.B. Rosser (1939) and S.C. Kleene (1943) ===\\nJ. Barkley Rosser defined an \\'effective [mathematical] method\\' in the following manner (italicization added):\\n\\n\"\\'Effective method\\' is used here in the rather special sense of a method each step of which is precisely determined and which is certain to produce the answer in a finite number of steps. With this special meaning, three different precise definitions have been given to date. [his footnote #5; see discussion immediately below]. The simplest of these to state (due to Post and Turing) says essentially that an effective method of solving certain sets of problems exists if one can build a machine which will then solve any problem of the set with no human intervention beyond inserting the question and (later) reading the answer. All three definitions are equivalent, so it doesn\\'t matter which one is used. Moreover, the fact that all three are equivalent is a very strong argument for the correctness of any one.\" (Rosser 1939:225–226)Rosser\\'s footnote No. 5 references the work of (1) Church and Kleene and their definition of λ-definability, in particular Church\\'s use of it in his An Unsolvable Problem of Elementary Number Theory (1936); (2) Herbrand and Gödel and their use of recursion in particular Gödel\\'s use in his famous paper On Formally Undecidable Propositions of Principia Mathematica and Related Systems I (1931); and (3) Post (1936) and Turing (1936–37) in their mechanism-models of computation.\\nStephen C. Kleene defined as his now-famous \"Thesis I\" known as the Church–Turing thesis. But he did this in the following context (boldface in original):\\n\\n\"12. Algorithmic theories... In setting up a complete algorithmic theory, what we do is to describe a procedure, performable for each set of values of the independent variables, which procedure necessarily terminates and in such manner that from the outcome we can read a definite answer, \"yes\" or \"no,\" to the question, \"is the predicate value true?\"\" (Kleene 1943:273)\\n\\n\\n=== History after 1950 ===\\nA number of efforts have been directed toward further refinement of the definition of \"algorithm\", and activity is on-going because of issues surrounding, in particular, foundations of mathematics (especially the Church–Turing thesis) and philosophy of mind (especially arguments about artificial intelligence). For more, see Algorithm characterizations.\\n\\n\\n== See also ==\\n\\n\\n== Notes ==\\n\\n\\n== Bibliography ==\\n\\n\\n== Further reading ==\\n\\n\\n== External links ==\\n\"Algorithm\", Encyclopedia of Mathematics, EMS Press, 2001 [1994]\\nAlgorithms at Curlie\\nWeisstein, Eric W. \"Algorithm\". MathWorld.\\nDictionary of Algorithms and Data Structures – National Institute of Standards and TechnologyAlgorithm repositoriesThe Stony Brook Algorithm Repository – State University of New York at Stony Brook\\nCollected Algorithms of the ACM – Association for Computing Machinery\\nThe Stanford GraphBase – Stanford University', 'Computer programming is the process of designing and building an executable computer program to accomplish a specific computing result or to perform a specific task. Programming involves tasks such as: analysis, generating algorithms, profiling algorithms\\' accuracy and resource consumption, and the implementation of algorithms in a chosen programming language (commonly referred to as coding). The source code of a program is written in one or more languages that are intelligible to programmers, rather than machine code, which is directly executed by the central processing unit. The purpose of programming is to find a sequence of instructions that will automate the performance of a task (which can be as complex as an operating system) on a computer, often for solving a given problem. Proficient programming thus often requires expertise in several different subjects, including knowledge of the application domain, specialized algorithms, and formal logic.\\nTasks accompanying and related to programming include: testing, debugging, source code maintenance, implementation of build systems, and management of derived artifacts, such as the machine code of computer programs. These might be considered part of the programming process, but often the term software development is used for this larger process with the term programming, implementation, or coding reserved for the actual writing of code. Software engineering combines engineering techniques with software development practices. Reverse engineering is a related process used by designers, analysts and programmers to understand and re-create/re-implement.\\n\\n\\n== History ==\\n\\nProgrammable devices have existed for centuries. As early as the 9th century, a programmable music sequencer was invented by the Persian Banu Musa brothers, who described an automated mechanical flute player in the Book of Ingenious Devices. In 1206, the Arab engineer Al-Jazari invented a programmable drum machine where a musical mechanical automaton could be made to play different rhythms and drum patterns, via pegs and cams. In 1801, the Jacquard loom could produce entirely different weaves by changing the \"program\" – a series of pasteboard cards with holes punched in them.\\nCode-breaking algorithms have also existed for centuries. In the 9th century, the Arab mathematician Al-Kindi described a cryptographic algorithm for deciphering encrypted code, in A Manuscript on Deciphering Cryptographic Messages. He gave the first description of cryptanalysis by frequency analysis, the earliest code-breaking algorithm.The first computer program is generally dated to 1843, when mathematician Ada Lovelace published an algorithm to calculate a sequence of Bernoulli numbers, intended to be carried out by Charles Babbage\\'s Analytical Engine.\\n\\nIn the 1880s Herman Hollerith invented the concept of storing data in machine-readable form. Later a control panel (plug board) added to his 1906 Type I Tabulator allowed it to be programmed for different jobs, and by the late 1940s, unit record equipment such as the IBM 602 and IBM 604, were programmed by control panels in a similar way, as were the first electronic computers. However, with the concept of the stored-program computer introduced in 1949, both programs and data were stored and manipulated in the same way in computer memory.\\n\\n\\n=== Machine language ===\\nMachine code was the language of early programs, written in the instruction set of the particular machine, often in binary notation. Assembly languages were soon developed that let the programmer specify instruction in a text format, (e.g., ADD X, TOTAL), with abbreviations for each operation code and meaningful names for specifying addresses. However, because an assembly language is little more than a different notation for a machine language, any two machines with different instruction sets also have different assembly languages.\\n\\n\\n=== Compiler languages ===\\nHigh-level languages made the process of developing a program simpler and more understandable, and less bound to the underlying hardware. \\nThe first compiler related tool, the A-0 System, was developed in 1952 by Grace Hopper, who also coined the term \\'compiler\\'. FORTRAN, the first widely used high-level language to have a functional implementation, came out in 1957, and many other languages were soon developed—in particular, COBOL aimed at commercial data processing, and Lisp for computer research.\\nThese compiled languages allow the programmer to write programs in terms that are syntactically richer, and more capable of abstracting the code, making it easy to target for varying machine instruction sets via compilation declarations and heuristics. Compilers harnessed the power of computers to make programming easier by allowing programmers to specify calculations by entering a formula using infix notation.\\n\\n\\n=== Source code entry ===\\n\\nPrograms were mostly still entered using punched cards or paper tape. By the late 1960s, data storage devices and computer terminals became inexpensive enough that programs could be created by typing directly into the computers. Text editors were also developed that allowed changes and corrections to be made much more easily than with punched cards. \\n\\n\\n== Modern programming ==\\n\\n\\n=== Quality requirements ===\\nWhatever the approach to development may be, the final program must satisfy some fundamental properties. The following properties are among the most important:\\nReliability: how often the results of a program are correct. This depends on conceptual correctness of algorithms and minimization of programming mistakes, such as mistakes in resource management (e.g., buffer overflows and race conditions) and logic errors (such as division by zero or off-by-one errors).\\nRobustness: how well a program anticipates problems due to errors (not bugs). This includes situations such as incorrect, inappropriate or corrupt data, unavailability of needed resources such as memory, operating system services, and network connections, user error, and unexpected power outages.\\nUsability: the ergonomics of a program: the ease with which a person can use the program for its intended purpose or in some cases even unanticipated purposes. Such issues can make or break its success even regardless of other issues. This involves a wide range of textual, graphical, and sometimes hardware elements that improve the clarity, intuitiveness, cohesiveness and completeness of a program\\'s user interface.\\nPortability: the range of computer hardware and operating system platforms on which the source code of a program can be compiled/interpreted and run. This depends on differences in the programming facilities provided by the different platforms, including hardware and operating system resources, expected behavior of the hardware and operating system, and availability of platform-specific compilers (and sometimes libraries) for the language of the source code.\\nMaintainability: the ease with which a program can be modified by its present or future developers in order to make improvements or to customize, fix bugs and security holes, or adapt it to new environments. Good practices during initial development make the difference in this regard. This quality may not be directly apparent to the end user but it can significantly affect the fate of a program over the long term.\\nEfficiency/performance: Measure of system resources a program consumes (processor time, memory space, slow devices such as disks, network bandwidth and to some extent even user interaction): the less, the better. This also includes careful management of resources, for example cleaning up temporary files and eliminating memory leaks. This is often discussed under the shadow of a chosen programming language. Although the language certainly affects performance, even slower languages, such as Python, can execute programs instantly from a human perspective. Speed, resource usage, and performance are important for programs that bottleneck the system, but efficient use of programmer time is also important and is related to cost: more hardware may be cheaper.\\n\\n\\n=== Readability of source code ===\\nIn computer programming, readability refers to the ease with which a human reader can comprehend the purpose, control flow, and operation of source code. It affects the aspects of quality above, including portability, usability and most importantly maintainability.\\nReadability is important because programmers spend the majority of their time reading, trying to understand and modifying existing source code, rather than writing new source code. Unreadable code often leads to bugs, inefficiencies, and duplicated code. A study found that a few simple readability transformations made code shorter and drastically reduced the time to understand it.Following a consistent programming style often helps readability. However, readability is more than just programming style. Many factors, having little or nothing to do with the ability of the computer to efficiently compile and execute the code, contribute to readability. Some of these factors include:\\n\\nDifferent indent styles (whitespace)\\nComments\\nDecomposition\\nNaming conventions for objects (such as variables, classes, functions, procedures, etc.)The presentation aspects of this (such as indents, line breaks, color highlighting, and so on) are often handled by the source code editor, but the content aspects reflect the programmer\\'s talent and skills.\\nVarious visual programming languages have also been developed with the intent to resolve readability concerns by adopting non-traditional approaches to code structure and display. Integrated development environments (I.D.Es) aim to integrate all such help. Techniques like Code refactoring can enhance readability.\\n\\n\\n=== Algorithmic complexity ===\\nThe academic field and the engineering practice of computer programming are both largely concerned with discovering and implementing the most efficient algorithms for a given class of problems. For this purpose, algorithms are classified into orders using so-called Big O notation, which expresses resource use, such as execution time or memory consumption, in terms of the size of an input. Expert programmers are familiar with a variety of well-established algorithms and their respective complexities and use this knowledge to choose algorithms that are best suited to the circumstances.\\n\\n\\n==== Chess algorithms as an example ====\\n\"Programming a Computer for Playing Chess\" was a 1950 paper that evaluated a \"minimax\" algorithm that is part of the history of algorithmic complexity; a course on IBM\\'s Deep Blue (chess computer) is part of the computer science curriculum at Stanford University.\\n\\n\\n=== Methodologies ===\\nThe first step in most formal software development processes is requirements analysis, followed by testing to determine value modeling, implementation, and failure elimination (debugging). There exist a lot of different approaches for each of those tasks. One approach popular for requirements analysis is Use Case analysis. Many programmers use forms of Agile software development where the various stages of formal software development are more integrated together into short cycles that take a few weeks rather than years. There are many approaches to the Software development process.\\nPopular modeling techniques include Object-Oriented Analysis and Design (OOAD) and Model-Driven Architecture (MDA). The Unified Modeling Language (UML) is a notation used for both the OOAD and MDA.\\nA similar technique used for database design is Entity-Relationship Modeling (ER Modeling).\\nImplementation techniques include imperative languages (object-oriented or procedural), functional languages, and logic languages.\\n\\n\\n=== Measuring language usage ===\\n\\nIt is very difficult to determine what are the most popular modern programming languages. Methods of measuring programming language popularity include: counting the number of job advertisements that mention the language, the number of books sold and courses teaching the language (this overestimates the importance of newer languages), and estimates of the number of existing lines of code written in the language (this underestimates the number of users of business languages such as COBOL).\\nSome languages are very popular for particular kinds of applications, while some languages are regularly used to write many different kinds of applications. For example, COBOL is still strong in corporate data centers often on large mainframe computers, Fortran in engineering applications, scripting languages in Web development, and C in embedded software. Many applications use a mix of several languages in their construction and use.  New languages are generally designed around the syntax of a prior language with new functionality added, (for example C++ adds object-orientation to C, and Java adds memory management and bytecode to C++, but as a result, loses efficiency and the ability for low-level manipulation).\\n\\n\\n=== Debugging ===\\n\\nDebugging is a very important task in the software development process since having defects in a program can have significant consequences for its users. Some languages are more prone to some kinds of faults because their specification does not require compilers to perform as much checking as other languages. Use of a static code analysis tool can help detect some possible problems. Normally the first step in debugging is to attempt to reproduce the problem. This can be a non-trivial task, for example as with parallel processes or some unusual software bugs. Also, specific user environment and usage history can make it difficult to reproduce the problem.\\nAfter the bug is reproduced, the input of the program may need to be simplified to make it easier to debug. For example, when a bug in a compiler can make it crash when parsing some large source file, a simplification of the test case that results in only few lines from the original source file can be sufficient to reproduce the same crash. Trial-and-error/divide-and-conquer is needed: the programmer will try to remove some parts of the original test case and check if the problem still exists. When debugging the problem in a GUI, the programmer can try to skip some user interaction from the original problem description and check if remaining actions are sufficient for bugs to appear. Scripting and breakpointing is also part of this process.\\nDebugging is often done with IDEs. Standalone debuggers like GDB are also used, and these often provide less of a visual environment, usually using a command line. Some text editors such as Emacs allow GDB to be invoked through them, to provide a visual environment.\\n\\n\\n== Programming languages ==\\n\\nDifferent programming languages support different styles of programming (called programming paradigms). The choice of language used is subject to many considerations, such as company policy, suitability to task, availability of third-party packages, or individual preference. Ideally, the programming language best suited for the task at hand will be selected. Trade-offs from this ideal involve finding enough programmers who know the language to build a team, the availability of compilers for that language, and the efficiency with which programs written in a given language execute. Languages form an approximate spectrum from \"low-level\" to \"high-level\"; \"low-level\" languages are typically more machine-oriented and faster to execute, whereas \"high-level\" languages are more abstract and easier to use but execute less quickly. It is usually easier to code in \"high-level\" languages than in \"low-level\" ones.\\nAllen Downey, in his book How To Think Like A Computer Scientist, writes:\\n\\nThe details look different in different languages, but a few basic instructions appear in just about every language:\\nInput: Gather data from the keyboard, a file, or some other device.\\nOutput: Display data on the screen or send data to a file or other device.\\nArithmetic: Perform basic arithmetical operations like addition and multiplication.\\nConditional Execution: Check for certain conditions and execute the appropriate sequence of statements.\\nRepetition: Perform some action repeatedly, usually with some variation.Many computer languages provide a mechanism to call functions provided by shared libraries. Provided the functions in a library follow the appropriate run-time conventions (e.g., method of passing arguments), then these functions may be written in any other language.\\n\\n\\n== Programmers ==\\n\\nComputer programmers are those who write computer software. Their jobs usually involve:\\n\\nAlthough programming has been presented in the media as a somewhat mathematical subject, some research shows that good programmers have strong skills in natural human languages, and that learning to code is similar to learning a foreign language.\\n\\n\\n== See also ==\\n\\n\\n== References ==\\n\\n\\n=== Sources ===\\nCeruzzi, Paul E. (1998). History of Computing. Cambridge, Massachusetts: MIT Press. ISBN 9780262032551 – via EBSCOhost.\\nEvans, Claire L. (2018). Broad Band: The Untold Story of the Women Who Made the Internet. New York: Portfolio/Penguin. ISBN 9780735211759.\\nGürer, Denise (1995). \"Pioneering Women in Computer Science\" (PDF). Communications of the ACM. 38 (1): 45–54. doi:10.1145/204865.204875. S2CID 6626310.\\nSmith, Erika E. (2013). \"Recognizing a Collective Inheritance through the History of Women in Computing\". CLCWeb: Comparative Literature & Culture: A WWWeb Journal. 15 (1): 1–9 – via EBSCOhost.\\n\\n\\n== Further reading ==\\nA.K. Hartmann, Practical Guide to Computer Simulations, Singapore: World Scientific (2009)\\nA. Hunt, D. Thomas, and W. Cunningham, The Pragmatic Programmer. From Journeyman to Master, Amsterdam: Addison-Wesley Longman (1999)\\nBrian W. Kernighan, The Practice of Programming, Pearson (1999)\\nWeinberg, Gerald M., The Psychology of Computer Programming, New York: Van Nostrand Reinhold (1971)\\nEdsger W. Dijkstra, A Discipline of Programming, Prentice-Hall (1976)\\nO.-J. Dahl, E.W.Dijkstra, C.A.R. Hoare, Structured Programming, Academic Press (1972)\\nDavid Gries, The Science of Programming, Springer-Verlag (1981)\\n\\n\\n== External links ==\\n Media related to Computer programming at Wikimedia Commons\\n Quotations related to Programming at Wikiquote\\nSoftware engineering at Curlie', 'Human–computer interaction (HCI) is research in the design and the use of computer technology, which focuses on the interfaces between people (users) and computers. HCI researchers observe the ways humans interact with computers and design technologies that allow humans to interact with computers in novel ways.\\nAs a field of research, human-computer interaction is situated at the intersection of computer science, behavioral sciences, design, media studies, and several other fields of study. The term was popularized by Stuart K. Card, Allen Newell, and Thomas P. Moran in their 1983 book, The Psychology of Human-Computer Interaction, although the authors first used the term in 1980, and the first known use was in 1975. The term is intended to convey that, unlike other tools with specific and limited uses, computers have many uses which often involve an open-ended dialogue between the user and the computer. The notion of dialogue likens human-computer interaction to human-to-human interaction: an analogy that is crucial to theoretical considerations in the field.\\n\\n\\n== Introduction ==\\nHumans interact with computers in many ways, and the interface between the two is crucial to facilitating this interaction. HCI is also sometimes termed human–machine interaction (HMI), man-machine interaction (MMI) or computer-human interaction (CHI). Desktop applications, internet browsers, handheld computers, and computer kiosks make use of the prevalent graphical user interfaces (GUI) of today. Voice user interfaces (VUI) are used for speech recognition and synthesizing systems, and the emerging multi-modal and Graphical user interfaces (GUI) allow humans to engage with embodied character agents in a way that cannot be achieved with other interface paradigms. The growth in human–computer interaction field has led to an increase in the quality of interaction, and resulted in many new areas of research beyond. Instead of designing regular interfaces, the different research branches focus on the concepts of multimodality over unimodality, intelligent adaptive interfaces over command/action based ones, and active interfaces over passive interfaces.The Association for Computing Machinery (ACM) defines human-computer interaction as \"a discipline that is concerned with the design, evaluation, and implementation of interactive computing systems for human use and with the study of major phenomena surrounding them\". An important facet of HCI is user satisfaction (or End-User Computing Satisfaction). It goes on to say:\\n\"Because human-computer interaction studies a human and a machine in communication, it draws from supporting knowledge on both the machine and the human side. On the machine side, techniques in computer graphics, operating systems, programming languages, and development environments are relevant. On the human side, communication theory, graphic and industrial design disciplines, linguistics, social sciences, cognitive psychology, social psychology, and human factors such as computer user satisfaction are relevant. And, of course, engineering and design methods are relevant.\"Due to the multidisciplinary nature of HCI, people with different backgrounds contribute to its success.\\nPoorly designed human-machine interfaces can lead to many unexpected problems. A classic example is the Three Mile Island accident, a nuclear meltdown accident, where investigations concluded that the design of the human-machine interface was at least partly responsible for the disaster. Similarly, accidents in aviation have resulted from manufacturers\\' decisions to use non-standard flight instruments or throttle quadrant layouts: even though the new designs were proposed to be superior in basic human-machine interaction, pilots had already ingrained the \"standard\" layout. Thus, the conceptually good idea had unintended results.\\n\\n\\n== Human–computer interface ==\\n\\nThe human-computer interface can be described as the point of communication between the human user and the computer. The flow of information between the human and computer is defined as the loop of interaction. The loop of interaction has several aspects to it, including:\\n\\nVisual Based: The visual-based human-computer interaction is probably the most widespread human-computer interaction (HCI) research area.\\nAudio Based: The audio-based interaction between a computer and a human is another important area of HCI systems. This area deals with information acquired by different audio signals.\\nTask environment: The conditions and goals set upon the user.\\nMachine environment: The computer\\'s environment is connected to, e.g., a laptop in a college student\\'s dorm room.\\nAreas of the interface: Non-overlapping areas involve processes of the human and computer, not about their interaction. Meanwhile, the overlapping areas only concern themselves with the processes of their interaction.\\nInput flow: The flow of information begins in the task environment when the user has some task requiring using their computer.\\nOutput: The flow of information that originates in the machine environment.\\nFeedback: Loops through the interface that evaluate, moderate, and confirm processes as they pass from the human through the interface to the computer and back.\\nFit: This matches the computer design, the user, and the task to optimize the human resources needed to accomplish the task.\\n\\n\\n== Goals for computers ==\\nHuman–computer interaction studies the ways in which humans make—or do not make—use of computational artifacts, systems, and infrastructures. Much of the research in this field seeks to improve the human–computer interaction by improving the usability of computer interfaces. How usability is to be precisely understood, how it relates to other social and cultural values, and when it is, and when it may not be a desirable property of computer interfaces is increasingly debated.Much of the research in the field of human-computer interaction takes an interest in:\\n\\nMethods for designing new computer interfaces, thereby optimizing a design for a desired property such as learnability, findability, the efficiency of use.\\nMethods for implementing interfaces, e.g., by means of software libraries.\\nMethods for evaluating and comparing interfaces with respect to their usability and other desirable properties.\\nMethods for studying human-computer use and its sociocultural implications more broadly.\\nMethods for determining whether or not the user is human or computer.\\nModels and theories of human-computer use as well as conceptual frameworks for the design of computer interfaces, such as cognitivist user models, Activity Theory, or ethnomethodological accounts of human-computer use.\\nPerspectives that critically reflect upon the values that underlie computational design, computer use, and HCI research practice.Visions of what researchers in the field seek to achieve might vary. When pursuing a cognitivist perspective, researchers of HCI may seek to align computer interfaces with the mental model that humans have of their activities. When pursuing a post-cognitivist perspective, researchers of HCI may seek to align computer interfaces with existing social practices or existing sociocultural values.\\nResearchers in HCI are interested in developing design methodologies, experimenting with devices, prototyping software and hardware systems, exploring interaction paradigms, and developing models and theories of interaction.\\n\\n\\n== Design ==\\n\\n\\n=== Principles ===\\n\\nThe following experimental design principles are considered, when evaluating a current user interface, or designing a new user interface:\\n\\nEarly focus is placed on the user(s) and task(s): How many users are needed to perform the task(s) is established and who the appropriate users should be is determined (someone who has never used the interface, and will not use the interface in the future, is most likely not a valid user). In addition, the task(s) the users will be performing and how often the task(s) need to be performed is defined.\\nEmpirical measurement: the interface is tested with real users who come in contact with the interface daily. The results can vary with the performance level of the user and the typical human-computer interaction may not always be represented. Quantitative usability specifics, such as the number of users performing the task(s), the time to complete the task(s), and the number of errors made during the task(s) are determined.\\nIterative design: After determining what users, tasks, and empirical measurements to include, the following iterative design steps are performed:\\nDesign the user interface\\nTest\\nAnalyze results\\nRepeatThe iterative design process is repeated until a sensible, user-friendly interface is created.\\n\\n\\n=== Methodologies ===\\nVarious strategies delineating methods for human–PC interaction design have developed since the conception of the field during the 1980s. Most plan philosophies come from a model for how clients, originators, and specialized frameworks interface. Early techniques treated clients\\' psychological procedures as unsurprising and quantifiable and urged plan specialists to look at subjective science to establish zones, (for example, memory and consideration) when structuring UIs. Present-day models, in general, center around a steady input and discussion between clients, creators, and specialists and push for specialized frameworks to be folded with the sorts of encounters clients need to have, as opposed to wrapping user experience around a finished framework.\\n\\nActivity theory: utilized in HCI to characterize and consider the setting where human cooperations with PCs occur. Action hypothesis gives a structure for reasoning about activities in these specific circumstances and illuminates the design of interactions from an action-driven perspective.\\nUser-centered design (UCD): a cutting-edge, broadly-rehearsed plan theory established on the possibility that clients must become the overwhelming focus in the plan of any PC framework. Clients, architects, and specialized experts cooperate to determine the requirements and restrictions of the client and make a framework to support these components. Frequently, client-focused plans are informed by ethnographic investigations of situations in which clients will associate with the framework. This training is like participatory design, which underscores the likelihood for end-clients to contribute effectively through shared plan sessions and workshops.\\nPrinciples of UI design: these standards may be considered during the design of a client interface: resistance, effortlessness, permeability, affordance, consistency, structure, and feedback.\\nValue sensitive design (VSD): a technique for building innovation that accounts for the individuals who utilize the design straightforwardly, and just as well for those who the design influences, either directly or indirectly. VSD utilizes an iterative plan process that includes three kinds of examinations: theoretical, exact, and specialized. Applied examinations target the understanding and articulation of the different parts of the design, and its qualities or any clashes that may emerge for the users of the design. Exact examinations are subjective or quantitative plans to explore things used to advise the creators\\' understanding regarding the clients\\' qualities, needs, and practices. Specialized examinations can include either investigation of how individuals use related advances or the framework plans.\\n\\n\\n== Display designs ==\\nDisplays are human-made artifacts designed to support the perception of relevant system variables and facilitate further processing of that information. Before a display is designed, the task that the display is intended to support must be defined (e.g., navigating, controlling, decision making, learning, entertaining, etc.). A user or operator must be able to process whatever information a system generates and displays; therefore, the information must be displayed according to principles to support perception, situation awareness, and understanding.\\n\\n\\n=== Thirteen principles of display design ===\\nChristopher Wickens et al. defined 13 principles of display design in their book An Introduction to Human Factors Engineering.These principles of human perception and information processing can be utilized to create an effective display design. A reduction in errors, a reduction in required training time, an increase in efficiency, and an increase in user satisfaction are a few of the many potential benefits that can be achieved by utilizing these principles.\\nCertain principles may not apply to different displays or situations. Some principles may also appear to be conflicting, and there is no simple solution to say that one principle is more important than another. The principles may be tailored to a specific design or situation. Striking a functional balance among the principles is critical for an effective design.\\n\\n\\n==== Perceptual principles ====\\n1.\\tMake displays legible (or audible). A display\\'s legibility is critical and necessary for designing a usable display. If the characters or objects being displayed cannot be discernible, the operator cannot effectively use them.\\n2.\\tAvoid absolute judgment limits. Do not ask the user to determine the level of a variable based on a single sensory variable (e.g., color, size, loudness). These sensory variables can contain many possible levels.\\n3.\\tTop-down processing. Signals are likely perceived and interpreted by what is expected based on a user\\'s experience. If a signal is presented contrary to the user\\'s expectation, more physical evidence of that signal may need to be presented to assure that it is understood correctly.\\n4.\\tRedundancy gain. If a signal is presented more than once, it is more likely to be understood correctly. This can be done by presenting the signal in alternative physical forms (e.g., color and shape, voice and print, etc.), as redundancy does not imply repetition. A traffic light is a good example of redundancy, as color and position are redundant.\\n5.\\tSimilarity causes confusion: Use distinguishable elements. Signals that appear to be similar will likely be confused. The ratio of similar features to different features causes signals to be similar. For example, A423B9 is more similar to A423B8 than 92 is to 93. Unnecessarily similar features should be removed, and dissimilar features should be highlighted.\\n\\n\\n==== Mental model principles ====\\n6.\\tPrinciple of pictorial realism. A display should look like the variable that it represents (e.g., the high temperature on a thermometer shown as a higher vertical level). If there are multiple elements, they can be configured in a manner that looks like they would in the represented environment.\\n7.\\tPrinciple of the moving part. Moving elements should move in a pattern and direction compatible with the user\\'s mental model of how it actually moves in the system. For example, the moving element on an altimeter should move upward with increasing altitude.\\n\\n\\n==== Principles based on attention ====\\n8.\\tMinimizing information access cost or interaction cost. When the user\\'s attention is diverted from one location to another to access necessary information, there is an associated cost in time or effort. A display design should minimize this cost by allowing frequently accessed sources to be located at the nearest possible position. However, adequate legibility should not be sacrificed to reduce this cost.\\n9.\\tProximity compatibility principle. Divided attention between two information sources may be necessary for the completion of one task. These sources must be mentally integrated and are defined to have close mental proximity. Information access costs should be low, which can be achieved in many ways (e.g., proximity, linkage by common colors, patterns, shapes, etc.). However, close display proximity can be harmful by causing too much clutter.\\n10.\\tPrinciple of multiple resources. A user can more easily process information across different resources. For example, visual and auditory information can be presented simultaneously rather than presenting all visual or all auditory information.\\n\\n\\n==== Memory principles ====\\n11.\\tReplace memory with visual information: knowledge in the world. A user should not need to retain important information solely in working memory or retrieve it from long-term memory. A menu, checklist, or another display can aid the user by easing the use of their memory. However, memory use may sometimes benefit the user by eliminating the need to reference some knowledge globally (e.g., an expert computer operator would rather use direct commands from memory than refer to a manual). The use of knowledge in a user\\'s head and knowledge in the world must be balanced for an effective design.\\n12.\\tPrinciple of predictive aiding. Proactive actions are usually more effective than reactive actions. A display should eliminate resource-demanding cognitive tasks and replace them with simpler perceptual tasks to reduce the user\\'s mental resources. This will allow the user to focus on current conditions and to consider possible future conditions. An example of a predictive aid is a road sign displaying the distance to a certain destination.\\n13.\\tPrinciple of consistency. Old habits from other displays will easily transfer to support the processing of new displays if they are designed consistently. A user\\'s long-term memory will trigger actions that are expected to be appropriate. A design must accept this fact and utilize consistency among different displays.\\n\\n\\n== Current research ==\\nTopics in human-computer interaction include the following:\\n\\n\\n=== Social computing ===\\n\\nSocial computing is an interactive and collaborative behavior considered between technology and people. In recent years, there has been an explosion of social science research focusing on interactions as the unit of analysis, as there are a lot of social computing technologies that include blogs, emails,  social networking, quick messaging, and various others. Much of this research draws from psychology, social psychology, and sociology. For example, one study found out that people expected a computer with a man\\'s name to cost more than a machine with a woman\\'s name. Other research finds that individuals perceive their interactions with computers more negatively than humans, despite behaving the same way towards these machines.\\n\\n\\n=== Knowledge-driven human–computer interaction ===\\nIn human and computer interactions, a semantic gap usually exists between human and computer\\'s understandings towards mutual behaviors. Ontology, as a formal representation of domain-specific knowledge, can be used to address this problem by solving the semantic ambiguities between the two parties.\\n\\n\\n=== Emotions and human-computer interaction ===\\n\\nIn the interaction of humans and computers, research has studied how computers can detect, process, and react to human emotions to develop emotionally intelligent information systems. Researchers have suggested several \\'affect-detection channels\\'. The potential of telling human emotions in an automated and digital fashion lies in improvements to the effectiveness of human-computer interaction. The influence of emotions in human-computer interaction has been studied in fields such as financial decision-making using ECG and organizational knowledge sharing using eye-tracking and face readers as affect-detection channels. In these fields, it has been shown that affect-detection channels have the potential to detect human emotions and those information systems can incorporate the data obtained from affect-detection channels to improve decision models.\\n\\n\\n=== Brain–computer interfaces ===\\n\\nA brain-computer interface (BCI), is a direct communication pathway between an enhanced or wired brain and an external device. BCI differs from neuromodulation in that it allows for bidirectional information flow. BCIs are often directed at researching, mapping, assisting, augmenting, or repairing human cognitive or sensory-motor functions.\\n\\n\\n== Factors of change ==\\nTraditionally, computer use was modeled as a human-computer dyad in which the two were connected by a narrow explicit communication channel, such as text-based terminals. Much work has been done to make the interaction between a computing system and a human more reflective of the multidimensional nature of everyday communication. Because of potential issues, human-computer interaction shifted focus beyond the interface to respond to observations as articulated by D. Engelbart: \"If ease of use were the only valid criterion, people would stick to tricycles and never try bicycles.\"How humans interact with computers continues to evolve rapidly. Human–computer interaction is affected by developments in computing. These forces include:\\n\\nDecreasing hardware costs leading to larger memory and faster systems\\nMiniaturization of hardware leading to portability\\nReduction in power requirements leading to portability\\nNew display technologies leading to the packaging of computational devices in new forms\\nSpecialized hardware leading to new functions\\nIncreased development of network communication and distributed computing\\nIncreasingly widespread use of computers, especially by people who are outside of the computing profession\\nIncreasing innovation in input techniques (e.g., voice, gesture, pen), combined with lowering cost, leading to rapid computerization by people formerly left out of the computer revolution.\\nWider social concerns leading to improved access to computers by currently disadvantaged groupsAs of 2010 the future for HCI is expected to include the following characteristics:\\n\\nUbiquitous computing and communication. Computers are expected to communicate through high-speed local networks, nationally over wide-area networks, and portably via infrared, ultrasonic, cellular, and other technologies. Data and computational services will be portably accessible from many if not most locations to which a user travels.\\nhigh-functionality systems. Systems can have large numbers of functions associated with them. There are so many systems that most users, technical or non-technical, do not have time to learn about traditionally (e.g., through thick user manuals).\\nThe mass availability of computer graphics. Computer graphics capabilities such as image processing, graphics transformations, rendering, and interactive animation become widespread as inexpensive chips become available for inclusion in general workstations and mobile devices.\\nMixed media. Commercial systems can handle images, voice, sounds, video, text, formatted data. These are exchangeable over communication links among users. The separate consumer electronics fields (e.g., stereo sets, DVD players, televisions) and computers are beginning to merge. Computer and print fields are expected to cross-assimilate.\\nHigh-bandwidth interaction. The rate at which humans and machines interact is expected to increase substantially due to the changes in speed, computer graphics, new media, and new input/output devices. This can lead to qualitatively different interfaces, such as virtual reality or computational video.\\nLarge and thin displays. New display technologies are maturing, enabling huge displays and displays that are thin, lightweight, and low in power use. This has large effects on portability and will likely enable developing paper-like, pen-based computer interaction systems very different in feel from present desktop workstations.\\nInformation utilities. Public information utilities (such as home banking and shopping) and specialized industry services (e.g., weather for pilots) are expected to proliferate. The proliferation rate can accelerate with the introduction of high-bandwidth interaction and the improvement in the quality of interfaces.\\n\\n\\n== Scientific conferences ==\\nOne of the main conferences for new research in human-computer interaction is the annually held Association for Computing Machinery\\'s (ACM) Conference on Human Factors in Computing Systems, usually referred to by its short name CHI (pronounced kai, or Khai). CHI is organized by ACM Special Interest Group on Computer-Human Interaction (SIGCHI). CHI is a large conference, with thousands of attendants, and is quite broad in scope. It is attended by academics, practitioners, and industry people, with company sponsors such as Google, Microsoft, and PayPal.\\nThere are also dozens of other smaller, regional, or specialized HCI-related conferences held around the world each year, including:\\n\\n\\n== See also ==\\n Human–computer interaction portal\\nOutline of human–computer interaction\\nInformation design\\nInformation architecture\\nUser experience design\\nMindfulness and technology\\nCAPTCHA\\nTuring test\\nHCI Bibliography, a web-based project to provide a bibliography of Human Computer Interaction literature\\n\\n\\n== Footnotes ==\\n\\n\\n== Further reading ==\\nAcademic overviews of the fieldJulie A. Jacko (Ed.). (2012). Human-Computer Interaction Handbook (3rd Edition). CRC Press. ISBN 1-4398-2943-8\\nAndrew Sears and Julie A. Jacko (Eds.). (2007). Human-Computer Interaction Handbook (2nd Edition). CRC Press. ISBN 0-8058-5870-9\\nJulie A. Jacko and Andrew Sears (Eds.). (2003). Human-Computer Interaction Handbook. Mahwah: Lawrence Erlbaum & Associates. ISBN 0-8058-4468-6Historically important classicStuart K. Card, Thomas P. Moran, Allen Newell (1983): The Psychology of Human–Computer Interaction. Erlbaum, Hillsdale 1983 ISBN 0-89859-243-7Overviews of history of the fieldJonathan Grudin: A moving target: The evolution of human-computer interaction. In Andrew Sears and Julie A. Jacko (Eds.). (2007). Human-Computer Interaction Handbook (2nd Edition). CRC Press. ISBN 0-8058-5870-9\\nMyers, Brad (1998). \"A brief history of human–computer interaction technology\". Interactions. 5 (2): 44–54. CiteSeerX 10.1.1.23.2422. doi:10.1145/274430.274436. S2CID 8278771.\\nJohn M. Carroll: Human-Computer Interaction: History and Status. Encyclopedia Entry at Interaction-Design.org\\nCarroll, John M. (2010). \"Conceptualizing a possible discipline of human–computer interaction\". Interacting with Computers. 22 (1): 3–12. doi:10.1016/j.intcom.2009.11.008.\\nSara Candeias, S. and A. Veiga The dialogue between man and machine: the role of language theory and technology, Sandra M. Aluísio & Stella E. O. Tagnin, New Language Technologies, and Linguistic Research, A Two-Way Road: cap. 11. Cambridge Scholars Publishing. (ISBN 978-1-4438-5377-4)Social science and HCINass, Clifford; Fogg, B. J.; Moon, Youngme (1996). \"Can computers be teammates?\". International Journal of Human-Computer Studies. 45 (6): 669–678. doi:10.1006/ijhc.1996.0073.\\nNass, Clifford; Moon, Youngme (2000). \"Machines and mindlessness: Social responses to computers\". Journal of Social Issues. 56 (1): 81–103. doi:10.1111/0022-4537.00153. S2CID 15851410.\\nPosard, Marek N (2014). \"Status processes in human–computer interactions: Does gender matter?\". Computers in Human Behavior. 37: 189–195. doi:10.1016/j.chb.2014.04.025.\\nPosard, Marek N.; Rinderknecht, R. Gordon (2015). \"Do people like working with computers more than human beings?\". Computers in Human Behavior. 51: 232–238. doi:10.1016/j.chb.2015.04.057.Academic journalsACM Transactions on Computer-Human Interaction\\nBehaviour & Information Technology [1]\\nInteracting with Computers\\nInternational Journal of Human–Computer Interaction\\nInternational Journal of Human–Computer Studies\\nHuman–Computer Interaction [2] [3]Collection of papersRonald M. Baecker, Jonathan Grudin, William A. S. Buxton, Saul Greenberg (Eds.) (1995): Readings in human-computer interaction. Toward the Year 2000. 2. ed. Morgan Kaufmann, San Francisco 1995 ISBN 1-55860-246-1\\nMithun Ahamed, Developing a Message Interface Architecture for Android Operating Systems, (2015). [4]Treatments by one or few authors, often aimed at a more general audienceJakob Nielsen: Usability Engineering. Academic Press, Boston 1993 ISBN 0-12-518405-0\\nDonald A. Norman: The Psychology of Everyday Things. Basic Books, New York 1988 ISBN 0-465-06709-3\\nJef Raskin: The Humane Interface. New directions for designing interactive systems. Addison-Wesley, Boston 2000 ISBN 0-201-37937-6\\nBruce Tognazzini: Tog on Interface. Addison-Wesley, Reading 1991 ISBN 0-201-60842-1TextbooksAlan Dix, Janet Finlay, Gregory Abowd, and Russell Beale (2003): Human–Computer Interaction. 3rd Edition. Prentice Hall, 2003. http://hcibook.com/e3/ ISBN 0-13-046109-1\\nYvonne Rogers, Helen Sharp & Jenny Preece: Interaction Design: Beyond Human-Computer Interaction, 3rd ed. John Wiley & Sons Ltd., 2011 ISBN 0-470-66576-9\\nHelen Sharp, Yvonne Rogers & Jenny Preece: Interaction Design: Beyond Human-Computer Interaction, 2nd ed. John Wiley & Sons Ltd., 2007 ISBN 0-470-01866-6\\nMatt Jones (interaction designer) and Gary Marsden (2006). Mobile Interaction Design, John Wiley and Sons Ltd.\\n\\n\\n== External links ==\\nBad Human Factors Designs\\nThe HCI Wiki Bibliography with over 100,000 publications.\\nThe HCI Bibliography Over 100,000 publications about HCI.\\nHuman-Centered Computing Education Digital Library\\nHCI Webliography', 'Reverse engineering (also known as backwards engineering or back engineering) is a process or method through the application of which one attempts to understand through deductive reasoning how a device, process, system, or piece of software accomplishes a task with very little (if any) insight into exactly how it does so.\\nReverse engineering is applicable in the fields of computer engineering, mechanical engineering, design, electronic engineering, software engineering, chemical engineering, and systems biology.\\n\\n\\n== Overview ==\\nThere are many reasons for performing reverse engineering in various fields. Reverse engineering has its origins in the analysis of hardware for commercial or military advantage.:\\u200a13\\u200a However, the reverse engineering process, as such, is not concerned with creating a copy or changing the artifact in some way. It is only an analysis to deduce design features from products with little or no additional knowledge about the procedures involved in their original production.:\\u200a15\\u200aIn some cases, the goal of the reverse engineering process can simply be a redocumentation of legacy systems.:\\u200a15\\u200a Even when the reverse-engineered product is that of a competitor, the goal may not be to copy it but to perform competitor analysis. Reverse engineering may also be used to create interoperable products and despite some narrowly-tailored United States and European Union legislation, the legality of using specific reverse engineering techniques for that purpose has been hotly contested in courts worldwide for more than two decades.Software reverse engineering can help to improve the understanding of the underlying source code for the maintenance and improvement of the software, relevant information can be extracted to make a decision for software development and graphical representations of the code can provide alternate views regarding the source code, which can help to detect and fix a software bug or vulnerability. Frequently, as some software develops, its design information and improvements are often lost over time, but that lost information can usually be recovered with reverse engineering. The process can also help to cut down the time required to understand the source code, thus reducing the overall cost of the software development. Reverse engineering can also help to detect and to eliminate a malicious code written to the software with better code detectors. Reversing a source code can be used to find alternate uses of the source code, such as detecting the unauthorized replication of the source code where it was not intended to be used, or revealing how a competitor\\'s product was built. That process is commonly used for \"cracking\" software and media to remove their copy protection,:\\u200a7\\u200a or to create a possibly-improved copy or even a knockoff, which is usually the goal of a competitor or a hacker.:\\u200a8\\u200aMalware developers often use reverse engineering techniques to find vulnerabilities in an operating system to build a computer virus that can exploit the system vulnerabilities.:\\u200a5\\u200a Reverse engineering is also being used in cryptanalysis to find vulnerabilities in substitution cipher, symmetric-key algorithm or public-key cryptography.:\\u200a6\\u200aThere are other uses to reverse engineering:\\n\\nInterfacing. Reverse engineering can be used when a system is required to interface to another system and how both systems would negotiate is to be established. Such requirements typically exist for interoperability.\\nMilitary or commercial espionage. Learning about an enemy\\'s or competitor\\'s latest research by stealing or capturing a prototype and dismantling it may result in the development of a similar product or a better countermeasure against it.\\nObsolescence. Integrated circuits are often designed on proprietary systems and built on production lines, which become obsolete in only a few years. When systems using those parts can no longer be maintained since the parts are no longer made, the only way to incorporate the functionality into new technology is to reverse-engineer the existing chip and then to redesign it using newer tools by using the understanding gained as a guide. Another obsolescence originated problem that can be solved by reverse engineering is the need to support (maintenance and supply for continuous operation) existing legacy devices that are no longer supported by their original equipment manufacturer. The problem is particularly critical in military operations.\\nProduct security analysis. That examines how a product works by determining the specifications of its components and estimate costs and identifies potential patent infringement. Also part of product security analysis is acquiring sensitive data by disassembling and analyzing the design of a system component. Another intent may be to remove copy protection or to circumvent access restrictions.\\nCompetitive technical intelligence. That is to understand what one\\'s competitor is actually doing, rather than what it says that it is doing.\\nSaving money. Finding out what a piece of electronics can do may spare a user from purchasing a separate product.\\nRepurposing. Obsolete objects are then reused in a different-but-useful manner.\\nDesign. Production and design companies applied Reverse Engineering to practical craft-based manufacturing process. The companies can work on “historical” manufacturing collections through 3D scanning, 3D re-modeling and re-design. In 2013 Italian manufactures Baldi and Savio Firmino together with University of Florence optimized their innovation, design, and production processes.\\n\\n\\n== Common situations ==\\n\\n\\n=== Machines ===\\nAs computer-aided design (CAD) has become more popular, reverse engineering has become a viable method to create a 3D virtual model of an existing physical part for use in 3D CAD, CAM, CAE, or other software. The reverse-engineering process involves measuring an object and then reconstructing it as a 3D model. The physical object can be measured using 3D scanning technologies like CMMs, laser scanners, structured light digitizers, or industrial CT scanning (computed tomography). The measured data alone, usually represented as a point cloud, lacks topological information and design intent. The former may be recovered by converting the point cloud to a triangular-faced mesh. Reverse engineering aims to go beyond producing such a mesh and to recover the design intent in terms of simple analytical surfaces where appropriate (planes, cylinders, etc.) as well as possibly NURBS surfaces to produce a boundary-representation CAD model. Recovery of such a model allows a design to be modified to meet new requirements, a manufacturing plan to be generated, etc.\\nHybrid modeling is a commonly used term when NURBS and parametric modeling are implemented together. Using a combination of geometric and freeform surfaces can provide a powerful method of 3D modeling. Areas of freeform data can be combined with exact geometric surfaces to create a hybrid model. A typical example of this would be the reverse engineering of a cylinder head, which includes freeform cast features, such as water jackets and high-tolerance machined areas.Reverse engineering is also used by businesses to bring existing physical geometry into digital product development environments, to make a digital 3D record of their own products, or to assess competitors\\' products. It is used to analyze how a product works, what it does, what components it has; estimate costs; identify potential patent infringement; etc.\\nValue engineering, a related activity that is also used by businesses, involves deconstructing and analyzing products. However, the objective is to find opportunities for cost-cutting.\\n\\n\\n=== Software ===\\nIn 1990, the Institute of Electrical and Electronics Engineers (IEEE) defined (software) reverse engineering (SRE) as \"the process of analyzing a\\nsubject system to identify the system\\'s components and their interrelationships and to create representations of the system in another form or at a higher\\nlevel of abstraction\" in which the \"subject system\" is the end product of software development. Reverse engineering is a process of examination only, and the software system under consideration is not modified, which would otherwise be re-engineering or restructuring. Reverse engineering can be performed from any stage of the product cycle, not necessarily from the functional end product.There are two components in reverse engineering: redocumentation and design recovery. Redocumentation is the creation of new representation of the computer code so that it is easier to understand. Meanwhile, design recovery is the use of deduction or reasoning from general knowledge or personal experience of the product to understand the product\\'s functionality fully. It can also be seen as \"going backwards through the development cycle.\" In this model, the output of the implementation phase (in source code form) is reverse-engineered back to the analysis phase, in an inversion of the traditional waterfall model. Another term for this technique is program comprehension. The Working Conference on Reverse Engineering (WCRE) has been held yearly to explore and expand the techniques of reverse engineering. Computer-aided software engineering (CASE) and automated code generation have contributed greatly in the field of reverse engineering.Software anti-tamper technology like obfuscation is used to deter both reverse engineering and re-engineering of proprietary software and software-powered systems. In practice, two main types of reverse engineering emerge. In the first case, source code is already available for the software, but higher-level aspects of the program, which are perhaps poorly documented or documented but no longer valid, are discovered. In the second case, there is no source code available for the software, and any efforts towards discovering one possible source code for the software are regarded as reverse engineering. The second usage of the term is more familiar to most people. Reverse engineering of software can make use of the clean room design technique to avoid copyright infringement.\\nOn a related note, black box testing in software engineering has a lot in common with reverse engineering. The tester usually has the API but has the goals to find bugs and undocumented features by bashing the product from outside.Other purposes of reverse engineering include security auditing, removal of copy protection (\"cracking\"), circumvention of access restrictions often present in consumer electronics, customization of embedded systems (such as engine management systems), in-house repairs or retrofits, enabling of additional features on low-cost \"crippled\" hardware (such as some graphics card chip-sets), or even mere satisfaction of curiosity.\\n\\n\\n==== Binary software ====\\nBinary reverse engineering is performed if source code for a software is unavailable. This process is sometimes termed reverse code engineering, or RCE. For example, decompilation of binaries for the Java platform can be accomplished by using Jad. One famous case of reverse engineering was the first non-IBM implementation of the PC BIOS, which launched the historic IBM PC compatible industry that has been the overwhelmingly-dominant computer hardware platform for many years. Reverse engineering of software is protected in the US by the fair use exception in copyright law. The Samba software, which allows systems that do not run Microsoft Windows systems to share files with systems that run it, is a classic example of software reverse engineering since the Samba project had to reverse-engineer unpublished information about how Windows file sharing worked so that non-Windows computers could emulate it. The Wine project does the same thing for the Windows API, and OpenOffice.org is one party doing that for the Microsoft Office file formats. The ReactOS project is even more ambitious in its goals by striving to provide binary (ABI and API) compatibility with the current Windows operating systems of the NT branch, which allows software and drivers written for Windows to run on a clean-room reverse-engineered free software (GPL) counterpart. WindowsSCOPE allows for reverse-engineering the full contents of a Windows system\\'s live memory including a binary-level, graphical reverse engineering of all running processes.\\nAnother classic, if not well-known, example is that in 1987 Bell Laboratories reverse-engineered the Mac OS System 4.1, originally running on the Apple Macintosh SE, so that it could run it on RISC machines of their own.\\n\\n\\n===== Binary software techniques =====\\nReverse engineering of software can be accomplished by various methods.\\nThe three main groups of software reverse engineering are\\n\\nAnalysis through observation of information exchange, most prevalent in protocol reverse engineering, which involves using bus analyzers and packet sniffers, such as for accessing a computer bus or computer network connection and revealing the traffic data thereon. Bus or network behavior can then be analyzed to produce a standalone implementation that mimics that behavior. That is especially useful for reverse engineering device drivers. Sometimes, reverse engineering on embedded systems is greatly assisted by tools deliberately introduced by the manufacturer, such as JTAG ports or other debugging means. In Microsoft Windows, low-level debuggers such as SoftICE are popular.\\nDisassembly using a disassembler, meaning the raw machine language of the program is read and understood in its own terms, only with the aid of machine-language mnemonics. It works on any computer program but can take quite some time, especially for those who are not used to machine code. The Interactive Disassembler is a particularly popular tool.\\nDecompilation using a decompiler, a process that tries, with varying results, to recreate the source code in some high-level language for a program only available in machine code or bytecode.\\n\\n\\n==== Software classification ====\\nSoftware classification is the process of identifying similarities between different software binaries (such as two different versions of the same binary) used to detect code relations between software samples. The task was traditionally done manually for several reasons (such as patch analysis for vulnerability detection and copyright infringement), but it can now be done somewhat automatically for large numbers of samples.\\nThis method is being used mostly for long and thorough reverse engineering tasks (complete analysis of a complex algorithm or big piece of software). In general, statistical classification is considered to be a hard problem, which is also true for software classification, and so few solutions/tools that handle this task well.\\n\\n\\n=== Source code ===\\nA number of UML tools refer to the process of importing and analysing source code to generate UML diagrams as \"reverse engineering.\" See List of UML tools.\\nAlthough UML is one approach in providing \"reverse engineering\" more recent advances in international standards activities have resulted in the development of the Knowledge Discovery Metamodel (KDM). The standard delivers an ontology for the intermediate (or abstracted) representation of programming language constructs and their interrelationships. An Object Management Group standard (on its way to becoming an ISO standard as well), KDM has started to take hold in industry with the development of tools and analysis environments that can deliver the extraction and analysis of source, binary, and byte code. For source code analysis, KDM\\'s granular standards\\' architecture enables the extraction of software system flows (data, control, and call maps), architectures, and business layer knowledge (rules, terms, and process). The standard enables the use of a common data format (XMI) enabling the correlation of the various layers of system knowledge for either detailed analysis (such as root cause, impact) or derived analysis (such as business process extraction). Although efforts to represent language constructs can be never-ending because of the number of languages, the continuous evolution of software languages, and the development of new languages, the standard does allow for the use of extensions to support the broad language set as well as evolution. KDM is compatible with UML, BPMN, RDF, and other standards enabling migration into other environments and thus leverage system knowledge for efforts such as software system transformation and enterprise business layer analysis.\\n\\n\\n=== Protocols ===\\nProtocols are sets of rules that describe message formats and how messages are exchanged: the protocol state machine. Accordingly, the problem of protocol reverse-engineering can be partitioned into two subproblems: message format and state-machine reverse-engineering.\\nThe message formats have traditionally been reverse-engineered by a tedious manual process, which involved analysis of how protocol implementations process messages, but recent research proposed a number of automatic solutions. Typically, the automatic approaches group observe messages into clusters by using various clustering analyses, or they emulate the protocol implementation tracing the message processing.\\nThere has been less work on reverse-engineering of state-machines of protocols. In general, the protocol state-machines can be learned either through a process of offline learning, which passively observes communication and attempts to build the most general state-machine accepting all observed sequences of messages, and online learning, which allows interactive generation of probing sequences of messages and listening to responses to those probing sequences. In general, offline learning of small state-machines is known to be NP-complete, but online learning can be done in polynomial time. An automatic offline approach has been demonstrated by Comparetti et al. and an online approach by Cho et al.Other components of typical protocols, like encryption and hash functions, can be reverse-engineered automatically as well. Typically, the automatic approaches trace the execution of protocol implementations and try to detect buffers in memory holding unencrypted packets.\\n\\n\\n=== Integrated circuits/smart cards ===\\nReverse engineering is an invasive and destructive form of analyzing a smart card. The attacker uses chemicals to etch away layer after layer of the smart card and takes pictures with a scanning electron microscope (SEM). That technique can reveal the complete hardware and software part of the smart card. The major problem for the attacker is to bring everything into the right order to find out how everything works. The makers of the card try to hide keys and operations by mixing up memory positions, such as by bus scrambling.In some cases, it is even possible to attach a probe to measure voltages while the smart card is still operational. The makers of the card employ sensors to detect and prevent that attack. That attack is not very common because it requires both a large investment in effort and special equipment that is generally available only to large chip manufacturers. Furthermore, the payoff from this attack is low since other security techniques are often used such as shadow accounts. It is still uncertain whether attacks against chip-and-PIN cards to replicate encryption data and then to crack PINs would provide a cost-effective attack on multifactor authentication.\\nFull reverse engineering proceeds in several major steps.\\nThe first step after images have been taken with a SEM is stitching the images together, which is necessary because each layer cannot be captured by a single shot. A SEM needs to sweep across the area of the circuit and take several hundred images to cover the entire layer. Image stitching takes as input several hundred pictures and outputs a single properly-overlapped picture of the complete layer.\\nNext, the stitched layers need to be aligned because the sample, after etching, cannot be put into the exact same position relative to the SEM each time. Therefore, the stitched versions will not overlap in the correct fashion, as on the real circuit. Usually, three corresponding points are selected, and a transformation applied on the basis of that.\\nTo extract the circuit structure, the aligned, stitched images need to be segmented, which highlights the important circuitry and separates it from the uninteresting background and insulating materials.\\nFinally, the wires can be traced from one layer to the next, and the netlist of the circuit, which contains all of the circuit\\'s information, can be reconstructed.\\n\\n\\n=== Military applications ===\\nReverse engineering is often used by people to copy other nations\\' technologies, devices, or information that have been obtained by regular troops in the fields or by intelligence operations. It was often used during the Second World War and the Cold War. Here are well-known examples from the Second World War and later:\\n\\nJerry can: British and American forces in WW2 noticed that the Germans had gasoline cans with an excellent design. They reverse-engineered copies of those cans, which cans were popularly known as \"Jerry cans.\"\\nPanzerschreck: The Germans captured an American bazooka during the Second World War and reverse engineered it to create the larger Panzerschreck.\\nTupolev Tu-4: In 1944, three American B-29 bombers on missions over Japan were forced to land in the Soviet Union. The Soviets, who did not have a similar strategic bomber, decided to copy the B-29. Within three years, they had developed the Tu-4, a nearly-perfect copy.\\nSCR-584 radar: copied by the Soviet Union after the Second World War, it is known for a few modifications - СЦР-584, Бинокль-Д.\\nV-2 rocket: Technical documents for the V-2 and related technologies were captured by the Western Allies at the end of the war. The Americans focused their reverse engineering efforts via Operation Paperclip, which led to the development of the PGM-11 Redstone rocket. The Soviets used captured German engineers to reproduce technical documents and plans and worked from captured hardware to make their clone of the rocket, the R-1. Thus began the postwar Soviet rocket program, which led to the R-7 and the beginning of the space race.\\nK-13/R-3S missile (NATO reporting name AA-2 Atoll), a Soviet reverse-engineered copy of the AIM-9 Sidewinder, was made possible after a Taiwanese AIM-9B hit a Chinese MiG-17 without exploding in September 1958. The missile became lodged within the airframe, and the pilot returned to base with what Soviet scientists would describe as a university course in missile development.\\nBGM-71 TOW missile: In May 1975, negotiations between Iran and Hughes Missile Systems on co-production of the TOW and Maverick missiles stalled over disagreements in the pricing structure, the subsequent 1979 revolution ending all plans for such co-production. Iran was later successful in reverse-engineering the missile and now produces its own copy, the Toophan.\\nChina has reversed engineered many examples of Western and Russian hardware, from fighter aircraft to missiles and HMMWV cars, such as the MiG-15 (which became the J-7) and the Su-33 (which became the J-15). More recent analyses of China\\'s military growth have pointed to the inherent limitations of reverse engineering for advanced weapon systems.\\nDuring the Second World War, Polish and British cryptographers studied captured German \"\"Enigma\" message encryption machines for weaknesses. Their operation was then simulated on electromechanical devices, \"bombes, which tried all the possible scrambler settings of the \"Enigma\" machines that helped the breaking of coded messages that had been sent by the Germans.\\nAlso during the Second World War, British scientists analyzed and defeated a series of increasingly-sophisticated radio navigation systems used by the Luftwaffe to perform guided bombing missions at night. The British countermeasures to the system were so effective that in some cases, German aircraft were led by signals to land at RAF bases since they believed that they had returned to German territory.\\n\\n\\n=== Gene networks ===\\nReverse engineering concepts have been applied to biology as well, specifically to the task of understanding the structure and function of gene regulatory networks. They regulate almost every aspect of biological behavior and allow cells to carry out physiological processes and responses to perturbations. Understanding the structure and the dynamic behavior of gene networks is therefore one of the paramount challenges of systems biology, with immediate practical repercussions in several applications that are beyond basic research.\\nThere are several methods for reverse engineering gene regulatory networks by using molecular biology and data science methods. They have been generally divided into six classes:\\n\\nCoexpression methods are based on the notion that if two genes exhibit a similar expression profile, they may be related although no causation can be simply inferred from coexpression.\\nSequence motif methods analyze gene promoters to find specific transcription factor binding domains. If a transcription factor is predicted to bind a promoter of a specific gene, a regulatory connection can be hypothesized.\\nChromatin ImmunoPrecipitation (ChIP) methods investigate the genome-wide profile of DNA binding of chosen transcription factors to infer their downstream gene networks.\\nOrthology methods transfer gene network knowledge from one species to another.\\nLiterature methods implement text mining and manual research to identify putative or experimentally-proven gene network connections.\\nTranscriptional complexes methods leverage information on protein-protein interactions between transcription factors, thus extending the concept of gene networks to include transcriptional regulatory complexes.Often, gene network reliability is tested by genetic perturbation experiments followed by dynamic modelling, based on the principle that removing one network node has predictable effects on the functioning of the remaining nodes of the network.\\nApplications of the reverse engineering of gene networks range from understanding mechanisms of plant physiology to the highlighting of new targets for anticancer therapy.\\n\\n\\n=== Overlap with patent law ===\\nReverse engineering applies primarily to gaining understanding of a process or artifact in which the manner of its construction, use, or internal processes has not been made clear by its creator.\\nPatented items do not of themselves have to be reverse-engineered to be studied, for the essence of a patent is that inventors provide a detailed public disclosure themselves, and in return receive legal protection of the invention that is involved. However, an item produced under one or more patents could also include other technology that is not patented and not disclosed. Indeed, one common motivation of reverse engineering is to determine whether a competitor\\'s product contains patent infringement or copyright infringement.\\n\\n\\n== Legality ==\\n\\n\\n=== United States ===\\nIn the United States, even if an artifact or process is protected by trade secrets, reverse-engineering the artifact or process is often lawful if it has been legitimately obtained.Reverse engineering of computer software often falls under both contract law as a breach of contract as well as any other relevant laws. That is because most end user license agreements specifically prohibit it, and US courts have ruled that if such terms are present, they override the copyright law that expressly permits it (see Bowers v. Baystate Technologies). According to Section 103(f) of the Digital Millennium Copyright Act (17 U.S.C. § 1201 (f)), a person in legal possession of a program may reverse-engineer and circumvent its protection if that is necessary to achieve \"interoperability,\" a term that broadly covers other devices and programs that can interact with it, make use of it, and to use and transfer data to and from it in useful ways. A limited exemption exists that allows the knowledge thus gained to be shared and used for interoperability purposes.\\n\\n\\n=== European Union ===\\nEU Directive 2009/24 on the legal protection of computer programs, which superseded an earlier (1991) directive, governs reverse engineering in the European Union.\\n\\n\\n== See also ==\\n\\n\\n== References ==\\n\\n\\n== Sources ==', 'Computability theory, also known as recursion theory, is a branch of mathematical logic, computer science, and the theory of computation that originated in the 1930s with the study of computable functions and Turing degrees. The field has since expanded to include the study of generalized computability and definability. In these areas, computability theory overlaps with proof theory and effective descriptive set theory.\\nBasic questions addressed by computability theory include:\\n\\nWhat does it mean for a function on the natural numbers to be computable?\\nHow can noncomputable functions be classified into a hierarchy based on their level of noncomputability?Although there is considerable overlap in terms of knowledge and methods, mathematical computability theorists study the theory of relative computability, reducibility notions, and degree structures; those in the computer science field focus on the theory of subrecursive hierarchies, formal methods, and formal languages.\\n\\n\\n== Computable and uncomputable sets ==\\nComputability theory originated in the 1930s, with work of Kurt Gödel, Alonzo Church, Rózsa Péter, Alan Turing, Stephen Kleene, and Emil Post.The fundamental results the researchers obtained established Turing computability as the correct formalization of the informal idea of effective calculation. These results led Stephen Kleene (1952) to coin the two names \"Church\\'s thesis\" (Kleene 1952:300) and \"Turing\\'s Thesis\" (Kleene 1952:376). Nowadays these are often considered as a single hypothesis, the Church–Turing thesis, which states that any function that is computable by an algorithm is a computable function. Although initially skeptical, by 1946 Gödel argued in favor of this thesis:\\n\\n\"Tarski has stressed in his lecture (and I think justly) the great importance of the concept of general recursiveness (or Turing\\'s computability). It seems to me that this importance is largely due to the fact that with this concept one has for the first time succeeded in giving an absolute notion to an interesting epistemological notion, i.e., one not depending on the formalism chosen.*\"(Gödel 1946 in Davis 1965:84).With a definition of effective calculation came the first proofs that there are problems in mathematics that cannot be effectively decided. Church (1936a, 1936b) and Turing (1936), inspired by techniques used by Gödel (1931) to prove his incompleteness theorems, independently demonstrated that the Entscheidungsproblem is not effectively decidable. This result showed that there is no algorithmic procedure that can correctly decide whether arbitrary mathematical propositions are true or false.\\nMany problems in mathematics have been shown to be undecidable after these initial examples were established.  In 1947, Markov and Post published independent papers showing that the word problem for semigroups cannot be effectively decided. Extending this result, Pyotr Novikov and William Boone showed independently in the 1950s that the word problem for groups is not effectively solvable: there is no effective procedure that, given a word in a finitely presented group, will decide whether the element represented by the word is the identity element of the group. In 1970, Yuri Matiyasevich proved (using results of Julia Robinson) Matiyasevich\\'s theorem, which implies that Hilbert\\'s tenth problem has no effective solution; this problem asked whether there is an effective procedure to decide whether a Diophantine equation over the integers has a solution in the integers. The list of undecidable problems gives additional examples of problems with no computable solution.\\nThe study of which mathematical constructions can be effectively performed is sometimes called recursive mathematics; the Handbook of Recursive Mathematics (Ershov et al. 1998) covers many of the known results in this field.\\n\\n\\n== Turing computability ==\\nThe main form of computability studied in computability theory was introduced by Turing (1936).  A set of natural numbers is said to be a computable set (also called a decidable,  recursive, or Turing computable set) if there is a Turing machine that, given a number n, halts with output 1 if n is in the set and halts with output 0 if n is not in the set.   A function f from natural numbers to natural numbers is a (Turing) computable, or recursive function if there is a Turing machine that, on input n, halts and returns output f(n). The use of Turing machines here is not necessary; there are many other models of computation that have the same computing power as Turing machines; for example the μ-recursive functions obtained from primitive recursion and the μ operator.\\nThe terminology for computable functions and sets is not completely standardized. \\nThe definition in terms of μ-recursive functions as well as a different definition of rekursiv functions by Gödel led to the traditional name recursive for sets and functions computable by a Turing machine. The word decidable stems from the German word Entscheidungsproblem which was used in the original papers of Turing and others. In contemporary use, the term \"computable function\" has various definitions: according to Cutland (1980), it is a partial recursive function (which can be undefined for some inputs), while according to Soare (1987) it is a total recursive (equivalently, general recursive) function. This article follows the second of these conventions.  Soare (1996) gives additional comments about the terminology.\\nNot every set of natural numbers is computable. The halting problem, which is the set of (descriptions of) Turing machines that halt on input 0, is a well-known example of a noncomputable set.  The existence of many noncomputable sets follows from the facts that there are only countably many Turing machines, and thus only countably many computable sets, but according to the Cantor\\'s theorem, there are uncountably many sets of natural numbers.\\nAlthough the halting problem is not computable, it is possible to simulate program execution and produce an infinite list of the programs that do halt. Thus the halting problem is an example of a computably enumerable (c.e.) set, which is a set that can be enumerated by a Turing machine (other terms for computably enumerable include recursively enumerable and semidecidable). Equivalently, a set is c.e. if and only if it is the range of some computable function.  The c.e. sets, although not decidable in general, have been studied in detail in computability theory.\\n\\n\\n== Areas of research ==\\nBeginning with the theory of computable sets and functions described above, the field of computability theory has grown to include the study of many closely related topics. These are not independent areas of research: each of these areas draws ideas and results from the others, and most computability theorists are familiar with the majority of them.\\n\\n\\n=== Relative computability and the Turing degrees ===\\n\\nComputability theory in mathematical logic has traditionally focused on relative computability, a generalization of Turing computability defined using oracle Turing machines, introduced by Turing (1939).  An oracle Turing machine is a hypothetical device which, in addition to performing the actions of a regular Turing machine, is able to ask questions of an oracle, which is a particular set of natural numbers.  The oracle machine may only ask questions of the form \"Is n in the oracle set?\". Each question will be immediately answered correctly, even if the oracle set is not computable. Thus an oracle machine with a noncomputable oracle will be able to compute sets that a Turing machine without an oracle cannot.\\nInformally, a set of natural numbers A is Turing reducible to a set B if there is an oracle machine that correctly tells whether numbers are in A when run with B as the oracle set (in this case, the set A is also said to be (relatively) computable from B and recursive in B).  If a set A is Turing reducible to a set B and B is Turing reducible to A then the sets are said to have the same Turing degree (also called degree of unsolvability).  The Turing degree of a set gives a precise measure of how uncomputable the set is.\\nThe natural examples of sets that are not computable, including many different sets that encode variants of the halting problem, have two properties in common:\\n\\nThey are computably enumerable, and\\nEach can be translated into any other via a many-one reduction. That is, given such sets A and B, there is a total computable function f such that A = {x : f(x) ∈ B}. These sets are said to be many-one equivalent (or m-equivalent).Many-one reductions are \"stronger\" than Turing reductions: if a set A is many-one reducible to a set B, then A is Turing reducible to B, but the converse does not always hold. Although the natural examples of noncomputable sets are all many-one equivalent, it is possible to construct computably enumerable sets A and B such that A is Turing reducible to B but not many-one reducible to B. It can be shown that every computably enumerable set is many-one reducible to the halting problem, and thus the halting problem is the most complicated computably enumerable set with respect to many-one reducibility and with respect to Turing reducibility. Post (1944) asked whether every computably enumerable set is either computable or Turing equivalent to the halting problem, that is, whether there is no computably enumerable set with a Turing degree intermediate between those two.\\nAs intermediate results, Post defined natural types of computably enumerable sets like the simple, hypersimple and hyperhypersimple sets. Post showed that these sets are strictly between the computable sets and the halting problem with respect to many-one reducibility. Post also showed that some of them are strictly intermediate under other reducibility notions stronger than Turing reducibility.  But Post left open the main problem of the existence of computably enumerable sets of intermediate Turing degree; this problem became known as Post\\'s problem. After ten years, Kleene and Post showed in 1954 that there are intermediate Turing degrees between those of the computable sets and the halting problem, but they failed to show that any of these degrees contains a computably enumerable set. Very soon after this, Friedberg and Muchnik independently solved Post\\'s problem by establishing the existence of computably enumerable sets of intermediate degree. This groundbreaking result opened a wide study of the Turing degrees of the computably enumerable sets which turned out to possess a very complicated and non-trivial structure.\\nThere are uncountably many sets that are not computably enumerable, and the investigation of the Turing degrees of all sets is as central in computability theory as the investigation of the computably enumerable Turing degrees. Many degrees with special properties were constructed: hyperimmune-free degrees where every function computable relative to that degree is majorized by a (unrelativized) computable function; high degrees relative to which one can compute a function f which dominates every computable function g in the sense that there is a constant c depending on g such that g(x) < f(x) for all x > c; random degrees containing algorithmically random sets; 1-generic degrees of 1-generic sets; and the degrees below the halting problem of limit-computable sets.\\nThe study of arbitrary (not necessarily computably enumerable) Turing degrees involves the study of the Turing jump.  Given a set A, the Turing jump of A is a set of natural numbers encoding a solution to the halting problem for oracle Turing machines running with oracle A.  The Turing jump of any set is always of higher Turing degree than the original set, and a theorem of Friedburg shows that any set that computes the Halting problem can be obtained as the Turing jump of another set. Post\\'s theorem establishes a close relationship between the Turing jump operation and the arithmetical hierarchy, which is a classification of certain subsets of the natural numbers based on their definability in arithmetic.\\nMuch recent research on Turing degrees has focused on the overall structure of the set of Turing degrees and the set of Turing degrees containing computably enumerable sets.  A deep theorem of Shore and Slaman (1999) states that the function mapping a degree x to the degree of its Turing jump is definable in the partial order of the Turing degrees.  A recent survey by Ambos-Spies and Fejer (2006) gives an overview of this research and its historical progression.\\n\\n\\n=== Other reducibilities ===\\n\\nAn ongoing area of research in computability theory studies reducibility relations other than Turing reducibility. Post (1944) introduced several strong reducibilities, so named because they imply truth-table reducibility. A Turing machine implementing a strong reducibility will compute a total function regardless of which oracle it is presented with.  Weak reducibilities are those where a reduction process may not terminate for all oracles; Turing reducibility is one example.\\nThe strong reducibilities include:\\n\\nOne-one reducibility\\nA is one-one reducible (or 1-reducible) to B if there is a total computable injective function f such that each n is in A if and only if f(n) is in B.\\nMany-one reducibility\\nThis is essentially one-one reducibility without the constraint that f be injective.  A is many-one reducible (or m-reducible) to B if there is a total computable function f such that each n is in A if and only if f(n) is in B.\\nTruth-table reducibility\\nA is truth-table reducible to B if A is Turing reducible to B via an oracle Turing machine that computes a total function regardless of the oracle it is given.  Because of compactness of Cantor space, this is equivalent to saying that the reduction presents a single list of questions (depending only on the input) to the oracle simultaneously, and then having seen their answers is able to produce an output without asking additional questions regardless of the oracle\\'s answer to the initial queries. Many variants of truth-table reducibility have also been studied.Further reducibilities (positive, disjunctive, conjunctive, linear and their weak and bounded versions) are discussed in the article Reduction (computability theory).\\nThe major research on strong reducibilities has been to compare their theories, both for the class of all computably enumerable sets as well as for the class of all subsets of the natural numbers. Furthermore, the relations between the reducibilities has been studied. For example, it is known that every Turing degree is either a truth-table degree or is the union of infinitely many truth-table degrees.\\nReducibilities weaker than Turing reducibility (that is, reducibilities that are implied by Turing reducibility) have also been studied.  The most well known are arithmetical reducibility and hyperarithmetical reducibility. These reducibilities are closely connected to definability over the standard model of arithmetic.\\n\\n\\n=== Rice\\'s theorem and the arithmetical hierarchy ===\\nRice showed that for every nontrivial class C (which contains some but not all c.e. sets) the index set E = {e: the eth c.e. set We is in C} has the property that either the halting problem or its complement is many-one reducible to E, that is, can be mapped using a many-one reduction to E (see Rice\\'s theorem for more detail). But, many of these index sets are even more complicated than the halting problem. These type of sets can be classified using the arithmetical hierarchy. For example, the index set FIN of the class of all finite sets is on the level Σ2, the index set REC of the class of all recursive sets is on the level Σ3, the index set COFIN of all cofinite sets is also on the level Σ3 and the index set COMP of the class of all Turing-complete sets Σ4. These hierarchy levels are defined inductively, Σn+1 contains just all sets which are computably enumerable relative to Σn; Σ1 contains the computably enumerable sets. The index sets given here are even complete for their levels, that is, all the sets in these levels can be many-one reduced to the given index sets.\\n\\n\\n=== Reverse mathematics ===\\n\\nThe program of reverse mathematics asks which set-existence axioms are necessary to prove particular theorems of mathematics in subsystems of second-order arithmetic.  This study was initiated by Harvey Friedman and was studied in detail by Stephen Simpson and others; Simpson (1999) gives a detailed discussion of the program. The set-existence axioms in question correspond informally to axioms saying that the powerset of the natural numbers is closed under various reducibility notions. The weakest such axiom studied in reverse mathematics is recursive comprehension, which states that the powerset of the naturals is closed under Turing reducibility.\\n\\n\\n=== Numberings ===\\nA numbering is an enumeration of functions; it has two parameters, e and x and outputs the value of the e-th function in the numbering on the input x. Numberings can be partial-computable although some of its members are total computable functions. Admissible numberings are those into which all others can be translated. A Friedberg numbering (named after its discoverer) is a one-one numbering of all partial-computable functions; it is necessarily not an admissible numbering. Later research dealt also with numberings of other classes like classes of computably enumerable sets. Goncharov discovered for example a class of computably enumerable sets for which the numberings fall into exactly two classes with respect to computable isomorphisms.\\n\\n\\n=== The priority method ===\\n\\nPost\\'s problem was solved with a method called the priority method; a proof using this method is called a priority argument.  This method is primarily used to construct computably enumerable sets with particular properties. To use this method, the desired properties of the set to be constructed are broken up into an infinite list of goals, known as requirements, so that satisfying all the requirements will cause the set constructed to have the desired properties.   Each requirement is assigned to a natural number representing the priority of the requirement; so 0 is assigned to the most important priority, 1 to the second most important, and so on.  The set is then constructed in stages, each stage attempting to satisfy one of more of the requirements by either adding numbers to the set or banning numbers from the set so that the final set will satisfy the requirement. It may happen that satisfying one requirement will cause another to become unsatisfied; the priority order is used to decide what to do in such an event.\\nPriority arguments have been employed to solve many problems in computability theory, and have been classified into a hierarchy based on their complexity (Soare 1987). Because complex priority arguments can be technical and difficult to follow, it has \\ntraditionally been considered desirable to prove results without priority arguments, or to see if results proved with priority arguments can also be proved without them. \\nFor example, Kummer published a paper on a proof for the existence of Friedberg numberings without using the priority method.\\n\\n\\n=== The lattice of computably enumerable sets ===\\nWhen Post defined the notion of a simple set as an c.e. set with an infinite complement not containing any infinite c.e. set, he started to study the structure of the computably enumerable sets under inclusion. This lattice became a well-studied structure. Computable sets can be defined in this structure by the basic result that a set is computable if and only if the set and its complement are both computably enumerable. Infinite c.e. sets have always infinite computable subsets; but on the other hand, simple sets exist but do not always have a coinfinite computable superset. Post (1944) introduced already hypersimple and hyperhypersimple sets; later maximal sets were constructed which are c.e. sets such that every c.e. superset is either a finite variant of the given maximal set or is co-finite. Post\\'s original motivation in the study of this lattice was to find a structural notion such that every set which satisfies this property is neither in the Turing degree of the computable sets nor in the Turing degree of the halting problem. Post did not find such a property and the solution to his problem applied priority methods instead; Harrington and Soare (1991) found eventually such a property.\\n\\n\\n=== Automorphism problems ===\\nAnother important question is the existence of automorphisms in computability-theoretic structures. One of these structures is that one of computably enumerable sets under inclusion modulo finite difference; in this structure, A is below B if and only if the set difference B − A is finite. Maximal sets (as defined in the previous paragraph) have the property that they cannot be automorphic to non-maximal sets, that is, if there is an automorphism of the computably enumerable sets under the structure just mentioned, then every maximal set is mapped to another maximal set. Soare (1974) showed that also the converse holds, that is, every two maximal sets are automorphic. So the maximal sets form an orbit, that is, every automorphism preserves maximality and any two maximal sets are transformed into each other by some automorphism. Harrington gave a further example of an automorphic property: that of the creative sets, the sets which are many-one equivalent to the halting problem.\\nBesides the lattice of computably enumerable sets, automorphisms are also studied for the structure of the Turing degrees of all sets as well as for the structure of the Turing degrees of c.e. sets. In both cases, Cooper claims to have constructed nontrivial automorphisms which map some degrees to other degrees; this construction has, however, not been verified and some colleagues believe that the construction contains errors and that the question of whether there is a nontrivial automorphism of the Turing degrees is still one of the main unsolved questions in this area (Slaman and Woodin 1986, Ambos-Spies and Fejer 2006).\\n\\n\\n=== Kolmogorov complexity ===\\n\\nThe field of Kolmogorov complexity and algorithmic randomness was developed during the 1960s and 1970s by Chaitin, Kolmogorov, Levin, Martin-Löf and Solomonoff (the names are given here in alphabetical order; much of the research was independent, and the unity of the concept of randomness was not understood at the time). The main idea is to consider a universal Turing machine U and to measure the complexity of a number (or string) x as the length of the shortest input p such that U(p) outputs x. This approach revolutionized earlier ways to determine when an infinite sequence (equivalently, characteristic function of a subset of the natural numbers) is random or not by invoking a notion of randomness for finite objects. Kolmogorov complexity became not only a subject of independent study but is also applied to other subjects as a tool for obtaining proofs.\\nThere are still many open problems in this area. For that reason, a recent research conference in this area was held in January 2007 and a list of open problems is maintained by Joseph Miller and Andre Nies.\\n\\n\\n=== Frequency computation ===\\nThis branch of computability theory analyzed the following question: For fixed m and n with 0 < m < n, for which functions A is it possible to compute for any different n inputs x1, x2, ..., xn a tuple of n numbers y1,y2,...,yn such that at least m of the equations A(xk) = yk are true. Such sets are known as (m, n)-recursive sets. The first major result in this branch of computability theory is Trakhtenbrot\\'s result that a set is computable if it is (m, n)-recursive for some m, n with 2m > n. On the other hand, Jockusch\\'s semirecursive sets (which were already known informally before Jockusch introduced them 1968) are examples of a set which is (m, n)-recursive if and only if 2m < n + 1. There are uncountably many of these sets and also some computably enumerable but noncomputable sets of this type. Later, Degtev established a hierarchy of computably enumerable sets that are (1, n + 1)-recursive but not (1, n)-recursive. After a long phase of research by Russian scientists, this subject became repopularized in the west by Beigel\\'s thesis on bounded queries, which linked frequency computation to the above-mentioned bounded reducibilities and other related notions. One of the major results was Kummer\\'s Cardinality Theory which states that a set A is computable if and only if there is an n such that some algorithm enumerates for each tuple of n different numbers up to n many possible choices of the cardinality of this set of n numbers intersected with A; these choices must contain the true cardinality but leave out at least one false one.\\n\\n\\n=== Inductive inference ===\\nThis is the computability-theoretic branch of learning theory. It is based on E. Mark Gold\\'s model of learning in the limit from 1967 and has developed since then more and more models of learning. The general scenario is the following: Given a class S of computable functions, is there a learner (that is, computable functional) which outputs for any input of the form (f(0),f(1),...,f(n)) a hypothesis. A learner M learns a function f if almost all hypotheses are the same index e of f with respect to a previously agreed on acceptable numbering of all computable functions; M learns S if M learns every f in S. Basic results are that all computably enumerable classes of functions are learnable while the class REC of all computable functions is not learnable. Many related models have been considered and also the learning of classes of computably enumerable sets from positive data is a topic studied from Gold\\'s pioneering paper in 1967 onwards.\\n\\n\\n=== Generalizations of Turing computability ===\\ncomputability theory includes the study of generalized notions of this field such as arithmetic reducibility, hyperarithmetical reducibility and α-recursion theory, as described by Sacks (1990).  These generalized notions include reducibilities that cannot be executed by Turing machines but are nevertheless natural generalizations of Turing reducibility. These studies include approaches to investigate the analytical hierarchy which differs from the arithmetical hierarchy by permitting quantification over sets of natural numbers in addition to quantification over individual numbers. These areas are linked to the theories of well-orderings and trees; for example the set of all indices of computable (nonbinary) trees without infinite branches is complete for level \\n  \\n    \\n      \\n        \\n          Π\\n          \\n            1\\n          \\n          \\n            1\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle \\\\Pi _{1}^{1}}\\n   of the analytical hierarchy. Both Turing reducibility and hyperarithmetical reducibility are important in the field of effective descriptive set theory.  The even more general notion of degrees of constructibility is studied in set theory.\\n\\n\\n=== Continuous computability theory ===\\nComputability theory for digital computation is well developed. Computability theory is less well developed for analog computation that occurs in analog computers, analog signal processing, analog electronics, neural networks and continuous-time control theory, modelled by differential equations and continuous dynamical systems (Orponen 1997; Moore 1996).\\n\\n\\n== Relationships between definability, proof and computability ==\\nThere are close relationships between the Turing degree of a set of natural numbers and the difficulty (in terms of the arithmetical hierarchy) of defining that set using a first-order formula. One such relationship is made precise by Post\\'s theorem. A weaker relationship was demonstrated by Kurt Gödel in the proofs of his completeness theorem and incompleteness theorems. Gödel\\'s proofs show that the set of logical consequences of an effective first-order theory is a computably enumerable set, and that if the theory is strong enough this set will be uncomputable.  Similarly, Tarski\\'s indefinability theorem can be interpreted both in terms of definability and in terms of computability.\\nComputability theory is also linked to second-order arithmetic, a formal theory of natural numbers and sets of natural numbers.  The fact that certain sets are computable or relatively computable often implies that these sets can be defined in weak subsystems of second-order arithmetic.  The program of reverse mathematics uses these subsystems to measure the noncomputability inherent in well known mathematical theorems. Simpson (1999) discusses many aspects of second-order arithmetic and reverse mathematics.\\nThe field of proof theory includes the study of second-order arithmetic and Peano arithmetic, as well as formal theories of the natural numbers weaker than Peano arithmetic.  One method of classifying the strength of these weak systems is by characterizing which computable functions the system can prove to be total (see Fairtlough and Wainer (1998)).  For example, in primitive recursive arithmetic any computable function that is provably total is actually primitive recursive, while Peano arithmetic proves that functions like the Ackermann function, which are not primitive recursive, are total. Not every total computable function is provably total in Peano arithmetic, however; an example of such a function is provided by Goodstein\\'s theorem.\\n\\n\\n== Name ==\\nThe field of mathematical logic dealing with computability and its generalizations has been called \"recursion theory\" since its early days. Robert I. Soare, a prominent researcher in the field, has proposed (Soare 1996) that the field should be called \"computability theory\" instead. He argues that Turing\\'s terminology using the word \"computable\" is more natural and more widely understood than the terminology using the word \"recursive\" introduced by Kleene. Many contemporary researchers have begun to use this alternate terminology. These researchers also use terminology such as partial computable function and computably enumerable (c.e.) set instead of partial recursive function and recursively enumerable (r.e.) set. Not all researchers have been convinced, however, as explained by Fortnow and Simpson.\\nSome commentators argue that both the names recursion theory and computability theory fail to convey the fact that most of the objects studied in computability theory are not computable.Rogers (1967) has suggested that a key property of computability theory is that its results and structures should be invariant under computable bijections on the natural numbers (this suggestion draws on the ideas of the Erlangen program in geometry). The idea is that a computable bijection merely renames numbers in a set, rather than indicating any structure in the set, much as a rotation of the Euclidean plane does not change any geometric aspect of lines drawn on it. Since any two infinite computable sets are linked by a computable bijection, this proposal identifies all the infinite computable sets (the finite computable sets are viewed as trivial). According to Rogers, the sets of interest in computability theory are the noncomputable sets, partitioned into equivalence classes by computable bijections of the natural numbers.\\n\\n\\n== Professional organizations ==\\nThe main professional organization for computability theory is the Association for Symbolic Logic, which holds several research conferences each year. The interdisciplinary research Association  Computability in Europe (CiE) also organizes a series of annual conferences.\\n\\n\\n== See also ==\\n\\nRecursion (computer science)\\nComputability logic\\nTranscomputational problem\\n\\n\\n== Notes ==\\n\\n\\n== References ==\\nUndergraduate level texts\\n\\nCooper, S.B. (2004). Computability Theory. Chapman & Hall/CRC. ISBN 1-58488-237-9.\\nCutland, N. (1980). Computability, An introduction to recursive function theory. Cambridge University Press. ISBN 0-521-29465-7.\\nMatiyasevich, Y. (1993). Hilbert\\'s Tenth Problem. MIT Press. ISBN 0-262-13295-8.Advanced texts\\n\\nJain, S.; Osherson, D.; Royer, J.; Sharma, A. (1999). Systems that learn, an introduction to learning theory (2nd ed.). Bradford Book. ISBN 0-262-10077-0.\\nKleene, S. (1952). Introduction to Metamathematics. North-Holland. ISBN 0-7204-2103-9.\\nLerman, M. (1983). Degrees of unsolvability. Perspectives in Mathematical Logic. Springer-Verlag. ISBN 3-540-12155-2.\\nNies, Andre (2009). Computability and Randomness. Oxford University Press. ISBN 978-0-19-923076-1.\\nOdifreddi, P. (1989). Classical Recursion Theory. North-Holland. ISBN 0-444-87295-7.\\nOdifreddi, P. (1999). Classical Recursion Theory. II. Elsevier. ISBN 0-444-50205-X.\\nRogers, Jr., H. (1987). The Theory of Recursive Functions and Effective Computability (2nd ed.). MIT Press. ISBN 0-262-68052-1.\\nSacks, G. (1990). Higher Recursion Theory. Springer-Verlag. ISBN 3-540-19305-7.\\nSimpson, S.G. (1999). Subsystems of Second Order Arithmetic. Springer-Verlag. ISBN 3-540-64882-8.\\nSoare, R.I. (1987). Recursively Enumerable Sets and Degrees. Perspectives in Mathematical Logic. Springer-Verlag. ISBN 0-387-15299-7.Survey papers and collections\\n\\nAmbos-Spies, K.; Fejer, P. (2006). \"Degrees of Unsolvability\" (PDF). Archived from the original (PDF) on 2013-04-20. Retrieved 2006-10-27. Unpublished preprint.\\nEnderton, H. (1977). \"Elements of Recursion Theory\".  In Barwise, J. (ed.). Handbook of Mathematical Logic. North-Holland. pp. 527–566. ISBN 0-7204-2285-X.\\nErshov, Y.L.; Goncharov, S.S.; Nerode, A.; Remmel, J.B. (1998). Handbook of Recursive Mathematics. North-Holland. ISBN 0-7204-2285-X.\\nFairtlough, M.; Wainer, S.S. (1998). \"Hierarchies of Provably Recursive Functions\".  In Buss, S.R. (ed.). Handbook of Proof Theory. Elsevier. pp. 149–208. ISBN 978-0-08-053318-6.\\nSoare, R.I. (1996). \"Computability and recursion\" (PDF). Bulletin of Symbolic Logic. 2 (3): 284–321. doi:10.2307/420992. JSTOR 420992.Research papers and collections\\n\\nBurgin, M.; Klinger, A. (2004). \"Experience, Generations, and Limits in Machine Learning\". Theoretical Computer Science. 317 (1–3): 71–91. doi:10.1016/j.tcs.2003.12.005.\\nChurch, A. (1936). \"An unsolvable problem of elementary number theory\". American Journal of Mathematics. 58 (2): 345–363. doi:10.2307/2371045. JSTOR 2371045. Reprinted in Davis 1965.\\nChurch, A. (1936). \"A note on the Entscheidungsproblem\". Journal of Symbolic Logic. 1 (1): 40–41. doi:10.2307/2269326. JSTOR 2269326.  Reprinted in Davis 1965.\\nDavis, Martin, ed. (2004) [1965]. The Undecidable: Basic Papers on Undecidable Propositions, Unsolvable Problems and Computable Functions. Courier. ISBN 978-0-486-43228-1.\\nFriedberg, R.M. (1958). \"Three theorems on recursive enumeration: I. Decomposition, II. Maximal Set, III. Enumeration without repetition\". The Journal of Symbolic Logic. 23 (3): 309–316. doi:10.2307/2964290. JSTOR 2964290.\\nGold, E. Mark (1967). \"Language Identification in the Limit\" (PDF). Information and Control. 10 (5): 447–474. doi:10.1016/s0019-9958(67)91165-5. [1]\\nHarrington, L.; Soare, R.I. (1991). \"Post\\'s Program and incomplete recursively enumerable sets\". Proc. Natl. Acad. Sci. U.S.A. 88 (22): 10242–6. Bibcode:1991PNAS...8810242H. doi:10.1073/pnas.88.22.10242. PMC 52904. PMID 11607241.\\nJockusch jr, C.G. (1968). \"Semirecursive sets and positive reducibility\". Trans. Amer. Math. Soc. 137 (2): 420–436. doi:10.1090/S0002-9947-1968-0220595-7. JSTOR 1994957.\\nKleene, S.C.; Post, E.L. (1954). \"The upper semi-lattice of degrees of recursive unsolvability\". Annals of Mathematics. Second. 59 (3): 379–407. doi:10.2307/1969708. JSTOR 1969708.\\nMoore, C. (1996). \"Recursion theory on the reals and continuous-time computation\". Theoretical Computer Science. 162 (1): 23–44. CiteSeerX 10.1.1.6.5519. doi:10.1016/0304-3975(95)00248-0.\\nMyhill, J. (1956). \"The lattice of recursively enumerable sets\". The Journal of Symbolic Logic. 21: 215–220. doi:10.1017/S002248120008525X.\\nOrponen, P. (1997). \"A survey of continuous-time computation theory\". Advances in Algorithms, Languages, and Complexity: 209–224. CiteSeerX 10.1.1.53.1991. doi:10.1007/978-1-4613-3394-4_11. ISBN 978-1-4613-3396-8.\\nPost, E. (1944). \"Recursively enumerable sets of positive integers and their decision problems\". Bulletin of the American Mathematical Society. 50 (5): 284–316. doi:10.1090/S0002-9904-1944-08111-1. MR 0010514.\\nPost, E. (1947). \"Recursive unsolvability of a problem of Thue\". Journal of Symbolic Logic. 12 (1): 1–11. doi:10.2307/2267170. JSTOR 2267170. Reprinted in Davis 1965.\\nShore, Richard A.; Slaman, Theodore A. (1999). \"Defining the Turing jump\" (PDF). Mathematical Research Letters. 6 (6): 711–722. doi:10.4310/mrl.1999.v6.n6.a10. MR 1739227.\\nSlaman, T.; Woodin, W.H. (1986). \"Definability in the Turing degrees\". Illinois J. Math. 30 (2): 320–334. doi:10.1215/ijm/1256044641. MR 0840131.\\nSoare, R.I. (1974). \"Automorphisms of the lattice of recursively enumerable sets, Part I: Maximal sets\". Annals of Mathematics. 100 (1): 80–120. doi:10.2307/1970842. JSTOR 1970842.\\nTuring, A. (1937). \"On computable numbers, with an application to the Entscheidungsproblem\". Proceedings of the London Mathematical Society. s2-42 (1): 230–265. doi:10.1112/plms/s2-42.1.230. Turing, A.M. (1938). \"On Computable Numbers, with an Application to the Entscheidungsproblem. A Correction\". Proceedings of the London Mathematical Society. s2-43 (1): 544–6. doi:10.1112/plms/s2-43.6.544. Reprinted in Davis 1965. PDF from comlab.ox.ac.uk\\nTuring, A.M. (1939). \"Systems of logic based on ordinals\". Proceedings of the London Mathematical Society. s2-45 (1): 161–228. doi:10.1112/plms/s2-45.1.161. hdl:21.11116/0000-0001-91CE-3. Reprinted in Davis 1965.\\n\\n\\n== External links ==\\nAssociation for Symbolic Logic homepage\\nComputability in Europe homepage\\nWebpage on Recursion Theory Course at Graduate Level with approximately 100 pages of lecture notes\\nGerman language lecture notes on inductive inference', \"This article is a list of notable unsolved problems in computer science. A problem in computer science is considered unsolved when no solution is known, or when experts in the field disagree about proposed solutions.\\n\\n\\n== Computational complexity ==\\n\\nP versus NP problem\\nWhat is the relationship between BQP and NP?\\nNC = P problem\\nNP = co-NP problem\\nP = BPP problem\\nP = PSPACE problem\\nL = NL problem\\nPH = PSPACE problem\\nL = P problem\\nL = RL problem\\nUnique games conjecture\\nIs the exponential time hypothesis true?\\nIs the strong exponential time hypothesis (SETH) true?\\nDo one-way functions exist?\\nIs public-key cryptography possible?\\nLog-rank conjecture\\n\\n\\n== Polynomial versus non-polynomial time for specific algorithmic problems ==\\n\\nCan integer factorization be done in polynomial time on a classical (non-quantum) computer?\\nCan the discrete logarithm be computed in polynomial time on a classical (non-quantum) computer?\\nCan the shortest vector of a lattice be computed in polynomial time on a classical or quantum computer?\\nCan clustered planar drawings be found in polynomial time?\\nCan the graph isomorphism problem be solved in polynomial time?\\nCan leaf powers and k-leaf powers be recognized in polynomial time?\\nCan parity games be solved in polynomial time?\\nCan the rotation distance between two binary trees be computed in polynomial time?\\nCan graphs of bounded clique-width be recognized in polynomial time?\\nCan one find a simple closed quasigeodesic on a convex polyhedron in polynomial time?\\nCan a simultaneous embedding with fixed edges for two given graphs be found in polynomial time?\\n\\n\\n== Other algorithmic problems ==\\nThe dynamic optimality conjecture: do splay trees have a bounded competitive ratio?\\nIs there a k-competitive online algorithm for the k-server problem?\\nCan a depth-first search tree be constructed in NC?\\nCan the fast Fourier transform be computed in o(n log n) time?\\nWhat is the fastest algorithm for multiplication of two n-digit numbers?\\nWhat is the lowest possible average-case time complexity of Shellsort with a deterministic, fixed gap sequence?\\nCan 3SUM be solved in strongly sub-quadratic time, that is, in time O(n2−ϵ) for some ϵ>0?\\nCan the edit distance between two strings of length n be computed in strongly sub-quadratic time?  (This is only possible if the strong exponential time hypothesis is false.)\\nCan X + Y sorting be done in o(n2 log n) time?\\nWhat is the fastest algorithm for matrix multiplication?\\nCan all-pairs shortest paths be computed in strongly sub-cubic time, that is, in time O(V3−ϵ) for some ϵ>0?\\nCan the Schwartz–Zippel lemma for polynomial identity testing be derandomized?\\nDoes linear programming admit a strongly polynomial-time algorithm?  (This is problem #9 in Smale's list of problems.)\\nHow many queries are required for envy-free cake-cutting?\\nWhat is the algorithm for the lookup table that consistently generates playable mazes in the 1982 Atari 2600 game Entombed merely from the values of the five pixels adjacent to the next ones to be generated?\\nWhat is the algorithmic complexity of the minimum spanning tree problem? Equivalently, what is the decision tree complexity of the MST problem? The optimal algorithm to compute MSTs is known, but it relies on decision trees, so its complexity is unknown.\\n\\n\\n== Natural language processing algorithms ==\\n\\nIs there any perfect syllabification algorithm in the English language?\\nIs there any perfect stemming algorithm in the English language?\\nIs there any perfect phrase chunking algorithm in the English language?\\nHow can computers discern pronoun ambiguity in the English Language? (Also known as the Winograd Schema Challenge).\\n\\n\\n== Programming language theory ==\\n\\nPOPLmark\\nBarendregt–Geuvers–Klop conjecture\\n\\n\\n== Other problems ==\\nAanderaa–Karp–Rosenberg conjecture\\nČerný Conjecture\\nGeneralized star height problem\\nSeparating words problem\\n\\n\\n== References ==\\n\\n\\n== External links ==\\nOpen problems around exact algorithms by Gerhard J. Woeginger, Discrete Applied Mathematics 156 (2008) 397–405.\\nThe RTA list of open problems – open problems in rewriting.\\nThe TLCA List of Open Problems – open problems in area typed lambda calculus.\", 'Computational complexity theory focuses on classifying computational problems according to their resource usage, and relating these classes to each other. A computational problem is a task solved by a computer. A computation problem is solvable by mechanical application of mathematical steps, such as an algorithm.\\nA problem is regarded as inherently difficult if its solution requires significant resources, whatever the algorithm used. The theory formalizes this intuition, by introducing mathematical models of computation to study these problems and quantifying their computational complexity, i.e., the amount of resources needed to solve them, such as time and storage. Other measures of complexity are also used, such as the amount of communication (used in communication complexity), the number of gates in a circuit (used in circuit complexity) and the number of processors (used in parallel computing). One of the roles of computational complexity theory is to determine the practical limits on what computers can and cannot do. The P versus NP problem, one of the seven Millennium Prize Problems, is dedicated to the field of computational complexity.Closely related fields in theoretical computer science are analysis of algorithms and computability theory. A key distinction between analysis of algorithms and computational complexity theory is that the former is devoted to analyzing the amount of resources needed by a particular algorithm to solve a problem, whereas the latter asks a more general question about all possible algorithms that could be used to solve the same problem. More precisely, computational complexity theory tries to classify problems that can or cannot be solved with appropriately restricted resources. In turn, imposing restrictions on the available resources is what distinguishes computational complexity from computability theory: the latter theory asks what kinds of problems can, in principle, be solved algorithmically.\\n\\n\\n== Computational problems ==\\n\\n\\n=== Problem instances ===\\nA computational problem can be viewed as an infinite collection of instances together with a set (possibly empty) of solutions for every instance. The input string for a computational problem is referred to as a problem instance, and should not be confused with the problem itself. In computational complexity theory, a problem refers to the abstract question to be solved. In contrast, an instance of this problem is a rather concrete utterance, which can serve as the input for a decision problem. For example, consider the problem of primality testing. The instance is a number (e.g., 15) and the solution is \"yes\" if the number is prime and \"no\" otherwise (in this case, 15 is not prime and the answer is \"no\"). Stated another way, the instance is a particular input to the problem, and the solution is the output corresponding to the given input.\\nTo further highlight the difference between a problem and an instance, consider the following instance of the decision version of the traveling salesman problem: Is there a route of at most 2000 kilometres passing through all of Germany\\'s 15 largest cities? The quantitative answer to this particular problem instance is of little use for solving other instances of the problem, such as asking for a round trip through all sites in Milan whose total length is at most 10 km. For this reason, complexity theory addresses computational problems and not particular problem instances.\\n\\n\\n=== Representing problem instances ===\\nWhen considering computational problems, a problem instance is a string over an alphabet. Usually, the alphabet is taken to be the binary alphabet (i.e., the set {0,1}), and thus the strings are bitstrings. As in a real-world computer, mathematical objects other than bitstrings must be suitably encoded. For example, integers can be represented in binary notation, and graphs can be encoded directly via their adjacency matrices, or by encoding their adjacency lists in binary.\\nEven though some proofs of complexity-theoretic theorems regularly assume some concrete choice of input encoding, one tries to keep the discussion abstract enough to be independent of the choice of encoding. This can be achieved by ensuring that different representations can be transformed into each other efficiently.\\n\\n\\n=== Decision problems as formal languages ===\\n\\nDecision problems are one of the central objects of study in computational complexity theory. A decision problem is a special type of computational problem whose answer is either yes or no, or alternately either 1 or 0. A decision problem can be viewed as a formal language, where the members of the language are instances whose output is yes, and the non-members are those instances whose output is no. The objective is to decide, with the aid of an algorithm, whether a given input string is a member of the formal language under consideration. If the algorithm deciding this problem returns the answer yes, the algorithm is said to accept the input string, otherwise it is said to reject the input.\\nAn example of a decision problem is the following. The input is an arbitrary graph. The problem consists in deciding whether the given graph is connected or not. The formal language associated with this decision problem is then the set of all connected graphs — to obtain a precise definition of this language, one has to decide how graphs are encoded as binary strings.\\n\\n\\n=== Function problems ===\\nA function problem is a computational problem where a single output (of a total function) is expected for every input, but the output is more complex than that of a decision problem—that is, the output isn\\'t just yes or no. Notable examples include the traveling salesman problem and the integer factorization problem.\\nIt is tempting to think that the notion of function problems is much richer than the notion of decision problems. However, this is not really the case, since function problems can be recast as decision problems. For example, the multiplication of two integers can be expressed as the set of triples (a, b, c) such that the relation a × b = c holds. Deciding whether a given triple is a member of this set corresponds to solving the problem of multiplying two numbers.\\n\\n\\n=== Measuring the size of an instance ===\\nTo measure the difficulty of solving a computational problem, one may wish to see how much time the best algorithm requires to solve the problem. However, the running time may, in general, depend on the instance. In particular, larger instances will require more time to solve. Thus the time required to solve a problem (or the space required, or any measure of complexity) is calculated as a function of the size of the instance. This is usually taken to be the size of the input in bits. Complexity theory is interested in how algorithms scale with an increase in the input size. For instance, in the problem of finding whether a graph is connected, how much more time does it take to solve a problem for a graph with 2n vertices compared to the time taken for a graph with n vertices?\\nIf the input size is n, the time taken can be expressed as a function of n. Since the time taken on different inputs of the same size can be different, the worst-case time complexity T(n) is defined to be the maximum time taken over all inputs of size n. If T(n) is a polynomial in n, then the algorithm is said to be a polynomial time algorithm. Cobham\\'s thesis argues that a problem can be solved with a feasible amount of resources if it admits a polynomial time algorithm.\\n\\n\\n== Machine models and complexity measures ==\\n\\n\\n=== Turing machine ===\\n\\nA Turing machine is a mathematical model of a general computing machine. It is a theoretical device that manipulates symbols contained on a strip of tape. Turing machines are not intended as a practical computing technology, but rather as a general model of a computing machine—anything from an advanced supercomputer to a mathematician with a pencil and paper. It is believed that if a problem can be solved by an algorithm, there exists a Turing machine that solves the problem. Indeed, this is the statement of the Church–Turing thesis. Furthermore, it is known that everything that can be computed on other models of computation known to us today, such as a RAM machine, Conway\\'s Game of Life, cellular automata or any programming language can be computed on a Turing machine. Since Turing machines are easy to analyze mathematically, and are believed to be as powerful as any other model of computation, the Turing machine is the most commonly used model in complexity theory.\\nMany types of Turing machines are used to define complexity classes, such as deterministic Turing machines, probabilistic Turing machines, non-deterministic Turing machines, quantum Turing machines, symmetric Turing machines and alternating Turing machines. They are all equally powerful in principle, but when resources (such as time or space) are bounded, some of these may be more powerful than others.\\nA deterministic Turing machine is the most basic Turing machine, which uses a fixed set of rules to determine its future actions. A probabilistic Turing machine is a deterministic Turing machine with an extra supply of random bits. The ability to make probabilistic decisions often helps algorithms solve problems more efficiently. Algorithms that use random bits are called randomized algorithms. A non-deterministic Turing machine is a deterministic Turing machine with an added feature of non-determinism, which allows a Turing machine to have multiple possible future actions from a given state. One way to view non-determinism is that the Turing machine branches into many possible computational paths at each step, and if it solves the problem in any of these branches, it is said to have solved the problem. Clearly, this model is not meant to be a physically realizable model, it is just a theoretically interesting abstract machine that gives rise to particularly interesting complexity classes. For examples, see non-deterministic algorithm.\\n\\n\\n=== Other machine models ===\\nMany machine models different from the standard multi-tape Turing machines have been proposed in the literature, for example random-access machines. Perhaps surprisingly, each of these models can be converted to another without providing any extra computational power. The time and memory consumption of these alternate models may vary. What all these models have in common is that the machines operate deterministically.\\nHowever, some computational problems are easier to analyze in terms of more unusual resources. For example, a non-deterministic Turing machine is a computational model that is allowed to branch out to check many different possibilities at once. The non-deterministic Turing machine has very little to do with how we physically want to compute algorithms, but its branching exactly captures many of the mathematical models we want to analyze, so that non-deterministic time is a very important resource in analyzing computational problems.\\n\\n\\n=== Complexity measures ===\\nFor a precise definition of what it means to solve a problem using a given amount of time and space, a computational model such as the deterministic Turing machine is used. The time required by a deterministic Turing machine M on input x is the total number of state transitions, or steps, the machine makes before it halts and outputs the answer (\"yes\" or \"no\"). A Turing machine M is said to operate within time f(n) if the time required by M on each input of length n is at most f(n). A decision problem A can be solved in time f(n) if there exists a Turing machine operating in time f(n) that solves the problem. Since complexity theory is interested in classifying problems based on their difficulty, one defines sets of problems based on some criteria. For instance, the set of problems solvable within time f(n) on a deterministic Turing machine is then denoted by DTIME(f(n)).\\nAnalogous definitions can be made for space requirements. Although time and space are the most well-known complexity resources, any complexity measure can be viewed as a computational resource. Complexity measures are very generally defined by the Blum complexity axioms. Other complexity measures used in complexity theory include communication complexity, circuit complexity, and decision tree complexity.\\nThe complexity of an algorithm is often expressed using big O notation.\\n\\n\\n=== Best, worst and average case complexity ===\\n\\nThe best, worst and average case complexity refer to three different ways of measuring the time complexity (or any other complexity measure) of different inputs of the same size. Since some inputs of size n may be faster to solve than others, we define the following complexities:\\n\\nBest-case complexity: This is the complexity of solving the problem for the best input of size n.\\nAverage-case complexity: This is the complexity of solving the problem on an average. This complexity is only defined with respect to a probability distribution over the inputs. For instance, if all inputs of the same size are assumed to be equally likely to appear, the average case complexity can be defined with respect to the uniform distribution over all inputs of size n.\\nAmortized analysis: Amortized analysis considers both the costly and less costly operations together over the whole series of operations of the algorithm.\\nWorst-case complexity: This is the complexity of solving the problem for the worst input of size n.The order from cheap to costly is: Best, average (of discrete uniform distribution), amortized, worst.\\nFor example, consider the deterministic sorting algorithm quicksort. This solves the problem of sorting a list of integers that is given as the input. The worst-case is when the pivot is always the largest or smallest value in the list (so the list is never divided). In this case the algorithm takes time O(n2). If we assume that all possible permutations of the input list are equally likely, the average time taken for sorting is O(n log n). The best case occurs when each pivoting divides the list in half, also needing O(n log n) time.\\n\\n\\n=== Upper and lower bounds on the complexity of problems ===\\nTo classify the computation time (or similar resources, such as space consumption), it is helpful to demonstrate upper and lower bounds on the maximum amount of time required by the most efficient algorithm to solve a given problem. The complexity of an algorithm is usually taken to be its worst-case complexity, unless specified otherwise. Analyzing a particular algorithm falls under the field of analysis of algorithms. To show an upper bound T(n) on the time complexity of a problem, one needs to show only that there is a particular algorithm with running time at most T(n). However, proving lower bounds is much more difficult, since lower bounds make a statement about all possible algorithms that solve a given problem. The phrase \"all possible algorithms\" includes not just the algorithms known today, but any algorithm that might be discovered in the future. To show a lower bound of T(n) for a problem requires showing that no algorithm can have time complexity lower than T(n).\\nUpper and lower bounds are usually stated using the big O notation, which hides constant factors and smaller terms. This makes the bounds independent of the specific details of the computational model used. For instance, if T(n) = 7n2 + 15n + 40, in big O notation one would write T(n) = O(n2).\\n\\n\\n== Complexity classes ==\\n\\n\\n=== Defining complexity classes ===\\nA complexity class is a set of problems of related complexity. Simpler complexity classes are defined by the following factors:\\n\\nThe type of computational problem: The most commonly used problems are decision problems. However, complexity classes can be defined based on function problems, counting problems, optimization problems, promise problems, etc.\\nThe model of computation: The most common model of computation is the deterministic Turing machine, but many complexity classes are based on non-deterministic Turing machines, Boolean circuits, quantum Turing machines, monotone circuits, etc.\\nThe resource (or resources) that is being bounded and the bound: These two properties are usually stated together, such as \"polynomial time\", \"logarithmic space\", \"constant depth\", etc.Some complexity classes have complicated definitions that do not fit into this framework. Thus, a typical complexity class has a definition like the following:\\n\\nThe set of decision problems solvable by a deterministic Turing machine within time f(n). (This complexity class is known as DTIME(f(n)).)But bounding the computation time above by some concrete function f(n) often yields complexity classes that depend on the chosen machine model. For instance, the language {xx | x is any binary string} can be solved in linear time on a multi-tape Turing machine, but necessarily requires quadratic time in the model of single-tape Turing machines. If we allow polynomial variations in running time, Cobham-Edmonds thesis states that \"the time complexities in any two reasonable and general models of computation are polynomially related\" (Goldreich 2008, Chapter 1.2). This forms the basis for the complexity class P, which is the set of decision problems solvable by a deterministic Turing machine within polynomial time. The corresponding set of function problems is FP.\\n\\n\\n=== Important complexity classes ===\\n\\nMany important complexity classes can be defined by bounding the time or space used by the algorithm. Some important complexity classes of decision problems defined in this manner are the following:\\n\\nThe logarithmic-space classes (necessarily) do not take into account the space needed to represent the problem.\\nIt turns out that PSPACE = NPSPACE and EXPSPACE = NEXPSPACE by Savitch\\'s theorem.\\nOther important complexity classes include BPP, ZPP and RP, which are defined using probabilistic Turing machines; AC and NC, which are defined using Boolean circuits; and BQP and QMA, which are defined using quantum Turing machines. #P is an important complexity class of counting problems (not decision problems). Classes like IP and AM are defined using Interactive proof systems. ALL is the class of all decision problems.\\n\\n\\n=== Hierarchy theorems ===\\n\\nFor the complexity classes defined in this way, it is desirable to prove that relaxing the requirements on (say) computation time indeed defines a bigger set of problems. In particular, although DTIME(n) is contained in DTIME(n2), it would be interesting to know if the inclusion is strict. For time and space requirements, the answer to such questions is given by the time and space hierarchy theorems respectively. They are called hierarchy theorems because they induce a proper hierarchy on the classes defined by constraining the respective resources. Thus there are pairs of complexity classes such that one is properly included in the other. Having deduced such proper set inclusions, we can proceed to make quantitative statements about how much more additional time or space is needed in order to increase the number of problems that can be solved.\\nMore precisely, the time hierarchy theorem states that\\n\\n  \\n    \\n      \\n        \\n          \\n            D\\n            T\\n            I\\n            M\\n            E\\n          \\n        \\n        \\n          \\n            (\\n          \\n        \\n        f\\n        (\\n        n\\n        )\\n        \\n          \\n            )\\n          \\n        \\n        ⊊\\n        \\n          \\n            D\\n            T\\n            I\\n            M\\n            E\\n          \\n        \\n        \\n          \\n            (\\n          \\n        \\n        f\\n        (\\n        n\\n        )\\n        ⋅\\n        \\n          log\\n          \\n            2\\n          \\n        \\n        \\u2061\\n        (\\n        f\\n        (\\n        n\\n        )\\n        )\\n        \\n          \\n            )\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\mathsf {DTIME}}{\\\\big (}f(n){\\\\big )}\\\\subsetneq {\\\\mathsf {DTIME}}{\\\\big (}f(n)\\\\cdot \\\\log ^{2}(f(n)){\\\\big )}}\\n  .The space hierarchy theorem states that\\n\\n  \\n    \\n      \\n        \\n          \\n            D\\n            S\\n            P\\n            A\\n            C\\n            E\\n          \\n        \\n        \\n          \\n            (\\n          \\n        \\n        f\\n        (\\n        n\\n        )\\n        \\n          \\n            )\\n          \\n        \\n        ⊊\\n        \\n          \\n            D\\n            S\\n            P\\n            A\\n            C\\n            E\\n          \\n        \\n        \\n          \\n            (\\n          \\n        \\n        f\\n        (\\n        n\\n        )\\n        ⋅\\n        log\\n        \\u2061\\n        (\\n        f\\n        (\\n        n\\n        )\\n        )\\n        \\n          \\n            )\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\mathsf {DSPACE}}{\\\\big (}f(n){\\\\big )}\\\\subsetneq {\\\\mathsf {DSPACE}}{\\\\big (}f(n)\\\\cdot \\\\log(f(n)){\\\\big )}}\\n  .The time and space hierarchy theorems form the basis for most separation results of complexity classes. For instance, the time hierarchy theorem tells us that P is strictly contained in EXPTIME, and the space hierarchy theorem tells us that L is strictly contained in PSPACE.\\n\\n\\n=== Reduction ===\\n\\nMany complexity classes are defined using the concept of a reduction. A reduction is a transformation of one problem into another problem. It captures the informal notion of a problem being at most as difficult as another problem. For instance, if a problem X can be solved using an algorithm for Y, X is no more difficult than Y, and we say that X reduces to Y. There are many different types of reductions, based on the method of reduction, such as Cook reductions, Karp reductions and Levin reductions, and the bound on the complexity of reductions, such as polynomial-time reductions or log-space reductions.\\nThe most commonly used reduction is a polynomial-time reduction. This means that the reduction process takes polynomial time. For example, the problem of squaring an integer can be reduced to the problem of multiplying two integers. This means an algorithm for multiplying two integers can be used to square an integer. Indeed, this can be done by giving the same input to both inputs of the multiplication algorithm. Thus we see that squaring is not more difficult than multiplication, since squaring can be reduced to multiplication.\\nThis motivates the concept of a problem being hard for a complexity class. A problem X is hard for a class of problems C if every problem in C can be reduced to X. Thus no problem in C is harder than X, since an algorithm for X allows us to solve any problem in C. The notion of hard problems depends on the type of reduction being used. For complexity classes larger than P, polynomial-time reductions are commonly used. In particular, the set of problems that are hard for NP is the set of NP-hard problems.\\nIf a problem X is in C and hard for C, then X is said to be complete for C. This means that X is the hardest problem in C. (Since many problems could be equally hard, one might say that X is one of the hardest problems in C.) Thus the class of NP-complete problems contains the most difficult problems in NP, in the sense that they are the ones most likely not to be in P. Because the problem P = NP is not solved, being able to reduce a known NP-complete problem, Π2, to another problem, Π1, would indicate that there is no known polynomial-time solution for Π1. This is because a polynomial-time solution to Π1 would yield a polynomial-time solution to Π2. Similarly, because all NP problems can be reduced to the set, finding an NP-complete problem that can be solved in polynomial time would mean that P = NP.\\n\\n\\n== Important open problems ==\\n\\n\\n=== P versus NP problem ===\\n\\nThe complexity class P is often seen as a mathematical abstraction modeling those computational tasks that admit an efficient algorithm. This hypothesis is called the Cobham–Edmonds thesis. The complexity class NP, on the other hand, contains many problems that people would like to solve efficiently, but for which no efficient algorithm is known, such as the Boolean satisfiability problem, the Hamiltonian path problem and the vertex cover problem. Since deterministic Turing machines are special non-deterministic Turing machines, it is easily observed that each problem in P is also member of the class NP.\\nThe question of whether P equals NP is one of the most important open questions in theoretical computer science because of the wide implications of a solution. If the answer is yes, many important problems can be shown to have more efficient solutions. These include various types of integer programming problems in operations research, many problems in logistics, protein structure prediction in biology, and the ability to find formal proofs of pure mathematics theorems. The P versus NP problem is one of the Millennium Prize Problems proposed by the Clay Mathematics Institute. There is a US$1,000,000 prize for resolving the problem.\\n\\n\\n=== Problems in NP not known to be in P or NP-complete ===\\nIt was shown by Ladner that if P ≠ NP then there exist problems in NP that are neither in P nor NP-complete. Such problems are called NP-intermediate problems. The graph isomorphism problem, the discrete logarithm problem and the integer factorization problem are examples of problems believed to be NP-intermediate. They are some of the very few NP problems not known to be in P or to be NP-complete.\\nThe graph isomorphism problem is the computational problem of determining whether two finite graphs are isomorphic. An important unsolved problem in complexity theory is whether the graph isomorphism problem is in P, NP-complete, or NP-intermediate. The answer is not known, but it is believed that the problem is at least not NP-complete. If graph isomorphism is NP-complete, the polynomial time hierarchy collapses to its second level. Since it is widely believed that the polynomial hierarchy does not collapse to any finite level, it is believed that graph isomorphism is not NP-complete. The best algorithm for this problem, due to László Babai and Eugene Luks has run time \\n  \\n    \\n      \\n        O\\n        (\\n        \\n          2\\n          \\n            \\n              n\\n              log\\n              \\u2061\\n              n\\n            \\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle O(2^{\\\\sqrt {n\\\\log n}})}\\n   for graphs with n vertices, although some recent work by Babai offers some potentially new perspectives on this.The integer factorization problem is the computational problem of determining the prime factorization of a given integer. Phrased as a decision problem, it is the problem of deciding whether the input has a prime factor less than k. No efficient integer factorization algorithm is known, and this fact forms the basis of several modern cryptographic systems, such as the RSA algorithm. The integer factorization problem is in NP and in co-NP (and even in UP and co-UP). If the problem is NP-complete, the polynomial time hierarchy will collapse to its first level (i.e., NP will equal co-NP). The best known algorithm for integer factorization is the general number field sieve, which takes time \\n  \\n    \\n      \\n        O\\n        (\\n        \\n          e\\n          \\n            \\n              (\\n              \\n                \\n                  \\n                    64\\n                    9\\n                  \\n                  \\n                    3\\n                  \\n                \\n              \\n              )\\n            \\n            \\n              \\n                \\n                  (\\n                  log\\n                  \\u2061\\n                  n\\n                  )\\n                \\n                \\n                  3\\n                \\n              \\n            \\n            \\n              \\n                \\n                  (\\n                  log\\n                  \\u2061\\n                  log\\n                  \\u2061\\n                  n\\n                  \\n                    )\\n                    \\n                      2\\n                    \\n                  \\n                \\n                \\n                  3\\n                \\n              \\n            \\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle O(e^{\\\\left({\\\\sqrt[{3}]{\\\\frac {64}{9}}}\\\\right){\\\\sqrt[{3}]{(\\\\log n)}}{\\\\sqrt[{3}]{(\\\\log \\\\log n)^{2}}}})}\\n   to factor an odd integer n. However, the best known quantum algorithm for this problem, Shor\\'s algorithm, does run in polynomial time. Unfortunately, this fact doesn\\'t say much about where the problem lies with respect to non-quantum complexity classes.\\n\\n\\n=== Separations between other complexity classes ===\\nMany known complexity classes are suspected to be unequal, but this has not been proved. For instance P ⊆ NP ⊆ PP ⊆ PSPACE, but it is possible that P = PSPACE. If P is not equal to NP, then P is not equal to PSPACE either. Since there are many known complexity classes between P and PSPACE, such as RP, BPP, PP, BQP, MA, PH, etc., it is possible that all these complexity classes collapse to one class. Proving that any of these classes are unequal would be a major breakthrough in complexity theory.\\nAlong the same lines, co-NP is the class containing the complement problems (i.e. problems with the yes/no answers reversed) of NP problems. It is believed that NP is not equal to co-NP; however, it has not yet been proven. It is clear that if these two complexity classes are not equal then P is not equal to NP, since P=co-P.  Thus if P=NP we would have co-P=co-NP whence NP=P=co-P=co-NP. \\nSimilarly, it is not known if L (the set of all problems that can be solved in logarithmic space) is strictly contained in P or equal to P. Again, there are many complexity classes between the two, such as NL and NC, and it is not known if they are distinct or equal classes.\\nIt is suspected that P and BPP are equal. However, it is currently open if BPP = NEXP.\\n\\n\\n== Intractability ==\\n\\nA problem that can be solved in theory (e.g. given large but finite resources, especially time), but for which in practice any solution takes too many resources to be useful, is known as an intractable problem. Conversely, a problem that can be solved in practice is called a tractable problem, literally \"a problem that can be handled\". The term infeasible (literally \"cannot be done\") is sometimes used interchangeably with intractable, though this risks confusion with a feasible solution in mathematical optimization.Tractable problems are frequently identified with problems that have polynomial-time solutions (P, PTIME); this is known as the Cobham–Edmonds thesis. Problems that are known to be intractable in this sense include those that are EXPTIME-hard. If NP is not the same as P, then NP-hard problems are also intractable in this sense.\\nHowever, this identification is inexact: a polynomial-time solution with large degree or large leading coefficient grows quickly, and may be impractical for practical size problems; conversely, an exponential-time solution that grows slowly may be practical on realistic input, or a solution that takes a long time in the worst case may take a short time in most cases or the average case, and thus still be practical. Saying that a problem is not in P does not imply that all large cases of the problem are hard or even that most of them are. For example, the decision problem in Presburger arithmetic has been shown not to be in P, yet algorithms have been written that solve the problem in reasonable times in most cases. Similarly, algorithms can solve the NP-complete knapsack problem over a wide range of sizes in less than quadratic time and SAT solvers routinely handle large instances of the NP-complete Boolean satisfiability problem.\\nTo see why exponential-time algorithms are generally unusable in practice, consider a program that makes 2n operations before halting. For small n, say 100, and assuming for the sake of example that the computer does 1012 operations each second, the program would run for about 4 × 1010 years, which is the same order of magnitude as the age of the universe. Even with a much faster computer, the program would only be useful for very small instances and in that sense the intractability of a problem is somewhat independent of technological progress. However, an exponential-time algorithm that takes 1.0001n operations is practical until n gets relatively large.\\nSimilarly, a polynomial time algorithm is not always practical. If its running time is, say, n15, it is unreasonable to consider it efficient and it is still useless except on small instances. Indeed, in practice even n3 or n2 algorithms are often impractical on realistic sizes of problems.\\n\\n\\n== Continuous complexity theory ==\\nContinuous complexity theory can refer to complexity theory of problems that involve continuous functions that are approximated by discretizations, as studied in numerical analysis. One approach to complexity theory of numerical analysis is information based complexity.\\nContinuous complexity theory can also refer to complexity theory of the use of analog computation, which uses continuous dynamical systems and differential equations. Control theory can be considered a form of computation and differential equations are used in the modelling of continuous-time and hybrid discrete-continuous-time systems.\\n\\n\\n== History ==\\nAn early example of algorithm complexity analysis is the running time analysis of the Euclidean algorithm done by Gabriel Lamé in 1844.\\nBefore the actual research explicitly devoted to the complexity of algorithmic problems started off, numerous foundations were laid out by various researchers. Most influential among these was the definition of Turing machines by Alan Turing in 1936, which turned out to be a very robust and flexible simplification of a computer.\\nThe beginning of systematic studies in computational complexity is attributed to the seminal 1965 paper \"On the Computational Complexity of Algorithms\" by Juris Hartmanis and Richard E. Stearns, which laid out the definitions of time complexity and space complexity, and proved the hierarchy theorems. In addition, in 1965 Edmonds suggested to consider a \"good\" algorithm to be one with running time bounded by a polynomial of the input size.Earlier papers studying problems solvable by Turing machines with specific bounded resources include John Myhill\\'s definition of linear bounded automata (Myhill 1960), Raymond Smullyan\\'s study of rudimentary sets (1961), as well as Hisao Yamada\\'s paper on real-time computations (1962). Somewhat earlier, Boris Trakhtenbrot (1956), a pioneer in the field from the USSR, studied another specific complexity measure. As he remembers:\\n\\nHowever, [my] initial interest [in automata theory] was increasingly set aside in favor of computational complexity, an exciting fusion of combinatorial methods, inherited from switching theory, with the conceptual arsenal of the theory of algorithms. These ideas had occurred to me earlier in 1955 when I coined the term \"signalizing function\", which is nowadays commonly known as \"complexity measure\".\\nIn 1967, Manuel Blum formulated a set of axioms (now known as Blum axioms) specifying desirable properties of complexity measures on the set of computable functions and proved an important result, the so-called speed-up theorem. The field began to flourish in 1971 when the Stephen Cook and Leonid Levin proved the existence of practically relevant problems that are NP-complete. In 1972, Richard Karp took this idea a leap forward with his landmark paper, \"Reducibility Among Combinatorial Problems\", in which he showed that 21 diverse combinatorial and graph theoretical problems, each infamous for its computational intractability, are NP-complete.\\n\\n\\n== See also ==\\n\\n\\n== Works on complexity ==\\nWuppuluri, Shyam; Doria, Francisco A., eds. (2020), Unravelling Complexity: The Life and Work of Gregory Chaitin, World Scientific, doi:10.1142/11270, ISBN 978-981-12-0006-9\\n\\n\\n== References ==\\n\\n\\n=== Citations ===\\n\\n\\n=== Textbooks ===\\nArora, Sanjeev; Barak, Boaz (2009), Computational Complexity: A Modern Approach, Cambridge University Press, ISBN 978-0-521-42426-4, Zbl 1193.68112\\nDowney, Rod; Fellows, Michael (1999), Parameterized complexity, Monographs in Computer Science, Berlin, New York: Springer-Verlag, ISBN 9780387948836\\nDu, Ding-Zhu; Ko, Ker-I (2000), Theory of Computational Complexity, John Wiley & Sons, ISBN 978-0-471-34506-0\\nGarey, Michael R.; Johnson, David S. (1979), Computers and Intractability: A Guide to the Theory of NP-Completeness, W. H. Freeman, ISBN 0-7167-1045-5\\nGoldreich, Oded (2008), Computational Complexity: A Conceptual Perspective, Cambridge University Press\\nvan Leeuwen, Jan, ed. (1990), Handbook of theoretical computer science (vol. A): algorithms and complexity, MIT Press, ISBN 978-0-444-88071-0\\nPapadimitriou, Christos (1994), Computational Complexity (1st ed.), Addison Wesley, ISBN 978-0-201-53082-7\\nSipser, Michael (2006), Introduction to the Theory of Computation (2nd ed.), USA: Thomson Course Technology, ISBN 978-0-534-95097-2\\n\\n\\n=== Surveys ===\\nKhalil, Hatem; Ulery, Dana (1976), \"A Review of Current Studies on Complexity of Algorithms for Partial Differential Equations\", Proceedings of the Annual Conference on - ACM 76, ACM \\'76: 197–201, doi:10.1145/800191.805573, S2CID 15497394\\nCook, Stephen (1983), \"An overview of computational complexity\", Commun. ACM, 26 (6): 400–408, doi:10.1145/358141.358144, ISSN 0001-0782, S2CID 14323396\\nFortnow, Lance; Homer, Steven (2003), \"A Short History of Computational Complexity\" (PDF), Bulletin of the EATCS, 80: 95–133\\nMertens, Stephan (2002), \"Computational Complexity for Physicists\", Computing in Science and Eng., 4 (3): 31–47, arXiv:cond-mat/0012185, Bibcode:2002CSE.....4c..31M, doi:10.1109/5992.998639, ISSN 1521-9615, S2CID 633346\\n\\n\\n== External links ==\\nThe Complexity Zoo\\n\"Computational complexity classes\", Encyclopedia of Mathematics, EMS Press, 2001 [1994]\\nWhat are the most important results (and papers) in complexity theory that every one should know?\\nScott Aaronson: Why Philosophers Should Care About Computational Complexity', 'Quantum computing is a type of computation that harnesses the collective properties of quantum states, such as superposition, interference, and entanglement, to perform calculations. The devices that perform quantum computations are known as quantum computers.:\\u200aI-5\\u200a They are believed to be able to solve certain computational problems, such as integer factorization (which underlies RSA encryption), substantially faster than classical computers. The study of quantum computing is a subfield of quantum information science. Expansion is expected in the next few years as the field shifts toward real-world use in pharmaceutical, data security and other applications.Quantum computing began in 1980 when physicist Paul Benioff proposed a quantum mechanical model of the Turing machine. Richard Feynman and Yuri Manin later suggested that a quantum computer had the potential to simulate things a classical computer could not feasibly do.  In 1994, Peter Shor developed  a quantum algorithm for factoring integers with the potential to decrypt RSA-encrypted communications. Despite ongoing experimental progress since the late 1990s, most researchers believe that \"fault-tolerant quantum computing [is] still a rather distant dream.\" In recent years, investment in quantum computing research has increased in the public and private sectors. On 23 October 2019, Google AI, in partnership with the U.S. National Aeronautics and Space Administration (NASA), claimed to have performed a quantum computation that was infeasible on any classical computer, but whether this claim was or is still valid is a topic of active research.There are several types of quantum computers (also known as quantum computing systems), including the quantum circuit model, quantum Turing machine, adiabatic quantum computer, one-way quantum computer, and various quantum cellular automata. The most widely used model is the quantum circuit, based on the quantum bit, or \"qubit\", which is somewhat analogous to the bit in classical computation. A qubit can be in a 1 or 0 quantum state, or in a superposition of the 1 and 0 states. When it is measured, however, it is always 0 or 1; the probability of either outcome depends on the qubit\\'s quantum state immediately prior to measurement.\\nEfforts towards building a physical quantum computer focus on technologies such as transmons, ion traps and topological quantum computers, which aim to create high-quality qubits.:\\u200a2–13\\u200a These qubits may be designed differently, depending on the full quantum computer\\'s computing model, whether quantum logic gates, quantum annealing, or adiabatic quantum computation. There are currently a number of significant obstacles to constructing useful quantum computers. It is particularly difficult to maintain qubits\\' quantum states, as they suffer from quantum decoherence and state fidelity. Quantum computers therefore require error correction.Any computational problem that can be solved by a classical computer can also be solved by a quantum computer. Conversely, any problem that can be solved by a quantum computer can also be solved by a classical computer, at least in principle given enough time. In other words, quantum computers obey the Church–Turing thesis. This means that while quantum computers provide no additional advantages over classical computers in terms of computability, quantum algorithms for certain problems have significantly lower time complexities than corresponding known classical algorithms. Notably, quantum computers are believed to be able to quickly solve certain problems that no classical computer could solve in any feasible amount of time—a feat known as \"quantum supremacy.\" The study of the computational complexity of problems with respect to quantum computers is known as quantum complexity theory.\\n\\n\\n== Quantum circuit ==\\n\\n\\n=== Definition ===\\n\\nThe prevailing model of quantum computation describes the computation in terms of a network of quantum logic gates. This model can be thought of as an abstract linear-algebraic generalization of a classical circuit. Since this circuit model obeys quantum mechanics, a quantum computer capable of efficiently running these circuits is believed to be physically realizable.\\nA memory consisting of \\n  \\n    \\n      \\n        n\\n      \\n    \\n    {\\\\textstyle n}\\n   bits of information has \\n  \\n    \\n      \\n        \\n          2\\n          \\n            n\\n          \\n        \\n      \\n    \\n    {\\\\textstyle 2^{n}}\\n   possible states. A vector representing all memory states thus has \\n  \\n    \\n      \\n        \\n          2\\n          \\n            n\\n          \\n        \\n      \\n    \\n    {\\\\textstyle 2^{n}}\\n   entries (one for each state). This vector is viewed as a probability vector and represents the fact that the memory is to be found in a particular state.\\nIn the classical view, one entry would have a value of 1 (i.e. a 100% probability of being in this state) and all other entries would be zero. In quantum mechanics, probability vectors can be generalized to density operators. The quantum state vector formalism is usually introduced first because it is conceptually simpler, and because it can be used instead of the density matrix formalism for pure states, where the whole quantum system is known.\\nWe begin by considering a simple memory consisting of only one bit. This memory may be found in one of two states: the zero state or the one state. We may represent the state of this memory using Dirac notation so that\\n\\nA quantum memory may then be found in any quantum superposition \\n  \\n    \\n      \\n        \\n          |\\n        \\n        ψ\\n        ⟩\\n      \\n    \\n    {\\\\textstyle |\\\\psi \\\\rangle }\\n   of the two classical states \\n  \\n    \\n      \\n        \\n          |\\n        \\n        0\\n        ⟩\\n      \\n    \\n    {\\\\textstyle |0\\\\rangle }\\n   and \\n  \\n    \\n      \\n        \\n          |\\n        \\n        1\\n        ⟩\\n      \\n    \\n    {\\\\textstyle |1\\\\rangle }\\n  :\\n\\nIn general, the coefficients \\n  \\n    \\n      \\n        α\\n      \\n    \\n    {\\\\textstyle \\\\alpha }\\n   and \\n  \\n    \\n      \\n        β\\n      \\n    \\n    {\\\\textstyle \\\\beta }\\n   are complex numbers. In this scenario, one qubit of information is said to be encoded into the quantum memory. The state \\n  \\n    \\n      \\n        \\n          |\\n        \\n        ψ\\n        ⟩\\n      \\n    \\n    {\\\\textstyle |\\\\psi \\\\rangle }\\n   is not itself a probability vector but can be connected with a probability vector via a measurement operation. If the quantum memory is measured to determine whether the state is \\n  \\n    \\n      \\n        \\n          |\\n        \\n        0\\n        ⟩\\n      \\n    \\n    {\\\\textstyle |0\\\\rangle }\\n   or \\n  \\n    \\n      \\n        \\n          |\\n        \\n        1\\n        ⟩\\n      \\n    \\n    {\\\\textstyle |1\\\\rangle }\\n   (this is known as a computational basis measurement), the zero state would be observed with probability \\n  \\n    \\n      \\n        \\n          |\\n        \\n        α\\n        \\n          \\n            |\\n          \\n          \\n            2\\n          \\n        \\n      \\n    \\n    {\\\\textstyle |\\\\alpha |^{2}}\\n   and the one state with probability \\n  \\n    \\n      \\n        \\n          |\\n        \\n        β\\n        \\n          \\n            |\\n          \\n          \\n            2\\n          \\n        \\n      \\n    \\n    {\\\\textstyle |\\\\beta |^{2}}\\n  . The numbers \\n  \\n    \\n      \\n        α\\n      \\n    \\n    {\\\\textstyle \\\\alpha }\\n   and \\n  \\n    \\n      \\n        β\\n      \\n    \\n    {\\\\textstyle \\\\beta }\\n   are called quantum amplitudes.\\nThe state of this one-qubit quantum memory can be manipulated by applying quantum logic gates, analogous to how classical memory can be manipulated with classical logic gates. One important gate for both classical and quantum computation is the NOT gate, which can be represented by a matrix\\n\\nMathematically, the application of such a logic gate to a quantum state vector is modelled with matrix multiplication. Thus \\n  \\n    \\n      \\n        X\\n        \\n          |\\n        \\n        0\\n        ⟩\\n        =\\n        \\n          |\\n        \\n        1\\n        ⟩\\n      \\n    \\n    {\\\\textstyle X|0\\\\rangle =|1\\\\rangle }\\n   and \\n  \\n    \\n      \\n        X\\n        \\n          |\\n        \\n        1\\n        ⟩\\n        =\\n        \\n          |\\n        \\n        0\\n        ⟩\\n      \\n    \\n    {\\\\textstyle X|1\\\\rangle =|0\\\\rangle }\\n  .\\nThe mathematics of single qubit gates can be extended to operate on multi-qubit quantum memories in two important ways. One way is simply to select a qubit and apply that gate to the target qubit whilst leaving the remainder of the memory unaffected. Another way is to apply the gate to its target only if another part of the memory is in a desired state. These two choices can be illustrated using another example. The possible states of a two-qubit quantum memory are\\n\\nThe CNOT gate can then be represented using the following matrix:\\n\\nAs a mathematical consequence of this definition, \\n  \\n    \\n      \\n        CNOT\\n        \\u2061\\n        \\n          |\\n        \\n        00\\n        ⟩\\n        =\\n        \\n          |\\n        \\n        00\\n        ⟩\\n      \\n    \\n    {\\\\textstyle \\\\operatorname {CNOT} |00\\\\rangle =|00\\\\rangle }\\n  , \\n  \\n    \\n      \\n        CNOT\\n        \\u2061\\n        \\n          |\\n        \\n        01\\n        ⟩\\n        =\\n        \\n          |\\n        \\n        01\\n        ⟩\\n      \\n    \\n    {\\\\textstyle \\\\operatorname {CNOT} |01\\\\rangle =|01\\\\rangle }\\n  , \\n  \\n    \\n      \\n        CNOT\\n        \\u2061\\n        \\n          |\\n        \\n        10\\n        ⟩\\n        =\\n        \\n          |\\n        \\n        11\\n        ⟩\\n      \\n    \\n    {\\\\textstyle \\\\operatorname {CNOT} |10\\\\rangle =|11\\\\rangle }\\n  , and \\n  \\n    \\n      \\n        CNOT\\n        \\u2061\\n        \\n          |\\n        \\n        11\\n        ⟩\\n        =\\n        \\n          |\\n        \\n        10\\n        ⟩\\n      \\n    \\n    {\\\\textstyle \\\\operatorname {CNOT} |11\\\\rangle =|10\\\\rangle }\\n  . In other words, the CNOT applies a NOT gate (\\n  \\n    \\n      \\n        X\\n      \\n    \\n    {\\\\textstyle X}\\n   from before) to the second qubit if and only if the first qubit is in the state \\n  \\n    \\n      \\n        \\n          |\\n        \\n        1\\n        ⟩\\n      \\n    \\n    {\\\\textstyle |1\\\\rangle }\\n  . If the first qubit is \\n  \\n    \\n      \\n        \\n          |\\n        \\n        0\\n        ⟩\\n      \\n    \\n    {\\\\textstyle |0\\\\rangle }\\n  , nothing is done to either qubit.\\nIn summary, a quantum computation can be described as a network of quantum logic gates and measurements. However, any measurement can be deferred to the end of quantum computation, though this deferment may come at a computational cost, so most quantum circuits depict a network consisting only of quantum logic gates and no measurements.\\nAny quantum computation (which is, in the above formalism, any unitary matrix over \\n  \\n    \\n      \\n        n\\n      \\n    \\n    {\\\\displaystyle n}\\n   qubits) can be represented as a network of quantum logic gates from a fairly small family of gates. A choice of gate family that enables this construction is known as a universal gate set, since a computer that can run such circuits is a universal quantum computer. One common such set includes all single-qubit gates as well as the CNOT gate from above. This means any quantum computation can be performed by executing a sequence of single-qubit gates together with CNOT gates. Though this gate set is infinite, it can be replaced with a finite gate set by appealing to the Solovay-Kitaev theorem.\\n\\n\\n=== Quantum algorithms ===\\n\\nProgress in finding quantum algorithms typically focuses on this quantum circuit model, though exceptions like the quantum adiabatic algorithm exist. Quantum algorithms can be roughly categorized by the type of speedup achieved over corresponding classical algorithms.Quantum algorithms that offer more than a polynomial speedup over the best known classical algorithm include Shor\\'s algorithm for factoring and the related quantum algorithms for computing discrete logarithms, solving Pell\\'s equation, and more generally solving the hidden subgroup problem for abelian finite groups. These algorithms depend on the primitive of the quantum Fourier transform. No mathematical proof has been found that shows that an equally fast classical algorithm cannot be discovered, although this is considered unlikely. Certain oracle problems like Simon\\'s problem and the Bernstein–Vazirani problem do give provable speedups, though this is in the quantum query model, which is a restricted model where lower bounds are much easier to prove and doesn\\'t necessarily translate to speedups for practical problems.\\nOther problems, including the simulation of quantum physical processes from chemistry and solid-state physics, the approximation of certain Jones polynomials, and the quantum algorithm for linear systems of equations have quantum algorithms appearing to give super-polynomial speedups and are BQP-complete. Because these problems are BQP-complete, an equally fast classical algorithm for them would imply that no quantum algorithm gives a super-polynomial speedup, which is believed to be unlikely.Some quantum algorithms, like Grover\\'s algorithm and amplitude amplification, give polynomial speedups over corresponding classical algorithms. Though these algorithms give comparably modest quadratic speedup, they are widely applicable and thus give speedups for a wide range of problems. Many examples of provable quantum speedups for query problems are related to Grover\\'s algorithm, including Brassard, Høyer, and Tapp\\'s algorithm for finding collisions in two-to-one functions, which uses Grover\\'s algorithm, and Farhi, Goldstone, and Gutmann\\'s algorithm for evaluating NAND trees, which is a variant of the search problem.\\n\\n\\n== Potential applications ==\\n\\n\\n=== Cryptography ===\\n\\nA notable application of quantum computation is for attacks on cryptographic systems that are currently in use. Integer factorization, which underpins the security of public key cryptographic systems, is believed to be computationally infeasible with an ordinary computer for large integers if they are the product of few prime numbers (e.g., products of two 300-digit primes). By comparison, a quantum computer could efficiently solve this problem using Shor\\'s algorithm to find its factors. This ability would allow a quantum computer to break many of the cryptographic systems in use today, in the sense that there would be a polynomial time (in the number of digits of the integer) algorithm for solving the problem. In particular, most of the popular public key ciphers are based on the difficulty of factoring integers or the discrete logarithm problem, both of which can be solved by Shor\\'s algorithm. In particular, the RSA, Diffie–Hellman, and elliptic curve Diffie–Hellman algorithms could be broken. These are used to protect secure Web pages, encrypted email, and many other types of data. Breaking these would have significant ramifications for electronic privacy and security.\\nIdentifying cryptographic systems that may be secure against quantum algorithms is an actively researched topic under the field of post-quantum cryptography. Some public-key algorithms are based on problems other than the integer factorization and discrete logarithm problems to which Shor\\'s algorithm applies, like the McEliece cryptosystem based on a problem in coding theory. Lattice-based cryptosystems are also not known to be broken by quantum computers, and finding a polynomial time algorithm for solving the dihedral hidden subgroup problem, which would break many lattice based cryptosystems, is a well-studied open problem. It has been proven that applying Grover\\'s algorithm to break a symmetric (secret key) algorithm by brute force requires time equal to roughly 2n/2 invocations of the underlying cryptographic algorithm, compared with roughly 2n in the classical case, meaning that symmetric key lengths are effectively halved: AES-256 would have the same security against an attack using Grover\\'s algorithm that AES-128 has against classical brute-force search (see Key size).\\nQuantum cryptography could potentially fulfill some of the functions of public key cryptography. Quantum-based cryptographic systems could, therefore, be more secure than traditional systems against quantum hacking.\\n\\n\\n=== Search problems ===\\nThe most well-known example of a problem admitting a polynomial quantum speedup is unstructured search, finding a marked item out of a list of \\n  \\n    \\n      \\n        n\\n      \\n    \\n    {\\\\displaystyle n}\\n   items in a database. This can be solved by Grover\\'s algorithm using \\n  \\n    \\n      \\n        O\\n        (\\n        \\n          \\n            n\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle O({\\\\sqrt {n}})}\\n   queries to the database, quadratically fewer than the \\n  \\n    \\n      \\n        Ω\\n        (\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle \\\\Omega (n)}\\n   queries required for classical algorithms. In this case, the advantage is not only provable but also optimal: it has been shown that Grover\\'s algorithm gives the maximal possible probability of finding the desired element for any number of oracle lookups.\\nProblems that can be addressed with Grover\\'s algorithm have the following properties:\\nThere is no searchable structure in the collection of possible answers,\\nThe number of possible answers to check is the same as the number of inputs to the algorithm, and\\nThere exists a boolean function that evaluates each input and determines whether it is the correct answerFor problems with all these properties, the running time of Grover\\'s algorithm on a quantum computer scales as the square root of the number of inputs (or elements in the database), as opposed to the linear scaling of classical algorithms. A general class of problems to which Grover\\'s algorithm can be applied is Boolean satisfiability problem, where the database through which the algorithm iterates is that of all possible answers. An example and (possible) application of this is a password cracker that attempts to guess a password. Symmetric ciphers such as Triple DES and AES are particularly vulnerable to this kind of attack. This application of quantum computing is a major interest of government agencies.\\n\\n\\n=== Simulation of quantum systems ===\\n\\nSince chemistry and nanotechnology rely on understanding quantum systems, and such systems are impossible to simulate in an efficient manner classically, many believe quantum simulation will be one of the most important applications of quantum computing. Quantum simulation could also be used to simulate the behavior of atoms and particles at unusual conditions such as the reactions inside a collider.\\nQuantum simulations might be used to predict future paths of particles and protons under superposition in the double-slit experiment.\\nAbout 2% of the annual global energy output is used for nitrogen fixation to produce ammonia for the Haber process in the agricultural fertilizer industry while naturally occurring organisms also produce ammonia. Quantum simulations might be used to understand this process increasing production.\\n\\n\\n=== Quantum annealing and adiabatic optimization ===\\nQuantum annealing or Adiabatic quantum computation relies on the adiabatic theorem to undertake calculations. A system is placed in the ground state for a simple Hamiltonian, which is slowly evolved to a more complicated Hamiltonian whose ground state represents the solution to the problem in question. The adiabatic theorem states that if the evolution is slow enough the system will stay in its ground state at all times through the process.\\n\\n\\n=== Machine learning ===\\n\\nSince quantum computers can produce outputs that classical computers cannot produce efficiently, and since quantum computation is fundamentally linear algebraic, some express hope in developing quantum algorithms that can speed up machine learning tasks.\\nFor example, the quantum algorithm for linear systems of equations, or \"HHL Algorithm\", named after its discoverers Harrow, Hassidim, and Lloyd, is believed to provide speedup over classical counterparts. Some research groups have recently explored the use of quantum annealing hardware for training Boltzmann machines and deep neural networks.\\n\\n\\n=== Computational biology ===\\n\\nIn the field of computational biology, computing has played a big role in solving many biological problems. One of the well-known examples would be in computational genomics and how computing has drastically reduced the time to sequence a human genome. Given how computational biology is using generic data modeling and storage, its applications to computational biology are expected to arise as well.\\n\\n\\n=== Computer-aided drug design and generative chemistry ===\\n\\nDeep generative chemistry models emerge as powerful tools to expedite drug discovery. However, the immense size and complexity of the structural space of all possible drug-like molecules pose significant obstacles, which could be overcome in the future by quantum computers. Quantum computers are naturally good for solving complex quantum many-body problems  and thus may be instrumental in applications involving quantum chemistry.  Therefore, one can expect that quantum-enhanced generative models including quantum GANs may eventually be developed into ultimate generative chemistry algorithms. Hybrid architectures combining quantum computers with deep classical networks, such as Quantum Variational Autoencoders, can already be trained on commercially available annealers and used to generate novel drug-like molecular structures.\\n\\n\\n=== Quantum supremacy ===\\n\\nJohn Preskill has introduced the term quantum supremacy to refer to the hypothetical speedup advantage that a quantum computer would have over a classical computer in a certain field. Google announced in 2017 that it expected to achieve quantum supremacy by the end of the year though that did not happen; however, by October 2019 Google, with the help of NASA, achieved Quantum Supremacy (https://www.nasa.gov/feature/ames/quantum-supremacy). IBM said in 2018 that the best classical computers will be beaten on some practical task within about five years and views the quantum supremacy test only as a potential future benchmark. Although skeptics like Gil Kalai doubt that quantum supremacy will ever be achieved, in October 2019, a Sycamore processor created in conjunction with Google AI Quantum was reported to have achieved quantum supremacy, with calculations more than 3,000,000 times as fast as those of Summit, generally considered the world\\'s fastest computer. In December 2020, a group at USTC implemented a type of Boson sampling on 76 photons with a photonic quantum computer Jiuzhang to demonstrate quantum supremacy. The authors claim that a classical contemporary supercomputer would require a computational time of 600 million years to generate the number of samples their quantum processor can generate in 20 seconds. Bill Unruh doubted the practicality of quantum computers in a paper published back in 1994. Paul Davies argued that a 400-qubit computer would even come into conflict with the cosmological information bound implied by the holographic principle.\\n\\n\\n== Obstacles ==\\nThere are a number of technical challenges in building a large-scale quantum computer. Physicist David DiVincenzo has listed these requirements for a practical quantum computer:\\nPhysically scalable to increase the number of qubits\\nQubits that can be initialized to arbitrary values\\nQuantum gates that are faster than decoherence time\\nUniversal gate set\\nQubits that can be read easilySourcing parts for quantum computers is also very difficult. Many quantum computers, like those constructed by Google and IBM, need Helium-3, a nuclear research byproduct, and special superconducting cables made only by the Japanese company Coax Co.The control of multi-qubit systems requires the generation and coordination of a large number of electrical signals with tight and deterministic timing resolution. This has led to the development of quantum controllers which enable interfacing with the qubits. Scaling these systems to support a growing number of qubits is an additional challenge.\\n\\n\\n=== Quantum decoherence ===\\n\\nOne of the greatest challenges involved with constructing quantum computers is controlling or removing quantum decoherence. This usually means isolating the system from its environment as interactions with the external world cause the system to decohere. However, other sources of decoherence also exist. Examples include the quantum gates, and the lattice vibrations and background thermonuclear spin of the physical system used to implement the qubits. Decoherence is irreversible, as it is effectively non-unitary, and is usually something that should be highly controlled, if not avoided. Decoherence times for candidate systems in particular, the transverse relaxation time T2 (for NMR and MRI technology, also called the dephasing time), typically range between nanoseconds and seconds at low temperature. Currently, some quantum computers require their qubits to be cooled to 20 millikelvins in order to prevent significant decoherence. A 2020 study argues that ionizing radiation such as cosmic rays can nevertheless cause certain systems to decohere within milliseconds.As a result, time-consuming tasks may render some quantum algorithms inoperable, as maintaining the state of qubits for a long enough duration will eventually corrupt the superpositions.These issues are more difficult for optical approaches as the timescales are orders of magnitude shorter and an often-cited approach to overcoming them is optical pulse shaping. Error rates are typically proportional to the ratio of operating time to decoherence time, hence any operation must be completed much more quickly than the decoherence time.\\nAs described in the Quantum threshold theorem, if the error rate is small enough, it is thought to be possible to use quantum error correction to suppress errors and decoherence. This allows the total calculation time to be longer than the decoherence time if the error correction scheme can correct errors faster than decoherence introduces them. An often cited figure for the required error rate in each gate for fault-tolerant computation is 10−3, assuming the noise is depolarizing.\\nMeeting this scalability condition is possible for a wide range of systems. However, the use of error correction brings with it the cost of a greatly increased number of required qubits. The number required to factor integers using Shor\\'s algorithm is still polynomial, and thought to be between L and L2, where L is the number of digits in the number to be factored; error correction algorithms would inflate this figure by an additional factor of L. For a 1000-bit number, this implies a need for about 104 bits without error correction. With error correction, the figure would rise to about 107 bits. Computation time is about L2 or about 107 steps and at 1 MHz, about 10 seconds.\\nA very different approach to the stability-decoherence problem is to create a topological quantum computer with anyons, quasi-particles used as threads and relying on braid theory to form stable logic gates.Physicist Mikhail Dyakonov has expressed skepticism of quantum computing as follows:\\n\\n\"So the number of continuous parameters describing the state of such a useful quantum computer at any given moment must be... about 10300... Could we ever learn to control the more than 10300 continuously variable parameters defining the quantum state of such a system? My answer is simple. No, never.\"\\n\\n\\n== Developments ==\\n\\n\\n=== Quantum computing models ===\\nThere are a number of quantum computing models, distinguished by the basic elements in which the computation is decomposed. The four main models of practical importance are:\\n\\nQuantum gate array (computation decomposed into a sequence of few-qubit quantum gates)\\nOne-way quantum computer (computation decomposed into a sequence of one-qubit measurements applied to a highly entangled initial state or cluster state)\\nAdiabatic quantum computer, based on quantum annealing (computation decomposed into a slow continuous transformation of an initial Hamiltonian into a final Hamiltonian, whose ground states contain the solution)\\nTopological quantum computer (computation decomposed into the braiding of anyons in a 2D lattice)The quantum Turing machine is theoretically important but the physical implementation of this model is not feasible. All four models of computation have been shown to be equivalent; each can simulate the other with no more than polynomial overhead.\\n\\n\\n=== Physical realizations ===\\nFor physically implementing a quantum computer, many different candidates are being pursued, among them (distinguished by the physical system used to realize the qubits):\\n\\nSuperconducting quantum computing (qubit implemented by the state of small superconducting circuits [Josephson junctions])\\nTrapped ion quantum computer (qubit implemented by the internal state of trapped ions)\\nNeutral atoms in optical lattices (qubit implemented by internal states of neutral atoms trapped in an optical lattice)\\nQuantum dot computer, spin-based (e.g. the Loss-DiVincenzo quantum computer) (qubit given by the spin states of trapped electrons)\\nQuantum dot computer, spatial-based (qubit given by electron position in double quantum dot)\\nQuantum computing using engineered quantum wells, which could in principle enable the construction of quantum computers that operate at room temperature\\nCoupled quantum wire (qubit implemented by a pair of quantum wires coupled by a quantum point contact)\\nNuclear magnetic resonance quantum computer (NMRQC) implemented with the nuclear magnetic resonance of molecules in solution, where qubits are provided by nuclear spins within the dissolved molecule and probed with radio waves\\nSolid-state NMR Kane quantum computers (qubit realized by the nuclear spin state of phosphorus donors in silicon)\\nElectrons-on-helium quantum computers (qubit is the electron spin)\\nCavity quantum electrodynamics (CQED) (qubit provided by the internal state of trapped atoms coupled to high-finesse cavities)\\nMolecular magnet (qubit given by spin states)\\nFullerene-based ESR quantum computer (qubit based on the electronic spin of atoms or molecules encased in fullerenes)\\nNonlinear optical quantum computer (qubits realized by processing states of different modes of light through both linear and nonlinear elements)\\nLinear optical quantum computer (qubits realized by processing states of different modes of light through linear elements e.g. mirrors, beam splitters and phase shifters)\\nDiamond-based quantum computer (qubit realized by the electronic or nuclear spin of nitrogen-vacancy centers in diamond)\\nBose-Einstein condensate-based quantum computer\\nTransistor-based quantum computer – string quantum computers with entrainment of positive holes using an electrostatic trap\\nRare-earth-metal-ion-doped inorganic crystal based quantum computers (qubit realized by the internal electronic state of dopants in optical fibers)\\nMetallic-like carbon nanospheres-based quantum computersThe large number of candidates demonstrates that quantum computing, despite rapid progress, is still in its infancy.\\n\\n\\n== Relation to computability and complexity theory ==\\n\\n\\n=== Computability theory ===\\n\\nAny computational problem solvable by a classical computer is also solvable by a quantum computer. Intuitively, this is because it is believed that all physical phenomena, including the operation of classical computers, can be described using quantum mechanics, which underlies the operation of quantum computers.\\nConversely, any problem solvable by a quantum computer is also solvable by a classical computer; or more formally, any quantum computer can be simulated by a Turing machine. In other words, quantum computers provide no additional power over classical computers in terms of computability. This means that quantum computers cannot solve undecidable problems like the halting problem and the existence of quantum computers does not disprove the Church–Turing thesis.As of yet, quantum computers do not satisfy the strong Church thesis. While hypothetical machines have been realized, a universal quantum computer has yet to be physically constructed. The strong version of Church\\'s thesis requires a physical computer, and therefore there is no quantum computer that yet satisfies the strong Church thesis.\\n\\n\\n=== Quantum complexity theory ===\\n\\nWhile quantum computers cannot solve any problems that classical computers cannot already solve, it is suspected that they can solve certain problems faster than classical computers. For instance, it is known that quantum computers can efficiently factor integers, while this is not believed to be the case for classical computers.\\nThe class of problems that can be efficiently solved by a quantum computer with bounded error is called BQP, for \"bounded error, quantum, polynomial time\". More formally, BQP is the class of problems that can be solved by a polynomial-time quantum Turing machine with an error probability of at most 1/3. As a class of probabilistic problems, BQP is the quantum counterpart to BPP (\"bounded error, probabilistic, polynomial time\"), the class of problems that can be solved by polynomial-time probabilistic Turing machines with bounded error. It is known that BPP\\n  \\n    \\n      \\n        ⊆\\n      \\n    \\n    {\\\\displaystyle \\\\subseteq }\\n  BQP and is widely suspected that BQP\\n  \\n    \\n      \\n        ⊊\\n      \\n    \\n    {\\\\displaystyle \\\\subsetneq }\\n  BPP, which intuitively would mean that quantum computers are more powerful than classical computers in terms of time complexity.\\n\\nThe exact relationship of BQP to P, NP, and PSPACE is not known. However, it is known that P\\n  \\n    \\n      \\n        ⊆\\n      \\n    \\n    {\\\\displaystyle \\\\subseteq }\\n  BQP\\n  \\n    \\n      \\n        ⊆\\n      \\n    \\n    {\\\\displaystyle \\\\subseteq }\\n  PSPACE; that is, all problems that can be efficiently solved by a deterministic classical computer can also be efficiently solved by a quantum computer, and all problems that can be efficiently solved by a quantum computer can also be solved by a deterministic classical computer with polynomial space resources. It is further suspected that BQP is a strict superset of P, meaning there are problems that are efficiently solvable by quantum computers that are not efficiently solvable by deterministic classical computers. For instance, integer factorization and the discrete logarithm problem are known to be in BQP and are suspected to be outside of P. On the relationship of BQP to NP, little is known beyond the fact that some NP problems that are believed not to be in P are also in BQP (integer factorization and the discrete logarithm problem are both in NP, for example). It is suspected that NP\\n  \\n    \\n      \\n        ⊈\\n      \\n    \\n    {\\\\displaystyle \\\\nsubseteq }\\n  BQP; that is, it is believed that there are efficiently checkable problems that are not efficiently solvable by a quantum computer. As a direct consequence of this belief, it is also suspected that BQP is disjoint from the class of NP-complete problems (if an NP-complete problem were in BQP, then it would follow from NP-hardness that all problems in NP are in BQP).The relationship of BQP to the basic classical complexity classes can be summarized as follows:\\n\\n  \\n    \\n      \\n        \\n          \\n            P\\n            ⊆\\n            B\\n            P\\n            P\\n            ⊆\\n            B\\n            Q\\n            P\\n            ⊆\\n            P\\n            P\\n            ⊆\\n            P\\n            S\\n            P\\n            A\\n            C\\n            E\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\mathsf {P\\\\subseteq BPP\\\\subseteq BQP\\\\subseteq PP\\\\subseteq PSPACE}}}\\n  It is also known that BQP is contained in the complexity class #P (or more precisely in the associated class of decision problems P#P), which is a subclass of PSPACE.\\nIt has been speculated that further advances in physics could lead to even faster computers. For instance, it has been shown that a non-local hidden variable quantum computer based on Bohmian Mechanics could implement a search of an \\n  \\n    \\n      \\n        N\\n      \\n    \\n    {\\\\displaystyle N}\\n  -item database in at most \\n  \\n    \\n      \\n        O\\n        (\\n        \\n          \\n            N\\n            \\n              3\\n            \\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle O({\\\\sqrt[{3}]{N}})}\\n   steps, a slight speedup over Grover\\'s algorithm, which runs in \\n  \\n    \\n      \\n        O\\n        (\\n        \\n          \\n            N\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle O({\\\\sqrt {N}})}\\n   steps. Note, however, that neither search method would allow quantum computers to solve NP-complete problems in polynomial time. Theories of quantum gravity, such as M-theory and loop quantum gravity, may allow even faster computers to be built. However, defining computation in these theories is an open problem due to the problem of time; that is, within these physical theories there is currently no obvious way to describe what it means for an observer to submit input to a computer at one point in time and then receive output at a later point in time.\\n\\n\\n== See also ==\\n\\n\\n== References ==\\n\\n\\n== Further reading ==\\n\\n\\n=== Textbooks ===\\nNielsen, Michael; Chuang, Isaac (2000). Quantum Computation and Quantum Information. Cambridge: Cambridge University Press. ISBN 978-0-521-63503-5. OCLC 174527496.\\nMermin, N. David (2007). Quantum Computer Science: An Introduction. Cambridge University Press. ISBN 978-0-521-87658-2.\\nAkama, Seiki (2014). Elements of Quantum Computing: History, Theories and Engineering Applications. Springer International Publishing. ISBN 978-3-319-08284-4.\\nBenenti, Giuliano (2004). Principles of Quantum Computation and Information Volume 1. New Jersey: World Scientific. ISBN 978-981-238-830-8. OCLC 179950736.\\nStolze, Joachim; Suter, Dieter (2004). Quantum Computing. Wiley-VCH. ISBN 978-3-527-40438-4.\\nWichert, Andreas (2014). Principles of Quantum Artificial Intelligence. World Scientific Publishing Co. ISBN 978-981-4566-74-2.\\nHiroshi, Imai; Masahito, Hayashi (2006). Quantum Computation and Information. Berlin: Springer. ISBN 978-3-540-33132-2.\\nJaeger, Gregg (2006). Quantum Information: An Overview. Berlin: Springer. ISBN 978-0-387-35725-6. OCLC 255569451.\\n\\n\\n=== Academic papers ===\\nAbbot, Derek; Doering, Charles R.; Caves, Carlton M.; Lidar, Daniel M.; Brandt, Howard E.; Hamilton, Alexander R.; Ferry, David K.; Gea-Banacloche, Julio; Bezrukov, Sergey M.; Kish, Laszlo B. (2003). \"Dreams versus Reality: Plenary Debate Session on Quantum Computing\". Quantum Information Processing. 2 (6): 449–472. arXiv:quant-ph/0310130. doi:10.1023/B:QINP.0000042203.24782.9a. hdl:2027.42/45526. S2CID 34885835.\\nDiVincenzo, David P. (2000). \"The Physical Implementation of Quantum Computation\". Fortschritte der Physik. 48 (9–11): 771–783. arXiv:quant-ph/0002077. Bibcode:2000ForPh..48..771D. doi:10.1002/1521-3978(200009)48:9/11<771::AID-PROP771>3.0.CO;2-E.\\nBerthiaume, Andre (1997). \"Quantum Computation\".\\nDiVincenzo, David P. (1995). \"Quantum Computation\". Science. 270 (5234): 255–261. Bibcode:1995Sci...270..255D. CiteSeerX 10.1.1.242.2165. doi:10.1126/science.270.5234.255. S2CID 220110562. Table 1 lists switching and dephasing times for various systems.\\nFeynman, Richard (1982). \"Simulating physics with computers\". International Journal of Theoretical Physics. 21 (6–7): 467–488. Bibcode:1982IJTP...21..467F. CiteSeerX 10.1.1.45.9310. doi:10.1007/BF02650179. S2CID 124545445.\\nMitchell, Ian (1998). \"Computing Power into the 21st Century: Moore\\'s Law and Beyond\".\\nSimon, Daniel R. (1994). \"On the Power of Quantum Computation\". Institute of Electrical and Electronic Engineers Computer Society Press.\\n\\n\\n== External links ==\\nStanford Encyclopedia of Philosophy: \"Quantum Computing\" by Amit Hagar and Michael E. Cuffaro.\\n\"Quantum computation, theory of\", Encyclopedia of Mathematics, EMS Press, 2001 [1994]\\nQuantum computing for the very curious by Andy Matuschak and Michael Nielsen\\nQuantum Computing Made Easy on Satalia blogLecturesQuantum computing for the determined – 22 video lectures by Michael Nielsen\\nVideo Lectures by David Deutsch\\nLectures at the Institut Henri Poincaré (slides and videos)\\nOnline lecture on An Introduction to Quantum Computing, Edward Gerjuoy (2008)\\nLomonaco, Sam. Four Lectures on Quantum Computing given at Oxford University in July 2006', 'Quantum superposition is a fundamental principle of quantum mechanics.  It states that, much like waves in classical physics, any two (or more) quantum states can be added together (\"superposed\") and the result will be another valid quantum state; and conversely, that every quantum state can be represented as a sum of two or more other distinct states.  Mathematically, it refers to a property of solutions to the Schrödinger equation; since the Schrödinger equation is linear, any linear combination of solutions will also be a solution.\\nAn example of a physically observable manifestation of the wave nature of quantum systems is the interference peaks from an electron beam in a double-slit experiment. The pattern is very similar to the one obtained by diffraction of classical waves.\\nAnother example is a quantum logical qubit state, as used in quantum information processing, which is a quantum superposition of the \"basis states\" \\n  \\n    \\n      \\n        \\n          |\\n        \\n        0\\n        ⟩\\n      \\n    \\n    {\\\\displaystyle |0\\\\rangle }\\n   and \\n  \\n    \\n      \\n        \\n          |\\n        \\n        1\\n        ⟩\\n      \\n    \\n    {\\\\displaystyle |1\\\\rangle }\\n  .\\nHere \\n  \\n    \\n      \\n        \\n          |\\n        \\n        0\\n        ⟩\\n      \\n    \\n    {\\\\displaystyle |0\\\\rangle }\\n   is the Dirac notation for the quantum state that will always give the result 0 when converted to classical logic by a measurement. Likewise \\n  \\n    \\n      \\n        \\n          |\\n        \\n        1\\n        ⟩\\n      \\n    \\n    {\\\\displaystyle |1\\\\rangle }\\n   is the state that will always convert to 1. Contrary to a classical bit that can only be in the state corresponding to 0 or the state corresponding to 1, a qubit may be in a superposition of both states.  This means that the probabilities of measuring 0 or 1 for a qubit are in general neither 0.0 nor 1.0, and multiple measurements made on qubits in identical states will not always give the same result.\\n\\n\\n== Concept ==\\nThe principle of quantum superposition states that if a physical system may be in one of many configurations—arrangements of particles or fields—then the most general state is a combination of all of these possibilities, where the amount in each configuration is specified by a complex number.\\nFor example, if there are two configurations labelled by 0 and 1, the most general state would be\\n\\n  \\n    \\n      \\n        \\n          c\\n          \\n            0\\n          \\n        \\n        \\n          ∣\\n        \\n        0\\n        ⟩\\n        +\\n        \\n          c\\n          \\n            1\\n          \\n        \\n        \\n          ∣\\n        \\n        1\\n        ⟩\\n      \\n    \\n    {\\\\displaystyle c_{0}{\\\\mid }0\\\\rangle +c_{1}{\\\\mid }1\\\\rangle }\\n  where the coefficients are complex numbers describing how much goes into each configuration.\\nThe principle was described by Paul Dirac as follows:\\n\\nThe general principle of superposition of quantum mechanics applies to the states [that are theoretically possible without mutual interference or contradiction] ... of any one dynamical system. It requires us to assume that between these states there exist peculiar relationships such that whenever the system is definitely in one state we can consider it as being partly in each of two or more other states. The original state must be regarded as the result of a kind of superposition of the two or more new states, in a way that cannot be conceived on classical ideas. Any state may be considered as the result of a superposition of two or more other states, and indeed in an infinite number of ways. Conversely, any two or more states may be superposed to give a new state...\\nThe non-classical nature of the superposition process is brought out clearly if we consider the superposition of two states, A and B, such that there exists an observation which, when made on the system in state A, is certain to lead to one particular result, a say, and when made on the system in state B is certain to lead to some different result, b say. What will be the result of the observation when made on the system in the superposed state? The answer is that the result will be sometimes a and sometimes b, according to a probability law depending on the relative weights of A and B in the superposition process. It will never be different from both a and b [i.e., either a or b]. The intermediate character of the state formed by superposition thus expresses itself through the probability of a particular result for an observation being intermediate between the corresponding probabilities for the original states, not through the result itself being intermediate between the corresponding results for the original states.\\nAnton Zeilinger, referring to the prototypical example of the double-slit experiment, has elaborated regarding the creation and destruction of quantum superposition:\\n\\n\"[T]he superposition of amplitudes ... is only valid if there is no way to know, even in principle, which path the particle took. It is important to realize that this does not imply that an observer actually takes note of what happens. It is sufficient to destroy the interference pattern, if the path information is accessible in principle from the experiment or even if it is dispersed in the environment and beyond any technical possibility to be recovered, but in principle still ‘‘out there.’’ The absence of any such information is the essential criterion for quantum interference to appear.\\n\\n\\n== Theory ==\\n\\n\\n=== Examples ===\\nFor an equation describing a physical phenomenon, the superposition principle states that a combination of solutions to a linear equation is also a solution of it. When this is true the equation is said to obey the superposition principle. Thus, if state vectors f1, f2 and f3 each solve the linear equation on ψ, then ψ = c1\\u2009f1 + c2\\u2009f2 + c3\\u2009f3 would also be a solution, in which each c is a coefficient. The Schrödinger equation is linear, so quantum mechanics follows this.\\nFor example, consider an electron with two possible configurations, up and down. This describes the physical system of a qubit.\\n\\n  \\n    \\n      \\n        \\n          c\\n          \\n            1\\n          \\n        \\n        \\n          ∣\\n        \\n        \\n          ↑\\n        \\n        ⟩\\n        +\\n        \\n          c\\n          \\n            2\\n          \\n        \\n        \\n          ∣\\n        \\n        \\n          ↓\\n        \\n        ⟩\\n      \\n    \\n    {\\\\displaystyle c_{1}{\\\\mid }{\\\\uparrow }\\\\rangle +c_{2}{\\\\mid }{\\\\downarrow }\\\\rangle }\\n  is the most general state. But these coefficients dictate probabilities for the system to be in either configuration. The probability for a specified configuration is given by the square of the absolute value of the coefficient. So the probabilities should add up to 1. The electron is in one of those two states for sure.\\n\\n  \\n    \\n      \\n        \\n          p\\n          \\n            up\\n          \\n        \\n        =\\n        \\n          ∣\\n        \\n        \\n          c\\n          \\n            1\\n          \\n        \\n        \\n          \\n            ∣\\n          \\n          \\n            2\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle p_{\\\\text{up}}={\\\\mid }c_{1}{\\\\mid }^{2}}\\n  \\n\\n  \\n    \\n      \\n        \\n          p\\n          \\n            down\\n          \\n        \\n        =\\n        \\n          ∣\\n        \\n        \\n          c\\n          \\n            2\\n          \\n        \\n        \\n          ∣\\n          \\n            2\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle p_{\\\\text{down}}={\\\\mid }c_{2}\\\\mid ^{2}}\\n  \\n\\n  \\n    \\n      \\n        \\n          p\\n          \\n            up or down\\n          \\n        \\n        =\\n        \\n          p\\n          \\n            up\\n          \\n        \\n        +\\n        \\n          p\\n          \\n            down\\n          \\n        \\n        =\\n        1\\n      \\n    \\n    {\\\\displaystyle p_{\\\\text{up or down}}=p_{\\\\text{up}}+p_{\\\\text{down}}=1}\\n  Continuing with this example: If a particle can be in state  up and  down, it can also be in a state where it is an amount 3i/5 in up and an amount 4/5 in down.\\n\\n  \\n    \\n      \\n        \\n          |\\n        \\n        ψ\\n        ⟩\\n        =\\n        \\n          \\n            3\\n            5\\n          \\n        \\n        i\\n        \\n          ∣\\n        \\n        \\n          ↑\\n        \\n        ⟩\\n        +\\n        \\n          \\n            4\\n            5\\n          \\n        \\n        \\n          ∣\\n        \\n        \\n          ↓\\n        \\n        ⟩\\n        .\\n      \\n    \\n    {\\\\displaystyle |\\\\psi \\\\rangle ={3 \\\\over 5}i{\\\\mid }{\\\\uparrow }\\\\rangle +{4 \\\\over 5}{\\\\mid }{\\\\downarrow }\\\\rangle .}\\n  In this, the probability for up is \\n  \\n    \\n      \\n        \\n          \\n            |\\n            \\n              \\n                \\n                  3\\n                  i\\n                \\n                5\\n              \\n            \\n            |\\n          \\n          \\n            2\\n          \\n        \\n        =\\n        \\n          \\n            9\\n            25\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle \\\\left|{\\\\frac {3i}{5}}\\\\right|^{2}={\\\\frac {9}{25}}}\\n  . The probability for down is \\n  \\n    \\n      \\n        \\n          \\n            |\\n            \\n              \\n                4\\n                5\\n              \\n            \\n            |\\n          \\n          \\n            2\\n          \\n        \\n        =\\n        \\n          \\n            16\\n            25\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle \\\\left|{\\\\frac {4}{5}}\\\\right|^{2}={\\\\frac {16}{25}}}\\n  . Note that \\n  \\n    \\n      \\n        \\n          \\n            9\\n            25\\n          \\n        \\n        +\\n        \\n          \\n            16\\n            25\\n          \\n        \\n        =\\n        1\\n      \\n    \\n    {\\\\displaystyle {\\\\frac {9}{25}}+{\\\\frac {16}{25}}=1}\\n  .\\nIn the description, only the relative size of the different components matter, and their angle to each other on the complex plane. This is usually stated by declaring that two states which are a multiple of one another are the same as far as the description of the situation is concerned. Either of these describe the same state for any nonzero \\n  \\n    \\n      \\n        α\\n      \\n    \\n    {\\\\displaystyle \\\\alpha }\\n  \\n\\n  \\n    \\n      \\n        \\n          |\\n        \\n        ψ\\n        ⟩\\n        ≈\\n        α\\n        \\n          |\\n        \\n        ψ\\n        ⟩\\n      \\n    \\n    {\\\\displaystyle |\\\\psi \\\\rangle \\\\approx \\\\alpha |\\\\psi \\\\rangle }\\n  The fundamental law of quantum mechanics is that the evolution is linear, meaning that if state A turns into A′ and B turns into B′ after 10 seconds, then after 10 seconds the superposition \\n  \\n    \\n      \\n        ψ\\n      \\n    \\n    {\\\\displaystyle \\\\psi }\\n   turns into a mixture of A′ and B′ with the same coefficients as A and B.\\nFor example, if we have the following\\n\\n  \\n    \\n      \\n        \\n          ∣\\n        \\n        \\n          ↑\\n        \\n        ⟩\\n        →\\n        \\n          ∣\\n        \\n        \\n          ↓\\n        \\n        ⟩\\n      \\n    \\n    {\\\\displaystyle {\\\\mid }{\\\\uparrow }\\\\rangle \\\\to {\\\\mid }{\\\\downarrow }\\\\rangle }\\n  \\n\\n  \\n    \\n      \\n        \\n          ∣\\n        \\n        \\n          ↓\\n        \\n        ⟩\\n        →\\n        \\n          \\n            \\n              3\\n              i\\n            \\n            5\\n          \\n        \\n        \\n          ∣\\n        \\n        \\n          ↑\\n        \\n        ⟩\\n        +\\n        \\n          \\n            4\\n            5\\n          \\n        \\n        \\n          ∣\\n        \\n        \\n          ↓\\n        \\n        ⟩\\n      \\n    \\n    {\\\\displaystyle {\\\\mid }{\\\\downarrow }\\\\rangle \\\\to {\\\\frac {3i}{5}}{\\\\mid }{\\\\uparrow }\\\\rangle +{\\\\frac {4}{5}}{\\\\mid }{\\\\downarrow }\\\\rangle }\\n  Then after those 10 seconds our state will change to\\n\\n  \\n    \\n      \\n        \\n          c\\n          \\n            1\\n          \\n        \\n        \\n          ∣\\n        \\n        \\n          ↑\\n        \\n        ⟩\\n        +\\n        \\n          c\\n          \\n            2\\n          \\n        \\n        \\n          ∣\\n        \\n        \\n          ↓\\n        \\n        ⟩\\n        →\\n        \\n          c\\n          \\n            1\\n          \\n        \\n        \\n          (\\n          \\n            \\n              ∣\\n            \\n            \\n              ↓\\n            \\n            ⟩\\n          \\n          )\\n        \\n        +\\n        \\n          c\\n          \\n            2\\n          \\n        \\n        \\n          (\\n          \\n            \\n              \\n                \\n                  3\\n                  i\\n                \\n                5\\n              \\n            \\n            \\n              ∣\\n            \\n            \\n              ↑\\n            \\n            ⟩\\n            +\\n            \\n              \\n                4\\n                5\\n              \\n            \\n            \\n              ∣\\n            \\n            \\n              ↓\\n            \\n            ⟩\\n          \\n          )\\n        \\n      \\n    \\n    {\\\\displaystyle c_{1}{\\\\mid }{\\\\uparrow }\\\\rangle +c_{2}{\\\\mid }{\\\\downarrow }\\\\rangle \\\\to c_{1}\\\\left({\\\\mid }{\\\\downarrow }\\\\rangle \\\\right)+c_{2}\\\\left({\\\\frac {3i}{5}}{\\\\mid }{\\\\uparrow }\\\\rangle +{\\\\frac {4}{5}}{\\\\mid }{\\\\downarrow }\\\\rangle \\\\right)}\\n  So far there have just been 2 configurations, but there can be infinitely many. \\nIn illustration, a particle can have any position, so that there are different configurations which have any value of the position x. These are written:\\n\\n  \\n    \\n      \\n        \\n          |\\n        \\n        x\\n        ⟩\\n      \\n    \\n    {\\\\displaystyle |x\\\\rangle }\\n  The principle of superposition guarantees that there are states which are arbitrary superpositions of all the positions with complex coefficients:\\n\\n  \\n    \\n      \\n        \\n          ∑\\n          \\n            x\\n          \\n        \\n        ψ\\n        (\\n        x\\n        )\\n        \\n          |\\n        \\n        x\\n        ⟩\\n      \\n    \\n    {\\\\displaystyle \\\\sum _{x}\\\\psi (x)|x\\\\rangle }\\n  This sum is defined only if the index x is discrete. If the index is over \\n  \\n    \\n      \\n        \\n          R\\n        \\n      \\n    \\n    {\\\\displaystyle \\\\mathbb {R} }\\n  , then the sum is replaced by an integral. The quantity \\n  \\n    \\n      \\n        ψ\\n        (\\n        x\\n        )\\n      \\n    \\n    {\\\\displaystyle \\\\psi (x)}\\n   is called the wavefunction of the particle.\\nIf we consider a qubit with both position and spin, the state is a superposition of all possibilities for both:\\n\\n  \\n    \\n      \\n        \\n          ∑\\n          \\n            x\\n          \\n        \\n        \\n          ψ\\n          \\n            +\\n          \\n        \\n        (\\n        x\\n        )\\n        \\n          |\\n        \\n        x\\n        ,\\n        \\n          ↑\\n        \\n        ⟩\\n        +\\n        \\n          ψ\\n          \\n            −\\n          \\n        \\n        (\\n        x\\n        )\\n        \\n          |\\n        \\n        x\\n        ,\\n        \\n          ↓\\n        \\n        ⟩\\n        \\n      \\n    \\n    {\\\\displaystyle \\\\sum _{x}\\\\psi _{+}(x)|x,{\\\\uparrow }\\\\rangle +\\\\psi _{-}(x)|x,{\\\\downarrow }\\\\rangle \\\\,}\\n  The configuration space of a quantum mechanical system cannot be worked out without some physical knowledge. The input is usually the allowed different classical configurations, but without the duplication of including both position and momentum.\\nA pair of particles can be in any combination of pairs of positions. A state where one particle is at position x and the other is at position y is written \\n  \\n    \\n      \\n        \\n          |\\n        \\n        x\\n        ,\\n        y\\n        ⟩\\n      \\n    \\n    {\\\\displaystyle |x,y\\\\rangle }\\n  . The most general state is a superposition of the possibilities:\\n\\n  \\n    \\n      \\n        \\n          ∑\\n          \\n            x\\n            y\\n          \\n        \\n        A\\n        (\\n        x\\n        ,\\n        y\\n        )\\n        \\n          |\\n        \\n        x\\n        ,\\n        y\\n        ⟩\\n        \\n      \\n    \\n    {\\\\displaystyle \\\\sum _{xy}A(x,y)|x,y\\\\rangle \\\\,}\\n  The description of the two particles is much larger than the description of one particle—it is a function in twice the number of dimensions. This is also true in probability, when the statistics of two random variables are correlated. If two particles are uncorrelated, the probability distribution for their joint position P(x,\\u2009y) is a product of the probability of finding one at one position and the other at the other position:\\n\\n  \\n    \\n      \\n        P\\n        (\\n        x\\n        ,\\n        y\\n        )\\n        =\\n        \\n          P\\n          \\n            x\\n          \\n        \\n        (\\n        x\\n        )\\n        \\n          P\\n          \\n            y\\n          \\n        \\n        (\\n        y\\n        )\\n        \\n      \\n    \\n    {\\\\displaystyle P(x,y)=P_{x}(x)P_{y}(y)\\\\,}\\n  This means that the wave function \\n  \\n    \\n      \\n        A\\n        (\\n        x\\n        ,\\n        y\\n        )\\n      \\n    \\n    {\\\\displaystyle A(x,y)}\\n   of the system can be represented as a product of the wave functions \\n  \\n    \\n      \\n        \\n          ψ\\n          \\n            x\\n          \\n        \\n        (\\n        x\\n        )\\n      \\n    \\n    {\\\\displaystyle \\\\psi _{x}(x)}\\n   and \\n  \\n    \\n      \\n        \\n          ψ\\n          \\n            y\\n          \\n        \\n        (\\n        y\\n        )\\n      \\n    \\n    {\\\\displaystyle \\\\psi _{y}(y)}\\n    of its parts:\\n\\n  \\n    \\n      \\n        A\\n        (\\n        x\\n        ,\\n        y\\n        )\\n        =\\n        \\n          ψ\\n          \\n            x\\n          \\n        \\n        (\\n        x\\n        )\\n        \\n          ψ\\n          \\n            y\\n          \\n        \\n        (\\n        y\\n        )\\n        \\n      \\n    \\n    {\\\\displaystyle A(x,y)=\\\\psi _{x}(x)\\\\psi _{y}(y)\\\\,}\\n  .\\nIn 1927, Heitler and London, attempted to quantitatively mechanically calculate the ground steady state of the H2 molecule.  The calculations were based on the quantum superposition of the two hydrogen atoms that make up the system - H2 molecule. The success of this attempt became the basis for all further development of covalent bond.\\n\\n\\n=== Analogy with probability ===\\nIn probability theory there is a similar principle. If a system has a probabilistic description, this description gives the probability of any configuration, and given any two different configurations, there is a state which is partly this and partly that, with positive real number coefficients, the probabilities, which say how much of each there is.\\nFor example, if we have a probability distribution for where a particle is, it is described by the \"state\"\\n\\n  \\n    \\n      \\n        \\n          ∑\\n          \\n            x\\n          \\n        \\n        ρ\\n        (\\n        x\\n        )\\n        \\n          |\\n        \\n        x\\n        ⟩\\n      \\n    \\n    {\\\\displaystyle \\\\sum _{x}\\\\rho (x)|x\\\\rangle }\\n  Where \\n  \\n    \\n      \\n        ρ\\n      \\n    \\n    {\\\\displaystyle \\\\rho }\\n   is the probability density function, a positive number that measures the probability that the particle will be found at a certain location.\\nThe evolution equation is also linear in probability, for fundamental reasons. If the particle has some probability for going from position x to y, and from z to y, the probability of going to y starting from a state which is half-x and half-z is a half-and-half mixture of the probability of going to y from each of the options. This is the principle of linear superposition in probability.\\nQuantum mechanics is different, because the numbers can be positive or negative. While the complex nature of the numbers is just a doubling, if you consider the real and imaginary parts separately, the sign of the coefficients is important. In probability, two different possible outcomes always add together, so that if there are more options to get to a point z, the probability always goes up. In quantum mechanics, different possibilities can cancel.\\nIn probability theory with a finite number of states, the probabilities can always be multiplied by a positive number to make their sum equal to one. For example, if there is a three state probability system:\\n\\n  \\n    \\n      \\n        x\\n        \\n          |\\n        \\n        1\\n        ⟩\\n        +\\n        y\\n        \\n          |\\n        \\n        2\\n        ⟩\\n        +\\n        z\\n        \\n          |\\n        \\n        3\\n        ⟩\\n        \\n      \\n    \\n    {\\\\displaystyle x|1\\\\rangle +y|2\\\\rangle +z|3\\\\rangle \\\\,}\\n  where the probabilities \\n  \\n    \\n      \\n        x\\n        ,\\n        y\\n        ,\\n        z\\n      \\n    \\n    {\\\\displaystyle x,y,z}\\n   are positive numbers. Rescaling x,y,z so that\\n\\n  \\n    \\n      \\n        x\\n        +\\n        y\\n        +\\n        z\\n        =\\n        1\\n        \\n      \\n    \\n    {\\\\displaystyle x+y+z=1\\\\,}\\n  The geometry of the state space is a revealed to be a triangle. In general it is a simplex. There are special points in a triangle or simplex corresponding to the corners, and these points are those where one of the probabilities is equal to 1 and the others are zero. These are the unique locations where the position is known with certainty.\\nIn a quantum mechanical system with three states, the quantum mechanical wavefunction is a superposition of states again, but this time twice as many quantities with no restriction on the sign:\\n\\n  \\n    \\n      \\n        A\\n        \\n          |\\n        \\n        1\\n        ⟩\\n        +\\n        B\\n        \\n          |\\n        \\n        2\\n        ⟩\\n        +\\n        C\\n        \\n          |\\n        \\n        3\\n        ⟩\\n        =\\n        (\\n        \\n          A\\n          \\n            r\\n          \\n        \\n        +\\n        i\\n        \\n          A\\n          \\n            i\\n          \\n        \\n        )\\n        \\n          |\\n        \\n        1\\n        ⟩\\n        +\\n        (\\n        \\n          B\\n          \\n            r\\n          \\n        \\n        +\\n        i\\n        \\n          B\\n          \\n            i\\n          \\n        \\n        )\\n        \\n          |\\n        \\n        2\\n        ⟩\\n        +\\n        (\\n        \\n          C\\n          \\n            r\\n          \\n        \\n        +\\n        i\\n        \\n          C\\n          \\n            i\\n          \\n        \\n        )\\n        \\n          |\\n        \\n        3\\n        ⟩\\n        \\n      \\n    \\n    {\\\\displaystyle A|1\\\\rangle +B|2\\\\rangle +C|3\\\\rangle =(A_{r}+iA_{i})|1\\\\rangle +(B_{r}+iB_{i})|2\\\\rangle +(C_{r}+iC_{i})|3\\\\rangle \\\\,}\\n  rescaling the variables so that the sum of the squares is 1, the geometry of the space is revealed to be a high-dimensional sphere\\n\\n  \\n    \\n      \\n        \\n          A\\n          \\n            r\\n          \\n          \\n            2\\n          \\n        \\n        +\\n        \\n          A\\n          \\n            i\\n          \\n          \\n            2\\n          \\n        \\n        +\\n        \\n          B\\n          \\n            r\\n          \\n          \\n            2\\n          \\n        \\n        +\\n        \\n          B\\n          \\n            i\\n          \\n          \\n            2\\n          \\n        \\n        +\\n        \\n          C\\n          \\n            r\\n          \\n          \\n            2\\n          \\n        \\n        +\\n        \\n          C\\n          \\n            i\\n          \\n          \\n            2\\n          \\n        \\n        =\\n        1\\n        \\n      \\n    \\n    {\\\\displaystyle A_{r}^{2}+A_{i}^{2}+B_{r}^{2}+B_{i}^{2}+C_{r}^{2}+C_{i}^{2}=1\\\\,}\\n  .A sphere has a large amount of symmetry, it can be viewed in different coordinate systems or bases. So unlike a probability theory, a quantum theory has a large number of different bases in which it can be equally well described. The geometry of the phase space can be viewed as a hint that the quantity in quantum mechanics which corresponds to the probability is the absolute square of the coefficient of the superposition.\\n\\n\\n=== Hamiltonian evolution ===\\nThe numbers that describe the amplitudes for different possibilities define the kinematics, the space of different states. The dynamics describes how these numbers change with time. For a particle that can be in any one of infinitely many discrete positions, a particle on a lattice, the superposition principle tells you how to make a state:\\n\\n  \\n    \\n      \\n        \\n          ∑\\n          \\n            n\\n          \\n        \\n        \\n          ψ\\n          \\n            n\\n          \\n        \\n        \\n          |\\n        \\n        n\\n        ⟩\\n        \\n      \\n    \\n    {\\\\displaystyle \\\\sum _{n}\\\\psi _{n}|n\\\\rangle \\\\,}\\n  So that the infinite list of amplitudes \\n  \\n    \\n      \\n        (\\n        …\\n        ,\\n        \\n          ψ\\n          \\n            −\\n            2\\n          \\n        \\n        ,\\n        \\n          ψ\\n          \\n            −\\n            1\\n          \\n        \\n        ,\\n        \\n          ψ\\n          \\n            0\\n          \\n        \\n        ,\\n        \\n          ψ\\n          \\n            1\\n          \\n        \\n        ,\\n        \\n          ψ\\n          \\n            2\\n          \\n        \\n        ,\\n        …\\n        )\\n      \\n    \\n    {\\\\textstyle (\\\\ldots ,\\\\psi _{-2},\\\\psi _{-1},\\\\psi _{0},\\\\psi _{1},\\\\psi _{2},\\\\ldots )}\\n   completely describes the quantum state of the particle. This list is called the state vector, and formally it is an element of a Hilbert space, an infinite-dimensional complex vector space. It is usual to represent the state so that the sum of the absolute squares of the amplitudes is one:\\n\\n  \\n    \\n      \\n        ∑\\n        \\n          ψ\\n          \\n            n\\n          \\n          \\n            ∗\\n          \\n        \\n        \\n          ψ\\n          \\n            n\\n          \\n        \\n        =\\n        1\\n      \\n    \\n    {\\\\displaystyle \\\\sum \\\\psi _{n}^{*}\\\\psi _{n}=1}\\n  For a particle described by probability theory random walking on a line, the analogous thing is the list of probabilities \\n  \\n    \\n      \\n        (\\n        …\\n        ,\\n        \\n          P\\n          \\n            −\\n            2\\n          \\n        \\n        ,\\n        \\n          P\\n          \\n            −\\n            1\\n          \\n        \\n        ,\\n        \\n          P\\n          \\n            0\\n          \\n        \\n        ,\\n        \\n          P\\n          \\n            1\\n          \\n        \\n        ,\\n        \\n          P\\n          \\n            2\\n          \\n        \\n        ,\\n        …\\n        )\\n      \\n    \\n    {\\\\textstyle (\\\\ldots ,P_{-2},P_{-1},P_{0},P_{1},P_{2},\\\\ldots )}\\n  , which give the probability of any position. The quantities that describe how they change in time are the transition probabilities \\n  \\n    \\n      \\n        \\n          \\n            K\\n            \\n              x\\n              →\\n              y\\n            \\n          \\n          (\\n          t\\n          )\\n        \\n      \\n    \\n    {\\\\displaystyle \\\\scriptstyle K_{x\\\\rightarrow y}(t)}\\n  , which gives the probability that, starting at x, the particle ends up at y time t later. The total probability of ending up at y is given by the sum over all the possibilities\\n\\n  \\n    \\n      \\n        \\n          P\\n          \\n            y\\n          \\n        \\n        (\\n        \\n          t\\n          \\n            0\\n          \\n        \\n        +\\n        t\\n        )\\n        =\\n        \\n          ∑\\n          \\n            x\\n          \\n        \\n        \\n          P\\n          \\n            x\\n          \\n        \\n        (\\n        \\n          t\\n          \\n            0\\n          \\n        \\n        )\\n        \\n          K\\n          \\n            x\\n            →\\n            y\\n          \\n        \\n        (\\n        t\\n        )\\n        \\n      \\n    \\n    {\\\\displaystyle P_{y}(t_{0}+t)=\\\\sum _{x}P_{x}(t_{0})K_{x\\\\rightarrow y}(t)\\\\,}\\n  The condition of conservation of probability states that starting at any x, the total probability to end up somewhere must add up to 1:\\n\\n  \\n    \\n      \\n        \\n          ∑\\n          \\n            y\\n          \\n        \\n        \\n          K\\n          \\n            x\\n            →\\n            y\\n          \\n        \\n        =\\n        1\\n        \\n      \\n    \\n    {\\\\displaystyle \\\\sum _{y}K_{x\\\\rightarrow y}=1\\\\,}\\n  So that the total probability will be preserved, K is what is called a stochastic matrix.\\nWhen no time passes, nothing changes: for 0 elapsed time \\n  \\n    \\n      \\n        \\n          K\\n          \\n            x\\n            →\\n            y\\n          \\n          (\\n          0\\n          )\\n          =\\n          \\n            δ\\n            \\n              x\\n              y\\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle \\\\scriptstyle K{x\\\\rightarrow y}(0)=\\\\delta _{xy}}\\n  , the K matrix is zero except from a state to itself. So in the case that the time is short, it is better to talk about the rate of change of the probability instead of the absolute change in the probability.\\n\\n  \\n    \\n      \\n        \\n          P\\n          \\n            y\\n          \\n        \\n        (\\n        t\\n        +\\n        d\\n        t\\n        )\\n        =\\n        \\n          P\\n          \\n            y\\n          \\n        \\n        (\\n        t\\n        )\\n        +\\n        d\\n        t\\n        \\n        \\n          ∑\\n          \\n            x\\n          \\n        \\n        \\n          P\\n          \\n            x\\n          \\n        \\n        \\n          R\\n          \\n            x\\n            →\\n            y\\n          \\n        \\n        \\n      \\n    \\n    {\\\\displaystyle P_{y}(t+dt)=P_{y}(t)+dt\\\\,\\\\sum _{x}P_{x}R_{x\\\\rightarrow y}\\\\,}\\n  where \\n  \\n    \\n      \\n        \\n          \\n            R\\n            \\n              x\\n              →\\n              y\\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle \\\\scriptstyle R_{x\\\\rightarrow y}}\\n   is the time derivative of the K matrix:\\n\\n  \\n    \\n      \\n        \\n          R\\n          \\n            x\\n            →\\n            y\\n          \\n        \\n        =\\n        \\n          \\n            \\n              \\n                K\\n                \\n                  x\\n                  →\\n                  y\\n                \\n              \\n              \\n              d\\n              t\\n              −\\n              \\n                δ\\n                \\n                  x\\n                  y\\n                \\n              \\n            \\n            \\n              d\\n              t\\n            \\n          \\n        \\n        .\\n        \\n      \\n    \\n    {\\\\displaystyle R_{x\\\\rightarrow y}={K_{x\\\\rightarrow y}\\\\,dt-\\\\delta _{xy} \\\\over dt}.\\\\,}\\n  The equation for the probabilities is a differential equation that is sometimes called the master equation:\\n\\n  \\n    \\n      \\n        \\n          \\n            \\n              d\\n              \\n                P\\n                \\n                  y\\n                \\n              \\n            \\n            \\n              d\\n              t\\n            \\n          \\n        \\n        =\\n        \\n          ∑\\n          \\n            x\\n          \\n        \\n        \\n          P\\n          \\n            x\\n          \\n        \\n        \\n          R\\n          \\n            x\\n            →\\n            y\\n          \\n        \\n        \\n      \\n    \\n    {\\\\displaystyle {dP_{y} \\\\over dt}=\\\\sum _{x}P_{x}R_{x\\\\rightarrow y}\\\\,}\\n  The R matrix is the probability per unit time for the particle to make a transition from x to y. The condition that the K matrix elements add up to one becomes the condition that the R matrix elements add up to zero:\\n\\n  \\n    \\n      \\n        \\n          ∑\\n          \\n            y\\n          \\n        \\n        \\n          R\\n          \\n            x\\n            →\\n            y\\n          \\n        \\n        =\\n        0\\n        \\n      \\n    \\n    {\\\\displaystyle \\\\sum _{y}R_{x\\\\rightarrow y}=0\\\\,}\\n  One simple case to study is when the R matrix has an equal probability to go one unit to the left or to the right, describing a particle that has a constant rate of random walking. In this case \\n  \\n    \\n      \\n        \\n          \\n            R\\n            \\n              x\\n              →\\n              y\\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle \\\\scriptstyle R_{x\\\\rightarrow y}}\\n   is zero unless y is either x + 1, x, or x − 1, when y is x + 1 or x − 1, the R matrix has value c, and in order for the sum of the R matrix coefficients to equal zero, the value of \\n  \\n    \\n      \\n        \\n          R\\n          \\n            x\\n            →\\n            x\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle R_{x\\\\rightarrow x}}\\n   must be −2c. So the probabilities obey the discretized diffusion equation:\\n\\n  \\n    \\n      \\n        \\n          \\n            \\n              d\\n              \\n                P\\n                \\n                  x\\n                \\n              \\n            \\n            \\n              d\\n              t\\n            \\n          \\n        \\n        =\\n        c\\n        (\\n        \\n          P\\n          \\n            x\\n            +\\n            1\\n          \\n        \\n        −\\n        2\\n        \\n          P\\n          \\n            x\\n          \\n        \\n        +\\n        \\n          P\\n          \\n            x\\n            −\\n            1\\n          \\n        \\n        )\\n        \\n      \\n    \\n    {\\\\displaystyle {dP_{x} \\\\over dt}=c(P_{x+1}-2P_{x}+P_{x-1})\\\\,}\\n  which, when c is scaled appropriately and the P distribution is smooth enough to think of the system in a continuum limit becomes:\\n\\n  \\n    \\n      \\n        \\n          \\n            \\n              ∂\\n              P\\n              (\\n              x\\n              ,\\n              t\\n              )\\n            \\n            \\n              ∂\\n              t\\n            \\n          \\n        \\n        =\\n        c\\n        \\n          \\n            \\n              \\n                ∂\\n                \\n                  2\\n                \\n              \\n              P\\n            \\n            \\n              ∂\\n              \\n                x\\n                \\n                  2\\n                \\n              \\n            \\n          \\n        \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\partial P(x,t) \\\\over \\\\partial t}=c{\\\\partial ^{2}P \\\\over \\\\partial x^{2}}\\\\,}\\n  Which is the diffusion equation.\\nQuantum amplitudes give the rate at which amplitudes change in time, and they are mathematically exactly the same except that they are complex numbers. The analog of the finite time K matrix is called the U matrix:\\n\\n  \\n    \\n      \\n        \\n          ψ\\n          \\n            n\\n          \\n        \\n        (\\n        t\\n        )\\n        =\\n        \\n          ∑\\n          \\n            m\\n          \\n        \\n        \\n          U\\n          \\n            n\\n            m\\n          \\n        \\n        (\\n        t\\n        )\\n        \\n          ψ\\n          \\n            m\\n          \\n        \\n        \\n      \\n    \\n    {\\\\displaystyle \\\\psi _{n}(t)=\\\\sum _{m}U_{nm}(t)\\\\psi _{m}\\\\,}\\n  Since the sum of the absolute squares of the amplitudes must be constant, \\n  \\n    \\n      \\n        U\\n      \\n    \\n    {\\\\displaystyle U}\\n   must be unitary:\\n\\n  \\n    \\n      \\n        \\n          ∑\\n          \\n            n\\n          \\n        \\n        \\n          U\\n          \\n            n\\n            m\\n          \\n          \\n            ∗\\n          \\n        \\n        \\n          U\\n          \\n            n\\n            p\\n          \\n        \\n        =\\n        \\n          δ\\n          \\n            m\\n            p\\n          \\n        \\n        \\n      \\n    \\n    {\\\\displaystyle \\\\sum _{n}U_{nm}^{*}U_{np}=\\\\delta _{mp}\\\\,}\\n  or, in matrix notation,\\n\\n  \\n    \\n      \\n        \\n          U\\n          \\n            †\\n          \\n        \\n        U\\n        =\\n        I\\n        \\n      \\n    \\n    {\\\\displaystyle U^{\\\\dagger }U=I\\\\,}\\n  The rate of change of U is called the Hamiltonian H, up to a traditional factor of i:\\n\\n  \\n    \\n      \\n        \\n          H\\n          \\n            m\\n            n\\n          \\n        \\n        =\\n        i\\n        \\n          \\n            d\\n            \\n              d\\n              t\\n            \\n          \\n        \\n        \\n          U\\n          \\n            m\\n            n\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle H_{mn}=i{d \\\\over dt}U_{mn}}\\n  The Hamiltonian gives the rate at which the particle has an amplitude to go from m to n. The reason it is multiplied by i is that the condition that U is unitary translates to the condition:\\n\\n  \\n    \\n      \\n        (\\n        I\\n        +\\n        i\\n        \\n          H\\n          \\n            †\\n          \\n        \\n        \\n        d\\n        t\\n        )\\n        (\\n        I\\n        −\\n        i\\n        H\\n        \\n        d\\n        t\\n        )\\n        =\\n        I\\n      \\n    \\n    {\\\\displaystyle (I+iH^{\\\\dagger }\\\\,dt)(I-iH\\\\,dt)=I}\\n  \\n\\n  \\n    \\n      \\n        \\n          H\\n          \\n            †\\n          \\n        \\n        −\\n        H\\n        =\\n        0\\n        \\n      \\n    \\n    {\\\\displaystyle H^{\\\\dagger }-H=0\\\\,}\\n  which says that H is Hermitian. The eigenvalues of the Hermitian matrix H are real quantities, which have a physical interpretation as energy levels. If the factor i were absent, the H matrix would be antihermitian and would have purely imaginary eigenvalues, which is not the traditional way quantum mechanics represents observable quantities like the energy.\\nFor a particle that has equal amplitude to move left and right, the Hermitian matrix H is zero except for nearest neighbors, where it has the value c. If the coefficient is everywhere constant, the condition that H is Hermitian demands that the amplitude to move to the left is the complex conjugate of the amplitude to move to the right. The equation of motion for \\n  \\n    \\n      \\n        ψ\\n      \\n    \\n    {\\\\displaystyle \\\\psi }\\n   is the time differential equation:\\n\\n  \\n    \\n      \\n        i\\n        \\n          \\n            \\n              d\\n              \\n                ψ\\n                \\n                  n\\n                \\n              \\n            \\n            \\n              d\\n              t\\n            \\n          \\n        \\n        =\\n        \\n          c\\n          \\n            ∗\\n          \\n        \\n        \\n          ψ\\n          \\n            n\\n            +\\n            1\\n          \\n        \\n        +\\n        c\\n        \\n          ψ\\n          \\n            n\\n            −\\n            1\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle i{d\\\\psi _{n} \\\\over dt}=c^{*}\\\\psi _{n+1}+c\\\\psi _{n-1}}\\n  In the case in which left and right are symmetric, c is real. By redefining the phase of the wavefunction in time, \\n  \\n    \\n      \\n        ψ\\n        →\\n        ψ\\n        \\n          e\\n          \\n            i\\n            2\\n            c\\n            t\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle \\\\psi \\\\rightarrow \\\\psi e^{i2ct}}\\n  , the amplitudes for being at different locations are only rescaled, so that the physical situation is unchanged. But this phase rotation introduces a linear term.\\n\\n  \\n    \\n      \\n        i\\n        \\n          \\n            \\n              d\\n              \\n                ψ\\n                \\n                  n\\n                \\n              \\n            \\n            \\n              d\\n              t\\n            \\n          \\n        \\n        =\\n        c\\n        \\n          ψ\\n          \\n            n\\n            +\\n            1\\n          \\n        \\n        −\\n        2\\n        c\\n        \\n          ψ\\n          \\n            n\\n          \\n        \\n        +\\n        c\\n        \\n          ψ\\n          \\n            n\\n            −\\n            1\\n          \\n        \\n        ,\\n      \\n    \\n    {\\\\displaystyle i{d\\\\psi _{n} \\\\over dt}=c\\\\psi _{n+1}-2c\\\\psi _{n}+c\\\\psi _{n-1},}\\n  which is the right choice of phase to take the continuum limit. When \\n  \\n    \\n      \\n        c\\n      \\n    \\n    {\\\\displaystyle c}\\n   is very large and \\n  \\n    \\n      \\n        ψ\\n      \\n    \\n    {\\\\displaystyle \\\\psi }\\n   is slowly varying so that the lattice can be thought of as a line, this becomes the free Schrödinger equation:\\n\\n  \\n    \\n      \\n        i\\n        \\n          \\n            \\n              ∂\\n              ψ\\n            \\n            \\n              ∂\\n              t\\n            \\n          \\n        \\n        =\\n        −\\n        \\n          \\n            \\n              \\n                ∂\\n                \\n                  2\\n                \\n              \\n              ψ\\n            \\n            \\n              ∂\\n              \\n                x\\n                \\n                  2\\n                \\n              \\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle i{\\\\partial \\\\psi  \\\\over \\\\partial t}=-{\\\\partial ^{2}\\\\psi  \\\\over \\\\partial x^{2}}}\\n  If there is an additional term in the H matrix that is an extra phase rotation that varies from point to point, the continuum limit is the Schrödinger equation with a potential energy:\\n\\n  \\n    \\n      \\n        i\\n        \\n          \\n            \\n              ∂\\n              ψ\\n            \\n            \\n              ∂\\n              t\\n            \\n          \\n        \\n        =\\n        −\\n        \\n          \\n            \\n              \\n                ∂\\n                \\n                  2\\n                \\n              \\n              ψ\\n            \\n            \\n              ∂\\n              \\n                x\\n                \\n                  2\\n                \\n              \\n            \\n          \\n        \\n        +\\n        V\\n        (\\n        x\\n        )\\n        ψ\\n      \\n    \\n    {\\\\displaystyle i{\\\\partial \\\\psi  \\\\over \\\\partial t}=-{\\\\partial ^{2}\\\\psi  \\\\over \\\\partial x^{2}}+V(x)\\\\psi }\\n  These equations describe the motion of a single particle in non-relativistic quantum mechanics.\\n\\n\\n=== Quantum mechanics in imaginary time ===\\nThe analogy between quantum mechanics and probability is very strong, so that there are many mathematical links between them. In a statistical system in discrete time, t=1,2,3, described by a transition matrix for one time step \\n  \\n    \\n      \\n        \\n          \\n            K\\n            \\n              m\\n              →\\n              n\\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle \\\\scriptstyle K_{m\\\\rightarrow n}}\\n  , the probability to go between two points after a finite number of time steps can be represented as a sum over all paths of the probability of taking each path:\\n\\n  \\n    \\n      \\n        \\n          K\\n          \\n            x\\n            →\\n            y\\n          \\n        \\n        (\\n        T\\n        )\\n        =\\n        \\n          ∑\\n          \\n            x\\n            (\\n            t\\n            )\\n          \\n        \\n        \\n          ∏\\n          \\n            t\\n          \\n        \\n        \\n          K\\n          \\n            x\\n            (\\n            t\\n            )\\n            x\\n            (\\n            t\\n            +\\n            1\\n            )\\n          \\n        \\n        \\n      \\n    \\n    {\\\\displaystyle K_{x\\\\rightarrow y}(T)=\\\\sum _{x(t)}\\\\prod _{t}K_{x(t)x(t+1)}\\\\,}\\n  where the sum extends over all paths \\n  \\n    \\n      \\n        x\\n        (\\n        t\\n        )\\n      \\n    \\n    {\\\\displaystyle x(t)}\\n   with the property that \\n  \\n    \\n      \\n        x\\n        (\\n        0\\n        )\\n        =\\n        0\\n      \\n    \\n    {\\\\displaystyle x(0)=0}\\n   and \\n  \\n    \\n      \\n        x\\n        (\\n        T\\n        )\\n        =\\n        y\\n      \\n    \\n    {\\\\displaystyle x(T)=y}\\n  . The analogous expression in quantum mechanics is the path integral.\\nA generic transition matrix in probability has a stationary distribution, which is the eventual probability to be found at any point no matter what the starting point. If there is a nonzero probability for any two paths to reach the same point at the same time, this stationary distribution does not depend on the initial conditions. In probability theory, the probability m for the stochastic matrix obeys detailed balance when the stationary distribution \\n  \\n    \\n      \\n        \\n          ρ\\n          \\n            n\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle \\\\rho _{n}}\\n   has the property:\\n\\n  \\n    \\n      \\n        \\n          ρ\\n          \\n            n\\n          \\n        \\n        \\n          K\\n          \\n            n\\n            →\\n            m\\n          \\n        \\n        =\\n        \\n          ρ\\n          \\n            m\\n          \\n        \\n        \\n          K\\n          \\n            m\\n            →\\n            n\\n          \\n        \\n        \\n      \\n    \\n    {\\\\displaystyle \\\\rho _{n}K_{n\\\\rightarrow m}=\\\\rho _{m}K_{m\\\\rightarrow n}\\\\,}\\n  Detailed balance says that the total probability of going from m to n in the stationary distribution, which is the probability of starting at m \\n  \\n    \\n      \\n        \\n          ρ\\n          \\n            m\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle \\\\rho _{m}}\\n   times the probability of hopping from m to n, is equal to the probability of going from n to m, so that the total back-and-forth flow of probability in equilibrium is zero along any hop. The condition is automatically satisfied when n=m, so it has the same form when written as a condition for the transition-probability R matrix.\\n\\n  \\n    \\n      \\n        \\n          ρ\\n          \\n            n\\n          \\n        \\n        \\n          R\\n          \\n            n\\n            →\\n            m\\n          \\n        \\n        =\\n        \\n          ρ\\n          \\n            m\\n          \\n        \\n        \\n          R\\n          \\n            m\\n            →\\n            n\\n          \\n        \\n        \\n      \\n    \\n    {\\\\displaystyle \\\\rho _{n}R_{n\\\\rightarrow m}=\\\\rho _{m}R_{m\\\\rightarrow n}\\\\,}\\n  When the R matrix obeys detailed balance, the scale of the probabilities can be redefined using the stationary distribution so that they no longer sum to 1:\\n\\n  \\n    \\n      \\n        \\n          p\\n          \\n            n\\n          \\n          ′\\n        \\n        =\\n        \\n          \\n            \\n              ρ\\n              \\n                n\\n              \\n            \\n          \\n        \\n        \\n        \\n          p\\n          \\n            n\\n          \\n        \\n        \\n      \\n    \\n    {\\\\displaystyle p\\'_{n}={\\\\sqrt {\\\\rho _{n}}}\\\\;p_{n}\\\\,}\\n  In the new coordinates, the R matrix is rescaled as follows:\\n\\n  \\n    \\n      \\n        \\n          \\n            \\n              ρ\\n              \\n                n\\n              \\n            \\n          \\n        \\n        \\n          R\\n          \\n            n\\n            →\\n            m\\n          \\n        \\n        \\n          \\n            1\\n            \\n              \\n                \\n                  ρ\\n                  \\n                    m\\n                  \\n                \\n              \\n            \\n          \\n        \\n        =\\n        \\n          H\\n          \\n            n\\n            m\\n          \\n        \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\sqrt {\\\\rho _{n}}}R_{n\\\\rightarrow m}{1 \\\\over {\\\\sqrt {\\\\rho _{m}}}}=H_{nm}\\\\,}\\n  and H is symmetric\\n\\n  \\n    \\n      \\n        \\n          H\\n          \\n            n\\n            m\\n          \\n        \\n        =\\n        \\n          H\\n          \\n            m\\n            n\\n          \\n        \\n        \\n      \\n    \\n    {\\\\displaystyle H_{nm}=H_{mn}\\\\,}\\n  This matrix H defines a quantum mechanical system:\\n\\n  \\n    \\n      \\n        i\\n        \\n          \\n            d\\n            \\n              d\\n              t\\n            \\n          \\n        \\n        \\n          ψ\\n          \\n            n\\n          \\n        \\n        =\\n        ∑\\n        \\n          H\\n          \\n            n\\n            m\\n          \\n        \\n        \\n          ψ\\n          \\n            m\\n          \\n        \\n        \\n      \\n    \\n    {\\\\displaystyle i{d \\\\over dt}\\\\psi _{n}=\\\\sum H_{nm}\\\\psi _{m}\\\\,}\\n  whose Hamiltonian has the same eigenvalues as those of the R matrix of the statistical system. The eigenvectors are the same too, except expressed in the rescaled basis. The stationary distribution of the statistical system is the ground state of the Hamiltonian and it has energy exactly zero, while all the other energies are positive. If H is exponentiated to find the U matrix:\\n\\n  \\n    \\n      \\n        U\\n        (\\n        t\\n        )\\n        =\\n        \\n          e\\n          \\n            −\\n            i\\n            H\\n            t\\n          \\n        \\n        \\n      \\n    \\n    {\\\\displaystyle U(t)=e^{-iHt}\\\\,}\\n  and t is allowed to take on complex values, the K\\' matrix is found by taking time imaginary.\\n\\n  \\n    \\n      \\n        \\n          K\\n          ′\\n        \\n        (\\n        t\\n        )\\n        =\\n        \\n          e\\n          \\n            −\\n            H\\n            t\\n          \\n        \\n        \\n      \\n    \\n    {\\\\displaystyle K\\'(t)=e^{-Ht}\\\\,}\\n  For quantum systems which are invariant under time reversal the Hamiltonian can be made real and symmetric, so that the action of time-reversal on the wave-function is just complex conjugation. If such a Hamiltonian has a unique lowest energy state with a positive real wave-function, as it often does for physical reasons, it is connected to a stochastic system in imaginary time. This relationship between stochastic systems and quantum systems sheds much light on supersymmetry.\\n\\n\\n== Experiments and applications ==\\nSuccessful experiments involving superpositions of relatively large (by the standards of quantum physics) objects have been performed.\\nA \"cat state\" has been achieved with photons.\\nA beryllium ion has been trapped in a superposed state.\\nA double slit experiment has been performed with molecules as large as buckyballs.\\nA 2013 experiment superposed molecules containing 15,000 each of protons, neutrons and electrons. The molecules were of compounds selected for their good thermal stability, and were evaporated into a beam at a temperature of 600 K. The beam was prepared from highly purified chemical substances, but still contained a mixture of different molecular species. Each species of molecule interfered only with itself, as verified by mass spectrometry.\\nAn experiment involving a superconducting quantum interference device (\"SQUID\") has been linked to the theme of the \"cat state\" thought experiment.By use of very low temperatures, very fine experimental arrangements were made to protect in near isolation and preserve the coherence of intermediate states, for a duration of time, between preparation and detection, of SQUID currents. Such a SQUID current is a coherent physical assembly of perhaps billions of electrons. Because of its coherence, such an assembly may be regarded as exhibiting \"collective states\" of a macroscopic quantal entity. For the principle of superposition, after it is prepared but before it is detected, it may be regarded as exhibiting an intermediate state. It is not a single-particle state such as is often considered in discussions of interference, for example by Dirac in his famous dictum stated above. Moreover, though the \\'intermediate\\' state may be loosely regarded as such, it has not been produced as an output of a secondary quantum analyser that was fed a pure state from a primary analyser, and so this is not an example of superposition as strictly and narrowly defined.Nevertheless, after preparation, but before measurement, such a SQUID state may be regarded in a manner of speaking as a \"pure\" state that is a superposition of a clockwise and an anti-clockwise current state.  In a SQUID, collective electron states can be physically prepared in near isolation, at very low temperatures, so as to result in protected coherent intermediate states. What is remarkable here is that there are two well-separated self-coherent collective states that exhibit such metastability. The crowd of electrons tunnels back and forth between the clockwise and the anti-clockwise states, as opposed to forming a single intermediate state in which there is no definite collective sense of current flow.An experiment involving a flu virus has been proposed.\\nA piezoelectric \"tuning fork\" has been constructed, which can be placed into a superposition of vibrating and non-vibrating states. The resonator comprises about 10 trillion atoms.\\nRecent research indicates that chlorophyll within plants appears to exploit the feature of quantum superposition to achieve greater efficiency in transporting energy, allowing pigment proteins to be spaced further apart than would otherwise be possible.\\nAn experiment has been proposed, with a bacterial cell cooled to 10 mK, using an electromechanical oscillator. At that temperature, all metabolism would be stopped, and the cell might behave virtually as a definite chemical species. For detection of interference, it would be necessary that the cells be supplied in large numbers as pure samples of identical and detectably recognizable virtual chemical species. It is not known whether this requirement can be met by bacterial cells. They would be in a state of suspended animation during the experiment.In quantum computing the phrase \"cat state\" often refers to the GHZ state, the special entanglement of qubits wherein the qubits are in an equal superposition of all being 0 and all being 1; i.e.,\\n\\n  \\n    \\n      \\n        \\n          |\\n        \\n        ψ\\n        ⟩\\n        =\\n        \\n          \\n            1\\n            \\n              2\\n            \\n          \\n        \\n        \\n          \\n            (\\n          \\n        \\n        \\n          |\\n        \\n        00\\n        …\\n        0\\n        ⟩\\n        +\\n        \\n          |\\n        \\n        11\\n        …\\n        1\\n        ⟩\\n        \\n          \\n            )\\n          \\n        \\n        .\\n      \\n    \\n    {\\\\displaystyle |\\\\psi \\\\rangle ={\\\\frac {1}{\\\\sqrt {2}}}{\\\\bigg (}|00\\\\ldots 0\\\\rangle +|11\\\\ldots 1\\\\rangle {\\\\bigg )}.}\\n  \\n\\n\\n== Formal interpretation ==\\nApplying the superposition principle to a quantum mechanical particle, the configurations of the particle are all positions, so the superpositions make a complex wave in space. The coefficients of the linear superposition are a wave which describes the particle as best as is possible, and whose amplitude interferes according to the Huygens principle.\\nFor any physical property in quantum mechanics, there is a list of all the states where that property has some value. These states are necessarily perpendicular to each other using the Euclidean notion of perpendicularity which comes from sums-of-squares length, except that they also must not be i multiples of each other. This list of perpendicular states has an associated value which is the value of the physical property. The superposition principle guarantees that any state can be written as a combination of states of this form with complex coefficients.Write each state with the value q of the physical quantity as a vector in some basis \\n  \\n    \\n      \\n        \\n          ψ\\n          \\n            n\\n          \\n          \\n            q\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle \\\\psi _{n}^{q}}\\n  , a list of numbers at each value of n for the vector which has value q for the physical quantity. Now form the outer product of the vectors by multiplying all the vector components and add them with coefficients to make the matrix\\n\\n  \\n    \\n      \\n        \\n          A\\n          \\n            n\\n            m\\n          \\n        \\n        =\\n        \\n          ∑\\n          \\n            q\\n          \\n        \\n        q\\n        \\n          ψ\\n          \\n            n\\n          \\n          \\n            ∗\\n            q\\n          \\n        \\n        \\n          ψ\\n          \\n            m\\n          \\n          \\n            q\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle A_{nm}=\\\\sum _{q}q\\\\psi _{n}^{*q}\\\\psi _{m}^{q}}\\n  where the sum extends over all possible values of q. This matrix is necessarily symmetric because it is formed from the orthogonal states, and has eigenvalues q. The matrix A is called the observable associated to the physical quantity. It has the property that the eigenvalues and eigenvectors determine the physical quantity and the states which have definite values for this quantity.\\nEvery physical quantity has a Hermitian linear operator associated to it, and the states where the value of this physical quantity is definite are the eigenstates of this linear operator. The linear combination of two or more eigenstates results in quantum superposition of two or more values of the quantity. If the quantity is measured, the value of the physical quantity will be random, with a probability equal to the square of the coefficient of the superposition in the linear combination. Immediately after the measurement, the state will be given by the eigenvector corresponding to the measured eigenvalue.\\n\\n\\n== Physical interpretation ==\\nIt is natural to ask why ordinary everyday objects and events do , not seem to display quantum mechanical features such as superposition. Indeed, this is sometimes regarded as \"mysterious\", for instance by Richard Feynman. In 1935, Erwin Schrödinger devised a well-known thought experiment, now known as Schrödinger\\'s cat, which highlighted this dissonance between quantum mechanics and classical physics.  One modern view is that this mystery is explained by quantum decoherence.  A macroscopic system (such as a cat) may evolve over time into a superposition of classically distinct quantum states (such as \"alive\" and \"dead\"). The mechanism that achieves this is a subject of significant research, one mechanism suggests that the state of the cat is entangled with the state of its environment (for instance, the molecules in the atmosphere surrounding it), when averaged over the possible quantum states of the environment (a physically reasonable procedure unless the quantum state of the environment can be controlled or measured precisely) the resulting mixed quantum state for the cat is very close to a classical probabilistic state where the cat has some definite probability to be dead or alive, just as a classical observer would expect in this situation. Another proposed class of theories is that the fundamental time evolution equation is incomplete, and requires the addition of some type of fundamental Lindbladian, the reason for this addition and the form of the additional term varies from theory to theory. A popular theory is Continuous spontaneous localization, where the lindblad term is proportional to the spatial separation of the states, this too results in a quasi-classical probabilistic state.\\n\\n\\n== See also ==\\n\\n\\n== References ==\\n\\n\\n=== Bibliography of cited references ===\\nBohr, N. (1927/1928). The quantum postulate and the recent development of atomic theory, Nature Supplement 14 April 1928, 121: 580–590.\\nCohen-Tannoudji, C., Diu, B., Laloë, F. (1973/1977). Quantum Mechanics, translated from the French by S. R. Hemley, N. Ostrowsky, D. Ostrowsky, second edition, volume 1, Wiley, New York, ISBN 0471164321.\\nDirac, P. A. M. (1930/1958). The Principles of Quantum Mechanics, 4th edition, Oxford University Press.\\nEinstein, A. (1949). Remarks concerning the essays brought together in this co-operative volume, translated from the original German by the editor, pp. 665–688 in Schilpp, P. A. editor (1949), Albert Einstein: Philosopher-Scientist, volume II, Open Court, La Salle IL.\\nFeynman, R. P., Leighton, R.B., Sands, M. (1965). The Feynman Lectures on Physics, volume 3, Addison-Wesley, Reading, MA.\\nMerzbacher, E. (1961/1970). Quantum Mechanics, second edition, Wiley, New York.\\nMessiah, A. (1961). Quantum Mechanics, volume 1, translated by G.M. Temmer from the French Mécanique Quantique, North-Holland, Amsterdam.\\nWheeler, J. A.; Zurek, W.H. (1983). Quantum Theory and Measurement. Princeton NJ: Princeton University Press.', 'The history of computer science began long before our modern discipline of computer science, usually appearing in forms like mathematics or physics. Developments in previous centuries alluded to the discipline that we now know as computer science. This progression, from mechanical inventions and mathematical theories towards modern computer concepts and machines, led to the development of a major academic field, massive technological advancement across the Western world, and the basis of a massive worldwide trade and culture.\\n\\n\\n== Prehistory ==\\n\\nThe earliest known tool for use in computation was the abacus, developed in the period between 2700 and 2300 BCE in Sumer. The Sumerians\\' abacus consisted of a table of successive columns which delimited the successive orders of magnitude of their sexagesimal number system.:\\u200a11\\u200a Its original style of usage was by lines drawn in sand with pebbles. Abaci of a more modern design are still used as calculation tools today, such as the Chinese abacus.In the 5th century BC in ancient India, the grammarian Pāṇini formulated the grammar of Sanskrit in 3959 rules known as the Ashtadhyayi which was highly systematized and technical. Panini used metarules, transformations and recursions.The Antikythera mechanism is believed to be an early mechanical analog computer.  It was designed to calculate astronomical positions. It was discovered in 1901 in the Antikythera wreck off the Greek island of Antikythera, between Kythera and Crete, and has been dated to circa 100 BC.Mechanical analog computer devices appeared again a thousand years later in the medieval Islamic world and were developed by Muslim astronomers, such as the mechanical geared astrolabe by Abū Rayhān al-Bīrūnī, and the torquetum by Jabir ibn Aflah. According to Simon Singh, Muslim mathematicians also made important advances in cryptography, such as the development of cryptanalysis and frequency analysis by Alkindus. Programmable machines were also invented by Muslim engineers, such as the automatic flute player by the Banū Mūsā brothers, and Al-Jazari\\'s programmable humanoid automata and castle clock, which is considered to be the first programmable analog computer. Technological artifacts of similar complexity appeared in 14th century Europe, with mechanical astronomical clocks.When John Napier discovered logarithms for computational purposes in the early 17th century, there followed a period of considerable progress by inventors and scientists in making calculating tools. In 1623 Wilhelm Schickard designed a calculating machine, but abandoned the project, when the prototype he had started building was destroyed by a fire in 1624. Around 1640, Blaise Pascal, a leading French mathematician, constructed a mechanical adding device based on a design described by Greek mathematician Hero of Alexandria. Then in 1672 Gottfried Wilhelm Leibniz invented the Stepped Reckoner which he completed in 1694.In 1837 Charles Babbage first described his Analytical Engine which is accepted as the first design for a modern computer. The analytical engine had expandable memory, an arithmetic unit, and logic processing capabilities able to interpret a programming language with loops and conditional branching. Although never built, the design has been studied extensively and is understood to be Turing equivalent. The analytical engine would have had a memory capacity of less than 1 kilobyte of memory and a clock speed of less than 10 Hertz.Considerable advancement in mathematics and electronics theory was required before the first modern computers could be designed.\\n\\n\\n== Binary logic ==\\n\\nIn 1702, Gottfried Wilhelm Leibniz developed logic in a formal, mathematical sense with his writings on the binary numeral system. In his system, the ones and zeros also represent true and false values or on and off states. But it took more than a century before George Boole published his Boolean algebra in 1854 with a complete system that allowed computational processes to be mathematically modeled.By this time, the first mechanical devices driven by a binary pattern had been invented. The industrial revolution had driven forward the mechanization of many tasks, and this included weaving. Punched cards controlled Joseph Marie Jacquard\\'s loom in 1801, where a hole punched in the card indicated a binary one and an unpunched spot indicated a binary zero. Jacquard\\'s loom was far from being a computer, but it did illustrate that machines could be driven by binary systems.\\n\\n\\n== Emergence of a discipline ==\\n\\n\\n=== Charles Babbage and Ada Lovelace ===\\n\\nCharles Babbage is often regarded as one of the first pioneers of computing. Beginning in the 1810s, Babbage had a vision of mechanically computing numbers and tables. Putting this into reality, Babbage designed a calculator to compute numbers up to 8 decimal points long. Continuing with the success of this idea, Babbage worked to develop a machine that could compute numbers with up to 20 decimal places. By the 1830s, Babbage had devised a plan to develop a machine that could use punched cards to perform arithmetical operations. The machine would store numbers in memory units, and there would be a form of sequential control.  This means that one operation would be carried out before another in such a way that the machine would produce an answer and not fail. This machine was to be known as the “Analytical Engine”, which was the first true representation of what is the modern computer.Ada Lovelace (Augusta Ada Byron) is credited as the pioneer of computer programming and is regarded as a mathematical genius. Lovelace began working with Charles Babbage as an assistant while Babbage was working on his “Analytical Engine”, the first mechanical computer. During her work with Babbage, Ada Lovelace became the designer of the first computer algorithm, which had the ability to compute Bernoulli numbers. Moreover, Lovelace\\'s work with Babbage resulted in her prediction of future computers to not only perform mathematical calculations, but also manipulate symbols, mathematical or not. While she was never able to see the results of her work, as the “Analytical Engine” was not created in her lifetime, her efforts in later years, beginning in the 1840s, did not go unnoticed.\\n\\n\\n=== Charles Sanders Peirce and electrical switching circuits ===\\n\\nIn an 1886 letter, Charles Sanders Peirce described how logical operations could be carried out by electrical switching circuits. During 1880–81 he showed that NOR gates alone (or alternatively NAND gates alone) can be used to reproduce the functions of all the other logic gates, but this work on it was unpublished until 1933. The first published proof was by Henry M. Sheffer in 1913, so the NAND logical operation is sometimes called Sheffer stroke; the logical NOR is sometimes called Peirce\\'s arrow. Consequently, these gates are sometimes called universal logic gates.Eventually, vacuum tubes replaced relays for logic operations. Lee De Forest\\'s modification, in 1907, of the Fleming valve can be used as a logic gate. Ludwig Wittgenstein introduced a version of the 16-row truth table as proposition 5.101 of Tractatus Logico-Philosophicus (1921). Walther Bothe, inventor of the coincidence circuit, got part of the 1954 Nobel Prize in physics, for the first modern electronic AND gate in 1924. Konrad Zuse designed and built electromechanical logic gates for his computer Z1 (from 1935 to 1938).\\nUp to and during the 1930s, electrical engineers were able to build electronic circuits to solve mathematical and logic problems, but most did so in an ad hoc manner, lacking any theoretical rigor.  This changed with switching circuit theory in the 1930s. From 1934 to 1936, Akira Nakashima, Claude Shannon, and Viktor Shetakov published a series of papers showing that the two-valued Boolean algebra, can describe the operation of switching circuits. This concept, of utilizing the properties of electrical switches to do logic, is the basic concept that underlies all electronic digital computers. Switching circuit theory provided the mathematical foundations and tools for digital system design in almost all areas of modern technology.While taking an undergraduate philosophy class, Shannon had been exposed to Boole\\'s work, and recognized that it could be used to arrange electromechanical relays (then used in telephone routing switches) to solve logic problems. His thesis became the foundation of practical digital circuit design when it became widely known among the electrical engineering community during and after World War II.\\n\\n\\n=== Alan Turing and the Turing machine ===\\n\\nBefore the 1920s, computers (sometimes computors) were human clerks that performed computations. They were usually under the lead of a physicist. Many thousands of computers were employed in commerce, government, and research establishments. Many of these clerks who served as human computers were women. Some performed astronomical calculations for calendars, others ballistic tables for the military.After the 1920s, the expression computing machine referred to any machine that performed the work of a human computer, especially those in accordance with effective methods of the Church-Turing thesis. The thesis states that a mathematical method is effective if it could be set out as a list of instructions able to be followed by a human clerk with paper and pencil, for as long as necessary, and without ingenuity or insight.\\nMachines that computed with continuous values became known as the analog kind. They used machinery that represented continuous numeric quantities, like the angle of a shaft rotation or difference in electrical potential.\\nDigital machinery, in contrast to analog, were able to render a state of a numeric value and store each individual digit. Digital machinery used difference engines or relays before the invention of faster memory devices.\\nThe phrase computing machine gradually gave way, after the late 1940s, to just computer as the onset of electronic digital machinery became common. These computers were able to perform the calculations that were performed by the previous human clerks.\\nSince the values stored by digital machines were not bound to physical properties like analog devices, a logical computer, based on digital equipment, was able to do anything that could be described \"purely mechanical.\" The theoretical Turing Machine, created by Alan Turing, is a hypothetical device theorized in order to study the properties of such hardware.\\nThe mathematical foundations of modern computer science began to be laid by Kurt Gödel with his incompleteness theorem (1931). In this theorem, he showed that there were limits to what could be proved and disproved within a formal system. This led to work by Gödel and others to define and describe these formal systems, including concepts such as mu-recursive functions and lambda-definable functions.In 1936  Alan Turing and Alonzo Church independently, and also together, introduced the formalization of an algorithm, with limits on what can be computed, and a \"purely mechanical\" model for computing. This became the Church–Turing thesis, a hypothesis about the nature of mechanical calculation devices, such as electronic computers. The thesis states that any calculation that is possible can be performed by an algorithm running on a computer, provided that sufficient time and storage space are available.In 1936, Alan Turing also published his seminal work on the Turing machines, an abstract digital computing machine which is now simply referred to as the Universal Turing machine. This machine invented the principle of the modern computer and was the birthplace of the stored program concept that almost all modern day computers use. These hypothetical machines were designed to formally determine, mathematically, what can be computed, taking into account limitations on computing ability. If a Turing machine can complete the task, it is considered Turing computable.The Los Alamos physicist Stanley Frankel, has described John von Neumann\\'s view of the fundamental importance of Turing\\'s 1936 paper, in a letter:\\n I know that in or about 1943 or ‘44 von Neumann was well aware of the fundamental importance of Turing\\'s paper of 1936… Von Neumann introduced me to that paper and at his urging I studied it with care. Many people have acclaimed von Neumann as the \"father of the computer\" (in a modern sense of the term) but I am sure that he would never have made that mistake himself. He might well be called the midwife, perhaps, but he firmly emphasized to me, and to others I am sure, that the fundamental conception is owing to Turing...\\n\\n\\n=== Early computer hardware ===\\nThe world\\'s first electronic digital computer, the Atanasoff–Berry computer, was built on the Iowa State campus from 1939 through 1942 by John V. Atanasoff, a professor of physics and mathematics, and Clifford Berry, an engineering graduate student.\\nIn 1941, Konrad Zuse developed the world\\'s first functional program-controlled computer, the Z3. In 1998, it was shown to be Turing-complete in principle. Zuse also developed the S2 computing machine, considered the first process control computer. He founded one of the earliest computer businesses in 1941, producing the Z4, which became the world\\'s first commercial computer.  In 1946, he designed the first high-level programming language, Plankalkül.In 1948, the Manchester Baby was completed; it was the world\\'s first electronic digital computer that ran programs stored in its memory, like almost all modern computers. The influence on Max Newman of Turing\\'s seminal 1936 paper on the Turing Machines and of his logico-mathematical contributions to the project, were both crucial to the successful development of the Baby.In 1950, Britain\\'s National Physical Laboratory completed Pilot ACE, a small scale programmable computer, based on Turing\\'s philosophy. With an operating speed of 1 MHz, the Pilot Model ACE was for some time the fastest computer in the world. Turing\\'s design for ACE had much in common with today\\'s RISC architectures and it called for a high-speed memory of roughly the same capacity as an early Macintosh computer, which was enormous by the standards of his day. Had Turing\\'s ACE been built as planned and in full, it would have been in a different league from the other early computers.\\n\\nThe first actual computer bug was a moth. It was stuck in between the relays on the Harvard Mark II.\\nWhile the invention of the term \\'bug\\' is often but erroneously attributed to Grace Hopper, a future rear admiral in the U.S. Navy, who supposedly logged the \"bug\" on September 9, 1945, most other accounts conflict at least with these details. According to these accounts, the actual date was September 9, 1947 when operators filed this \\'incident\\' — along with the insect and the notation \"First actual case of bug being found\" (see software bug for details).\\n\\n\\n=== Shannon and information theory ===\\nClaude Shannon went on to found the field of information theory with his 1948 paper titled A Mathematical Theory of Communication, which applied probability theory to the problem of how to best encode the information a sender wants to transmit.  This work is one of the theoretical foundations for many areas of study, including data compression and cryptography.\\n\\n\\n=== Wiener and cybernetics ===\\nFrom experiments with anti-aircraft systems that interpreted radar images to detect enemy planes, Norbert Wiener coined the term cybernetics from the Greek word for \"steersman.\" He published \"Cybernetics\" in 1948, which influenced artificial intelligence. Wiener also compared computation, computing machinery, memory devices, and other cognitive similarities with his analysis of brain waves.\\n\\n\\n=== John von Neumann and the von Neumann architecture ===\\n\\nIn 1946, a model for computer architecture was introduced and became known as Von Neumann architecture. Since 1950, the von Neumann model provided uniformity in subsequent computer designs. The von Neumann architecture was considered innovative as it introduced an idea of allowing machine instructions and data to share memory space.  The von Neumann model is composed of three major parts, the arithmetic logic unit (ALU), the memory, and the instruction processing unit (IPU). In von Neumann machine design, the IPU passes addresses to memory, and memory, in turn, is routed either back to the IPU if an instruction is being fetched or to the ALU if data is being fetched.Von Neumann\\'s machine design uses a RISC (Reduced instruction set computing) architecture, which means the instruction set uses a total of 21 instructions to perform all tasks. (This is in contrast to CISC, complex instruction set computing, instruction sets which have more instructions from which to choose.)  With von Neumann architecture, main memory along with the accumulator (the register that holds the result of logical operations) are the two memories that are addressed. Operations can be carried out as simple arithmetic (these are performed by the ALU and include addition, subtraction, multiplication and division), conditional branches (these are more commonly seen now as if statements or while loops. The branches serve as go to statements), and logical moves between the different components of the machine, i.e., a move from the accumulator to memory or vice versa. Von Neumann architecture accepts fractions and instructions as data types. Finally, as the von Neumann architecture is a simple one, its register management is also simple. The architecture uses a set of seven registers to manipulate and interpret fetched data and instructions. These registers include the \"IR\" (instruction register), \"IBR\" (instruction buffer register), \"MQ\" (multiplier quotient register), \"MAR\" (memory address register), and \"MDR\" (memory data register).\"  The architecture also uses a program counter (\"PC\") to keep track of where in the program the machine is.\\n\\n\\n=== John McCarthy, Marvin Minsky and artificial intelligence ===\\nThe term artificial intelligence was credited by John McCarthy to explain the research that they were doing for a proposal for the Dartmouth Summer Research. The naming of artificial intelligence also led to the birth of a new field in computer science. On August 31, 1955, a research project was proposed consisting of John McCarthy, Marvin L. Minsky, Nathaniel Rochester, and Claude E. Shannon. The official project began in 1956 that consisted of several significant parts they felt would help them better understand artificial intelligence\\'s makeup.McCarthy and his colleagues\\' ideas behind automatic computers was while a machine is capable of completing a task, then the same should be confirmed with a computer by compiling a program to perform the desired results. They also discovered that the human brain was too complex to replicate, not by the machine itself but by the program. The knowledge to produce a program that sophisticated was not there yet.The concept behind this was looking at how humans understand our own language and structure of how we form sentences, giving different meaning and rule sets and comparing them to a machine process. The way computers can understand is at a hardware level. This language is written in binary (1s and 0\\'s). This has to be written in a specific format that gives the computer the ruleset to run a particular hardware piece.Minsky\\'s process determined how these artificial neural networks could be arranged to have similar qualities to the human brain. However, he could only produce partial results and needed to further the research into this idea.McCarthy and Shannon\\'s idea behind this theory was to develop a way to use complex problems to determine and measure the machine\\'s efficiency through mathematical theory and computations. However, they were only to receive partial test results.The idea behind self-improvement is how a machine would use self-modifying code to make itself smarter. This would allow for a machine to grow in intelligence and increase calculation speeds. The group believed they could study this if a machine could improve upon the process of completing a task in the abstractions part of their research.The group thought that research in this category could be broken down into smaller groups. This would consist of sensory and other forms of information about artificial intelligence. Abstractions in computer science can refer to mathematics and programing language.Their idea of computational creativity is how the program or a machine can be seen in having similar ways of human thinking. They wanted to see if a machine could take a piece of incomplete information and improve upon it to fill in the missing details as the human mind can do. If this machine could do this; they needed to think of how did the machine determine the outcome.\\n\\n\\n== See also ==\\nComputer Museum\\nList of computer term etymologies, the origins of computer science words\\nList of pioneers in computer science\\nHistory of computing\\nHistory of computing hardware\\nHistory of software\\nHistory of personal computers\\nTimeline of algorithms\\nTimeline of women in computing\\nTimeline of computing 2020–2029\\n\\n\\n== References ==\\n\\n\\n=== Sources ===\\nEvans, Claire L. (2018). Broad Band: The Untold Story of the Women Who Made the Internet. New York: Portfolio/Penguin. ISBN 9780735211759.\\nGrier, David Alan (2013). When Computers Were Human. Princeton: Princeton University Press. ISBN 9781400849369 – via Project MUSE.\\n\\n\\n== Further reading ==\\nTedre, Matti (2014). The Science of Computing: Shaping a Discipline. Taylor and Francis / CRC Press. ISBN 978-1-4822-1769-8.\\nKak, Subhash : Computing Science in Ancient India; Munshiram Manoharlal Publishers Pvt. Ltd (2001)\\nThe Development of Computer Science: A Sociocultural Perspective Matti Tedre\\'s Ph.D. Thesis, University of Joensuu (2006)\\nCeruzzi, Paul E. (1998). A History of a Modern Computing. The MIT Press. ISBN 978-0-262-03255-1.\\nCopeland, B. Jack. \"The Modern History of Computing\".  In Zalta, Edward N. (ed.). Stanford Encyclopedia of Philosophy.\\n\\n\\n== External links ==\\nComputer History Museum\\nComputers: From the Past to the Present\\nThe First \"Computer Bug\" at the Naval History and Heritage Command Photo Archives.\\nBitsavers, an effort to capture, salvage, and archive historical computer software and manuals from minicomputers and mainframes of the 1950s, 1960s, 1970s, and 1980s\\nOral history interviews', 'This article presents a list of individuals who made transformative breakthroughs in the creation, development and imagining of what computers could do.\\n\\n\\n== Pioneers ==\\nTo put the list in chronological order, click the small \"up-down\" icon in the Date column. The Person column can also be sorted alphabetically, up-down.~ Items marked with a tilde are circa dates.\\n\\n\\n== See also ==\\n\\nComputer Pioneer Award\\nIEEE John von Neumann Medal\\nGrace Murray Hopper Award\\nList of computer science awards\\nList of computer scientists\\nList of Internet pioneers\\nList of people considered father or mother of a field § Computing\\nList of Russian IT developers\\nList of Women in Technology International Hall of Fame inductees\\nThe Man Who Invented the Computer (2010 book)\\nTimeline of computing\\nTuring Award\\nWomen in computing\\n\\n\\n== References ==\\n\\n\\n=== Sources ===\\nHamming, Richard W. (1950). \"Error detecting and error correcting codes\" (PDF). Bell System Technical Journal. 29 (2): 147–160. doi:10.1002/j.1538-7305.1950.tb00463.x. MR 0035935. Archived from the original (PDF) on 2006-05-25.\\nLing, San; Xing, Chaoping (2004). Coding Theory: a First Course. Cambridge: Cambridge University Press. ISBN 978-0-521-82191-9.\\nPless, Vera (1982). Introduction to the Theory of Error-Correcting Codes. New York: Wiley. ISBN 978-0-471-08684-0.\\nMorgan, Samuel P. (September 1998). \"Richard Wesley Hamming (1915–1998)\" (PDF). Notices of the AMS. 45 (8): 972–977. ISSN 0002-9920. Retrieved 2014-08-30.\\n\\n\\n== External links ==\\nPioneers of Computing from the Virtual Museum of Computing\\nInternet pioneers', 'A teacher, also called a schoolteacher or formally an educator, is a person who helps students to acquire knowledge, competence or virtue.\\nInformally the role of teacher may be taken on by anyone (e.g. when showing a colleague how to perform a specific task). \\nIn some countries, teaching young people of school age may be carried out in an informal setting, such as within the family (homeschooling), rather than in a formal setting such as a school or college. \\nSome other professions may involve a significant amount of teaching (e.g. youth worker, pastor).\\nIn most countries, formal teaching of students is usually carried out by paid professional teachers. This article focuses on those who are employed, as their main role, to teach others in a formal education context, such as at a school or other place of initial formal education or training.\\n\\n\\n== Duties and functions ==\\nA teacher\\'s role may vary among cultures.\\nTeachers may provide instruction in literacy and numeracy, craftsmanship or vocational training, the arts, religion, civics, community roles, or life skills.\\nFormal teaching tasks include preparing lessons according to agreed curricula, giving lessons, and assessing pupil progress.\\nA teacher\\'s professional duties may extend beyond formal teaching. Outside of the classroom teachers may accompany students on field trips, supervise study halls, help with the organization of school functions, and serve as supervisors for extracurricular activities. \\nIn some education systems, teachers may be responsible for student discipline.\\n\\n\\n== Competences and qualities required by teachers ==\\nTeaching is a highly complex activity.\\nThis is partially because teaching is a social practice, that takes place in a specific context (time, place, culture, socio-political-economic situation etc.) and therefore is shaped by the values of that specific context. Factors that influence what is expected (or required) of teachers include history and tradition, social views about the purpose of education, accepted theories about learning, etc.\\n\\n\\n=== Competences ===\\nThe competences required by a teacher are affected by the different ways in which the role is understood around the world. Broadly, there seem to be four models: \\n\\nthe teacher as manager of instruction;\\nthe teacher as caring person;\\nthe teacher as expert learner; and\\nthe teacher as cultural and civic person.The Organisation for Economic Co-operation and Development has argued that it is necessary to develop a shared definition of the skills and knowledge required by teachers, in order to guide teachers\\' career-long education and professional development. Some evidence-based international discussions have tried to reach such a common understanding. For example, the European Union has identified three broad areas of competences that teachers require:\\n\\nWorking with others\\nWorking with knowledge, technology and information, and\\nWorking in and with society.Scholarly consensus is emerging that what is required of teachers can be grouped under three headings:\\n\\nknowledge (such as: the subject matter itself and knowledge about how to teach it, curricular knowledge, knowledge about the educational sciences, psychology, assessment etc.)\\ncraft skills (such as lesson planning, using teaching technologies, managing students and groups, monitoring and assessing learning etc.) and\\ndispositions (such as essential values and attitudes, beliefs and commitment).\\n\\n\\n=== Qualities ===\\n\\n\\n==== Enthusiasm ====\\n\\nIt has been found that teachers who showed enthusiasm towards the course materials and students can create a positive learning experience. These teachers do not teach by rote but attempt to invigorate their teaching of the course materials everyday. Teachers who cover the same curriculum repeatedly may find it challenging to maintain their enthusiasm, lest their boredom with the content bore their students in turn. Enthusiastic teachers are rated higher by their students than teachers who didn\\'t show much enthusiasm for the course materials.\\n\\nTeachers that exhibit enthusiasm are more likely to have engaged, interested and energetic students who are curious about learning the subject matter. Recent research has found a correlation between teacher enthusiasm and students\\' intrinsic motivation to learn and vitality in the classroom. Controlled, experimental studies exploring intrinsic motivation of college students has shown that nonverbal expressions of enthusiasm, such as demonstrative gesturing, dramatic movements which are varied, and emotional facial expressions, result in college students reporting higher levels of intrinsic motivation to learn. But even while a teacher\\'s enthusiasm has been shown to improve motivation and increase task engagement, it does not necessarily improve learning outcomes or memory for the material.There are various mechanisms by which teacher enthusiasm may facilitate higher levels of intrinsic motivation. \\nTeacher enthusiasm may contribute to a classroom atmosphere of energy and enthusiasm which feeds student interest and excitement in learning the subject matter. Enthusiastic teachers may also lead to students becoming more self-determined in their own learning process. The concept of mere exposure indicates that the teacher\\'s enthusiasm may contribute to the student\\'s expectations about intrinsic motivation in the context of learning.  Also, enthusiasm may act as a \"motivational embellishment\", increasing a student\\'s interest by the variety, novelty, and surprise of the enthusiastic teacher\\'s presentation of the material. Finally, the concept of emotional contagion may also apply: students may become more intrinsically motivated by catching onto the enthusiasm and energy of the teacher.\\n\\n\\n==== Interaction with learners ====\\nResearch shows that student motivation and attitudes towards school are closely linked to student-teacher relationships. Enthusiastic teachers are particularly good at creating beneficial relations with their students. Their ability to create effective learning environments that foster student achievement depends on the kind of relationship they build with their students. Useful teacher-to-student interactions are crucial in linking academic success with personal achievement. Here, personal success is a student\\'s internal goal of improving themselves, whereas academic success includes the goals they receive from their superior. A teacher must guide their student in aligning their personal goals with their academic goals. Students who receive this positive influence show stronger self-confidence and greater personal and academic success than those without these teacher interactions.Students are likely to build stronger relations with teachers who are friendly and supportive and will show more interest in courses taught by these teachers. Teachers that spend more time interacting and working directly with students are perceived as supportive and effective teachers. Effective teachers have been shown to invite student participation and decision making, allow humor into their classroom, and demonstrate a willingness to play.\\n\\n\\n== Teaching qualifications ==\\nIn many countries, a person who wishes to become a teacher must first obtain specified professional qualifications or credentials from a university or college. These professional qualifications may include the study of pedagogy, the science of teaching. \\nTeachers, like other professionals, may have to, or choose to, continue their education after they qualify, a process known as continuing professional development.\\nThe issue of teacher qualifications is linked to the status of the profession. In some societies, teachers enjoy a status on a par with physicians, lawyers, engineers, and accountants, in others, the status of the profession is low. In the twentieth century, many intelligent women were unable to get jobs in corporations or governments so many chose teaching as a default profession. As women become more welcomed into corporations and governments today, it may be more difficult to attract qualified teachers in the future.\\nTeachers are often required to undergo a course of initial education at a College of Education to ensure that they possess the necessary knowledge, competences and adhere to relevant codes of ethics.\\nThere are a variety of bodies designed to instill, preserve and update the knowledge and professional standing of teachers. Around the world many teachers\\' colleges exist; they may be controlled by government or by the teaching profession itself.\\nThey are generally established to serve and protect the public interest through certifying, governing, quality controlling, and enforcing standards of practice for the teaching profession.\\n\\n\\n=== Professional standards ===\\nThe functions of the teachers\\' colleges may include setting out clear standards of practice, providing for the ongoing education of teachers, investigating complaints involving members, conducting hearings into allegations of professional misconduct and taking appropriate disciplinary action and accrediting teacher education programs. In many situations teachers in publicly funded schools must be members in good standing with the college, and private schools may also require their teachers to be college members. In other areas these roles may belong to the State Board of Education, the Superintendent of Public Instruction, the State Education Agency or other governmental bodies. In still other areas Teaching Unions may be responsible for some or all of these duties.\\n\\n\\n==== Professional misconduct ====\\n\\nMisconduct by teachers, especially sexual misconduct, has been getting increased scrutiny from the media and the courts. A study by the American Association of University Women reported that 9.6% of students in the United States claim to have received unwanted sexual attention from an adult associated with education; be they a volunteer, bus driver, teacher, administrator or other adult; sometime during their educational career.A study in England showed a 0.3% prevalence of sexual abuse by any professional, a group that included priests, religious leaders, and case workers as well as teachers. It is important to note, however, that this British study is the only one of its kind and consisted of \"a random ... probability sample of 2,869 young people between the ages of 18 and 24 in a computer-assisted study\" and that the questions referred to \"sexual abuse with a professional,\" not necessarily a teacher. It is therefore logical to conclude that information on the percentage of abuses by teachers in the United Kingdom is not explicitly available and therefore not necessarily reliable. The AAUW study, however, posed questions about fourteen types of sexual harassment and various degrees of frequency and included only abuses by teachers. \"The sample was drawn from a list of 80,000 schools to create a stratified two-stage sample design of 2,065 8th to 11th grade students\". Its reliability was gauged at 95% with a 4% margin of error.\\nIn the United States especially, several high-profile cases such as Debra LaFave, Pamela Rogers Turner, and Mary Kay Letourneau have caused increased scrutiny on teacher misconduct.\\nChris Keates, the general secretary of National Association of Schoolmasters Union of Women Teachers, said that teachers who have sex with pupils over the age of consent should not be placed on the sex offenders register and that prosecution for statutory rape \"is a real anomaly in the law that we are concerned about.\" This has led to outrage from child protection and parental rights groups. Fears of being labelled a pedophile or hebephile has led to several men who enjoy teaching avoiding the profession. This has in some jurisdictions reportedly led to a shortage of male teachers.\\n\\n\\n== Pedagogy and teaching ==\\n\\nTeachers facilitate student learning, often in a school or academy or perhaps in another environment such as outdoors.\\n\\nThe objective is typically accomplished through either an informal or formal approach to learning, including a course of study and lesson plan that teaches skills, knowledge or thinking skills. Different ways to teach are often referred to as pedagogy. When deciding what teaching method to use teachers consider students\\' background knowledge, environment, and their learning goals as well as standardized curricula as determined by the relevant authority. Many times, teachers assist in learning outside of the classroom by accompanying students on field trips. The increasing use of technology, specifically the rise of the internet over the past decade, has begun to shape the way teachers approach their roles in the classroom.\\nThe objective is typically a course of study, lesson plan, or a practical skill. A teacher may follow standardized curricula as determined by the relevant authority. The teacher may interact with students of different ages, from infants to adults, students with different abilities and students with learning disabilities.\\nTeaching using pedagogy also involve assessing the educational levels of the students on particular skills. Understanding the pedagogy of the students in a classroom involves using differentiated instruction as well as supervision to meet the needs of all students in the classroom. Pedagogy can be thought of in two manners. First, teaching itself can be taught in many different ways, hence, using a pedagogy of teaching styles. Second, the pedagogy of the learners comes into play when a teacher assesses the pedagogic diversity of their students and differentiates for the individual students accordingly. For example, an experienced teacher and parent described the place of a teacher in learning as follows: \"The real bulk of learning takes place in self-study and problem solving with a lot of feedback around that loop. The function of the teacher is to pressure the lazy, inspire the bored, deflate the cocky, encourage the timid, detect and correct individual flaws, and broaden the viewpoint of all. This function looks like that of a coach using the whole gamut of psychology to get each new class of rookies off the bench and into the game.\"Perhaps the most significant difference between primary school and secondary school teaching is the relationship between teachers and children. In primary schools each class has a teacher who stays with them for most of the week and will teach them the whole curriculum. In secondary schools they will be taught by different subject specialists each session during the week and may have ten or more different teachers. The relationship between children and their teachers tends to be closer in the primary school where they act as form tutor, specialist teacher and surrogate parent during the course of the day.\\nThis is true throughout most of the United States as well. However, alternative approaches for primary education do exist. One of these, sometimes referred to as a \"platoon\" system, involves placing a group of students together in one class that moves from one specialist to another for every subject. The advantage here is that students learn from teachers who specialize in one subject and who tend to be more knowledgeable in that one area than a teacher who teaches many subjects. Students still derive a strong sense of security by staying with the same group of peers for all classes.\\nCo-teaching has also become a new trend amongst educational institutions. Co-teaching is defined as two or more teachers working harmoniously to fulfill the needs of every student in the classroom. Co-teaching focuses the student on learning by providing a social networking support that allows them to reach their full cognitive potential. Co-teachers work in sync with one another to create a climate of learning.\\n\\n\\n=== Classroom management ===\\n\\n\\n==== Teachers and school discipline ====\\n\\nThroughout the history of education the most common form of school discipline was corporal punishment. While a child was in school, a teacher was expected to act as a substitute parent, with all the normal forms of parental discipline open to them. \\nIn past times, corporal punishment (spanking or paddling or caning or strapping or birching the student in order to cause physical pain) was one of the most common forms of school discipline throughout much of the world. Most Western countries, and some others, have now banned it, but it remains lawful in the United States following a US Supreme Court decision in 1977 which held that paddling did not violate the US Constitution.30 US states have banned corporal punishment, the others (mostly in the South) have not. It is still used to a significant (though declining) degree in some public schools in Alabama, Arkansas, Georgia, Louisiana, Mississippi, Oklahoma, Tennessee and Texas. Private schools in these and most other states may also use it. Corporal punishment in American schools is administered to the seat of the student\\'s trousers or skirt with a specially made wooden paddle. This often used to take place in the classroom or hallway, but nowadays the punishment is usually given privately in the principal\\'s office.\\nOfficial corporal punishment, often by caning, remains commonplace in schools in some Asian, African and Caribbean countries.\\nCurrently detention is one of the most common punishments in schools in the United States, the UK, Ireland, Singapore and other countries. It requires the pupil to remain in school at a given time in the school day (such as lunch, recess or after school); or even to attend school on a non-school day, e.g. \"Saturday detention\" held at some schools. During detention, students normally have to sit in a classroom and do work, write lines or a punishment essay, or sit quietly.\\nA modern example of school discipline in North America and Western Europe relies upon the idea of an assertive teacher who is prepared to impose their will upon a class. Positive reinforcement is balanced with immediate and fair punishment for misbehavior and firm, clear boundaries define what is appropriate and inappropriate behavior. Teachers are expected to respect their students; sarcasm and attempts to humiliate pupils are seen as falling outside of what constitutes reasonable discipline.Whilst this is the consensus viewpoint amongst the majority of academics, some teachers and parents advocate a more assertive and confrontational style of discipline (refer to Canter Model of Discipline). Such individuals claim that many problems with modern schooling stem from the weakness in school discipline and if teachers exercised firm control over the classroom they would be able to teach more efficiently. This viewpoint is supported by the educational attainment of countries—in East Asia for instance—that combine strict discipline with high standards of education.It\\'s not clear, however that this stereotypical view reflects the reality of East Asian classrooms or that the educational goals in these countries are commensurable with those in Western countries. In Japan, for example, although average attainment on standardized tests may exceed those in Western countries, classroom discipline and behavior is highly problematic. Although, officially, schools have extremely rigid codes of behavior, in practice many teachers find the students unmanageable and do not enforce discipline at all.\\nWhere school class sizes are typically 40 to 50 students, maintaining order in the classroom can divert the teacher from instruction, leaving little opportunity for concentration and focus on what is being taught. In response, teachers may concentrate their attention on motivated students, ignoring attention-seeking and disruptive students. The result of this is that motivated students, facing demanding university entrance examinations, receive disproportionate resources. Given the emphasis on attainment of university places, administrators and governors may regard this policy as appropriate.\\n\\n\\n==== Obligation to honor students rights ====\\n\\nSudbury model democratic schools claim that popularly based authority can maintain order more effectively than dictatorial authority for governments and schools alike. They also claim that in these schools the preservation of public order is easier and more efficient than anywhere else. Primarily because rules and regulations are made by the community as a whole, thence the school atmosphere is one of persuasion and negotiation, rather than confrontation since there is no one to confront. Sudbury model democratic schools\\' proponents argue that a school that has good, clear laws, fairly and democratically passed by the entire school community, and a good judicial system for enforcing these laws, is a school in which community discipline prevails, and in which an increasingly sophisticated concept of law and order develops, against other schools today, where rules are arbitrary, authority is absolute, punishment is capricious, and due process of law is unknown.\\n\\n\\n== Occupational hazards ==\\n\\nTeachers face several occupational hazards in their line of work, including occupational stress, which can negatively impact teachers\\' mental and physical health, productivity, and students\\' performance. Stress can be caused by organizational change, relationships with students, fellow teachers, and administrative personnel, working environment, expectations to substitute, long hours with a heavy workload, and inspections. Teachers are also at high risk for occupational burnout.A 2000 study found that 42% of UK teachers experienced occupational stress, twice the figure for the average profession. A 2012 study found that teachers experienced double the rate of anxiety, depression, and stress than average workers.There are several ways to mitigate the occupational hazards of teaching. Organizational interventions, like changing teachers\\' schedules, providing support networks and mentoring, changing the work environment, and offering promotions and bonuses, may be effective in helping to reduce occupational stress among teachers. Individual-level interventions, including stress-management training and counseling, are also used to relieve occupational stress among teachers.Apart from this, teachers are often not given sufficient opportunities for professional growth or promotions. This leads to some stagnancy, as there is not sufficient interests to enter the profession. An organisation in India called Centre for Teacher Accreditation (CENTA) is working to reduce this hazard, by trying to open opportunities for teachers in India.\\n\\n\\n== Teaching around the world ==\\n\\nThere are many similarities and differences among teachers around the world. In almost all countries teachers are educated in a university or college. Governments may require certification by a recognized body before they can teach in a school. In many countries, elementary school education certificate is earned after completion of high school. The high school student follows an education specialty track, obtain the prerequisite \"student-teaching\" time, and receive a special diploma to begin teaching after graduation. In addition to certification, many educational institutions especially within the US, require that prospective teachers pass a background check and psychiatric evaluation to be able to teach in classroom. This is not always the case with adult further learning institutions but is fast becoming the norm in many countries as security concerns grow.\\nInternational schools generally follow an English-speaking, Western curriculum and are aimed at expatriate communities.\\n\\n\\n=== Australia ===\\n\\nEducation in Australia is primarily the responsibility of the individual states and territories. Generally, education in Australia follows the three-tier model which includes primary education (primary schools), followed by secondary education (secondary schools/high schools) and tertiary education (universities or TAFE colleges).\\n\\n\\n=== Canada ===\\n\\nTeaching in Canada requires a post-secondary degree Bachelor\\'s Degree. In most provinces a second Bachelor\\'s Degree such as a Bachelor of Education is required to become a qualified teacher. Salary ranges from $40,000/year to $90,000/yr. Teachers have the option to teach for a public school which is funded by the provincial government or teaching in a private school which is funded by the private sector, businesses and sponsors.\\n\\n\\n=== France ===\\n\\nIn France, teachers, or professors, are mainly civil servants, recruited by competitive examination.\\n\\n\\n=== Germany ===\\n\\nIn Germany, teachers are mainly civil servants recruited in special university classes, called Lehramtstudien (Teaching Education Studies). There are many differences between the teachers for elementary schools (Grundschule), lower secondary schools (Hauptschule), middle level secondary schools (Realschule) and higher level secondary schools (Gymnasium).\\nSalaries for teachers depend on the civil servants\\' salary index scale (Bundesbesoldungsordnung).\\n\\n\\n=== India ===\\n\\nIn ancient India, the most common form of education was gurukula based on the guru-shishya tradition (teacher-disciple tradition) which involved the disciple and guru living in the same (or a nearby) residence. These gurukulam was supported by public donations and the guru would not accept any fees from the shishya. This organized system stayed the most prominent form of education in the Indian subcontinent until the British invasion. Through strong efforts in 1886 and 1948, the gurukula system was revived in India.The role and success of a teacher in the modern Indian education system is clearly defined. CENTA Standards define the competencies that a good teacher should possess. Schools look for competent teachers across grades. Teachers are appointed directly by schools in private sector, and through eligibility tests in government schools.\\n\\n\\n=== Ireland ===\\n\\nSalaries for primary teachers in Ireland depend mainly on seniority (i.e. holding the position of principal, deputy principal or assistant principal), experience and qualifications. Extra pay is also given for teaching through the Irish language, in a Gaeltacht area or on an island. The basic pay for a starting teacher is €27,814 p.a., rising incrementally to €53,423 for a teacher with 25 years service. A principal of a large school with many years experience and several qualifications (M.A., H.Dip., etc.) could earn over €90,000.Teachers are required to be registered with the Teaching Council; under Section 30 of the Teaching Council Act 2001, a person employed in any capacity in a recognised teaching post - who is not registered with the Teaching Council - may not be paid from Oireachtas funds.From 2006 Garda vetting has been introduced for new entrants to the teaching profession. These procedures apply to teaching and also to non-teaching posts and those who refuse vetting \"cannot be appointed or engaged by the school in any capacity including in a voluntary role\". Existing staff will be vetted on a phased basis.\\n\\n\\n=== United Kingdom ===\\n\\nEducation in the United Kingdom is a devolved matter with each of the countries of the United Kingdom having separate systems.\\n\\n\\n==== England ====\\n\\nSalaries for nursery, primary and secondary school teachers ranged from £20,133 to £41,004 in September 2007, although some salaries can go much higher depending on experience and extra responsibilities. Preschool teachers may earn an average salary of £19,543 annually. Teachers in state schools must have at least a bachelor\\'s degree, complete an approved teacher education program, and be licensed.\\nMany counties offer alternative licensing programs to attract people into teaching, especially for hard-to-fill positions. Excellent job opportunities are expected as retirements, especially among secondary school teachers, outweigh slowing enrollment growth; opportunities will vary by geographic area and subject taught.\\n\\n\\n==== Scotland ====\\n\\nIn Scotland, anyone wishing to teach must be registered with the General Teaching Council for Scotland (GTCS). Teaching in Scotland is an all graduate profession and the normal route for graduates wishing to teach is to complete a programme of Initial Teacher Education (ITE) at one of the seven Scottish Universities who offer these courses. Once successfully completed, \"Provisional Registration\" is given by the GTCS which is raised to \"Full Registration\" status after a year if there is sufficient evidence to show that the \"Standard for Full Registration\" has been met.For the salary year beginning April 2008, unpromoted teachers in Scotland earned from £20,427 for a Probationer, up to £32,583 after 6 years teaching, but could then go on to earn up to £39,942 as they complete the modules to earn Chartered Teacher Status (requiring at least 6 years at up to two modules per year.) Promotion to Principal Teacher positions attracts a salary of between £34,566 and £44,616; Deputy Head, and Head teachers earn from £40,290 to £78,642.\\nTeachers in Scotland can be registered members of trade unions with the main ones being the Educational Institute of Scotland and the Scottish Secondary Teachers\\' Association.\\n\\n\\n==== Wales ====\\n\\nEducation in Wales differs in certain respects from education elsewhere in the United Kingdom. For example, a significant number of students all over Wales are educated either wholly or largely through the medium of Welsh: in 2008/09, 22 per cent of classes in maintained primary schools used Welsh as the sole or main medium of instruction. Welsh medium education is available to all age groups through nurseries, schools, colleges and universities and in adult education; lessons in the language itself are compulsory for all pupils until the age of 16.\\nTeachers in Wales can be registered members of trade unions such as ATL, NUT or NASUWT and reports in recent years suggest that the average age of teachers in Wales is falling with teachers being younger than in previous years. A growing cause of concern are that attacks on teachers in Welsh schools which reached an all-time high between 2005 and 2010.\\n\\n\\n=== United States ===\\n\\nIn the United States, each state determines the requirements for getting a license to teach in public schools. Teaching certification generally lasts three years, but teachers can receive certificates that last as long as ten years. Public school teachers are required to have a bachelor\\'s degree and the majority must be certified by the state in which they teach. Many charter schools do not require that their teachers be certified, provided they meet the standards to be highly qualified as set by No Child Left Behind. Additionally, the requirements for substitute/temporary teachers are generally not as rigorous as those for full-time professionals. The Bureau of Labor Statistics estimates that there are 1.4 million elementary school teachers, 674,000 middle school teachers, and 1 million secondary school teachers employed in the U.S.In the past, teachers have been paid relatively low salaries. However, average teacher salaries have improved rapidly in recent years. US teachers are generally paid on graduated scales, with income depending on experience. Teachers with more experience and higher education earn more than those with a standard bachelor\\'s degree and certificate. Salaries vary greatly depending on state, relative cost of living, and grade taught. Salaries also vary within states where wealthy suburban school districts generally have higher salary schedules than other districts. The median salary for all primary and secondary teachers was $46,000 in 2004, with the average entry salary for a teacher with a bachelor\\'s degree being an estimated $32,000. Median salaries for preschool teachers, however, were less than half the national median for secondary teachers, clock in at an estimated $21,000 in 2004. For high school teachers, median salaries in 2007 ranged from $35,000 in South Dakota to $71,000 in New York, with a national median of $52,000. Some contracts may include long-term disability insurance, life insurance, emergency/personal leave and investment options.The American Federation of Teachers\\' teacher salary survey for the 2006–07 school year found that the average teacher salary was $51,009. In a salary survey report for K-12 teachers, elementary school teachers had the lowest median salary earning $39,259. High school teachers had the highest median salary earning $41,855. Many teachers take advantage of the opportunity to increase their income by supervising after-school programs and other extracurricular activities. In addition to monetary compensation, public school teachers may also enjoy greater benefits (like health insurance) compared to other occupations. Merit pay systems are on the rise for teachers, paying teachers extra money based on excellent classroom evaluations, high test scores and for high success at their overall school. Also, with the advent of the internet, many teachers are now selling their lesson plans to other teachers through the web in order to earn supplemental income, most notably on TeachersPayTeachers.com. The United Nations Sustainable Development Goal 4 also aims to substantially increase the supply of qualified teachers through international cooperation by 2030 in an effort to improve the quality of teaching around the world.\\n\\n\\n== Assistant teachers ==\\nAssistant teachers are additional teachers assisting the primary teacher, often in the same classroom.  There are different types around the world, as well as a variety of formal programs defining roles and responsibilities.\\nOne type is a Foreign Language Assistant, which in Germany is run by the Educational Exchange Service (Pädagogischer Austauschdienst).\\nBritish schools employ teaching assistants, who are not considered fully qualified teachers, and as such, are guided by teachers but may supervise and teach groups of pupils independently. In the United Kingdom, the term \"assistant teacher\" used to be used to refer to any qualified or unqualified teacher who was not a head or deputy head teacher.The Japanese education system employs Assistant Language Teachers in elementary, junior high and high schools.\\nLearning by teaching (German short form: LdL) is a method which allows pupils and students to prepare and teach lessons or parts of lessons, with the understanding that a student\\'s own learning is enhanced through the teaching process.\\n\\n\\n== See also ==\\n\\n\\n== References ==\\n\\n\\n== External links ==\\nOECD\\'s Education GPS, a review of education policy analysis and statistics: Teachers', 'Software engineering is the systematic application of engineering approaches to the development of software.A software engineer is a person who applies the principles of software engineering to design, develop, maintain, test, and evaluate computer software. The term programmer is sometimes used as a synonym, but may also lack connotations of engineering education or skills.\\nEngineering techniques are used to inform the software development process which involves the definition, implementation, assessment, measurement, management, change, and improvement of the software life cycle process itself. It heavily uses software configuration management which is about systematically controlling changes to the configuration, and maintaining the integrity and traceability of the configuration and code throughout the system life cycle. Modern processes use software versioning.\\n\\n\\n== History ==\\n\\nBeginning in the 1960s, software engineering was seen as its own type of engineering. Additionally, the development of software engineering was seen as a struggle. It was difficult to keep up with the hardware which caused many problems for software engineers. Problems included software that was over budget, exceeded deadlines, required extensive de-bugging and maintenance, and unsuccessfully met the needs of consumers or was never even completed. In 1968 NATO held the first Software Engineering conference where issues related to software were addressed: guidelines and best practices for the development of software were established. The origins of the term \"software engineering\" have been attributed to various sources. The term \"software engineering\" appeared in a list of services offered by companies in the June 1965 issue of COMPUTERS and AUTOMATION and was used more formally in the August 1966 issue of Communications of the ACM (Volume 9, number 8) “letter to the ACM membership” by the ACM President Anthony A. Oettinger,  it is also associated with the title of a NATO conference in 1968 by Professor Friedrich L. Bauer, the first conference on software engineering. Independently, Margaret Hamilton named the discipline \"software engineering\" during the Apollo missions to give what they were doing legitimacy.  At the time there was perceived to be a \"software crisis\". The 40th International Conference on Software Engineering (ICSE 2018) celebrates 50 years of \"Software Engineering\" with the Plenary Sessions\\' keynotes of Frederick Brooks and Margaret Hamilton.In 1984, the Software Engineering Institute (SEI) was established as a federally funded research and development center headquartered on the campus of Carnegie Mellon University in Pittsburgh, Pennsylvania, United States. Watts Humphrey founded the SEI Software Process Program, aimed at understanding and managing the software engineering process.  The Process Maturity Levels introduced would become the Capability Maturity Model Integration for Development(CMMI-DEV), which has defined how the US Government evaluates the abilities of a software development team.\\nModern, generally accepted best-practices for software engineering have been collected by the ISO/IEC JTC 1/SC 7 subcommittee and published as the Software Engineering Body of Knowledge (SWEBOK). Software engineering is considered one of major computing disciplines.\\n\\n\\n== Definitions and terminology controversies ==\\nNotable definitions of software engineering include:\\n\\n\"The systematic application of scientific and technological knowledge, methods, and experience to the design, implementation, testing, and documentation of software\"—The Bureau of Labor Statistics—IEEE Systems and software engineering – Vocabulary\\n\"The application of a systematic, disciplined, quantifiable approach to the development, operation, and maintenance of software\"—IEEE Standard Glossary of Software Engineering Terminology\\n\"an engineering discipline that is concerned with all aspects of software production\"—Ian Sommerville\\n\"the establishment and use of sound engineering principles in order to economically obtain software that is reliable and works efficiently on real machines\"—Fritz Bauer\\n\"a branch of computer science that deals with the design, implementation, and maintenance of complex computer programs\"—Merriam-Webster\\n\"\\'software engineering\\' encompasses not just the act of writing code, but all of the tools and processes an organization uses to build and maintain that code over time. [...] Software engineering can be thought of as \\'programming integrated over time.\\'\"—Software Engineering at GoogleThe term has also been used less formally:\\n\\nas the informal contemporary term for the broad range of activities that were formerly called computer programming and systems analysis;\\nas the broad term for all aspects of the practice of computer programming, as opposed to the theory of computer programming, which is formally studied as a sub-discipline of computer science;\\nas the term embodying the advocacy of a specific approach to computer programming, one that urges that it be treated as an engineering discipline rather than an art or a craft, and advocates the codification of recommended practices.\\n\\n\\n=== Etymology of \"software engineer\" ===\\nMargaret Hamilton promoted the term \"software engineering\" during her work on the Apollo program. The term \"engineering\" was used to acknowledge that the work should be taken just as seriously as other contributions toward the advancement of technology. Hamilton details her use of the term:When I first came up with the term, no one had heard of it before, at least in our world. It was an ongoing joke for a long time. They liked to kid me about my radical ideas. It was a memorable day when one of the most respected hardware gurus explained to everyone in a meeting that he agreed with me that the process of building software should also be considered an engineering discipline, just like with hardware. Not because of his acceptance of the new \"term\" per se, but because we had earned his and the acceptance of the others in the room as being in an engineering field in its own right.\\n\\n\\n=== Suitability of the term ===\\nIndividual commentators have disagreed sharply on how to define software engineering or its legitimacy as an engineering discipline. David Parnas has said that software engineering is, in fact, a form of engineering. Steve McConnell has said that it is not, but that it should be. Donald Knuth has said that programming is an art and a science. Edsger W. Dijkstra claimed that the terms software engineering and software engineer have been misused  and should be considered harmful, particularly in the United States.\\n\\n\\n== Tasks in large scale projects ==\\n\\n\\n=== Software requirements ===\\n\\nRequirements engineering is about the elicitation, analysis, specification, and validation of requirements for software. Software requirements can be of three different types. There are functional requirements, non-functional requirements, and domain requirements. The operation of the software should be performed and the proper output should be expected for the user to use. Non-functional requirements deal with issues like portability, security, maintainability, reliability, scalability, performance, reusability, and flexibility. They are classified into the following types: interference constraints, performance constraints (such as response time, security, storage space, etc.), operating constraints, life cycle constraints (maintainability, portability, etc.), and economic constraints. Knowledge of how the system or software works is needed when it comes to specifying non-functional requirements. Domain requirements have to do with the characteristic of a certain category or domain of projects.\\n\\n\\n=== Software design ===\\n\\nSoftware design is about the process of defining the architecture, components, interfaces, and other characteristics of a system or component. This is also called software architecture. Software design is divided into three different levels of design. The three levels are interface design, architectural design, and detailed design. Interface design is the interaction between a system and its environment. This happens at a high level of abstraction along with the inner workings of the system. Architectural design has to do with the major components of a system and their responsibilities, properties, interfaces, and their relationships and interactions that occur between them. Detailed design is the internal elements of all the major system components, their properties, relationships, processing, and usually their algorithms and the data structures. \\n\\n\\n=== Software construction ===\\n\\nSoftware construction, the main activity of software development, is the combination of programming, unit testing, integration testing, and debugging. Testing during this phase is generally performed by the programmer while the software is under construction, to verify what was just written and decide when the code is ready to be sent to the next step.\\n\\n\\n=== Software testing ===\\n\\nSoftware testing is an empirical, technical investigation conducted to provide stakeholders with information about the quality of the product or service under test, with different approaches such as unit testing and integration testing. It is one aspect of software quality. As a separate phase in software development, it is typically performed by quality assurance staff or a developer other than the one who wrote the code.\\n\\n\\n=== Software maintenance ===\\n\\nSoftware maintenance refers to the activities required to provide cost-effective support after shipping the software product. Software maintenance is modifying and updating software applications after distribution to correct faults and to improve its performance. Software has a lot to do with the real world and when the real world changes, software maintenance is required. Software maintenance includes: error correction, optimization, deletion of unused and discarded features, and enhancement of features that already exist. Usually, maintenance takes up about 40% to 80% of the project cost therefore, focusing on maintenance keeps the costs down.\\n\\n\\n== Education ==\\nKnowledge of computer programming is a prerequisite for becoming a software engineer. In 2004 the IEEE Computer Society produced the SWEBOK, which has been published as ISO/IEC Technical Report 1979:2005, describing the body of knowledge that they recommend to be mastered by a graduate software engineer with four years of experience.\\nMany software engineers enter the profession by obtaining a university degree or training at a vocational school. One standard international curriculum for undergraduate software engineering degrees was defined by the Joint Task Force on Computing Curricula of the IEEE Computer Society and the Association for Computing Machinery, and updated in 2014. A number of universities have Software Engineering degree programs; as of 2010, there were 244 Campus Bachelor of Software Engineering programs, 70 Online programs, 230 Masters-level programs, 41 Doctorate-level programs, and 69 Certificate-level programs in the United States.\\nIn addition to university education, many companies sponsor internships for students wishing to pursue careers in information technology. These internships can introduce the student to interesting real-world tasks that typical software engineers encounter every day. Similar experience can be gained through military service in software engineering.\\n\\n\\n=== Software engineering degree programs ===\\nHalf of all practitioners today have degrees in computer science, information systems, or information technology. A small, but growing, number of practitioners have software engineering degrees. In 1987, the Department of Computing at Imperial College London introduced the first three-year software engineering Bachelor\\'s degree in the UK and the world; in the following year, the University of Sheffield established a similar program.  In 1996, the Rochester Institute of Technology established the first software engineering bachelor\\'s degree program in the United States, however, it did not obtain ABET accreditation until 2003, the same time as Rice University, Clarkson University, Milwaukee School of Engineering and Mississippi State University obtained theirs. In 1997, PSG College of Technology in Coimbatore, India was the first to start a five-year integrated Master of Science degree in Software Engineering.Since then, software engineering undergraduate degrees have been established at many universities. A standard international curriculum for undergraduate software engineering degrees, SE2004, was defined by a steering committee between 2001 and 2004 with funding from the Association for Computing Machinery and the IEEE Computer Society. As of 2004, in the U.S., about 50 universities offer software engineering degrees, which teach both computer science and engineering principles and practices. The first software engineering Master\\'s degree was established at Seattle University in 1979. Since then graduate software engineering degrees have been made available from many more universities.  Likewise in Canada, the Canadian Engineering Accreditation Board (CEAB) of the Canadian Council of Professional Engineers has recognized several software engineering programs.\\nIn 1998, the US Naval Postgraduate School (NPS) established the first doctorate program in Software Engineering in the world. Additionally, many online advanced degrees in Software Engineering have appeared such as the Master of Science in Software Engineering (MSE) degree offered through the Computer Science and Engineering Department at California State University, Fullerton. Steve McConnell opines that because most universities teach computer science rather than software engineering, there is a shortage of true software engineers. ETS (École de technologie supérieure) University and UQAM (Université du Québec à Montréal) were mandated by IEEE to develop the Software Engineering Body of Knowledge (SWEBOK), which has become an ISO standard describing the body of knowledge covered by a software engineer.\\n\\n\\n== Profession ==\\n\\nLegal requirements for the licensing or certification of professional software engineers vary around the world. In the UK, there is no licensing or legal requirement to assume or use the job title Software Engineer.  In some areas of Canada, such as Alberta, British Columbia, Ontario, and Quebec, software engineers can hold the Professional Engineer (P.Eng) designation and/or the Information Systems Professional (I.S.P.) designation. In Europe, Software Engineers can obtain the European Engineer (EUR ING) professional title.\\nThe United States, since 2013, has offered an NCEES Professional Engineer exam for Software Engineering, thereby allowing Software Engineers to be licensed and recognized. NCEES will end the exam after April 2019 due to lack of participation. Mandatory licensing is currently still largely debated, and perceived as controversial. In some parts of the US such as Texas, the use of the term Engineer is regulated by law and reserved only for use by individuals who have a Professional Engineer license.The IEEE Computer Society and the ACM, the two main US-based professional organizations of software engineering, publish guides to the profession of software engineering. The IEEE\\'s Guide to the Software Engineering Body of Knowledge – 2004 Version, or SWEBOK, defines the field and describes the knowledge the IEEE expects a practicing software engineer to have. The most current SWEBOK v3 is an updated version and was released in 2014. The IEEE also promulgates a \"Software Engineering Code of Ethics\".\\n\\n\\n=== Employment ===\\n\\nThe U. S. Bureau of Labor Statistics (BLS) counted 1,365,500 software developers holding jobs in the U.S. in 2018. Due to its relative newness as a field of study, formal education in software engineering is often taught as part of a computer science curriculum, and many software engineers hold computer science degrees. The BLS estimates from 2014 to 2024 that computer software engineering would increase by 17% . This is down from the 2012 to 2022 BLS estimate of 22% for software engineering. And, is further down from their 30% 2010 to 2020 BLS estimate. Due to this trend, job growth may not be as fast as during the last decade, as jobs that would have gone to computer software engineers in the United States would instead be outsourced to computer software engineers in countries such as India and other foreign countries. In addition, the BLS Job Outlook for Computer Programmers, 2014–24 predicts an −8% (a decline, in their words), then a decline in the Job Outlook, 2019-29 of -9%, and a 10% decline for 2020-2030 for those who program computers. Furthermore, women in many software fields has also been declining over the years as compared to other engineering fields. However, this trend may change or slow in the future as many current software engineers in the U.S. market leave the profession or  age out of the market in the next few decades.\\nMany software engineers work as employees or contractors. Software engineers work with businesses, government agencies (civilian or military), and non-profit organizations. Some software engineers work for themselves as freelancers. Some organizations have specialists to perform each of the tasks in the software development process. Other organizations require software engineers to do many or all of them. In large projects, people may specialize in only one role. In small projects, people may fill several or all roles at the same time. Many companies hire interns, often university or college students during a summer break, or externships. Specializations include analysts, architects, developers, testers, technical support, middleware analysts, project managers, educators, and researchers.\\nMost software engineers and programmers work 40 hours a week, but about 15 percent of software engineers and 11 percent of programmers worked more than 50 hours a week in 2008. Potential injuries in these occupations are possible because like other workers who spend long periods sitting in front of a computer terminal typing at a keyboard, engineers and programmers are susceptible to eyestrain, back discomfort, and hand and wrist problems such as carpal tunnel syndrome.\\n\\n\\n=== Certification ===\\nThe Software Engineering Institute offers certifications on specific topics like security, process improvement and software architecture. IBM, Microsoft and other companies also sponsor their own certification examinations. Many IT certification programs are oriented toward specific technologies, and managed by the vendors of these technologies. These certification programs are tailored to the institutions that would employ people who use these technologies.\\nBroader certification of general software engineering skills is available through various professional societies. As of 2006, the IEEE had certified over 575 software professionals as a Certified Software Development Professional (CSDP). In 2008 they added an entry-level certification known as the Certified Software Development Associate (CSDA). The ACM had a professional certification program in the early 1980s, which was discontinued due to lack of interest. The ACM examined the possibility of professional certification of software engineers in the late 1990s, but eventually decided that such certification was inappropriate for the professional industrial practice of software engineering.In the U.K. the British Computer Society has developed a legally recognized professional certification called Chartered IT Professional (CITP), available to fully qualified members (MBCS). Software engineers may be eligible for membership of the Institution of Engineering and Technology and so qualify for Chartered Engineer status. In Canada the Canadian Information Processing Society has developed a legally recognized professional certification called Information Systems Professional (ISP). In Ontario, Canada, Software Engineers who graduate from a Canadian Engineering Accreditation Board (CEAB) accredited program, successfully complete PEO\\'s (Professional Engineers Ontario) Professional Practice Examination (PPE) and have at least 48 months of acceptable engineering experience are eligible to be licensed through the Professional Engineers Ontario and can become Professional Engineers P.Eng. The PEO does not recognize any online or distance education however; and does not consider Computer Science programs to be equivalent to software engineering programs despite the tremendous overlap between the two. This has sparked controversy and a certification war. It has also held the number of P.Eng holders for the profession exceptionally low. The vast majority of working professionals in the field hold a degree in CS, not SE. Given the difficult certification path for holders of non-SE degrees, most never bother to pursue the license.\\n\\n\\n=== Impact of globalization ===\\nThe initial impact of outsourcing, and the relatively lower cost of international human resources in developing third world countries led to a massive migration of software development activities from corporations in North America and Europe to India and later: China, Russia, and other developing countries. This approach had some flaws, mainly the distance / time zone difference that prevented human interaction between clients and developers and the massive job transfer. This had a negative impact on many aspects of the software engineering profession. For example, some students in the developed world avoid education related to software engineering because of the fear of offshore outsourcing (importing software products or services from other countries) and of being displaced by foreign visa workers. Although statistics do not currently show a threat to software engineering itself; a related career, computer programming does appear to have been affected. Nevertheless, the ability to smartly leverage offshore and near-shore resources via the follow-the-sun workflow has improved the overall operational capability of many organizations. When North Americans are leaving work, Asians are just arriving to work. When Asians are leaving work, Europeans are arriving to work. This provides a continuous ability to have human oversight on business-critical processes 24 hours per day, without paying overtime compensation or disrupting a key human resource, sleep patterns.\\nWhile global outsourcing has several advantages, global – and generally distributed – development can run into serious difficulties resulting from the distance between developers. This is due to the key elements of this type of distance that have been identified as geographical, temporal, cultural and communication (that includes the use of different languages and dialects of English in different locations). Research has been carried out in the area of global software development over the last 15 years and an extensive body of relevant work published that highlights the benefits and problems associated with the complex activity. As with other aspects of software engineering research is ongoing in this and related areas.\\n\\n\\n=== Prizes ===\\nThere are several prizes in the field of software engineering:\\nThe Codie awards is a yearly award issued by the Software and Information Industry Association for excellence in software development within the software industry.\\nJolt Awards are awards in the software industry.\\nStevens Award is a software engineering award given in memory of Wayne Stevens.\\n\\n\\n== Criticism ==\\nSoftware engineering sees its practitioners as individuals who follow well-defined engineering approaches to problem-solving. These approaches are specified in various software engineering books and research papers, always with the connotations of predictability, precision, mitigated risk and professionalism. This perspective has led to calls for licensing, certification and codified bodies of knowledge as mechanisms for spreading the engineering knowledge and maturing the field.\\nSoftware engineering extends engineering and draws on the engineering model, i.e. engineering process, engineering project management, engineering requirements, engineering design, engineering construction, and engineering validation. The concept is so new that it is rarely understood, and it is widely misinterpreted, including in software engineering textbooks, papers, and among the communities of programmers and crafters.\\nOne of the core issues in software engineering is that its approaches are not empirical enough because a real-world validation of approaches is usually absent, or very limited and hence software engineering is often misinterpreted as feasible only in a \"theoretical environment.\"\\nEdsger Dijkstra, the founder of many of the concepts used within software development today, rejected the idea of \"software engineering\" up until his death in 2002, arguing that those terms were poor analogies for what\\nhe called the \"radical novelty\" of computer science:\\n\\nA number of these phenomena have been bundled under the name \"Software Engineering\". As economics is known as \"The Miserable Science\", software engineering should be known as \"The Doomed Discipline\", doomed because it cannot even approach its goal since its goal is self-contradictory. Software engineering, of course, presents itself as another worthy cause, but that is eyewash: if you carefully read its literature and analyse what its devotees actually do, you will discover that software engineering has accepted as its charter \"How to program if you cannot.\"\\n\\n\\n== See also ==\\n\\n\\n=== Study and practice ===\\nComputer science\\nInformation engineering\\nSoftware craftsmanship\\nSoftware development\\nRelease engineering\\n\\n\\n=== Roles ===\\nProgrammer\\nSystems analyst\\nSystems architect\\n\\n\\n=== Professional aspects ===\\nBachelor of Science in Information Technology\\nBachelor of Software Engineering\\nList of software engineering conferences\\nList of computer science journals (including software engineering journals)\\nSoftware Engineering Institute\\n\\n\\n== References ==\\n\\n\\n=== Citations ===\\n\\n\\n=== Sources ===\\n\\n\\n== Further reading ==\\nGuide to the Software Engineering Body of Knowledge (SWEBOK Guide): Version 3.0. Pierre Bourque, Richard E. Fairley (eds.). IEEE Computer Society. 2014. ISBN 978-0-7695-5166-1.CS1 maint: others (link)\\nPressman, Roger S (2009). Software Engineering: A Practitioner\\'s Approach (7th ed.). Boston, Mass: McGraw-Hill. ISBN 978-0-07-337597-7.\\nSommerville, Ian (2010) [2010]. Software Engineering (9th ed.). Harlow, England: Pearson Education. ISBN 978-0-13-703515-1.\\nJalote, Pankaj (2005) [1991]. An Integrated Approach to Software Engineering (3rd ed.). Springer. ISBN 978-0-387-20881-7.\\nBruegge, Bernd; Dutoit, Allen (2009). Object-oriented software engineering : using UML, patterns, and Java (3rd ed.). Prentice Hall. ISBN 978-0-13-606125-0.\\nOshana, Robert (2019-06-21). Software engineering for embedded systems : methods, practical techniques, and applications (Second ed.). Kidlington, Oxford, United Kingdom. ISBN 978-0-12-809433-4.\\n\\n\\n== External links ==\\nGuide to the Software Engineering Body of Knowledge\\nThe Open Systems Engineering and Software Development Life Cycle Framework OpenSDLC.org the integrated Creative Commons SDLC\\nSoftware Engineering Institute Carnegie Mellon', 'A software architect is a software development expert who makes high-level design choices and tries to enforce technical standards, including software coding standards, tools, and platforms.\\n\\n\\n== History ==\\nThe software architect concept began to take hold when object-oriented programming or OOP, was coming into more widespread use (in the late 1990s and early years of the 21st century). OOP allowed ever-larger and more complex applications to be built, which in turn required increased high-level application and system oversight.\\n\\n\\n== Duties ==\\nThe role of software architect generally has certain common traits:\\nSoftware architects make high-level design choices based on their programming experience. In addition, the software architect may sometimes propose technical standards, including coding standards, tools, or platforms.\\nSoftware architects may also be engaged in the design of the architecture of the hardware environment, or may focus entirely on the design methodology of the code.\\nArchitects can use various architectural-oriented software packages that specialize in communicating architecture.\\n\\n\\n== Other types of IT-related architects ==\\nThe enterprise architect handles the interaction between the business and IT sides of an organization and is principally involved with determining the AS-IS and TO-BE states from a business and IT process perspective. Many organizations are bundling the software architect duties within the role of enterprise architecture.\\nAn application architect works with a single software application.\\nOther similar titles in use, but without consensus on their exact meaning, include:\\n\\nSolution architect, which may refer to a person directly involved in advancing a particular business solution needing interactions between multiple applications. May also refer to an application architect.\\nSystem architect (singular), which is often used as a synonym for application architect. However, if one subscribes to systems theory and the idea that an enterprise can be a system, then system architect could also mean enterprise architect.\\nSystems architect (plural), which is often used as a synonym for enterprise architect or solution architect.\\nCloud architect (plural), which is a software architect who has deep knowledge about architecting solutions and applications on cloud-based infrastructures.The table below indicates many of the differences between various kinds of software architects:\\n\\n\\n== See also ==\\nElectrical engineering\\nElectronics engineering\\nHardware architecture / hardware architect\\nRequirements analysis / requirements engineer\\nSoftware architectural model\\nSoftware architecture\\nSoftware engineering / software engineer\\nSystems architecture / systems architect\\nSystems design\\nSystems engineering / systems engineer\\n\\n\\n== References ==\\n\\n\\n== External links ==\\nInternational Association of Software Architects (IASA)', 'Software testing is an investigation conducted to provide stakeholders with information about the quality of the software product or service under test. Software testing can also provide an objective, independent view of the software to allow the business to appreciate and understand the risks of software implementation. Test techniques include the process of executing a program or application with the intent of finding failures,:\\u200a31\\u200a and verifying that the software product is fit for use.\\nSoftware testing involves the execution of a software component or system component to evaluate one or more properties of interest. In general, these properties indicate the extent to which the component or system under test:\\n\\nmeets the requirements that guided its design and development,\\nresponds correctly to all kinds of inputs,\\nperforms its functions within an acceptable time,\\nis sufficiently usable,\\ncan be installed and run in its intended environments\\nachieves the general result its stakeholders desire.As the number of possible tests for even simple software components is practically infinite, all software testing uses some strategy to select tests that are feasible for the available time and resources. As a result, software testing typically, but not exclusively, attempts to execute a program or application with the intent of finding failures:\\u200a31\\u200a due to software faults.:\\u200a31\\u200a The job of testing is an iterative process as when one fault is fixed, it can illuminate other failures due to deeper faults, or can even create new ones.\\nSoftware testing can provide objective, independent information about the quality of software and risk of its failure to users or sponsors.Software testing can be conducted as soon as executable software (even if partially complete) exists. The overall approach to software development often determines when and how testing is conducted. For example, in a phased process, most testing occurs after system requirements have been defined and then implemented in testable programs. In contrast, under an agile approach, requirements, programming, and testing are often done concurrently.\\n\\n\\n== Overview ==\\nAlthough software testing can determine the correctness of software under the assumption of some specific hypotheses (see the hierarchy of testing difficulty below), testing cannot identify all the failures within the software. Instead, it furnishes a criticism or comparison that compares the state and behavior of the product against test oracles — principles or mechanisms by which someone might recognize a problem. These oracles may include (but are not limited to) specifications, contracts, comparable products, past versions of the same product, inferences about intended or expected purpose, user or customer expectations, relevant standards, applicable laws, or other criteria.\\nA primary purpose of testing is to detect software failures so that defects may be discovered and corrected. Testing cannot establish that a product functions properly under all conditions, but only that it does not function properly under specific conditions. The scope of software testing may include the examination of code as well as the execution of that code in various environments and conditions as well as examining the aspects of code: does it do what it is supposed to do and do what it needs to do. In the current culture of software development, a testing organization may be separate from the development team. There are various roles for testing team members. Information derived from software testing may be used to correct the process by which software is developed.:\\u200a41–43\\u200aEvery software product has a target audience. For example, the audience for video game software is completely different from banking software. Therefore, when an organization develops or otherwise invests in a software product, it can assess whether the software product will be acceptable to its end users, its target audience, its purchasers, and other stakeholders. Software testing assists in making this assessment.\\n\\n\\n=== Faults and failures ===\\nSoftware faults occur through the following process: A programmer makes an error (mistake), which results in a fault (defect, bug) in the software source code. If this fault is executed, in certain situations the system will produce wrong results, causing a failure.:\\u200a31\\u200aNot all faults will necessarily result in failures. For example, faults in the dead code will never result in failures. A fault that did not reveal failures may result in a failure when the environment is changed. Examples of these changes in environment include the software being run on a new computer hardware platform, alterations in source data, or interacting with different software. A single fault may result in a wide range of failure symptoms.\\nNot all software faults are caused by coding errors. One common source of expensive defects is requirement gaps, i.e., unrecognized requirements that result in errors of omission by the program designer.:\\u200a426\\u200a Requirement gaps can often be non-functional requirements such as testability, scalability, maintainability, performance, and security.\\n\\n\\n=== Input combinations and preconditions ===\\nA fundamental problem with software testing is that testing under all combinations of inputs and preconditions (initial state) is not feasible, even with a simple product.:\\u200a17–18\\u200a This means that the number of faults in a software product can be very large and defects that occur infrequently are difficult to find in testing and debugging. More significantly, non-functional dimensions of quality (how it is supposed to be versus what it is supposed to do) — usability, scalability, performance, compatibility, and reliability — can be highly subjective; something that constitutes sufficient value to one person may be intolerable to another.\\nSoftware developers can\\'t test everything, but they can use combinatorial test design to identify the minimum number of tests needed to get the coverage they want. Combinatorial test design enables users to get greater test coverage with fewer tests. Whether they are looking for speed or test depth, they can use combinatorial test design methods to build structured variation into their test cases.\\n\\n\\n=== Economics ===\\nA study conducted by NIST in 2002 reports that software bugs cost the U.S. economy $59.5 billion annually. More than a third of this cost could be avoided, if better software testing was performed.Outsourcing software testing because of costs is very common, with China, the Philippines, and India being preferred destinations.\\n\\n\\n=== Roles ===\\nSoftware testing can be done by dedicated software testers; until the 1980s, the term \"software tester\" was used generally, but later it was also seen as a separate profession. Regarding the periods and the different goals in software testing, different roles have been established, such as test manager, test lead, test analyst, test designer, tester, automation developer, and test administrator. Software testing can also be performed by non-dedicated software testers.\\n\\n\\n== History ==\\nGlenford J. Myers initially introduced the separation of debugging from testing in 1979. Although his attention was on breakage testing (\"A successful test case is one that detects an as-yet undiscovered error.\":\\u200a16\\u200a), it illustrated the desire of the software engineering community to separate fundamental development activities, such as debugging, from that of verification.\\n\\n\\n== Testing approach ==\\n\\n\\n=== Static, dynamic, and passive testing ===\\nThere are many approaches available in software testing. Reviews, walkthroughs, or inspections are referred to as static testing, whereas executing programmed code with a given set of test cases is referred to as dynamic testing.Static testing is often implicit, like proofreading, plus when programming tools/text editors check source code structure or compilers (pre-compilers) check syntax and data flow as static program analysis. Dynamic testing takes place when the program itself is run. Dynamic testing may begin before the program is 100% complete in order to test particular sections of code and are applied to discrete functions or modules. Typical techniques for these are either using stubs/drivers or execution from a debugger environment.Static testing involves verification, whereas dynamic testing also involves validation.Passive testing means verifying the system behavior without any interaction with the software product. Contrary to active testing, testers do not provide any test data but look at system logs and traces. They mine for patterns and specific behavior in order to make some kind of decisions. This is related to offline runtime verification and log analysis.\\n\\n\\n=== Exploratory approach ===\\nExploratory testing is an approach to software testing that is concisely described as simultaneous learning, test design, and test execution. Cem Kaner, who coined the term in 1984,:\\u200a2\\u200a defines exploratory testing as \"a style of software testing that emphasizes the personal freedom and responsibility of the individual tester to continually optimize the quality of his/her work by treating test-related learning, test design, test execution, and test result interpretation as mutually supportive activities that run in parallel throughout the project.\":\\u200a36\\u200a\\n\\n\\n=== The \"box\" approach ===\\nSoftware testing methods are traditionally divided into white- and black-box testing. These two approaches are used to describe the point of view that the tester takes when designing test cases. A hybrid approach called grey-box testing may also be applied to software testing methodology. With the concept of grey-box testing—which develops tests from specific design elements—gaining prominence, this \"arbitrary distinction\" between black- and white-box testing has faded somewhat.\\n\\n\\n==== White-box testing ====\\n\\nWhite-box testing (also known as clear box testing, glass box testing, transparent box testing, and structural testing) verifies the internal structures or workings of a program, as opposed to the functionality exposed to the end-user. In white-box testing, an internal perspective of the system (the source code), as well as programming skills, are used to design test cases. The tester chooses inputs to exercise paths through the code and determine the appropriate outputs. This is analogous to testing nodes in a circuit, e.g., in-circuit testing (ICT).\\nWhile white-box testing can be applied at the unit, integration, and system levels of the software testing process, it is usually done at the unit level. It can test paths within a unit, paths between units during integration, and between subsystems during a system–level test. Though this method of test design can uncover many errors or problems, it might not detect unimplemented parts of the specification or missing requirements.\\nTechniques used in white-box testing include:\\nAPI testing – testing of the application using public and private APIs (application programming interfaces)\\nCode coverage – creating tests to satisfy some criteria of code coverage (e.g., the test designer can create tests to cause all statements in the program to be executed at least once)\\nFault injection methods – intentionally introducing faults to gauge the efficacy of testing strategies\\nMutation testing methods\\nStatic testing methodsCode coverage tools can evaluate the completeness of a test suite that was created with any method, including black-box testing. This allows the software team to examine parts of a system that are rarely tested and ensures that the most important function points have been tested. Code coverage as a software metric can be reported as a percentage for:\\nFunction coverage, which reports on functions executed\\nStatement coverage, which reports on the number of lines executed to complete the test\\nDecision coverage, which reports on whether both the True and the False branch of a given test has been executed100% statement coverage ensures that all code paths or branches (in terms of control flow) are executed at least once. This is helpful in ensuring correct functionality, but not sufficient since the same code may process different inputs correctly or incorrectly. Pseudo-tested functions and methods are those that are covered but not specified (it is possible to remove their body without breaking any test case).\\n\\n\\n==== Black-box testing ====\\n\\nBlack-box testing (also known as functional testing) treats the software as a \"black box,\" examining functionality without any knowledge of internal implementation, without seeing the source code. The testers are only aware of what the software is supposed to do, not how it does it. Black-box testing methods include: equivalence partitioning, boundary value analysis, all-pairs testing, state transition tables, decision table testing, fuzz testing, model-based testing, use case testing, exploratory testing, and specification-based testing.Specification-based testing aims to test the functionality of software according to the applicable requirements. This level of testing usually requires thorough test cases to be provided to the tester, who then can simply verify that for a given input, the output value (or behavior), either \"is\" or \"is not\" the same as the expected value specified in the test case.\\nTest cases are built around specifications and requirements, i.e., what the application is supposed to do. It uses external descriptions of the software, including specifications, requirements, and designs to derive test cases. These tests can be functional or non-functional, though usually functional.\\nSpecification-based testing may be necessary to assure correct functionality, but it is insufficient to guard against complex or high-risk situations.One advantage of the black box technique is that no programming knowledge is required. Whatever biases the programmers may have had, the tester likely has a different set and may emphasize different areas of functionality. On the other hand, black-box testing has been said to be \"like a walk in a dark labyrinth without a flashlight.\" Because they do not examine the source code, there are situations when a tester writes many test cases to check something that could have been tested by only one test case or leaves some parts of the program untested.\\nThis method of test can be applied to all levels of software testing: unit, integration, system and acceptance. It typically comprises most if not all testing at higher levels, but can also dominate unit testing as well.\\nComponent interface testing\\nComponent interface testing is a variation of black-box testing, with the focus on the data values beyond just the related actions of a subsystem component. The practice of component interface testing can be used to check the handling of data passed between various units, or subsystem components, beyond full integration testing between those units. The data being passed can be considered as \"message packets\" and the range or data types can be checked, for data generated from one unit, and tested for validity before being passed into another unit. One option for interface testing is to keep a separate log file of data items being passed, often with a timestamp logged to allow analysis of thousands of cases of data passed between units for days or weeks. Tests can include checking the handling of some extreme data values while other interface variables are passed as normal values. Unusual data values in an interface can help explain unexpected performance in the next unit.\\n\\n\\n===== Visual testing =====\\nThe aim of visual testing is to provide developers with the ability to examine what was happening at the point of software failure by presenting the data in such a way that the developer can easily find the information she or he requires, and the information is expressed clearly.At the core of visual testing is the idea that showing someone a problem (or a test failure), rather than just describing it, greatly increases clarity and understanding. Visual testing, therefore, requires the recording of the entire test process – capturing everything that occurs on the test system in video format. Output videos are supplemented by real-time tester input via picture-in-a-picture webcam and audio commentary from microphones.\\nVisual testing provides a number of advantages. The quality of communication is increased drastically because testers can show the problem (and the events leading up to it) to the developer as opposed to just describing it and the need to replicate test failures will cease to exist in many cases. The developer will have all the evidence she or he requires of a test failure and can instead focus on the cause of the fault and how it should be fixed.\\nAd hoc testing and exploratory testing are important methodologies for checking software integrity, because they require less preparation time to implement, while the important bugs can be found quickly. In ad hoc testing, where testing takes place in an improvised, impromptu way, the ability of the tester(s) to base testing off documented methods and then improvise variations of those tests can result in more rigorous examination of defect fixes. However, unless strict documentation of the procedures are maintained, one of the limits of ad hoc testing is lack of repeatability.\\n\\n\\n==== Grey-box testing ====\\n\\nGrey-box testing (American spelling: gray-box testing) involves having knowledge of internal data structures and algorithms for purposes of designing tests while executing those tests at the user, or black-box level. The tester will often have access to both \"the source code and the executable binary.\" Grey-box testing may also include reverse engineering (using dynamic code analysis) to determine, for instance, boundary values or error messages. Manipulating input data and formatting output do not qualify as grey-box, as the input and output are clearly outside of the \"black box\" that we are calling the system under test. This distinction is particularly important when conducting integration testing between two modules of code written by two different developers, where only the interfaces are exposed for the test.\\nBy knowing the underlying concepts of how the software works, the tester makes better-informed testing choices while testing the software from outside. Typically, a grey-box tester will be permitted to set up an isolated testing environment with activities such as seeding a database. The tester can observe the state of the product being tested after performing certain actions such as executing SQL statements against the database and then executing queries to ensure that the expected changes have been reflected. Grey-box testing implements intelligent test scenarios, based on limited information. This will particularly apply to data type handling, exception handling, and so on.\\n\\n\\n== Testing levels ==\\nBroadly speaking, there are at least three levels of testing: unit testing, integration testing, and system testing. However, a fourth level, acceptance testing, may be included by developers. This may be in the form of operational acceptance testing or be simple end-user (beta) testing, testing to ensure the software meets functional expectations. Based on the ISTQB Certified Test Foundation Level syllabus, test levels includes those four levels, and the fourth level is named acceptance testing. Tests are frequently grouped into one of these levels by where they are added in the software development process, or by the level of specificity of the test.\\n\\n\\n=== Unit testing ===\\n\\nUnit testing refers to tests that verify the functionality of a specific section of code, usually at the function level. In an object-oriented environment, this is usually at the class level, and the minimal unit tests include the constructors and destructors.These types of tests are usually written by developers as they work on code (white-box style), to ensure that the specific function is working as expected. One function might have multiple tests, to catch corner cases or other branches in the code. Unit testing alone cannot verify the functionality of a piece of software, but rather is used to ensure that the building blocks of the software work independently from each other.\\nUnit testing is a software development process that involves a synchronized application of a broad spectrum of defect prevention and detection strategies in order to reduce software development risks, time, and costs. It is performed by the software developer or engineer during the construction phase of the software development life cycle. Unit testing aims to eliminate construction errors before code is promoted to additional testing; this strategy is intended to increase the quality of the resulting software as well as the efficiency of the overall development process.\\nDepending on the organization\\'s expectations for software development, unit testing might include static code analysis, data-flow analysis, metrics analysis, peer code reviews, code coverage analysis and other software testing practices.\\n\\n\\n=== Integration testing ===\\n\\nIntegration testing is any type of software testing that seeks to verify the interfaces between components against a software design. Software components may be integrated in an iterative way or all together (\"big bang\"). Normally the former is considered a better practice since it allows interface issues to be located more quickly and fixed.\\nIntegration testing works to expose defects in the interfaces and interaction between integrated components (modules). Progressively larger groups of tested software components corresponding to elements of the architectural design are integrated and tested until the software works as a system.Integration tests usually involve a lot of code, and produce traces that are larger than those produced by unit tests. This has an impact on the ease of localizing the fault when an integration test fails. To overcome this issue, it has been proposed to automatically cut the large tests in smaller pieces to improve fault localization.\\n\\n\\n=== System testing ===\\n\\nSystem testing tests a completely integrated system to verify that the system meets its requirements.:\\u200a74\\u200a For example, a system test might involve testing a login interface, then creating and editing an entry, plus sending or printing results, followed by summary processing or deletion (or archiving) of entries, then logoff.\\n\\n\\n=== Acceptance testing ===\\n\\nCommonly this level of Acceptance testing include the following four types:\\nUser acceptance testing\\nOperational acceptance testing\\nContractual and regulatory acceptance testing\\nAlpha and beta testingUser acceptance testing and Alpha and beta testing are described in the next testing types section.\\nOperational acceptance is used to conduct operational readiness (pre-release) of a product, service or system as part of a quality management system. OAT is a common type of non-functional software testing, used mainly in software development and software maintenance projects. This type of testing focuses on the operational readiness of the system to be supported, or to become part of the production environment. Hence, it is also known as operational readiness testing (ORT) or Operations readiness and assurance (OR&A) testing. Functional testing within OAT is limited to those tests that are required to verify the non-functional aspects of the system.\\nIn addition, the software testing should ensure that the portability of the system, as well as working as expected, does not also damage or partially corrupt its operating environment or cause other processes within that environment to become inoperative.Contractual acceptance testing is performed based on the contract\\'s acceptance criteria defined during the agreement of the contract, while regulatory acceptance testing is performed based on the relevant regulations to the software product. Both of these two testings can be performed by users or independent testers. Regulation acceptance testing sometimes involves the regulatory agencies auditing the test results.\\n\\n\\n== Testing types, techniques and tactics ==\\nDifferent labels and ways of grouping testing may be testing types, software testing tactics or techniques.\\n\\n\\n=== Installation testing ===\\n\\nMost software systems have installation procedures that are needed before they can be used for their main purpose. Testing these procedures to achieve an installed software system that may be used is known as installation testing.\\n\\n\\n=== Compatibility testing ===\\n\\nA common cause of software failure (real or perceived) is a lack of its compatibility with other application software, operating systems (or operating system versions, old or new), or target environments that differ greatly from the original (such as a terminal or GUI application intended to be run on the desktop now being required to become a Web application, which must render in a Web browser). For example, in the case of a lack of backward compatibility, this can occur because the programmers develop and test software only on the latest version of the target environment, which not all users may be running. This results in the unintended consequence that the latest work may not function on earlier versions of the target environment, or on older hardware that earlier versions of the target environment were capable of using. Sometimes such issues can be fixed by proactively abstracting operating system functionality into a separate program module or library.\\n\\n\\n=== Smoke and sanity testing ===\\n\\nSanity testing determines whether it is reasonable to proceed with further testing.\\nSmoke testing consists of minimal attempts to operate the software, designed to determine whether there are any basic problems that will prevent it from working at all. Such tests can be used as build verification test.\\n\\n\\n=== Regression testing ===\\n\\nRegression testing focuses on finding defects after a major code change has occurred. Specifically, it seeks to uncover software regressions, as degraded or lost features, including old bugs that have come back. Such regressions occur whenever software functionality that was previously working correctly, stops working as intended. Typically, regressions occur as an unintended consequence of program changes, when the newly developed part of the software collides with the previously existing code. Regression testing is typically the largest test effort in commercial software development, due to checking numerous details in prior software features, and even new software can be developed while using some old test cases to test parts of the new design to ensure prior functionality is still supported.\\nCommon methods of regression testing include re-running previous sets of test cases and checking whether previously fixed faults have re-emerged. The depth of testing depends on the phase in the release process and the risk of the added features. They can either be complete, for changes added late in the release or deemed to be risky, or be very shallow, consisting of positive tests on each feature, if the changes are early in the release or deemed to be of low risk. In regression testing, it is important to have strong assertions on the existing behavior. For this, it is possible to generate and add new assertions in existing test cases, this is known as automatic test amplification.\\n\\n\\n=== Acceptance testing ===\\n\\nAcceptance testing can mean one of two things:\\n\\nA smoke test is used as a build acceptance test prior to further testing, e.g., before integration or regression.\\nAcceptance testing performed by the customer, often in their lab environment on their own hardware, is known as user acceptance testing (UAT). Acceptance testing may be performed as part of the hand-off process between any two phases of development.\\n\\n\\n=== Alpha testing ===\\nAlpha testing is simulated or actual operational testing by potential users/customers or an independent test team at the developers\\' site. Alpha testing is often employed for off-the-shelf software as a form of internal acceptance testing before the software goes to beta testing.\\n\\n\\n=== Beta testing ===\\n\\nBeta testing comes after alpha testing and can be considered a form of external user acceptance testing. Versions of the software, known as beta versions, are released to a limited audience outside of the programming team known as beta testers. The software is released to groups of people so that further testing can ensure the product has few faults or bugs. Beta versions can be made available to the open public to increase the feedback field to a maximal number of future users and to deliver value earlier, for an extended or even indefinite period of time (perpetual beta).\\n\\n\\n=== Functional vs non-functional testing ===\\nFunctional testing refers to activities that verify a specific action or function of the code. These are usually found in the code requirements documentation, although some development methodologies work from use cases or user stories. Functional tests tend to answer the question of \"can the user do this\" or \"does this particular feature work.\"\\nNon-functional testing refers to aspects of the software that may not be related to a specific function or user action, such as scalability or other performance, behavior under certain constraints, or security. Testing will determine the breaking point, the point at which extremes of scalability or performance leads to unstable execution. Non-functional requirements tend to be those that reflect the quality of the product, particularly in the context of the suitability perspective of its users.\\n\\n\\n=== Continuous testing ===\\n\\nContinuous testing is the process of executing automated tests as part of the software delivery pipeline to obtain immediate feedback on the business risks associated with a software release candidate. Continuous testing includes the validation of both functional requirements and non-functional requirements; the scope of testing extends from validating bottom-up requirements or user stories to assessing the system requirements associated with overarching business goals.\\n\\n\\n=== Destructive testing ===\\n\\nDestructive testing attempts to cause the software or a sub-system to fail. It verifies that the software functions properly even when it receives invalid or unexpected inputs, thereby establishing the robustness of input validation and error-management routines. Software fault injection, in the form of fuzzing, is an example of failure testing. Various commercial non-functional testing tools are linked from the software fault injection page; there are also numerous open-source and free software tools available that perform destructive testing.\\n\\n\\n=== Software performance testing ===\\n\\nPerformance testing is generally executed to determine how a system or sub-system performs in terms of responsiveness and stability under a particular workload. It can also serve to investigate, measure, validate or verify other quality attributes of the system, such as scalability, reliability and resource usage.\\nLoad testing is primarily concerned with testing that the system can continue to operate under a specific load, whether that be large quantities of data or a large number of users. This is generally referred to as software scalability. The related load testing activity of when performed as a non-functional activity is often referred to as endurance testing. Volume testing is a way to test software functions even when certain components (for example a file or database) increase radically in size. Stress testing is a way to test reliability under unexpected or rare workloads. Stability testing (often referred to as load or endurance testing) checks to see if the software can continuously function well in or above an acceptable period.\\nThere is little agreement on what the specific goals of performance testing are. The terms load testing, performance testing, scalability testing, and volume testing, are often used interchangeably.\\nReal-time software systems have strict timing constraints. To test if timing constraints are met, real-time testing is used.\\n\\n\\n=== Usability testing ===\\nUsability testing is to check if the user interface is easy to use and understand. It is concerned mainly with the use of the application.  This is not a kind of testing that can be automated; actual human users are needed, being monitored by skilled UI designers.\\n\\n\\n=== Accessibility testing ===\\nAccessibility testing may include compliance with standards such as:\\n\\nAmericans with Disabilities Act of 1990\\nSection 508 Amendment to the Rehabilitation Act of 1973\\nWeb Accessibility Initiative (WAI) of the World Wide Web Consortium (W3C)\\n\\n\\n=== Security testing ===\\nSecurity testing is essential for software that processes confidential data to prevent system intrusion by hackers.\\nThe International Organization for Standardization (ISO) defines this as a \"type of testing conducted to evaluate the degree to which a test item, and associated data and information, are protected so that unauthorised persons or systems cannot use, read or modify them, and authorized persons or systems are not denied access to them.\"\\n\\n\\n=== Internationalization and localization ===\\nTesting for internationalization and localization validates that the software can be used with different languages and geographic regions. The process of pseudolocalization is used to test the ability of an application to be translated to another language, and make it easier to identify when the localization process may introduce new bugs into the product.\\nGlobalization testing verifies that the software is adapted for a new culture (such as different currencies or time zones).Actual translation to human languages must be tested, too. Possible localization and globalization failures include:\\n\\nSoftware is often localized by translating a list of strings out of context, and the translator may choose the wrong translation for an ambiguous source string.\\nTechnical terminology may become inconsistent, if the project is translated by several people without proper coordination or if the translator is imprudent.\\nLiteral word-for-word translations may sound inappropriate, artificial or too technical in the target language.\\nUntranslated messages in the original language may be left hard coded in the source code.\\nSome messages may be created automatically at run time and the resulting string may be ungrammatical, functionally incorrect, misleading or confusing.\\nSoftware may use a keyboard shortcut that has no function on the source language\\'s keyboard layout, but is used for typing characters in the layout of the target language.\\nSoftware may lack support for the character encoding of the target language.\\nFonts and font sizes that are appropriate in the source language may be inappropriate in the target language; for example, CJK characters may become unreadable, if the font is too small.\\nA string in the target language may be longer than the software can handle. This may make the string partly invisible to the user or cause the software to crash or malfunction.\\nSoftware may lack proper support for reading or writing bi-directional text.\\nSoftware may display images with text that was not localized.\\nLocalized operating systems may have differently named system configuration files and environment variables and different formats for date and currency.\\n\\n\\n=== Development testing ===\\n\\nDevelopment Testing is a software development process that involves the synchronized application of a broad spectrum of defect prevention and detection strategies in order to reduce software development risks, time, and costs. It is performed by the software developer or engineer during the construction phase of the software development lifecycle. Development Testing aims to eliminate construction errors before code is promoted to other testing; this strategy is intended to increase the quality of the resulting software as well as the efficiency of the overall development process.\\nDepending on the organization\\'s expectations for software development, Development Testing might include static code analysis, data flow analysis, metrics analysis, peer code reviews, unit testing, code coverage analysis, traceability, and other software testing practices.\\n\\n\\n=== A/B testing ===\\n\\nA/B testing is a method of running a controlled experiment to determine if a proposed change is more effective than the current approach. Customers are routed to either a current version (control) of a feature, or to a modified version (treatment) and data is collected to determine which version is better at achieving the desired outcome.\\n\\n\\n=== Concurrent testing ===\\n\\nConcurrent or concurrency testing assesses the behaviour and performance of software and systems that use concurrent computing, generally under normal usage conditions. Typical problems this type of testing will expose are deadlocks, race conditions and problems with shared memory/resource handling.\\n\\n\\n=== Conformance testing or type testing ===\\n\\nIn software testing, conformance testing verifies that a product performs according to its specified standards. Compilers, for instance, are extensively tested to determine whether they meet the recognized standard for that language.\\n\\n\\n=== Output comparison testing ===\\nCreating a display expected output, whether as data comparison of text or screenshots of the UI,:\\u200a195\\u200a is sometimes called snapshot testing or Golden Master Testing unlike many other forms of testing, this cannot detect failures automatically and instead requires that a human evaluate the output for inconsistencies.\\n\\n\\n=== Property testing ===\\nProperty testing is a testing technique where, instead of asserting that specific inputs produce specific expected outputs, the practitioner randomly generates many inputs, runs the program on all of them, and asserts the truth of some \"property\" that should be true for every pair of input and output. For example, every input to a sort function should have the same length as its output. Every output from a sort function should be a monotonically increasing list.\\nProperty testing libraries allow the user to control the strategy by which random inputs are constructed, to ensure coverage of degenerate cases, or inputs featuring specific patterns that are needed to fully exercise aspects of the implementation under test.\\nProperty testing is also sometimes known as \"generative testing\" or \"QuickCheck testing\" since it was introduced and popularized by the Haskell library \"QuickCheck.\"\\n\\n\\n=== VCR testing ===\\nVCR testing, also known as \"playback testing\" or \"record/replay\" testing, is a testing technique for increasing the reliability and speed of regression tests that involve a component that is slow or unreliable to communicate with, often a third-party API outside of the tester\\'s control. It involves making a recording (\"cassette\") of the system\\'s interactions with the external component, and then replaying the recorded interactions as a substitute for communicating with the external system on subsequent runs of the test.\\nThe technique was popularized in web development by the Ruby library vcr.\\n\\n\\n== Testing process ==\\n\\n\\n=== Traditional waterfall development model ===\\nA common practice in waterfall development is that testing is performed by an independent group of testers. This can happen:\\n\\nafter the functionality is developed, but before it is shipped to the customer. This practice often results in the testing phase being used as a project buffer to compensate for project delays, thereby compromising the time devoted to testing.:\\u200a145–146\\u200a\\nat the same moment the development project starts, as a continuous process until the project finishes.However, even in the waterfall development model, unit testing is often done by the software development team even when further testing is done by a separate team.\\n\\n\\n=== Agile or XP development model ===\\nIn contrast, some emerging software disciplines such as extreme programming and the agile software development movement, adhere to a \"test-driven software development\" model. In this process, unit tests are written first, by the software engineers (often with pair programming in the extreme programming methodology). The tests are expected to fail initially. Each failing test is followed by writing just enough code to make it pass. This means the test suites are continuously updated as new failure conditions and corner cases are discovered, and they are integrated with any regression tests that are developed. Unit tests are maintained along with the rest of the software source code and generally integrated into the build process (with inherently interactive tests being relegated to a partially manual build acceptance process).\\nThe ultimate goals of this test process are to support continuous integration and to reduce defect rates.This methodology increases the testing effort done by development, before reaching any formal testing team. In some other development models, most of the test execution occurs after the requirements have been defined and the coding process has been completed.\\n\\n\\n=== A sample testing cycle ===\\nAlthough variations exist between organizations, there is a typical cycle for testing. The sample below is common among organizations employing the Waterfall development model. The same practices are commonly found in other development models, but might not be as clear or explicit.\\n\\nRequirements analysis: Testing should begin in the requirements phase of the software development life cycle. During the design phase, testers work to determine what aspects of a design are testable and with what parameters those tests work.\\nTest planning: Test strategy, test plan, testbed creation. Since many activities will be carried out during testing, a plan is needed.\\nTest development: Test procedures, test scenarios, test cases, test datasets, test scripts to use in testing software.\\nTest execution: Testers execute the software based on the plans and test documents then report any errors found to the development team. This part could be complex when running tests with a lack of programming knowledge.\\nTest reporting: Once testing is completed, testers generate metrics and make final reports on their test effort and whether or not the software tested is ready for release.\\nTest result analysis: Or Defect Analysis, is done by the development team usually along with the client, in order to decide what defects should be assigned, fixed, rejected (i.e. found software working properly) or deferred to be dealt with later.\\nDefect Retesting: Once a defect has been dealt with by the development team, it is retested by the testing team.\\nRegression testing: It is common to have a small test program built of a subset of tests, for each integration of new, modified, or fixed software, in order to ensure that the latest delivery has not ruined anything and that the software product as a whole is still working correctly.\\nTest Closure: Once the test meets the exit criteria, the activities such as capturing the key outputs, lessons learned, results, logs, documents related to the project are archived and used as a reference for future projects.\\n\\n\\n== Automated testing ==\\n\\nMany programming groups are relying more and more on automated testing, especially groups that use test-driven development. There are many frameworks to write tests in, and continuous integration software will run tests automatically every time code is checked into a version control system.\\nWhile automation cannot reproduce everything that a human can do (and all the ways they think of doing it), it can be very useful for regression testing. However, it does require a well-developed test suite of testing scripts in order to be truly useful.\\n\\n\\n=== Testing tools ===\\nProgram testing and fault detection can be aided significantly by testing tools and debuggers.\\nTesting/debug tools include features such as:\\n\\nProgram monitors, permitting full or partial monitoring of program code, including:\\nInstruction set simulator, permitting complete instruction level monitoring and trace facilities\\nHypervisor, permitting complete control of the execution of program code including:-\\nProgram animation, permitting step-by-step execution and conditional breakpoint at source level or in machine code\\nCode coverage reports\\nFormatted dump or symbolic debugging, tools allowing inspection of program variables on error or at chosen points\\nAutomated functional Graphical User Interface (GUI) testing tools are used to repeat system-level tests through the GUI\\nBenchmarks, allowing run-time performance comparisons to be made\\nPerformance analysis (or profiling tools) that can help to highlight hot spots and resource usageSome of these features may be incorporated into a single composite tool or an Integrated Development Environment (IDE).\\n\\n\\n=== Capture and replay ===\\nCapture and replay consists in collecting end-to-end usage scenario while interacting with an application and in turning these scenarios into test cases. Possible applications of capture and replay include the generation of regression tests. The SCARPE tool  selectively captures a subset of the application under study as it executes. JRapture  captures the sequence of interactions between an executing Java program and components on the host system such as files, or events on graphical user interfaces. These sequences can then be replayed for observation-based testing. \\nSaieva et al. propose to generate ad-hoc tests that replay recorded user execution traces in order to test candidate patches for critical security bugs. Pankti collects object profiles in production to generate focused differential unit tests. This tool enhances capture and replay with the systematic generation of  derived test oracles.\\n\\n\\n== Measurement in software testing ==\\n\\nQuality measures include such topics as correctness, completeness, security and ISO/IEC 9126 requirements such as capability, reliability, efficiency, portability, maintainability, compatibility, and usability.\\nThere are a number of frequently used software metrics, or measures, which are used to assist in determining the state of the software or the adequacy of the testing.\\n\\n\\n=== Hierarchy of testing difficulty ===\\nBased on the number of test cases required to construct a complete test suite in each context (i.e. a test suite such that, if it is applied to the implementation under test, then we collect enough information to precisely determine whether the system is correct or incorrect according to some specification), a hierarchy of testing difficulty has been proposed. It includes the following testability classes:\\n\\nClass I: there exists a finite complete test suite.\\nClass II: any partial distinguishing rate (i.e., any incomplete capability to distinguish correct systems from incorrect systems) can be reached with a finite test suite.\\nClass III: there exists a countable complete test suite.\\nClass IV: there exists a complete test suite.\\nClass V: all cases.It has been proved that each class is strictly included in the next. For instance, testing when we assume that the behavior of the implementation under test can be denoted by a deterministic finite-state machine for some known finite sets of inputs and outputs and with some known number of states belongs to Class I (and all subsequent classes). However, if the number of states is not known, then it only belongs to all classes from Class II on. If the implementation under test must be a deterministic finite-state machine failing the specification for a single trace (and its continuations), and its number of states is unknown, then it only belongs to classes from Class III on. Testing temporal machines where transitions are triggered if inputs are produced within some real-bounded interval only belongs to classes from Class IV on, whereas testing many non-deterministic systems only belongs to Class V (but not all, and some even belong to Class I). The inclusion into Class I does not require the simplicity of the assumed computation model, as some testing cases involving implementations written in any programming language, and testing implementations defined as machines depending on continuous magnitudes, have been proved to be in Class I. Other elaborated cases, such as the testing framework by Matthew Hennessy under must semantics, and temporal machines with rational timeouts, belong to Class II.\\n\\n\\n== Testing artifacts ==\\nA software testing process can produce several artifacts. The actual artifacts produced are a factor of the software development model used, stakeholder and organisational needs.\\n\\nTest plan\\nA test plan is a document detailing the approach that will be taken for intended test activities. The plan may include aspects such as objectives, scope, processes and procedures, personnel requirements, and contingency plans. The test plan could come in the form of a single plan that includes all test types (like an acceptance or system test plan) and planning considerations, or it may be issued as a master test plan that provides an overview of more than one detailed test plan (a plan of a plan). A test plan can be, in some cases, part of a wide \"test strategy\" which documents overall testing approaches, which may itself be a master test plan or even a separate artifact.Traceability matrix\\nA traceability matrix is a table that correlates requirements or design documents to test documents. It is used to change tests when related source documents are changed, to select test cases for execution when planning for regression tests by considering requirement coverage.Test case\\nA test case normally consists of a unique identifier, requirement references from a design specification, preconditions, events, a series of steps (also known as actions) to follow, input, output, expected result, and the actual result. Clinically defined, a test case is an input and an expected result. This can be as terse as \\'for condition x your derived result is y\\', although normally test cases describe in more detail the input scenario and what results might be expected. It can occasionally be a series of steps (but often steps are contained in a separate test procedure that can be exercised against multiple test cases, as a matter of economy) but with one expected result or expected outcome. The optional fields are a test case ID, test step, or order of execution number, related requirement(s), depth, test category, author, and check boxes for whether the test is automatable and has been automated. Larger test cases may also contain prerequisite states or steps, and descriptions. A test case should also contain a place for the actual result. These steps can be stored in a word processor document, spreadsheet, database, or other common repositories. In a database system, you may also be able to see past test results, who generated the results, and what system configuration was used to generate those results. These past results would usually be stored in a separate table.Test script\\nA test script is a procedure or programming code that replicates user actions. Initially, the term was derived from the product of work created by automated regression test tools. A test case will be a baseline to create test scripts using a tool or a program.Test suite\\nThe most common term for a collection of test cases is a test suite. The test suite often also contains more detailed instructions or goals for each collection of test cases. It definitely contains a section where the tester identifies the system configuration used during testing. A group of test cases may also contain prerequisite states or steps, and descriptions of the following tests.Test fixture or test data\\nIn most cases, multiple sets of values or data are used to test the same functionality of a particular feature. All the test values and changeable environmental components are collected in separate files and stored as test data. It is also useful to provide this data to the client and with the product or a project. There are techniques to generate test data.Test harness\\nThe software, tools, samples of data input and output, and configurations are all referred to collectively as a test harness.Test run\\nA report of the results from running a test case or a test suite\\n\\n\\n== Certifications ==\\n\\nSeveral certification programs exist to support the professional aspirations of software testers and quality assurance specialists. Note that a few practitioners argue that the testing field is not ready for certification, as mentioned in the controversy section.\\n\\n\\n== Controversy ==\\nSome of the major software testing controversies include:\\n\\nAgile vs. traditional\\nShould testers learn to work under conditions of uncertainty and constant change or should they aim at process \"maturity\"? The agile testing movement has received growing popularity since 2006 mainly in commercial circles, whereas government and military software providers use this methodology but also the traditional test-last models (e.g., in the Waterfall model).Manual vs. automated testing\\nSome writers believe that test automation is so expensive relative to its value that it should be used sparingly. The test automation then can be considered as a way to capture and implement the requirements. As a general rule, the larger the system and the greater the complexity, the greater the ROI in test automation. Also, the investment in tools and expertise can be amortized over multiple projects with the right level of knowledge sharing within an organization.Is the existence of the ISO 29119 software testing standard justified?\\nSignificant opposition has formed out of the ranks of the context-driven school of software testing about the ISO 29119 standard. Professional testing associations, such as the International Society for Software Testing, have attempted to have the standard withdrawn.Some practitioners declare that the testing field is not ready for certification\\n No certification now offered actually requires the applicant to show their ability to test software. No certification is based on a widely accepted body of knowledge. Certification itself cannot measure an individual\\'s productivity, their skill, or practical knowledge, and cannot guarantee their competence, or professionalism as a tester.Studies used to show the relative expense of fixing defects\\nThere are opposing views on the applicability of studies used to show the relative expense of fixing defects depending on their introduction and detection. For example:\\nIt is commonly believed that the earlier a defect is found, the cheaper it is to fix it. The following table shows the cost of fixing the defect depending on the stage it was found. For example, if a problem in the requirements is found only post-release, then it would cost 10–100 times more to fix than if it had already been found by the requirements review. With the advent of modern continuous deployment practices and cloud-based services, the cost of re-deployment and maintenance may lessen over time.\\n\\nThe data from which this table is extrapolated is scant. Laurent Bossavit says in his analysis:\\n\\nThe \"smaller projects\" curve turns out to be from only two teams of first-year students, a sample size so small that extrapolating to \"smaller projects in general\" is totally indefensible. The GTE study does not explain its data, other than to say it came from two projects, one large and one small. The paper cited for the Bell Labs \"Safeguard\" project specifically disclaims having collected the fine-grained data that Boehm\\'s data points suggest. The IBM study (Fagan\\'s paper) contains claims that seem to contradict Boehm\\'s graph and no numerical results that clearly correspond to his data points.\\n\\nBoehm doesn\\'t even cite a paper for the TRW data, except when writing for \"Making Software\" in 2010, and there he cited the original 1976 article. There exists a large study conducted at TRW at the right time for Boehm to cite it, but that paper doesn\\'t contain the sort of data that would support Boehm\\'s claims.\\n\\n\\n== Related processes ==\\n\\n\\n=== Software verification and validation ===\\n\\nSoftware testing is used in association with verification and validation:\\nVerification: Have we built the software right? (i.e., does it implement the requirements).\\nValidation: Have we built the right software? (i.e., do the deliverables satisfy the customer).The terms verification and validation are commonly used interchangeably in the industry; it is also common to see these two terms defined with contradictory definitions. According to the IEEE Standard Glossary of Software Engineering Terminology::\\u200a80–81\\u200a\\nVerification is the process of evaluating a system or component to determine whether the products of a given development phase satisfy the conditions imposed at the start of that phase.\\nValidation is the process of evaluating a system or component during or at the end of the development process to determine whether it satisfies specified requirements.And, according to the ISO 9000 standard:\\n\\nVerification is confirmation by examination and through provision of objective evidence that specified requirements have been fulfilled.\\nValidation is confirmation by examination and through provision of objective evidence that the requirements for a specific intended use or application have been fulfilled.The contradiction is caused by the use of the concepts of requirements and specified requirements but with different meanings.\\nIn the case of IEEE standards, the specified requirements, mentioned in the definition of validation, are the set of problems, needs and wants of the stakeholders that the software must solve and satisfy. Such requirements are documented in a Software Requirements Specification (SRS). And, the products mentioned in the definition of verification, are the output artifacts of every phase of the software development process. These products are, in fact, specifications such as Architectural Design Specification, Detailed Design Specification, etc. The SRS is also a specification, but it cannot be verified (at least not in the sense used here, more on this subject below).\\nBut, for the ISO 9000, the specified requirements are the set of specifications, as just mentioned above, that must be verified. A specification, as previously explained, is the product of a software development process phase that receives another specification as input. A specification is verified successfully when it correctly implements its input specification. All the specifications can be verified except the SRS because it is the first one (it can be validated, though). Examples: The Design Specification must implement the SRS; and, the Construction phase artifacts must implement the Design Specification.\\nSo, when these words are defined in common terms, the apparent contradiction disappears.\\nBoth the SRS and the software must be validated. The SRS can be validated statically by consulting with the stakeholders. Nevertheless, running some partial implementation of the software or a prototype of any kind (dynamic testing) and obtaining positive feedback from them, can further increase the certainty that the SRS is correctly formulated. On the other hand, the software, as a final and running product (not its artifacts and documents, including the source code) must be validated dynamically with the stakeholders by executing the software and having them to try it.\\nSome might argue that, for SRS, the input is the words of stakeholders and, therefore, SRS validation is the same as SRS verification. Thinking this way is not advisable as it only causes more confusion. It is better to think of verification as a process involving a formal and technical input document.\\n\\n\\n=== Software quality assurance ===\\nSoftware testing may be considered a part of a software quality assurance (SQA) process.:\\u200a347\\u200a In SQA, software process specialists and auditors are concerned with the software development process rather than just the artifacts such as documentation, code and systems. They examine and change the software engineering process itself to reduce the number of faults that end up in the delivered software: the so-called defect rate. What constitutes an acceptable defect rate depends on the nature of the software; a flight simulator video game would have much higher defect tolerance than software for an actual airplane. Although there are close links with SQA, testing departments often exist independently, and there may be no SQA function in some companies.Software testing is an activity to investigate software under test in order to provide quality-related information to stakeholders. By contrast, QA (quality assurance) is the implementation of policies and procedures intended to prevent defects from reaching customers.\\n\\n\\n== See also ==\\n\\n\\n== References ==\\n\\n\\n== Further reading ==\\nMeyer, Bertrand (August 2008). \"Seven Principles of Software Testing\" (PDF). Computer. Vol. 41 no. 8. pp. 99–101. doi:10.1109/MC.2008.306. Retrieved November 21, 2017.\\nWhat is Software Testing? - Answered by community of Software Testers at Software Testing Board\\n\\n\\n== External links ==\\nSoftware testing tools and products at Curlie\\n\"Software that makes Software better\" Economist.com', 'Computer engineering (CoE or CpE) is a branch of engineering that integrates several fields of computer science and electronic engineering required to develop computer hardware and software. Computer engineers usually have training in electronic engineering (or electrical engineering), software design, and hardware-software integration instead of only software engineering or electronic engineering. Computer engineers are involved in many hardware and software aspects of computing, from the design of individual microcontrollers, microprocessors, personal computers, and supercomputers, to circuit design. This field of engineering not only focuses on how computer systems themselves work but also how they integrate into the larger picture.Usual tasks involving computer engineers include writing software and firmware for embedded microcontrollers, designing VLSI chips, designing analog sensors, designing mixed signal circuit boards, and designing operating systems. Computer engineers are also suited for robotics research, which relies heavily on using digital systems to control and monitor electrical systems like motors, communications, and sensors.\\nIn many institutions of higher learning, computer engineering students are allowed to choose areas of in-depth study in their junior and senior year because the full breadth of knowledge used in the design and application of computers is beyond the scope of an undergraduate degree. Other institutions may require engineering students to complete one or two years of general engineering before declaring computer engineering as their primary focus.\\n\\n\\n== History ==\\n\\nComputer engineering began in 1939 when John Vincent Atanasoff and Clifford Berry began developing the world\\'s first electronic digital computer through physics, mathematics, and electrical engineering. John Vincent Atanasoff was once a physics and mathematics teacher for Iowa State University and Clifford Berry a former graduate under electrical engineering and physics. Together, they created the Atanasoff-Berry computer, also known as the ABC which took 5 years to complete.\\nWhile the original ABC was dismantled and discarded in the 1940s a tribute was made to the late inventors, a replica of the ABC was made in 1997 where it took a team of researchers and engineers four years and $350,000 to build.The modern personal computer emerged in the 1970s, after several breakthroughs in semiconductor technology. These include the first working transistor by William Shockley, John Bardeen and Walter Brattain at Bell Labs in 1947, the silicon surface passivation process (via thermal oxidation) by Mohamed Atalla at Bell Labs in 1957, the monolithic integrated circuit chip by Robert Noyce at Fairchild Semiconductor in 1959, the metal-oxide-semiconductor field-effect transistor (MOSFET, or MOS transistor) by Mohamed Atalla and Dawon Kahng at Bell Labs in 1959, and the single-chip microprocessor (Intel 4004) by Federico Faggin, Marcian Hoff, Masatoshi Shima and Stanley Mazor at Intel in 1971.\\n\\n\\n=== History of computer engineering education ===\\nThe first computer engineering degree program in the United States was established in 1971 at Case Western Reserve University in Cleveland, Ohio. As of 2015, there were 250 ABET-accredited computer engineering programs in the U.S. In Europe, accreditation of computer engineering schools is done by a variety of agencies part of the EQANIE network. Due to increasing job requirements for engineers who can concurrently design hardware, software, firmware, and manage all forms of computer systems used in industry, some tertiary institutions around the world offer a bachelor\\'s degree generally called computer engineering.  Both computer engineering and electronic engineering programs include analog and digital circuit design in their curriculum. As with most engineering disciplines, having a sound knowledge of mathematics and science is necessary for computer engineers.\\n\\n\\n== Education ==\\nComputer engineering is referred to as computer science and engineering at some universities. Most entry-level computer engineering jobs require at least a bachelor\\'s degree in computer engineering (or computer science and engineering). Typically one must learn an array of mathematics such as calculus, algebra and trigonometry and some computer science classes. Sometimes a degree in electronic engineering is accepted, due to the similarity of the two fields. Because hardware engineers commonly work with computer software systems, a strong background in computer programming is necessary. According to BLS, \"a computer engineering major is similar to electrical engineering but with some computer science courses added to the curriculum\". Some large firms or specialized jobs require a master\\'s degree.\\nIt is also important for computer engineers to keep up with rapid advances in technology. Therefore, many continue learning throughout their careers. This can be helpful, especially when it comes to learning new skills or improving existing ones. For example, as the relative cost of fixing a bug increases the further along it is in the software development cycle, there can be greater cost savings attributed to developing and testing for quality code as soon as possible in the process, and particularly before release.\\n\\n\\n== Profession: Computer engineer ==\\nA person with a profession in computer engineering is called a computer engineer.\\n\\n\\n== Applications and practice ==\\nThere are two major focuses in computer engineering: hardware and software.\\n\\n\\n=== Computer hardware engineering ===\\nAccording to the BLS, Job Outlook employment for computer hardware engineers, the expected ten-year growth from 2019 to 2029 for computer hardware engineering was an estimated 2% and a total of 71,100 jobs. (\"Slower than average\" in their own words when compared to other occupations)\". This is a decrease from the 2014 to 2024 BLS computer hardware engineering estimate of 3% and  a total of 77,700 jobs. \" and is down from 7% for the 2012 to 2022 BLS estimate and is further down from 9% in the BLS 2010 to 2020 estimate.\" Today, computer hardware is somehow equal to electronic and computer engineering (ECE) and has been divided into many subcategories; the most significant is embedded system design.\\n\\n\\n=== Computer software engineering ===\\nAccording to the U.S. Bureau of Labor Statistics (BLS), \"computer applications software engineers and computer systems software engineers are projected to be among the faster than average growing occupations\" The expected ten-year growth as of 2014 for computer software engineering was an estimated seventeen percent and there was a total of 1,114,000 jobs that same year. This is down from the 2012 to 2022 BLS estimate of 22% for software developers. And, further down from the 30% 2010 to 2020 BLS estimate. In addition, growing concerns over cybersecurity add up to put computer software engineering high above the average rate of increase for all fields. However, some of the work will be outsourced in foreign countries. Due to this, job growth will not be as fast as during the last decade, as jobs that would have gone to computer software engineers in the United States would instead go to computer software engineers in countries such as India. In addition, the BLS Job Outlook for Computer Programmers, 2014–24 has an −8% (a decline, in their words) and a Job Outlook, 2019-29 \\t-9% (Decline) for those who program computers (i.e. embedded systems) who are not computer application developers. Furthermore, women in software fields has been declining over the years even faster than other engineering fields.\\n\\n\\n=== Computer engineering licensing and practice ===\\nComputer engineering is generally practiced within larger product development firms, and such practice may not be subject to licensing.  However, independent consultants who advertise computer engineering, just like any form of engineering, may be subject to state laws which restrict professional engineer practice to only those who have received the appropriate License.  National Council of Examiners for Engineering and Surveying (NCEES) first offered a Principles and Practice of Engineering Examination for computer engineering in 2003. \\n\\n\\n== Specialty areas ==\\nThere are many specialty areas in the field of computer engineering.\\n\\n\\n=== Processor design ===\\n\\nProcessor design process involves choosing an instruction set and a certain execution paradigm (e.g. VLIW or RISC) and results in a microarchitecture, which might be described in e.g. VHDL or Verilog. CPU design is divided into design of the following components: datapaths (such as ALUs and pipelines), control unit: logic which controls the datapaths, memory components such as register files, caches, clock circuitry such as clock drivers, PLLs, clock distribution networks, pad transceiver circuitry, logic gate cell library which is used to implement the logic.\\n\\n\\n=== Coding, cryptography, and information protection ===\\n\\nComputer engineers work in coding, cryptography, and information protection to develop new methods for protecting various information, such as digital images and music, fragmentation, copyright infringement and other forms of tampering. Examples include work on wireless communications, multi-antenna systems, optical transmission, and digital watermarking.\\n\\n\\n=== Communications and wireless networks ===\\n\\nThose focusing on communications and wireless networks, work advancements in telecommunications systems and networks (especially wireless networks), modulation and error-control coding, and information theory. High-speed network design, interference suppression and modulation, design, and analysis of fault-tolerant system, and storage and transmission schemes are all a part of this specialty.\\n\\n\\n=== Compilers and operating systems ===\\n\\nThis specialty focuses on compilers and operating systems design and development. Engineers in this field develop new operating system architecture, program analysis techniques, and new techniques to assure quality. Examples of work in this field include post-link-time code transformation algorithm development and new operating system development.\\n\\n\\n=== Computational science and engineering ===\\n\\nComputational science and engineering is a relatively new discipline. According to the Sloan Career Cornerstone Center, individuals working in this area, \"computational methods are applied to formulate and solve complex mathematical problems in engineering and the physical and the social sciences. Examples include aircraft design, the plasma processing of nanometer features on semiconductor wafers, VLSI circuit design, radar detection systems, ion transport through biological channels, and much more\".\\n\\n\\n=== Computer networks, mobile computing, and distributed systems ===\\n\\nIn this specialty, engineers build integrated environments for computing, communications, and information access. Examples include shared-channel wireless networks, adaptive resource management in various systems, and improving the quality of service in mobile and ATM environments. Some other examples include work on wireless network systems and fast Ethernet cluster wired systems.\\n\\n\\n=== Computer systems: architecture, parallel processing, and dependability ===\\n\\nEngineers working in computer systems work on research projects that allow for reliable, secure, and high-performance computer systems. Projects such as designing processors for multi-threading and parallel processing are included in this field. Other examples of work in this field include the development of new theories, algorithms, and other tools that add performance to computer systems.Computer architecture includes CPU design, cache hierarchy layout, memory organization and load balancing.\\n\\n\\n=== Computer vision and robotics ===\\n\\nIn this specialty, computer engineers focus on developing visual sensing technology to sense an environment, representation of an environment, and manipulation of the environment. The gathered three-dimensional information is then implemented to perform a variety of tasks. These include improved human modeling, image communication, and human-computer interfaces, as well as devices such as special-purpose cameras with versatile vision sensors.\\n\\n\\n=== Embedded systems ===\\n\\nIndividuals working in this area design technology for enhancing the speed, reliability, and performance of systems. Embedded systems are found in many devices from a small FM radio to the space shuttle. According to the Sloan Cornerstone Career Center, ongoing developments in embedded systems include \"automated vehicles and equipment to conduct search and rescue, automated transportation systems, and human-robot coordination to repair equipment in space.\" As of 2018, computer embedded computer engineering specializations include system-on-chip design, architecture of edge computing and the Internet of things.\\n\\n\\n=== Integrated circuits, VLSI design, testing and CAD ===\\n\\nThis specialty of computer engineering requires adequate knowledge of electronics and electrical systems. Engineers working in this area work on enhancing the speed, reliability, and energy efficiency of next-generation very-large-scale integrated (VLSI) circuits and microsystems. An example of this specialty is work done on reducing the power consumption of VLSI algorithms and architecture.\\n\\n\\n=== Signal, image and speech processing ===\\n\\nComputer engineers in this area develop improvements in human-computer interaction, including speech recognition and synthesis, medical and scientific imaging, or communications systems. Other work in this area includes computer vision development such as recognition of human facial features.\\n\\n\\n=== Quantum computing ===\\n\\n\\n== See also ==\\n\\n\\n=== Related fields ===\\n\\n\\n=== Associations ===\\nAssociation of Computer Engineers and Technicians\\nIEEE Computer Society\\nAssociation for Computing Machinery\\n\\n\\n== References ==\\n\\n\\n== External links ==\\n Media related to Computer engineering at Wikimedia Commons', 'Data analysis is a process of inspecting, cleansing, transforming, and modelling data with the goal of discovering useful information, informing conclusions, and supporting decision-making. Data analysis has multiple facets and approaches, encompassing diverse techniques under a variety of names, and is used in different business, science, and social science domains. In today\\'s business world, data analysis plays a role in making decisions more scientific and helping businesses operate more effectively.Data mining is a particular data analysis technique that focuses on statistical modelling and knowledge discovery for predictive rather than purely descriptive purposes, while business intelligence covers data analysis that relies heavily on aggregation, focusing mainly on business information. In statistical applications, data analysis can be divided into descriptive statistics, exploratory data analysis (EDA), and confirmatory data analysis (CDA). EDA focuses on discovering new features in the data while CDA focuses on confirming or falsifying existing hypotheses. Predictive analytics focuses on the application of statistical models for predictive forecasting or classification, while text analytics applies statistical, linguistic, and structural techniques to extract and classify information from textual sources, a species of unstructured data. All of the above are varieties of data analysis.Data integration is a precursor to data analysis, and data analysis is closely linked to data visualization and data dissemination.\\n\\n\\n== The process of data analysis ==\\n\\nAnalysis, refers to dividing a whole into its separate components for individual examination. Data analysis, is a process for obtaining raw data, and subsequently converting it into information useful for decision-making by users. Data, is collected and analyzed to answer questions, test hypotheses, or disprove theories.\\nStatistician John Tukey, defined data analysis in 1961, as:\"Procedures for analyzing data, techniques for interpreting the results of such procedures, ways of planning the gathering of data to make its analysis easier, more precise or more accurate, and all the machinery and results of (mathematical) statistics which apply to analyzing data.\"There are several phases that can be distinguished, described below. The phases are iterative, in that feedback from later phases may result in additional work in earlier phases. The CRISP framework, used in data mining, has similar steps.\\n\\n\\n=== Data requirements ===\\nThe data is necessary as inputs to the analysis, which is specified based upon the requirements of those directing the analysis (or customers, who will use the finished product of the analysis). The general type of entity upon which the data will be collected is referred to as an experimental unit (e.g., a person or population of people). Specific variables regarding a population (e.g., age and income) may be specified and obtained.  Data may be numerical or categorical (i.e., a text label for numbers).\\n\\n\\n=== Data collection ===\\nData is collected from a variety of sources. The requirements may be communicated by analysts to custodians of the data; such as, Information Technology personnel within an organization. The data may also be collected from sensors in the environment, including traffic cameras, satellites, recording devices, etc. It may also be obtained through interviews, downloads from online sources, or reading documentation.\\n\\n\\n=== Data processing ===\\n\\nData, when initially obtained, must be processed or organized for analysis. For instance, these may involve placing data into rows and columns in a table format (known as structured data) for further analysis, often through the use of spreadsheet or statistical software.\\n\\n\\n=== Data cleaning ===\\n\\nOnce processed and organized, the data may be incomplete, contain duplicates, or contain errors. The need for data cleaning will arise from problems in the way that the datum are entered and stored. Data cleaning is the process of preventing and correcting these errors. Common tasks include record matching, identifying inaccuracy of data, overall quality of existing data, deduplication, and column segmentation. Such data problems can also be identified through a variety of analytical techniques. For example; with financial information, the totals for particular variables may be compared against separately published numbers that are believed to be reliable. Unusual amounts, above or below predetermined thresholds, may also be reviewed.  There are several types of data cleaning, that are dependent upon the type of data in the set; this could be phone numbers, email addresses, employers, or other values. Quantitative data methods for outlier detection, can be used to get rid of data that appears to have a higher likelihood of being input incorrectly. Textual data spell checkers can be used to lessen the amount of mis-typed words. However, it is harder to tell if the words themselves are correct.\\n\\n\\n=== Exploratory data analysis ===\\nOnce the datasets are cleaned, they can then be analyzed. Analysts may apply a variety of techniques, referred to as exploratory data analysis, to begin understanding the messages contained within the obtained data. The process of data exploration may result in additional data cleaning or additional requests for data; thus, the initialization of the iterative phases mentioned in the lead paragraph of this section. Descriptive statistics, such as, the average or median, can be generated to aid in understanding the data. Data visualization is also a technique used, in which the analyst is able to examine the data in a graphical format in order to obtain additional insights, regarding the messages within the data.\\n\\n\\n=== Modelling and algorithms ===\\nMathematical formulas or models (known as algorithms), may be applied to the data in order to identify relationships among the variables; for example, using correlation or causation. In general terms, models may be developed to evaluate a specific variable based on other variable(s) contained within the dataset, with some residual error depending on the implemented model\\'s accuracy (e.g., Data = Model + Error).Inferential statistics, includes utilizing techniques that measure the relationships between particular variables. For example, regression analysis may be used to model whether a change in advertising (independent variable X), provides an explanation for the variation in sales (dependent variable Y). In mathematical terms, Y (sales) is a function of X (advertising). It may be described as (Y = aX + b + error), where the model is designed such that (a) and (b) minimize the error when the model predicts Y for a given range of values of X. Analysts may also attempt to build models that are descriptive of the data, in an aim to simplify analysis and communicate results.\\n\\n\\n=== Data product ===\\nA data product is a computer application that takes data inputs and generates outputs, feeding them back into the environment. It may be based on a model or algorithm. For instance, an application that analyzes data about customer purchase history, and uses the results to recommend other purchases the customer might enjoy.\\n\\n\\n=== Communication ===\\n\\nOnce data is analyzed, it may be reported in many formats to the users of the analysis to support their requirements. The users may have feedback, which results in additional analysis. As such, much of the analytical cycle is iterative.When determining how to communicate the results, the analyst may consider implementing a variety of data visualization techniques to help communicate the message more clearly and efficiently to the audience. Data visualization uses information displays (graphics such as, tables and charts) to help communicate key messages contained in the data. Tables are a valuable tool by enabling the ability of a user to query and focus on specific numbers; while charts (e.g., bar charts or line charts), may help explain the quantitative messages contained in the data.\\n\\n\\n== Quantitative messages ==\\n\\nStephen Few described eight types of quantitative messages that users may attempt to understand or communicate from a set of data and the associated graphs used to help communicate the message. Customers specifying requirements and analysts performing the data analysis may consider these messages during the course of the process.\\nTime-series: A single variable is captured over a period of time, such as the unemployment rate over a 10-year period. A line chart may be used to demonstrate the trend.\\nRanking: Categorical subdivisions are ranked in ascending or descending order, such as a ranking of sales performance (the measure) by salespersons (the category, with each salesperson a categorical subdivision) during a single period.  A bar chart may be used to show the comparison across the salespersons.\\nPart-to-whole: Categorical subdivisions are measured as a ratio to the whole (i.e., a percentage out of 100%).  A pie chart or bar chart can show the comparison of ratios, such as the market share represented by competitors in a market.\\nDeviation: Categorical subdivisions are compared against a reference, such as a comparison of actual vs. budget expenses for several departments of a business for a given time period.  A bar chart can show the comparison of the actual versus the reference amount.\\nFrequency distribution: Shows the number of observations of a particular variable for a given interval, such as the number of years in which the stock market return is between intervals such as 0–10%, 11–20%, etc. A histogram, a type of bar chart, may be used for this analysis.\\nCorrelation: Comparison between observations represented by two variables (X,Y) to determine if they tend to move in the same or opposite directions. For example, plotting unemployment (X) and inflation (Y) for a sample of months. A scatter plot is typically used for this message.\\nNominal comparison: Comparing categorical subdivisions in no particular order, such as the sales volume by product code. A bar chart may be used for this comparison.\\nGeographic or geospatial: Comparison of a variable across a map or layout, such as the unemployment rate by state or the number of persons on the various floors of a building. A cartogram is a typical graphic used.\\n\\n\\n== Techniques for analyzing quantitative data ==\\n\\nAuthor Jonathan Koomey has recommended a series of best practices for understanding quantitative data.  These include:\\n\\nCheck raw data for anomalies prior to performing an analysis;\\nRe-perform important calculations, such as verifying columns of data that are formula driven;\\nConfirm main totals are the sum of subtotals;\\nCheck relationships between numbers that should be related in a predictable way, such as ratios over time;\\nNormalize numbers to make comparisons easier, such as analyzing amounts per person or relative to GDP or as an index value relative to a base year;\\nBreak problems into component parts by analyzing factors that led to the results, such as DuPont analysis of return on equity.For the variables under examination, analysts typically obtain descriptive statistics for them, such as the mean (average), median, and standard deviation. They may also analyze the distribution of the key variables to see how the individual values cluster around the mean.\\n The consultants at McKinsey and Company named a technique for breaking a quantitative problem down into its component parts called the MECE principle. Each layer can be broken down into its components; each of the sub-components must be mutually exclusive of each other and collectively add up to the layer above them. The relationship is referred to as \"Mutually Exclusive and Collectively Exhaustive\" or MECE.  For example, profit by definition can be broken down into total revenue and total cost. In turn, total revenue can be analyzed by its components, such as the revenue of divisions A, B, and C (which are mutually exclusive of each other) and should add to the total revenue (collectively exhaustive).Analysts may use robust statistical measurements to solve certain analytical problems. Hypothesis testing is used when a particular hypothesis about the true state of affairs is made by the analyst and data is gathered to determine whether that state of affairs is true or false. For example, the hypothesis might be that \"Unemployment has no effect on inflation\", which relates to an economics concept called the Phillips Curve. Hypothesis testing involves considering the likelihood of Type I and type II errors, which relate to whether the data supports accepting or rejecting the hypothesis.Regression analysis may be used when the analyst is trying to determine the extent to which independent variable X affects dependent variable Y (e.g., \"To what extent do changes in the unemployment rate (X) affect the inflation rate (Y)?\"). This is an attempt to model or fit an equation line or curve to the data, such that Y is a function of X.Necessary condition analysis (NCA) may be used when the analyst is trying to determine the extent to which independent variable X allows variable Y (e.g., \"To what extent is a certain unemployment rate (X) necessary for a certain inflation rate (Y)?\"). Whereas (multiple) regression analysis uses additive logic where each X-variable can produce the outcome and the X\\'s can compensate for each other (they are sufficient but not necessary), necessary condition analysis (NCA) uses necessity logic, where one or more X-variables allow the outcome to exist, but may not produce it (they are necessary but not sufficient). Each single necessary condition must be present and compensation is not possible.\\n\\n\\n== Analytical activities of data users ==\\nUsers may have particular data points of interest within a data set, as opposed to the general messaging outlined above. Such low-level user analytic activities are presented in the following table. The taxonomy can also be organized by three poles of activities: retrieving values, finding data points, and arranging data points.\\n\\n\\n== Barriers to effective analysis ==\\nBarriers to effective analysis may exist among the analysts performing the data analysis or among the audience. Distinguishing fact from opinion, cognitive biases, and innumeracy are all challenges to sound data analysis.\\n\\n\\n=== Confusing fact and opinion ===\\n\\nEffective analysis requires obtaining relevant facts to answer questions, support a conclusion or formal opinion, or test hypotheses. Facts by definition are irrefutable, meaning that any person involved in the analysis should be able to agree upon them. For example, in August 2010, the Congressional Budget Office (CBO) estimated that extending the Bush tax cuts of 2001 and 2003 for the 2011–2020 time period would add approximately $3.3 trillion to the national debt. Everyone should be able to agree that indeed this is what CBO reported; they can all examine the report. This makes it a fact. Whether persons agree or disagree with the CBO is their own opinion.As another example, the auditor of a public company must arrive at a formal opinion on whether financial statements of publicly traded corporations are \"fairly stated, in all material respects\". This requires extensive analysis of factual data and evidence to support their opinion. When making the leap from facts to opinions, there is always the possibility that the opinion is erroneous.\\n\\n\\n=== Cognitive biases ===\\nThere are a variety of cognitive biases that can adversely affect analysis. For example, confirmation bias is the tendency to search for or interpret information in a way that confirms one\\'s preconceptions. In addition, individuals may discredit information that does not support their views.Analysts may be trained specifically to be aware of these biases and how to overcome them. In his book Psychology of Intelligence Analysis, retired CIA analyst Richards Heuer wrote that analysts should clearly delineate their assumptions and chains of inference and specify the degree and source of the uncertainty involved in the conclusions. He emphasized procedures to help surface and debate alternative points of view.\\n\\n\\n=== Innumeracy ===\\nEffective analysts are generally adept with a variety of numerical techniques. However, audiences may not have such literacy with numbers or numeracy; they are said to be innumerate.  Persons communicating the data may also be attempting to mislead or misinform, deliberately using bad numerical techniques.For example, whether a number is rising or falling may not be the key factor. More important may be the number relative to another number, such as the size of government revenue or spending relative to the size of the economy (GDP) or the amount of cost relative to revenue in corporate financial statements. This numerical technique is referred to as normalization or common-sizing. There are many such techniques employed by analysts, whether adjusting for inflation (i.e., comparing real vs. nominal data) or considering population increases, demographics, etc. Analysts apply a variety of techniques to address the various quantitative messages described in the section above.Analysts may also analyze data under different assumptions or scenario. For example, when analysts perform financial statement analysis, they will often recast the financial statements under different assumptions to help arrive at an estimate of future cash flow, which they then discount to present value based on some interest rate, to determine the valuation of the company or its stock.  Similarly, the CBO analyzes the effects of various policy options on the government\\'s revenue, outlays and deficits, creating alternative future scenarios for key measures.\\n\\n\\n== Other topics ==\\n\\n\\n=== Smart buildings ===\\nA data analytics approach can be used in order to predict energy consumption in buildings. The different steps of the data analysis process are carried out in order to realise smart buildings, where the building management and control operations including heating, ventilation, air conditioning, lighting and security are realised automatically by miming the needs of the building users and optimising resources like energy and time.\\n\\n\\n=== Analytics and business intelligence ===\\n\\nAnalytics is the \"extensive use of data, statistical and quantitative analysis, explanatory and predictive models, and fact-based management to drive decisions and actions.\" It is a subset of business intelligence, which is a set of technologies and processes that uses data to understand and analyze business performance to drive decision-making .\\n\\n\\n=== Education ===\\n\\nIn education, most educators have access to a data system for the purpose of analyzing student data. These data systems present data to educators in an over-the-counter data format (embedding labels, supplemental documentation, and a help system and making key package/display and content decisions) to improve the accuracy of educators’ data analyses.\\n\\n\\n== Practitioner notes ==\\nThis section contains rather technical explanations that may assist practitioners but are beyond the typical scope of a Wikipedia article.\\n\\n\\n=== Initial data analysis ===\\nThe most important distinction between the initial data analysis phase and the main analysis phase, is that during initial data analysis one refrains from any analysis that is aimed at answering the original research question. The initial data analysis phase is guided by the following four questions:\\n\\n\\n==== Quality of data ====\\nThe quality of the data should be checked as early as possible. Data quality can be assessed in several ways, using different types of analysis: frequency counts, descriptive statistics (mean, standard deviation, median), normality (skewness, kurtosis, frequency histograms), normal imputation is needed.\\nAnalysis of extreme observations: outlying observations in the data are analyzed to see if they seem to disturb the distribution.\\nComparison and correction of differences in coding schemes: variables are compared with coding schemes of variables external to the data set, and possibly corrected if coding schemes are not comparable.\\nTest for common-method variance.The choice of analyses to assess the data quality during the initial data analysis phase depends on the analyses that will be conducted in the main analysis phase.\\n\\n\\n==== Quality of measurements ====\\nThe quality of the measurement instruments should only be checked during the initial data analysis phase when this is not the focus or research question of the study. One should check whether structure of measurement instruments corresponds to structure reported in the literature.\\nThere are two ways to assess measurement quality:\\n\\nConfirmatory factor analysis\\nAnalysis of homogeneity (internal consistency), which gives an indication of the reliability of a measurement instrument. During this analysis, one inspects the variances of the items and the scales, the Cronbach\\'s α of the scales, and the change in the Cronbach\\'s alpha when an item would be deleted from a scale\\n\\n\\n==== Initial transformations ====\\nAfter assessing the quality of the data and of the measurements, one might decide to impute missing data, or to perform initial transformations of one or more variables, although this can also be done during the main analysis phase.\\nPossible transformations of variables are:\\nSquare root transformation (if the distribution differs moderately from normal)\\nLog-transformation (if the distribution differs substantially from normal)\\nInverse transformation (if the distribution differs severely from normal)\\nMake categorical (ordinal / dichotomous) (if the distribution differs severely from normal, and no transformations help)\\n\\n\\n==== Did the implementation of the study fulfill the intentions of the research design? ====\\nOne should check the success of the randomization procedure, for instance by checking whether background and substantive variables are equally distributed within and across groups. If the study did not need or use a randomization procedure, one should check the success of the non-random sampling, for instance by checking whether all subgroups of the population of interest are represented in sample.Other possible data distortions that should be checked are:\\n\\ndropout (this should be identified during the initial data analysis phase)\\nItem non-response (whether this is random or not should be assessed during the initial data analysis phase)\\nTreatment quality (using manipulation checks).\\n\\n\\n==== Characteristics of data sample ====\\nIn any report or article, the structure of the sample must be accurately described. It is especially important to exactly determine the structure of the sample (and specifically the size of the subgroups) when subgroup analyses will be performed during the main analysis phase.The characteristics of the data sample can be assessed by looking at:\\n\\nBasic statistics of important variables\\nScatter plots\\nCorrelations and associations\\nCross-tabulations\\n\\n\\n==== Final stage of the initial data analysis ====\\nDuring the final stage, the findings of the initial data analysis are documented, and necessary, preferable, and possible corrective actions are taken.Also, the original plan for the main data analyses can and should be specified in more detail or rewritten. In order to do this, several decisions about the main data analyses can and should be made:\\n\\nIn the case of non-normals: should one transform variables; make variables categorical (ordinal/dichotomous); adapt the analysis method?\\nIn the case of missing data: should one neglect or impute the missing data; which imputation technique should be used?\\nIn the case of outliers: should one use robust analysis techniques?\\nIn case items do not fit the scale: should one adapt the measurement instrument by omitting items, or rather ensure comparability with other (uses of the) measurement instrument(s)?\\nIn the case of (too) small subgroups: should one drop the hypothesis about inter-group differences, or use small sample techniques, like exact tests or bootstrapping?\\nIn case the randomization procedure seems to be defective: can and should one calculate propensity scores and include them as covariates in the main analyses?\\n\\n\\n==== Analysis ====\\nSeveral analyses can be used during the initial data analysis phase:\\nUnivariate statistics (single variable)\\nBivariate associations (correlations)\\nGraphical techniques (scatter plots)It is important to take the measurement levels of the variables into account for the analyses, as special statistical techniques are available for each level:\\nNominal and ordinal variables\\nFrequency counts (numbers and percentages)\\nAssociations\\ncircumambulations (crosstabulations)\\nhierarchical loglinear analysis (restricted to a maximum of 8 variables)\\nloglinear analysis (to identify relevant/important variables and possible confounders)\\nExact tests or bootstrapping (in case subgroups are small)\\nComputation of new variables\\nContinuous variables\\nDistribution\\nStatistics (M, SD, variance, skewness, kurtosis)\\nStem-and-leaf displays\\nBox plots\\n\\n\\n==== Nonlinear analysis ====\\nNonlinear analysis is often necessary when the data is recorded from a nonlinear system. Nonlinear systems can exhibit complex dynamic effects including bifurcations, chaos, harmonics and subharmonics that cannot be analyzed using simple linear methods.  Nonlinear data analysis is closely related to nonlinear system identification.\\n\\n\\n=== Main data analysis ===\\nIn the main analysis phase, analyses aimed at answering the research question are performed as well as any other relevant analysis needed to write the first draft of the research report.\\n\\n\\n==== Exploratory and confirmatory approaches ====\\nIn the main analysis phase, either an exploratory or confirmatory approach can be adopted. Usually the approach is decided before data is collected. In an exploratory analysis no clear hypothesis is stated before analysing the data, and the data is searched for models that describe the data well. In a confirmatory analysis clear hypotheses about the data are tested.Exploratory data analysis should be interpreted carefully. When testing multiple models at once there is a high chance on finding at least one of them to be significant, but this can be due to a type 1 error. It is important to always adjust the significance level when testing multiple models with, for example, a Bonferroni correction. Also, one should not follow up an exploratory analysis with a confirmatory analysis in the same dataset. An exploratory analysis is used to find ideas for a theory, but not to test that theory as well. When a model is found exploratory in a dataset, then following up that analysis with a confirmatory analysis in the same dataset could simply mean that the results of the confirmatory analysis are due to the same type 1 error that resulted in the exploratory model in the first place. The confirmatory analysis therefore will not be more informative than the original exploratory analysis.\\n\\n\\n==== Stability of results ====\\nIt is important to obtain some indication about how generalizable the results are. While this is often difficult to check, one can look at the stability of the results. Are the results reliable and reproducible? There are two main ways of doing that.\\nCross-validation. By splitting the data into multiple parts, we can check if an analysis (like a fitted model) based on one part of the data generalizes to another part of the data as well. Cross-validation is generally inappropriate, though, if there are correlations within the data, e.g. with panel data. Hence other methods of validation sometimes need to be used. For more on this topic, see statistical model validation.\\nSensitivity analysis. A procedure to study the behavior of a system or model when global parameters are (systematically) varied. One way to do that is via bootstrapping.\\n\\n\\n== Free software for data analysis ==\\nNotable free software for data analysis include:\\n\\nDevInfo – A database system endorsed by the United Nations Development Group for monitoring and analyzing human development.\\nELKI – Data mining framework in Java with data mining oriented visualization functions.\\nKNIME – The Konstanz Information Miner, a user friendly and comprehensive data analytics framework.\\nOrange – A visual programming tool featuring interactive data visualization and methods for statistical data analysis, data mining, and machine learning.\\nPandas – Python library for data analysis.\\nPAW – FORTRAN/C data analysis framework developed at CERN.\\nR – A programming language and software environment for statistical computing and graphics.\\nROOT –  C++ data analysis framework developed at CERN.\\nSciPy – Python library for data analysis.\\nJulia - A programming language well-suited for numerical analysis and computational science.\\n\\n\\n== International data analysis contests ==\\nDifferent companies or organizations hold data analysis contests to encourage researchers to utilize their data or to solve a particular question using data analysis. A few examples of well-known international data analysis contests are as follows:\\nKaggle competition, which is held by Kaggle.\\nLTPP data analysis contest held by FHWA and ASCE.\\n\\n\\n== See also ==\\n\\n\\n== References ==\\n\\n\\n=== Citations ===\\n\\n\\n=== Bibliography ===\\nAdèr, Herman J. (2008a). \"Chapter 14: Phases and initial steps in data analysis\".  In Adèr, Herman J.; Mellenbergh, Gideon J.; Hand, David J (eds.). Advising on research methods : a consultant\\'s companion. Huizen, Netherlands: Johannes van Kessel Pub. pp. 333–356. ISBN 9789079418015. OCLC 905799857.\\nAdèr, Herman J. (2008b). \"Chapter 15: The main analysis phase\".  In Adèr, Herman J.; Mellenbergh, Gideon J.; Hand, David J (eds.). Advising on research methods : a consultant\\'s companion. Huizen, Netherlands: Johannes van Kessel Pub. pp. 357–386. ISBN 9789079418015. OCLC 905799857.\\nTabachnick, B.G. & Fidell, L.S. (2007). Chapter 4: Cleaning up your act. Screening data prior to analysis. In B.G. Tabachnick & L.S. Fidell (Eds.), Using Multivariate Statistics, Fifth Edition (pp. 60–116). Boston: Pearson Education, Inc. / Allyn and Bacon.\\n\\n\\n== Further reading ==\\nAdèr, H.J. & Mellenbergh, G.J. (with contributions by D.J. Hand) (2008). Advising on Research Methods: A Consultant\\'s Companion. Huizen, the Netherlands: Johannes van Kessel Publishing.  ISBN 978-90-79418-01-5\\nChambers, John M.; Cleveland, William S.; Kleiner, Beat; Tukey, Paul A. (1983). Graphical Methods for Data Analysis, Wadsworth/Duxbury Press. ISBN 0-534-98052-X\\nFandango, Armando (2017). Python Data Analysis, 2nd Edition. Packt Publishers. ISBN 978-1787127487\\nJuran, Joseph M.; Godfrey, A. Blanton (1999). Juran\\'s Quality Handbook, 5th Edition. New York: McGraw Hill. ISBN 0-07-034003-X\\nLewis-Beck, Michael S. (1995). Data Analysis: an Introduction, Sage Publications Inc, ISBN 0-8039-5772-6\\nNIST/SEMATECH (2008) Handbook of Statistical Methods,\\nPyzdek, T, (2003). Quality Engineering Handbook, ISBN 0-8247-4614-7\\nRichard Veryard (1984). Pragmatic Data Analysis. Oxford : Blackwell Scientific Publications. ISBN 0-632-01311-7\\nTabachnick, B.G.; Fidell, L.S. (2007). Using Multivariate Statistics, 5th Edition. Boston: Pearson Education, Inc. / Allyn and Bacon, ISBN 978-0-205-45938-4', 'Interaction design, often abbreviated as IxD, is \"the practice of designing interactive digital products, environments, systems, and services.\":\\u200axxxi,\\u200a1\\u200a Beyond the digital aspect, interaction design is also useful when creating physical (non-digital) products, exploring how a user might interact with it. Common topics of interaction design include design, human–computer interaction, and software development. While interaction design has an interest in form (similar to other design fields), its main area of focus rests on behavior.:\\u200a1\\u200a Rather than analyzing how things are, interaction design synthesizes and imagines things as they could be. This element of interaction design is what characterizes IxD as a design field as opposed to a science or engineering field.:\\u200axviii\\u200aWhile disciplines such as software engineering have a heavy focus on designing for technical stakeholders, interaction design is focused on meeting the needs and optimizing the experience of users, within relevant technical or business constraints.:\\u200axviii\\u200a\\n\\n\\n== History ==\\nThe term interaction design was coined by Bill Moggridge and Bill Verplank in the mid-1980s, but it took 10 years before the concept started to take hold.:\\u200axviii\\u200a To Verplank, it was an adaptation of the computer science term user interface design for the industrial design profession. To Moggridge, it was an improvement over soft-face, which he had coined in 1984 to refer to the application of industrial design to products containing software.The earliest programs in design for interactive technologies were the Visible Language Workshop, started by Muriel Cooper at MIT in 1975, and the Interactive Telecommunications Program founded at NYU in 1979 by Martin Elton and later headed by Red Burns.The first academic program officially named \"Interaction Design\" was established at Carnegie Mellon University in 1994 as a Master of Design in Interaction Design. At the outset, the program focused mainly on screen interfaces, before shifting to a greater emphasis on the \"big picture\" aspects of interaction—people, organizations, culture, service and system.\\nIn 1990, Gillian Crampton Smith founded the Computer-related Design MA at the Royal College of Art (RCA) in London, changed in 2005 to Design Interactions, headed by Anthony Dunne. In 2001, Crampton Smith helped found the Interaction Design Institute Ivrea, a small institute in Olivetti\\'s hometown in Northern Italy, dedicated solely to interaction design. The institute moved to Milan in October 2005 and merged with Domus Academy. In 2007, some of the people originally involved with IDII set up the Copenhagen Institute of Interaction Design (CIID). After Ivrea, Crampton Smith and Philip Tabor added the Interaction Design (IxD) track in the Visual and Multimedia Communication at Iuav, University of Venice, Italy, between 2006 and 2014.\\nIn 1998, the Swedish Foundation for Strategic Research founded The Interactive Institute—a Swedish research institute in the field of interaction design.\\n\\n\\n== Methodologies ==\\n\\n\\n=== Goal-oriented design ===\\nGoal-oriented design (or Goal-Directed design) \"is concerned with satisfying the needs and desires of the users of a product or service.\":\\u200axviii\\u200aAlan Cooper argues in The Inmates Are Running the Asylum that we need a new approach to solving interactive software-based problems.:\\u200a1\\u200a The problems with designing computer interfaces are fundamentally different from those that do not include software (e.g., hammers).  Cooper introduces the concept of cognitive friction, which is when the interface of a design is complex and difficult to use, and behaves inconsistently and unexpectedly, possessing different modes.:\\u200a22\\u200aAlternatively, interfaces can be designed to serve the needs of the service/product provider. User needs may be poorly served by this approach.\\n\\n\\n=== Usability ===\\nUsability answers the question \"can someone use this interface?\". Jakob Nielsen describes usability as the quality attribute that describes how usable the interface is. Shneiderman proposes principles for designing more usable interfaces called \"Eight Golden Rules of Interface Design\"—which are well-known heuristics for creating usable systems.\\n\\n\\n=== Personas ===\\nPersonas are archetypes that describe the various goals and observed behaviour patterns among users.A persona encapsulates critical behavioural data in a way that both designers and stakeholders can understand, remember, and relate to. Personas use storytelling to engage users\\' social and emotional aspects, which helps designers to either visualize the best product behaviour or see why the recommended design is successful.\\n\\n\\n=== Cognitive dimensions ===\\nThe cognitive dimensions framework provides a vocabulary to evaluate and modify design solutions. Cognitive dimensions offer a lightweight approach to analysis of a design quality, rather than an in-depth, detailed description. They provide a common vocabulary for discussing notation, user interface or programming language design.\\nDimensions provide high-level descriptions of the interface and how the user interacts with it: examples include consistency, error-proneness, hard mental operations, viscosity and premature commitment. These concepts aid the creation of new designs from existing ones through design maneuvers that alter the design within a particular dimension.\\n\\n\\n=== Affective interaction design ===\\nDesigners must be aware of elements that influence user emotional responses. For instance, products must convey positive emotions while avoiding negative ones. Other important aspects include motivational, learning, creative, social and persuasive influences. One method that can help convey such aspects is for example, the use of dynamic icons, animations and sound to help communicate, creating a sense of interactivity. Interface aspects such as fonts, color palettes and graphical layouts can influence acceptance. Studies showed that affective aspects can affect perceptions of usability.Emotion and pleasure theories exist to explain interface responses. These include Don Norman\\'s emotional design model, Patrick Jordan\\'s pleasure model and McCarthy and Wright\\'s Technology as Experience framework.\\n\\n\\n== Five dimensions ==\\nThe concept of dimensions of interaction design were introduced in Moggridge\\'s book Designing Interactions. Crampton Smith wrote that interaction design draws on four existing design languages, 1D, 2D, 3D, 4D. Silver later proposed a fifth dimension, behaviour.\\n\\n\\n=== Words ===\\nThis dimension defines interactions: words are the element that users interact with.\\n\\n\\n=== Visual representations ===\\nVisual representations are the elements of an interface that the user perceives; these may include but are not limited to \"typography, diagrams, icons, and other graphics\".\\n\\n\\n=== Physical objects or space ===\\nThis dimension defines the objects or space \"with which or within which users interact\".\\n\\n\\n=== Time ===\\nThe time during which the user interacts with the interface. An example of this includes \"content that changes over time such as sound, video or animation\".\\n\\n\\n=== Behavior ===\\nBehavior defines how users respond to the interface. Users may have different reactions in this interface.\\n\\n\\n== Interaction Design Association ==\\nThe Interaction Design Association was created in 2003 to serve the community. The organization has over 80,000 members and more than 173 local groups. IxDA hosts Interaction the annual interaction design conference, and the Interaction Awards.\\n\\n\\n== Related disciplines ==\\nIndustrial design\\nThe core principles of industrial design overlap with those of interaction design. Industrial designers use their knowledge of physical form, color, aesthetics, human perception and desire, and usability to create a fit of an object with the person using it.Human factors and ergonomics\\nCertain basic principles of ergonomics provide grounding for interaction design. These include anthropometry, biomechanics, kinesiology, physiology and psychology as they relate to human behavior in the built environment.Cognitive psychology\\nCertain basic principles of cognitive psychology provide grounding for interaction design. These include mental models, mapping, interface metaphors, and affordances. Many of these are laid out in Donald Norman\\'s influential book The Design of Everyday Things.Human–computer interaction\\nAcademic research in human–computer interaction (HCI) includes methods for describing and testing the usability of interacting with an interface, such as cognitive dimensions and the cognitive walkthrough.Design research\\nInteraction designers are typically informed through iterative cycles of user research. User research is used to identify the needs, motivations and behaviors of end users. They design with an emphasis on user goals and experience, and evaluate designs in terms of usability and affective influence.Architecture\\nAs interaction designers increasingly deal with ubiquitous computing, urban informatics and urban computing, the architects\\' ability to make, place, and create context becomes a point of contact between the disciplines.User interface design\\nLike user interface design and experience design, interaction design is often associated with the design of system interfaces in a variety of media but concentrates on the aspects of the interface that define and present its behavior over time, with a focus on developing the system to respond to the user\\'s experience and not the other way around.\\n\\n\\n== See also ==\\n\\n\\n== References ==\\n\\n\\n== Further reading ==\\nBolter, Jay D.; Gromala, Diane (2008). Windows and Mirrors: Interaction Design, Digital Art, and the Myth of Transparency. Cambridge, Massachusetts: MIT Press. ISBN 978-0-262-02545-4.\\nBuchenau, Marion; Suri, Jane Fulton. Experience Prototyping. DIS 2000. ISBN 1-58113-219-0.\\nBuxton, Bill (2005). Sketching the User Experience. New Riders Press. ISBN 0-321-34475-8.\\nCooper, Alan (1999). The Inmates are Running the Asylum. Sams. ISBN 0672316498.\\nCooper, Alan; Reimann, Robert; Cronin, David; Noessel, Christopher (2014). About Face (4th ed.). Wiley. ISBN 9781118766576.\\nDawes, Brendan (2007). Analog In, Digital Out. Berkeley, California: New Riders Press.\\nGoodwin, Kim (2009). Designing for the Digital Age: How to Create Human-Centered Products and Services. ISBN 978-0-470-22910-1.\\nHoude, Stephanie; Hill, Charles (1997). \"What Do Prototypes Prototype?\".  In Helander, M; Landauer, T; Prabhu, P (eds.). Handbook of Human–Computer Interaction (2nd ed.). Elsevier Science.\\nJones, Matt & Gary Marsden: Mobile Interaction Design, John Wiley & Sons, 2006, ISBN 0-470-09089-8.\\nKolko, Jon (2009). Thoughts on Interaction Design. ISBN 978-0-12-378624-1.\\nLaurel, Brenda; Lunenfeld, Peter (2003). Design Research: Methods and Perspectives. MIT Press. ISBN 0-262-12263-4.\\nTinauli, Musstanser; Pillan, Margherita (2008). \"Interaction Design and Experiential Factors: A Novel Case Study on Digital Pen and Paper\". Mobility \\'08: Proceedings of the International Conference on Mobile Technology, Applications, and Systems. New York: ACM. doi:10.1145/1506270.1506400. ISBN 978-1-60558-089-0.\\nNorman, Donald (1988). The Design of Everyday Things. New York: Basic Books. ISBN 978-0-465-06710-7.\\nRaskin, Jef (2000). The Humane Interface. ACM Press. ISBN 0-201-37937-6.\\nSaffer, Dan (2006). Designing for Interaction. New Riders Press. ISBN 0-321-43206-1.=', 'Network management is the process of administering and managing computer networks. Services provided by this discipline include fault analysis, performance management, provisioning of networks and maintaining quality of service. Network management software is used by network administrators to help perform these functions.\\n\\n\\n== Technologies ==\\nA small number of accessory methods exist to support network and network device management. Network management allows IT professionals to monitor network components within large network area. Access methods include the SNMP, command-line interface (CLI), custom XML, CMIP, Windows Management Instrumentation (WMI), Transaction Language 1 (TL1), CORBA, NETCONF, and the Java Management Extensions (JMX).\\nSchemas include the Structure of Management Information (SMI), WBEM, the Common Information Model (CIM Schema), and MTOSI amongst others.\\n\\n\\n== See also ==\\n\\n\\n== References ==\\n\\n\\n== External links ==\\nNetwork Management at Curlie\\nInternet Network Management at Curlie\\nNetwork Monitoring and Management Tools\\nSoftware-Defined Network Management', 'Data science is an interdisciplinary field that uses scientific methods, processes, algorithms and systems to extract knowledge and insights from noisy, structured and unstructured data, and apply knowledge and actionable insights from data across a broad range of application domains. Data science is related to data mining, machine learning and big data.\\nData science is a \"concept to unify statistics, data analysis, informatics, and their related methods\" in order to \"understand and analyze actual phenomena\" with data. It uses techniques and theories drawn from many fields within the context of mathematics, statistics, computer science, information science, and domain knowledge. However, data science is different from computer science and information science. Turing Award winner Jim Gray imagined data science as a \"fourth paradigm\" of science (empirical, theoretical, computational, and now data-driven) and asserted that \"everything about science is changing because of the impact of information technology\" and the data deluge.A data scientist is someone who creates programming code, and combines it with statistical knowledge to create insights from data.\\n\\n\\n== Foundations ==\\nData science is an interdisciplinary field focused on extracting knowledge from data sets, which are typically large (see big data), and applying the knowledge and actionable insights from data to solve problems in a wide range of application domains. The field encompasses preparing data for analysis, formulating data science problems, analyzing data, developing data-driven solutions, and presenting findings to inform high-level decisions in a broad range of application domains. As such, it incorporates skills from computer science, statistics, information science, mathematics, information visualization, data integration, graphic design, complex systems, communication and business. Statistician Nathan Yau, drawing on Ben Fry, also links data science to human-computer interaction: users should be able to intuitively control and explore data. In 2015, the American Statistical Association identified database management, statistics and machine learning, and distributed and parallel systems as the three emerging foundational professional communities.\\n\\n\\n=== Relationship to statistics ===\\nMany statisticians, including Nate Silver, have argued that data science is not a new field, but rather another name for statistics. Others argue that data science is distinct from statistics because it focuses on problems and techniques unique to digital data. Vasant Dhar writes that statistics emphasizes quantitative data and description. In contrast, data science deals with quantitative and qualitative data (e.g. images) and emphasizes prediction and action. Andrew Gelman of Columbia University has described statistics as a nonessential part of data science.\\nStanford professor David Donoho writes that data science is not distinguished from statistics by the size of datasets or use of computing, and that many graduate programs misleadingly advertise their analytics and statistics training as the essence of a data science program. He describes data science as an applied field growing out of traditional statistics. \\nIn summary, data science can be therefore described as an applied branch of statistics.\\n\\n\\n== Etymology ==\\n\\n\\n=== Early usage ===\\nIn 1962, John Tukey described a field he called \"data analysis\", which resembles modern data science. In 1985, in a lecture given to the Chinese Academy of Sciences in Beijing, C.F. Jeff Wu used the term Data Science for the first time as an alternative name for statistics. Later, attendees at a 1992 statistics symposium at the University of Montpellier II acknowledged the emergence of a new discipline focused on data of various origins and forms, combining established concepts and principles of statistics and data analysis with computing.The term \"data science\" has been traced back to 1974, when Peter Naur proposed it as an alternative name for computer science. In 1996, the International Federation of Classification Societies became the first conference to specifically feature data science as a topic. However, the definition was still in flux. After the 1985 lecture in the Chinese Academy of Sciences in Beijing, in 1997 C.F. Jeff Wu again suggested that statistics should be renamed data science. He reasoned that a new name would help statistics shed inaccurate stereotypes, such as being synonymous with accounting, or limited to describing data. In 1998, Hayashi Chikio argued for data science as a new, interdisciplinary concept, with three aspects: data design, collection, and analysis.During the 1990s, popular terms for the process of finding patterns in datasets (which were increasingly large) included \"knowledge discovery\" and \"data mining\".\\n\\n\\n=== Modern usage ===\\nThe modern conception of data science as an independent discipline is sometimes attributed to William S. Cleveland. In a 2001 paper, he advocated an expansion of statistics beyond theory into technical areas; because this would significantly change the field, it warranted a new name. \"Data science\" became more widely used in the next few years: in 2002, the Committee on Data for Science and Technology launched Data Science Journal. In 2003, Columbia University launched The Journal of Data Science. In 2014, the American Statistical Association\\'s Section on Statistical Learning and Data Mining changed its name to the Section on Statistical Learning and Data Science, reflecting the ascendant popularity of data science.The professional title of \"data scientist\" has been attributed to DJ Patil and Jeff Hammerbacher in 2008. Though it was used by the National Science Board in their 2005 report, \"Long-Lived Digital Data Collections: Enabling Research and Education in the 21st Century,\" it referred broadly to any key role in managing a digital data collection.There is still no consensus on the definition of data science and it is considered by some to be a buzzword.\\n\\n\\n== Market ==\\nBig data is becoming a tool for businesses and companies of all sizes. The availability and interpretation of big data has altered the business models of old industries and enabled the creation of new ones. Data scientists are responsible for breaking down big data into usable information and creating software and algorithms that help companies and organizations determine optimal operations.\\n\\n\\n== Technologies and techniques ==\\nThere is a variety of different technologies and techniques that are used for data science which depend on the application. More recently, full-featured, end-to-end platforms have been developed and heavily used for data science and machine learning.\\n\\n\\n=== Techniques ===\\n\\nLinear regression\\nLogistic regression\\nDecision trees are used as prediction models for classification and data fitting. The decision tree structure can be used to generate rules able to classify or predict target/class/label variable based on the observation attributes.\\nSupport-vector machine (SVM)\\nCluster analysis is a technique used to group data together.\\nDimensionality reduction is used to reduce the complexity of data computation so that it can be performed more quickly.\\nMachine learning is a technique used to perform tasks by inferencing patterns from data\\nNaive Bayes classifiers are used to classify by applying the Bayes\\' theorem. They are mainly used in datasets with large amounts of data, and can aptly generate accurate results.\\n\\n\\n== See also ==\\nInternational Journal of Population Data Science\\n\\n\\n== References ==', 'In computer science and computer programming, a data type or simply type is an attribute of data which tells the compiler or interpreter how the programmer intends to use the data. Most programming languages support basic data types of integer numbers (of varying sizes), floating-point numbers (which approximate real numbers), characters and Booleans. A data type constrains the values that an expression, such as a variable or a function, might take. This data type defines the operations that can be done on the data, the meaning of the data, and the way values of that type can be stored. A data type provides a set of values from which an expression (i.e. variable, function, etc.) may take its values.\\n\\n\\n== Concept ==\\nData types are used within type systems, which offer various ways of defining, implementing, and using them. Different type systems ensure varying degrees of type safety.\\nAlmost all programming languages explicitly include the notion of data type, though different languages may use different terminology.\\nCommon data types include:\\n\\nInteger\\nFloating-point number\\nCharacter\\nString\\nBooleanFor example, in the Java programming language, the type int represents the set of 32-bit integers ranging in value from −2,147,483,648 to 2,147,483,647, as well as the operations that can be performed on integers, such as addition, subtraction, and multiplication. A color, on the other hand, might be represented by three bytes denoting the amounts each of red, green, and blue, and a string representing the color\\'s name.\\nMost programming languages also allow the programmer to define additional data types, usually by combining multiple elements of other types and defining the valid operations of the new data type.  For example, a programmer might create a new data type named \"complex number\" that would include real and imaginary parts. \\nA data type also represents a constraint placed upon the interpretation of data in a type system, describing representation, interpretation and structure of values or objects stored in computer memory. The type system uses data type information to check correctness of computer programs that access or manipulate the data.\\nMost data types in statistics have comparable types in computer programming, and vice versa, as shown in the following table:\\n\\n\\n== Definition ==\\n(Parnas, Shore & Weiss 1976) identified five definitions of a \"type\" that were used—sometimes implicitly—in the literature.  Types including behavior align more closely with object-oriented models, whereas a structured programming model would tend to not include code, and are called plain old data structures.\\nThe five types are:\\n\\nSyntactic\\nA type is a purely syntactic label associated with a variable when it is declared. Such definitions of \"type\" do not give any semantic meaning to types.\\nRepresentation\\nA type is defined in terms of its composition of more primitive types—often machine types.\\nRepresentation and behaviour\\nA type is defined as its representation and a set of operators manipulating these representations.\\nValue space\\nA type is a set of possible values which a variable can possess. Such definitions make it possible to speak about (disjoint) unions or Cartesian products of types.\\nValue space and behaviour\\nA type is a set of values which a variable can possess and a set of functions that one can apply to these values.The definition in terms of a representation was often done in imperative languages such as ALGOL and Pascal, while the definition in terms of a value space and behaviour was used in higher-level languages such as Simula and CLU.\\n\\n\\n== Classes of data types ==\\n\\n\\n=== Primitive data types ===\\nPrimitive data types are typically types that are built-in or basic to a language implementation.\\n\\n\\n==== Machine data types ====\\nAll data in computers based on digital electronics is represented as bits (alternatives 0 and 1) on the lowest level. The smallest addressable unit of data is usually a group of bits called a byte (usually an octet, which is 8 bits). The unit processed by machine code instructions is called a word (as of 2011, typically 32 or 64 bits). Most instructions interpret the word as a binary number, such that a 32-bit word can represent unsigned integer values from 0 to \\n  \\n    \\n      \\n        \\n          2\\n          \\n            32\\n          \\n        \\n        −\\n        1\\n      \\n    \\n    {\\\\displaystyle 2^{32}-1}\\n   or signed integer values from \\n  \\n    \\n      \\n        −\\n        \\n          2\\n          \\n            31\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle -2^{31}}\\n   to \\n  \\n    \\n      \\n        \\n          2\\n          \\n            31\\n          \\n        \\n        −\\n        1\\n      \\n    \\n    {\\\\displaystyle 2^{31}-1}\\n  . Because of two\\'s complement, the machine language and machine doesn\\'t need to distinguish between these unsigned and signed data types for the most part.\\nFloating-point numbers used for floating-point arithmetic use a different interpretation of the bits in a word. See Floating-point arithmetic for details.\\nMachine data types need to be exposed or made available in systems or low-level programming languages, allowing fine-grained control over hardware. The C programming language, for instance, supplies integer types of various widths, such as short and long. If a corresponding native type does not exist on the target platform, the compiler will break them down into code using types that do exist. For instance, if a 32-bit integer is requested on a 16 bit platform, the compiler will tacitly treat it as an array of two 16 bit integers.\\nIn higher level programming, machine data types are often hidden or abstracted as an implementation detail that would render code less portable if exposed.  For instance, a generic numeric type might be supplied instead of integers of some specific bit-width.\\n\\n\\n==== Boolean type ====\\nThe Boolean type represents the values true and false. Although only two values are possible, they are rarely implemented as a single binary digit for efficiency reasons. Many programming languages do not have an explicit Boolean type, instead interpreting (for instance) 0 as false and other values as true.\\nBoolean data refers to the logical structure of how the language is interpreted to the machine language. In this case a Boolean 0 refers to the logic False. True is always a non zero, especially a one which is known as Boolean 1.\\n\\n\\n==== Enumerations ====\\nThe enumerated type has distinct values, which can be compared and assigned, but which do not necessarily have any particular concrete representation in the computer\\'s memory; compilers and interpreters can represent them arbitrarily. For example, the four suits in a deck of playing cards may be four enumerators named CLUB, DIAMOND, HEART, SPADE, belonging to an enumerated type named suit.  If a variable V is declared having suit as its data type, one can assign any of those four values to it. Some implementations allow programmers to assign integer values to the enumeration values, or even treat them as type-equivalent to integers.\\n\\n\\n==== Numeric types ====\\nSuch as:\\n\\nThe integer data types, or \"non-fractional numbers\". May be sub-typed according to their ability to contain negative values (e.g. unsigned in C and C++). May also have a small number of predefined subtypes (such as short and long in C/C++); or allow users to freely define subranges such as 1..12 (e.g. Pascal/Ada).\\nFloating point data types, usually represent values as high-precision fractional values (rational numbers, mathematically), but are sometimes misleadingly called reals (evocative of mathematical real numbers). They usually have predefined limits on both their maximum values and their precision. Typically stored internally in the form a × 2b (where a and b are integers), but displayed in familiar decimal form.\\nFixed point data types are convenient for representing monetary values. They are often implemented internally as integers, leading to predefined limits.\\nBignum or arbitrary precision numeric types lack predefined limits. They are not primitive types, and are used sparingly for efficiency reasons.\\n\\n\\n=== Composite types ===\\nComposite types are derived from more than one primitive type. This can be done in a number of ways. The ways they are combined are called data structures. Composing a primitive type into a compound type generally results in a new type, e.g. array-of-integer is a different type to integer.\\n\\nAn array (also called vector, list, or sequence) stores a number of elements and provide random access to individual elements. The elements of an array are typically (but not in all contexts) required to be of the same type. Arrays may be fixed-length or expandable. Indices into an array are typically required to be integers (if not, one may stress this relaxation by speaking about an associative array) from a specific range (if not all indices in that range correspond to elements, it may be a sparse array).\\nRecord (also called tuple or struct) Records are among the simplest data structures. A record is a value that contains other values, typically in fixed number and sequence and typically indexed by names. The elements of records are usually called fields or members.\\nUnion. A union type definition will specify which of a number of permitted primitive types may be stored in its instances, e.g. \"float or long integer\". Contrast with a record, which could be defined to contain a float and an integer; whereas, in a union, there is only one type allowed at a time.\\nA tagged union (also called a variant, variant record, discriminated union, or disjoint union) contains an additional field indicating its current type for enhanced type safety.\\nA set is an abstract data structure that can store certain values, without any particular order, and no repeated values. Values themselves are not retrieved from sets, rather one tests a value for membership to obtain a boolean \"in\" or \"not in\".\\nAn object contains a number of data fields, like a record, and also a number of subroutines for accessing or modifying them, called methods.Many others are possible, but they tend to be further variations and compounds of the above. For example, a linked list can store the same data as an array, but provides sequential access rather than random and is built up of records in dynamic memory; though arguably a data structure rather than a type per se, it is also common and distinct enough that including it in a discussion of composite types can be justified.\\n\\n\\n==== String and text types ====\\nSuch as:\\n\\nA character, which may be a letter of some alphabet, a digit, a blank space, a punctuation mark, etc.\\nA string, which is a sequence of characters. Strings are typically used to represent words and text, although text in all but the most trivial cases involves much more than a sequence of characters.Character and string types can store sequences of characters from a character set such as ASCII. Since most character sets include the digits, it is possible to have a numeric string, such as \"1234\". However, many languages treat these as belonging to a different type to the numeric value 1234.\\nCharacter and string types can have different subtypes according to the required character \"width\". The original 7-bit wide ASCII was found to be limited, and superseded by 8 and 16-bit sets, which can encode a wide variety of non-Latin alphabets (such as Hebrew and Chinese) and other symbols. Strings may be either stretch-to-fit or of fixed size, even in the same programming language. They may also be subtyped by their maximum size.\\nNote: Strings are not a primitive data type in all languages. In C, for instance, they are composed from an array of characters.\\n\\n\\n=== Abstract data types ===\\nAny data type that does not specify the concrete representation of the data is an abstract data type.  Instead, a formal specification based on the data type\\'s operations is used to describe it. Any implementation of a specification must fulfill the rules given. Abstract data types are used in formal semantics and program verification and, less strictly, in design.\\nBeyond verification, a specification might immediately be turned into an implementation. The OBJ family of programming languages for instance bases on this option using equations for specification and rewriting to run them. Algebraic specification was an important subject of research in CS around 1980 and almost a synonym for abstract data types at that time. It has a mathematical foundation in Universal algebra. The specification language can be made more expressive by allowing other formulas than only equations.\\nA typical example is the hierarchy of the list, bag and set data types. All these data types can be declared by three operations: null, which constructs the empty container, single, which constructs a container from a single element and append, which combines two containers of the same type. The complete specification for the three data types can then be given by the following rules over these operation:\\n\\nAccess to the data can be specified likely, e.g. a member function for these containers by:\\n\\n\\n=== Other types ===\\nTypes can be based on, or derived from, the basic types explained above. In some languages, such as C, functions have a type derived from the type of their return value.\\n\\n\\n==== Pointers and references ====\\n\\nThe main non-composite, derived type is the pointer, a data type whose value refers directly to (or \"points to\") another value stored elsewhere in the computer memory using its address. It is a primitive kind of reference. (In everyday terms, a page number in a book could be considered a piece of data that refers to another one). Pointers are often stored in a format similar to an integer; however, attempting to dereference or \"look up\" a pointer whose value was never a valid memory address would cause a program to crash. To ameliorate this potential problem, pointers are considered a separate type to the type of data they point to, even if the underlying representation is the same.\\n\\n\\n==== Function types ====\\n\\nWhile functions can be assigned a type, too, their type is not considered a data type in the setting of this article. Here, data is viewed as being distinct from algorithms. In programming, functions are strongly related to the latter. But, because a central tenet of universal data processing is that algorithms can be represented as data, e.g., textual description and binary programs, the contrast between data and functions has limits. In fact, not only can functions be represented by data, but functions can also be used to encode data. Many contemporary type systems focus strongly on function types and many modern languages allow functions to operate as first-class citizens.\\nTo exclude functions from the being treated as data types is not uncommon in related fields. Predicate logic for instance does not allow the application of quantifiers on function or predicate names.\\n\\n\\n==== Meta types ====\\n\\nSome programming languages represent the type information as data, enabling type introspection and reflection. In contrast, higher order type systems, while allowing types to be constructed from other types and passed to functions as values, typically avoid basing computational decisions on them.\\n\\n\\n==== Utility types ====\\nFor convenience, high-level languages may supply ready-made \"real world\" data types, for instance times, dates, monetary values, and memory, even where the language would allow them to be built from primitive types.\\n\\n\\n== Type systems ==\\nA type system associates types with computed values. By examining the flow of these values, a type system attempts to prove that no type errors can occur. The type system in question determines what constitutes a type error, but a type system generally seeks to guarantee that operations expecting a certain kind of value are not used with values for which that operation does not make sense.\\nA compiler may use the static type of a value to optimize the storage it needs and the choice of algorithms for operations on the value. In many C compilers the float data type, for example, is represented in 32 bits, in accord with the IEEE specification for single-precision floating point numbers. They will thus use floating-point-specific microprocessor operations on those values (floating-point addition, multiplication, etc.).\\nThe depth of type constraints and the manner of their evaluation affect the typing of the language. A programming language may further associate an operation with varying concrete algorithms on each type in the case of type polymorphism. Type theory is the study of type systems, although the concrete type systems of programming languages originate from practical issues of computer architecture, compiler implementation, and language design.\\nType systems may be variously static or dynamic, strong or weak typing, and so forth.\\n\\n\\n== See also ==\\nC data types\\nData dictionary\\nFunctional programming\\nKind\\nType theory for the mathematical models of types\\nType system for different choices in programming language typing\\nType conversion\\n\\n\\n== References ==\\n\\n\\n== Further reading ==\\nParnas, David L.; Shore, John E.; Weiss, David (1976). \"Abstract types defined as classes of variables\". Proceedings of the 1976 Conference on Data : Abstraction, Definition and Structure: 149–154. doi:10.1145/800237.807133. S2CID 14448258.\\nCardelli, Luca; Wegner, Peter (December 1985). \"On Understanding Types, Data Abstraction, and Polymorphism\" (PDF). ACM Computing Surveys. 17 (4): 471–523. CiteSeerX 10.1.1.117.695. doi:10.1145/6041.6042. ISSN 0360-0300. S2CID 2921816.\\nCleaveland, J. Craig (1986). An Introduction to Data Types. Addison-Wesley. ISBN 978-0201119404.\\n\\n\\n== External links ==\\n Media related to Data types at Wikimedia Commons', 'In computer science, an associative array, map, symbol table, or dictionary is an abstract data type composed of a collection of (key, value) pairs, such that each possible key appears at most once in the collection.  Not to be confused with Associative Processors\\nOperations associated with this data type allow to:\\nadd a pair to the collection;\\nremove a pair from the collection;\\nmodify an existing pair;\\nlookup a value associated with a particular key.Implementing associative arrays poses the dictionary problem, a classic computer science problem: the task of designing a data structure that maintains a set of data during \\'search\\', \\'delete\\', and \\'insert\\' operations.\\nThe two major solutions to the dictionary problem are a hash table and a search tree.\\nIn some cases it is also possible to solve the problem using directly addressed arrays, binary search trees, or other more specialized structures.\\nMany programming languages include associative arrays as primitive data types, and they are available in software libraries for many others. Content-addressable memory is a form of direct hardware-level support for associative arrays.\\nAssociative arrays have many applications including such fundamental programming patterns as memoization and the decorator pattern.The name does not come from the associative property known in mathematics. Rather, it arises from the fact that we associate values with keys.\\n\\n\\n== Operations ==\\nIn an associative array, the association between a key and a value is often known as a \"mapping\", and the same word mapping may also be used to refer to the process of creating a new association.\\nThe operations that are usually defined for an associative array are:\\nAdd or insert: add a new \\n  \\n    \\n      \\n        (\\n        k\\n        e\\n        y\\n        ,\\n        v\\n        a\\n        l\\n        u\\n        e\\n        )\\n      \\n    \\n    {\\\\displaystyle (key,value)}\\n   pair to the collection, mapping the new key to its new value. The arguments to this operation are the key and the value.\\nReassign: replace the value in one of the \\n  \\n    \\n      \\n        (\\n        k\\n        e\\n        y\\n        ,\\n        v\\n        a\\n        l\\n        u\\n        e\\n        )\\n      \\n    \\n    {\\\\displaystyle (key,value)}\\n   pairs that are already in the collection, mapping an existing key to a new value. As with an insertion, the arguments to this operation are the key and the value.\\nRemove or delete: remove a \\n  \\n    \\n      \\n        (\\n        k\\n        e\\n        y\\n        ,\\n        v\\n        a\\n        l\\n        u\\n        e\\n        )\\n      \\n    \\n    {\\\\displaystyle (key,value)}\\n   pair from the collection, unmapping a given key from its value. The argument to this operation is the key.\\nLookup: find the value (if any) that is bound to a given key. The argument to this operation is the key, and the value is returned from the operation. If no value is found, some associative array implementations raise an exception, while others create a pair with the given key and the default value of the value type (zero, empty container...).Often, instead of add or reassign there is a single set operation that adds a new \\n  \\n    \\n      \\n        (\\n        k\\n        e\\n        y\\n        ,\\n        v\\n        a\\n        l\\n        u\\n        e\\n        )\\n      \\n    \\n    {\\\\displaystyle (key,value)}\\n   pair if one does not already exist, and otherwise reassigns it.\\nIn addition, associative arrays may also include other operations such as determining the number of mappings or constructing an iterator to loop over all the mappings. Usually, for such an operation, the order in which the mappings are returned may be implementation-defined.\\nA multimap generalizes an associative array by allowing multiple values to be associated with a single key. A bidirectional map is a related abstract data type in which the mappings operate in both directions: each value must be associated with a unique key, and a second lookup operation takes a value as an argument and looks up the key associated with that value.\\n\\n\\n== Example ==\\nSuppose that the set of loans made by a library is represented in a data structure.  Each book in a library may be checked out only by a single library patron at a time.  However, a single patron may be able to check out multiple books.  Therefore, the information about which books are checked out to which patrons may be represented by an associative array, in which the books are the keys and the patrons are the values.  Using notation from Python or JSON, the data structure would be:\\n\\nA lookup operation on the key \"Great Expectations\" would return \"John\".  If John returns his book, that would cause a deletion operation, and if Pat checks out a book, that would cause an insertion operation, leading to a different state:\\n\\n\\n== Implementation ==\\nFor dictionaries with very small numbers of mappings, it may make sense to implement the dictionary using an association list, a linked list of mappings. With this implementation, the time to perform the basic dictionary operations is linear in the total number of mappings; however, it is easy to implement and the constant factors in its running time are small.Another very simple implementation technique, usable when the keys are restricted to a narrow range, is direct addressing into an array: the value for a given key k is stored at the array cell A[k], or if there is no mapping for k then the cell stores a special sentinel value that indicates the absence of a mapping. As well as being simple, this technique is fast: each dictionary operation takes constant time. However, the space requirement for this structure is the size of the entire keyspace, making it impractical unless the keyspace is small.The two major approaches to implementing dictionaries are a hash table or a search tree.\\n\\n\\n=== Hash table implementations ===\\n\\nThe most frequently used general purpose implementation of an associative array is with a hash table: an array combined with a hash function that separates each key into a separate \"bucket\" of the array. The basic idea behind a hash table is that accessing an element of an array via its index is a simple, constant-time operation. Therefore, the average overhead of an operation for a hash table is only the computation of the key\\'s hash, combined with accessing the corresponding bucket within the array. As such, hash tables usually perform in O(1) time, and outperform alternatives in most situations.\\nHash tables need to be able to handle collisions: when the hash function maps two different keys to the same bucket of the array. The two most widespread approaches to this problem are separate chaining and open addressing. In separate chaining, the array does not store the value itself but stores a pointer to another container, usually an association list, that stores all of the values matching the hash. On the other hand, in open addressing, if a hash collision is found, then the table seeks an empty spot in an array to store the value in a deterministic manner, usually by looking at the next immediate position in the array.\\nOpen addressing has a lower cache miss ratio than separate chaining when the table is mostly empty. However, as the table becomes filled with more elements, open addressing\\'s performance degrades exponentially. Additionally, separate chaining uses less memory in most cases, unless the entries are very small (less than four times the size of a pointer).\\n\\n\\n=== Tree implementations ===\\n\\n\\n==== Self-balancing binary search trees ====\\nAnother common approach is to implement an associative array with a self-balancing binary search tree, such as an AVL tree or a red–black tree.Compared to hash tables, these structures have both advantages and weaknesses. The worst-case performance of self-balancing binary search trees is significantly better than that of a hash table, with a time complexity in big O notation of O(log n). This is in contrast to hash tables, whose worst-case performance involves all elements sharing a single bucket, resulting in O(n) time complexity. In addition, and like all binary search trees, self-balancing binary search trees keep their elements in order. Thus, traversing its elements follows a least-to-greatest pattern, whereas traversing a hash table can result in elements being in seemingly random order. However, hash tables have a much better average-case time complexity than self-balancing binary search trees of O(1), and their worst-case performance is highly unlikely when a good hash function is used.\\nIt is worth noting that a self-balancing binary search tree can be used to implement the buckets for a hash table that uses separate chaining. This allows for average-case constant lookup, but assures a worst-case performance of O(log n). However, this introduces extra complexity into the implementation, and may cause even worse performance for smaller hash tables, where the time spent inserting into and balancing the tree is greater than the time needed to perform a linear search on all of the elements of a linked list or similar data structure.\\n\\n\\n==== Other trees ====\\nAssociative arrays may also be stored in unbalanced binary search trees or in data structures specialized to a particular type of keys such as radix trees, tries, Judy arrays, or van Emde Boas trees, though the ability of these implementation methods within comparison to hash tables varies; for instance, Judy trees remain indicated to perform with a smaller quantity of efficiency than hash tables, while carefully selected hash tables generally perform with increased efficiency in comparison to adaptive radix trees, with potentially greater restrictions on the types of data that they can handle. The advantages of these alternative structures come from their ability to handle operations beyond the basic ones of an associative array, such as finding the mapping whose key is the closest to a queried key, when the query is not itself present in the set of mappings.\\n\\n\\n=== Comparison ===\\n\\n\\n== Ordered dictionary ==\\nThe basic definition of the dictionary does not mandate an order. To guarantee a fixed order of enumeration, ordered versions of the associative array are often used. There are two senses of an ordered dictionary:\\n\\nThe order of enumeration is always deterministic for a given set of keys by sorting. This is the case for tree-based implementations, one representative being the <map> container of C++.\\nThe order of enumeration is key-independent and is instead based on the order of insertion. This is the case for the \"ordered dictionary\" in .NET Framework and Python.The latter sense of ordered dictionaries are more commonly encountered. They can be implemented using an association list, or by overlaying a doubly linked list on top of a normal dictionary. The latter approach, as used by CPython before version 3.6, has the advantage of keeping the potentially better complexity of another implementation. CPython 3.6+ makes dictionaries ordered by splitting the hash table into an insertion-ordered array of k-v pairs and a sparse array (\"hash table\") of indices.\\n\\n\\n== Language support ==\\n\\nAssociative arrays can be implemented in any programming language as a package and many language systems provide them as part of their standard library. In some languages, they are not only built into the standard system, but have special syntax, often using array-like subscripting.\\nBuilt-in syntactic support for associative arrays was introduced in 1969 by SNOBOL4, under the name \"table\". TMG offered tables with string keys and integer values. MUMPS made multi-dimensional associative arrays, optionally persistent, its key data structure. SETL supported them as one possible implementation of sets and maps. Most modern scripting languages, starting with AWK and including Rexx,  Perl, PHP, Tcl, JavaScript, Maple, Python, Ruby, Wolfram Language, Go, and Lua, support associative arrays as a primary container type. In many more languages, they are available as library functions without special syntax.\\nIn Smalltalk, Objective-C, .NET, Python, REALbasic, Swift, VBA and Delphi they are called dictionaries; in Perl, Ruby and Seed7 they are called hashes; in C++, Java, Go, Clojure, Scala, OCaml, Haskell they are called maps (see map (C++), unordered_map (C++), and Map); in Common Lisp and Windows PowerShell, they are called hash tables (since both typically use this implementation);  in Maple and Lua, they are called tables. In PHP, all arrays can be associative, except that the keys are limited to integers and strings. In JavaScript (see also JSON), all objects behave as associative arrays with string-valued keys, while the Map and WeakMap types take arbitrary objects as keys. In Lua, they are used as the primitive building block for all data structures. In Visual FoxPro, they are called Collections. The D language also has support for associative arrays.\\n\\n\\n== Permanent storage ==\\n\\nMany programs using associative arrays will at some point need to store that data in a more permanent form, like in a computer file. A common solution to this problem is a generalized concept known as archiving or serialization, which produces a text or binary representation of the original objects that can be written directly to a file. This is most commonly implemented in the underlying object model, like .Net or Cocoa, which include standard functions that convert the internal data into text form. The program can create a complete text representation of any group of objects by calling these methods, which are almost always already implemented in the base associative array class.For programs that use very large data sets, this sort of individual file storage is not appropriate, and a database management system (DB) is required. Some DB systems natively store associative arrays by serializing the data and then storing that serialized data and the key. Individual arrays can then be loaded or saved from the database using the key to refer to them. These key–value stores have been used for many years and have a history as long as that as the more common relational database (RDBs), but a lack of standardization, among other reasons, limited their use to certain niche roles. RDBs were used for these roles in most cases, although saving objects to a RDB can be complicated, a problem known as object-relational impedance mismatch.\\nAfter c.\\u20092010, the need for high performance databases suitable for cloud computing and more closely matching the internal structure of the programs using them led to a renaissance in the key–value store market. These systems can store and retrieve associative arrays in a native fashion, which can greatly improve performance in common web-related workflows.\\n\\n\\n== See also ==\\n\\nKey–value database\\nTuple\\nFunction (mathematics)\\nJSON\\n\\n\\n== References ==\\n\\n\\n== External links ==\\nNIST\\'s Dictionary of Algorithms and Data Structures: Associative Array', 'In computing, a hash table (hash map) is a data structure that implements an associative array abstract data type, a structure that can map keys to values. A hash table uses a hash function to compute an index, also called a hash code, into an array of buckets or slots, from which the desired value can be found. During lookup, the key is hashed and the resulting hash indicates where the corresponding value is stored.\\nIdeally, the hash function will assign each key to a unique bucket, but most hash table designs employ an imperfect hash function, which might cause hash collisions where the hash function generates the same index for more than one key. Such collisions are typically accommodated in some way.\\nIn a well-dimensioned hash table, the average cost (number of instructions) for each lookup is independent of the number of elements stored in the table. Many hash table designs also allow arbitrary insertions and deletions of key–value pairs, at (amortized) constant average cost per operation.In many situations, hash tables turn out to be on average more efficient than search trees or any other table lookup structure. For this reason, they are widely used in many kinds of computer software, particularly for associative arrays, database indexing, caches, and sets.\\n\\n\\n== Hashing ==\\n\\nThe advantage of using hashing is that the table address of a record can be directly computed from the key. Hashing implies a function \\n  \\n    \\n      \\n        h\\n      \\n    \\n    {\\\\displaystyle h}\\n  , when applied to a key \\n  \\n    \\n      \\n        k\\n      \\n    \\n    {\\\\displaystyle k}\\n  , produces a hash \\n  \\n    \\n      \\n        M\\n      \\n    \\n    {\\\\displaystyle M}\\n  . However, since \\n  \\n    \\n      \\n        M\\n      \\n    \\n    {\\\\displaystyle M}\\n   could be potentially large, the hash result should be mapped to finite entries in the hash table—or slots—several methods can be used to map the keys into the size of hash table \\n  \\n    \\n      \\n        N\\n      \\n    \\n    {\\\\displaystyle N}\\n  . The most common method is the division method, in which modular arithmetic is used in computing the slot.:\\u200a110\\u200a\\n\\n  \\n    \\n      \\n        h\\n        (\\n        k\\n        )\\n         \\n        =\\n         \\n        M\\n         \\n        m\\n        o\\n        d\\n         \\n        N\\n      \\n    \\n    {\\\\displaystyle h(k)\\\\ =\\\\ M\\\\ mod\\\\ N}\\n  This is often done in two steps,\\n\\n  \\n    \\n      \\n        \\n          Hash\\n        \\n        =\\n        \\n          Hash-Fuction(Key)\\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\text{Hash}}={\\\\text{Hash-Fuction(Key)}}}\\n  \\n\\n  \\n    \\n      \\n        \\n          Index\\n        \\n        =\\n        \\n          Hash\\n        \\n         \\n        %\\n         \\n        \\n          Hash-Table-Size\\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\text{Index}}={\\\\text{Hash}}\\\\ \\\\%\\\\ {\\\\text{Hash-Table-Size}}}\\n  \\n\\n\\n=== Choosing a hash function ===\\nA basic requirement is that the function should provide a uniform distribution of hash values. A non-uniform distribution increases the number of collisions and the cost of resolving them. Uniformity is sometimes difficult to ensure by design, but may be evaluated empirically using statistical tests, e.g., a Pearson\\'s chi-squared test for discrete uniform distributions.The distribution needs to be uniform only for table sizes that occur in the application. In particular, if one uses dynamic resizing with exact doubling and halving of the table size, then the hash function needs to be uniform only when the size is a power of two. Here the index can be computed as some range of bits of the hash function. On the other hand, some hashing algorithms prefer to have the size be a prime number. The modulus operation may provide some additional mixing; this is especially useful with a poor hash function.\\nFor open addressing schemes, the hash function should also avoid clustering, the mapping of two or more keys to consecutive slots. Such clustering may cause the lookup cost to skyrocket, even if the load factor is low and collisions are infrequent. The popular multiplicative hash is claimed to have particularly poor clustering behavior.Cryptographic hash functions are believed to provide good hash functions for any table size, either by modulo reduction or by bit masking. They may also be appropriate if there is a risk of malicious users trying to sabotage a network service by submitting requests designed to generate a large number of collisions in the server\\'s hash tables. However, the risk of sabotage can also be avoided by cheaper methods (such as applying a secret salt to the data). A drawback of cryptographic hashing functions is that they are often slower to compute, which means that in cases where the uniformity for  any size is not necessary, a non-cryptographic hashing function might be preferable.K-independent hashing offers a way to prove a certain hash function doesn\\'t have bad keysets for a given type of hashtable.\\nA number of such results are known for collision resolution schemes such as linear probing and cuckoo hashing.\\nSince K-independence can prove a hash function works, one can then focus on finding the fastest possible such hash function.\\nUniversal hash function is an approach of choosing a hash function randomly in a way that the hash function is independent of the keys that are to be hashed by the function. The possibility of collision between two distinct keys from a set is no more than \\n  \\n    \\n      \\n        1\\n        \\n          /\\n        \\n        m\\n      \\n    \\n    {\\\\displaystyle 1/m}\\n   where \\n  \\n    \\n      \\n        m\\n      \\n    \\n    {\\\\displaystyle m}\\n   is cardinality.:\\u200a264\\u200a\\n\\n\\n=== Perfect hash function ===\\n\\nIf all keys are known ahead of time, a perfect hash function can be used to create a perfect hash table that has no collisions. If minimal perfect hashing is used, every location in the hash table can be used as well.Perfect hashing allows for constant time lookups in all cases. This is in contrast to most chaining and open addressing methods, where the time for lookup is low on average, but may be very large, O(n), for instance when all the keys hash to a few values.\\n\\n\\n== Key statistics ==\\nA critical statistic for a hash table is the load factor, defined as\\n\\n  \\n    \\n      \\n        l\\n        o\\n        a\\n        d\\n        f\\n        a\\n        c\\n        t\\n        o\\n        r\\n         \\n        (\\n        α\\n        )\\n        =\\n        \\n          \\n            n\\n            k\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle loadfactor\\\\ (\\\\alpha )={\\\\frac {n}{k}}}\\n  ,where\\n\\n  \\n    \\n      \\n        n\\n      \\n    \\n    {\\\\displaystyle n}\\n   is the number of entries occupied in the hash table.\\n\\n  \\n    \\n      \\n        k\\n      \\n    \\n    {\\\\displaystyle k}\\n   is the number of buckets.The performance of the hash table worsens in relation to the load factor (\\n  \\n    \\n      \\n        α\\n      \\n    \\n    {\\\\displaystyle \\\\alpha }\\n  ) i.e. as \\n  \\n    \\n      \\n        α\\n      \\n    \\n    {\\\\displaystyle \\\\alpha }\\n   approaches 1. Hence, it\\'s essential to resize—or \"rehash\"—the hash table when the load factor exceeds an ideal value. It\\'s also efficient to resize the hash table if the size is smaller—which is usually done when load factor drops below \\n  \\n    \\n      \\n        \\n          α\\n          \\n            m\\n            a\\n            x\\n          \\n        \\n        \\n          /\\n        \\n        4\\n      \\n    \\n    {\\\\displaystyle \\\\alpha _{max}/4}\\n  . Generally, a load factor of 0.6 and 0.75 is an acceptable figure.:\\u200a110\\u200a\\n\\n\\n== Collision resolution ==\\n\\nThe search algorithm that uses hashing consists of two parts. The first part is computing a hash function which transforms the search key into an array index. The ideal case is such that no two search keys hashes to the same array index, however, this is not always the case, since it\\'s theoretically impossible.:\\u200a515\\u200a Hence the second part of the algorithm is collision resolution. The two common methods for collision resolution are separate chaining and open addressing.:\\u200a458\\u200a\\n\\n\\n=== Separate chaining ===\\n\\nHashing is an example of space-time tradeoff. If there exists a condition where the memory is infinite, single memory access using the key as an index in a (potentially huge) array would retrieve the value—which also implies possible key values are huge. On the other hand, if time is infinite, the values can be stored in minimum possible memory and a linear search through the array can be used to retrieve the element.:\\u200a458\\u200a In separate chaining, the process involves building a linked list with key-value pair for each search array indices. The collided items are chained together through a single linked list, which can be traversed to access the item with a unique search key.:\\u200a464\\u200a Collision resolution through charming i.e. with a linked list is a common method of implementation. Let \\n  \\n    \\n      \\n        T\\n      \\n    \\n    {\\\\displaystyle T}\\n   and \\n  \\n    \\n      \\n        x\\n      \\n    \\n    {\\\\displaystyle x}\\n   be the hash table and the node respectively, the operation involves as follows::\\u200a258\\u200a\\nIf the keys of the elements are ordered, it\\'s efficient to insert the item by maintaining the order when the key is comparable either numerically or lexically, thus resulting in faster insertions and unsuccessful searches.:\\u200a520-521\\u200a However, the standard method of using a linked list is not cache-conscious since there is little spatial locality—locality of reference—since the nodes of the list are scattered across the memory, hence doesn\\'t make efficient use of CPU cache.:\\u200a91\\u200a\\n\\n\\n==== Separate chaining with other structures ====\\nIf the keys are ordered, it could be efficient to use \"self-organizing\" concepts such as using a self-balancing binary search tree, through which the theoretical worse case could be bought down to \\n  \\n    \\n      \\n        O\\n        (\\n        log\\n        \\u2061\\n        \\n          n\\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle O(\\\\log {n})}\\n  , although it introduces additional complexities.:\\u200a521\\u200aIn cache-conscious variants, a dynamic array found to be more cache-friendly is used in the place where a linked list or self-balancing binary search trees is usually deployed for collision resolution through separate chaining, since the contiguous allocation patten of  the array could be exploited by hardware-cache prefetchers—such as translation lookaside buffer—resulting in reduced access time and memory consumption.In dynamic perfect hashing, two level hash tables are used to reduce the look-up complexity to be a guaranteed \\n  \\n    \\n      \\n        O\\n        (\\n        1\\n        )\\n      \\n    \\n    {\\\\displaystyle O(1)}\\n   in the worse case. In this technique, the buckets of \\n  \\n    \\n      \\n        k\\n      \\n    \\n    {\\\\displaystyle k}\\n   entries are organized as perfect hash tables with \\n  \\n    \\n      \\n        \\n          k\\n          \\n            2\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle k^{2}}\\n   slots providing constant worst-case lookup time, and low amortized time for insertion. A study shows array based separate chaining to be 97% more performant when compared to standard the linked list method under heavy load.:\\u200a99\\u200aTechniques such as using fusion tree for each buckets also result in constant time for all operations with high probability.\\n\\n\\n=== Open addressing ===\\n\\nIn another strategy, called open addressing, all entry records are stored in the bucket array itself. When a new entry has to be inserted, the buckets are examined, starting with the hashed-to slot and proceeding in some probe sequence, until an unoccupied slot is found. When searching for an entry, the buckets are scanned in the same sequence,  until either the target record is found, or an unused array slot is found, which indicates that there is no such key in the table. The name \"open addressing\" refers to the location (\"address\") of the item is not determined by its hash value. (This method is also called  closed hashing; it should not be confused with \"open hashing\" or \"closed addressing\" which usually means separate chaining.)\\nWell-known probe sequences include:\\n\\nLinear probing, in which the interval between probes is fixed (usually 1). Since the slots are located in successive locations, linear probing could lead to better utilization of CPU cache due to locality of references.\\nQuadratic probing, in which the interval between probes is increased by adding the successive outputs of a quadratic polynomial to the starting value given by the original hash computation\\nDouble hashing, in which the interval between probes is computed by a second hash functionIn practice, the performance of open addressing is slower than separate chaining when used in conjunction with an array of buckets for collusion resolution,:\\u200a93\\u200a since a longer sequence of array indices may need to be tried to find a given element when the load factor approaches 1. The load factor must be maintained below 1 since if the reaches 1—in case of a completely full table—a search miss would go into an infinite loop through the table.:\\u200a471\\u200a The average cost of linear probing depends on the chosen hash function\\'s ability to distribute the keys uniformly throughout the table to avoid clustering, since formation of clusters would result in increased search time leading to inefficiency.:\\u200a472\\u200a\\n\\n\\n==== Coalesced hashing ====\\n\\nCoalesced hashing is a hybrid of both separate charming and open addressing in which the buckets or nodes link within the table.:\\u200a6-8\\u200a The algorithm is ideally suited for fixed memory allocation.:\\u200a4\\u200a The collision in coalesced hashing is resolved by identifying the largest-indexed empty slot on the hash table, then the colliding value is inserted into that slot. The bucket is also linked to the inserted node\\'s slot which contains its colliding hash address.:\\u200a8\\u200a\\n\\n\\n==== Cuckoo hashing ====\\n\\nCuckoo hashing is a form of open addressing collision resolution technique which provides guarantees \\n  \\n    \\n      \\n        O\\n        (\\n        1\\n        )\\n      \\n    \\n    {\\\\displaystyle O(1)}\\n   worst-case lookup complexity and constant amortized time for insertions. The collision is resolved through maintaining two hash tables, each having its own hashing function, and collided slot gets replaced with the given item, and the preoccupied element of the slot gets displaced into the other hash table. The process continues until every key has its own spot in the empty buckets of the tables; if the procedure enters into infinite loop—which is identified through maintaining a threshold loop counter—both hash tables get rehashed with newer hash functions and the procedure continues.:\\u200a124-125\\u200a\\n\\n\\n==== Hopscotch hashing ====\\n\\nHopscotch hashing is an open addressing based algorithm which combines the elements of cuckoo hashing, linear probing and chaining through the notion of a neighbourhood of buckets—the subsequent buckets around any given occupied bucket, also called as \"virtual\" bucket.:\\u200a351-352\\u200a The algorithm is designed to deliver better performance when the load factor of the hash table grows beyond 90%; it also provides high throughput in concurrent settings, thus well suited for implementing resizable concurrent hash table.:\\u200a350\\u200a The neighbourhood characteristic of hopscotch hashing guarantees a property that, the cost of finding the desired item from any given buckets within the neighbourhood is very close to the cost of finding it in the bucket itself; the algorithm attempts to be an item into its neighbourhood—with a possible cost involved in displacing other items.:\\u200a352\\u200aEach bucket within the hash table includes an additional \"hop-information\"—an H-bit bit array for indicating the relative distance of the item which was originally hashed into the current virtual bucket within H-1 entries.:\\u200a352\\u200a Let \\n  \\n    \\n      \\n        k\\n      \\n    \\n    {\\\\displaystyle k}\\n   and \\n  \\n    \\n      \\n        B\\n        k\\n      \\n    \\n    {\\\\displaystyle Bk}\\n   be the key to be inserted and bucket to which the key is hashed into respectively; several cases are involved in the insertion procedure such that the neighbourhood property of the algorithm is vowed::\\u200a352-353\\u200a if \\n  \\n    \\n      \\n        B\\n        k\\n      \\n    \\n    {\\\\displaystyle Bk}\\n   is empty, the element is inserted, and the leftmost bit of bitmap is set to 1; if not empty, linear probing is used for finding an empty slot in the table, the bitmap of the bucket gets updated followed by the insertion; if the empty slot is not within the range of the neighbourhood, i.e. H-1, subsequent swap and hop-info bit array manipulation of each bucket is performed in accordance with its neighbourhood invariant properties.:\\u200a353\\u200a\\n\\n\\n==== Robin Hood hashing ====\\nRobin hood hashing is an open addressing based collision resolution algorithm; the collisions are resolved through flavouring the displacement of the element that is farthest—or highest probe sequence length (PSL)—from its \"home location\" i.e. the bucket to which the item was hashed into.:\\u200a12\\u200a Although robin hood hashing does not change the theoritical search cost, it significantly effects the variance of the distribution of the items on the buckets,:\\u200a2\\u200a i.e. dealing with cluster formation in the hash table. Each node within the hash table that uses robin hood hashing should be augmented to store an extra PSL value. Let \\n  \\n    \\n      \\n        x\\n      \\n    \\n    {\\\\displaystyle x}\\n   be the key to be inserted, \\n  \\n    \\n      \\n        x\\n        .\\n        p\\n        s\\n        l\\n      \\n    \\n    {\\\\displaystyle x.psl}\\n   be the (incremental) PSL length of \\n  \\n    \\n      \\n        x\\n      \\n    \\n    {\\\\displaystyle x}\\n  , \\n  \\n    \\n      \\n        T\\n      \\n    \\n    {\\\\displaystyle T}\\n   be the hash table and \\n  \\n    \\n      \\n        j\\n      \\n    \\n    {\\\\displaystyle j}\\n   be the index, the insertion procedure is as follows::\\u200a12-13\\u200a:\\u200a5\\u200a\\nIf \\n  \\n    \\n      \\n        x\\n        .\\n        p\\n        s\\n        l\\n         \\n        ≤\\n         \\n        T\\n        [\\n        j\\n        ]\\n        .\\n        p\\n        s\\n        l\\n      \\n    \\n    {\\\\displaystyle x.psl\\\\ \\\\leq \\\\ T[j].psl}\\n  : the iteration goes into next bucket without attempting an external prob.\\nIf \\n  \\n    \\n      \\n        x\\n        .\\n        p\\n        s\\n        l\\n         \\n        >\\n         \\n        T\\n        [\\n        j\\n        ]\\n        .\\n        p\\n        s\\n        l\\n      \\n    \\n    {\\\\displaystyle x.psl\\\\ >\\\\ T[j].psl}\\n  : insert the item \\n  \\n    \\n      \\n        x\\n      \\n    \\n    {\\\\displaystyle x}\\n   into the bucket \\n  \\n    \\n      \\n        j\\n      \\n    \\n    {\\\\displaystyle j}\\n  ; swap \\n  \\n    \\n      \\n        x\\n      \\n    \\n    {\\\\displaystyle x}\\n   with \\n  \\n    \\n      \\n        T\\n        [\\n        j\\n        ]\\n      \\n    \\n    {\\\\displaystyle T[j]}\\n  —let it be \\n  \\n    \\n      \\n        \\n          x\\n          ′\\n        \\n      \\n    \\n    {\\\\displaystyle x\\'}\\n  ; continue the prob from \\n  \\n    \\n      \\n        j\\n        +\\n        1\\n      \\n    \\n    {\\\\displaystyle j+1}\\n   bucket to insert \\n  \\n    \\n      \\n        \\n          x\\n          ′\\n        \\n      \\n    \\n    {\\\\displaystyle x\\'}\\n  ; repeat the procedure until every element is inserted.\\n\\n\\n== Dynamic resizing ==\\nWhen an insert is made such that the number of entries in a hash table exceeds the product of the load factor and the current capacity then the hash table will need to be rehashed. Rehashing includes increasing the size of the underlying data structure and mapping existing items to new bucket locations. In some implementations, if the initial capacity is greater than the maximum number of entries divided by the load factor, no rehash operations will ever occur.To limit the proportion of memory wasted due to empty buckets, some implementations also shrink the size of the table—followed by a rehash—when items are deleted. From the point of space–time tradeoffs, this operation is similar to the deallocation in dynamic arrays.\\n\\n\\n=== Resizing by copying all entries ===\\nA common approach is to automatically trigger a complete resizing when the load factor exceeds some threshold rmax. Then a new larger table is allocated, each entry is removed from the old table, and inserted into the new table. When all entries have been removed from the old table then the old table is returned to the free storage pool. Likewise, when the load factor falls below a second threshold rmin, all entries are moved to a new smaller table.\\nFor hash tables that shrink and grow frequently, the resizing downward can be skipped entirely. In this case, the table size is proportional to the maximum number of entries that ever were in the hash table at one time, rather than the current number. The disadvantage is that memory usage will be higher, and thus cache behavior may be worse. For best control, a \"shrink-to-fit\" operation can be provided that does this only on request.\\nIf the table size increases or decreases by a fixed percentage at each expansion, the total cost of these resizings, amortized over all insert and delete operations, is still a constant, independent of the number of entries n and of the number m of operations performed.\\nFor example, consider a table that was created with the minimum possible size and is doubled each time the load ratio exceeds some threshold. If m elements are inserted into that table, the total number of extra re-insertions that occur in all dynamic resizings of the table is at most m − 1. In other words, dynamic resizing roughly doubles the cost of each insert or delete operation.\\n\\n\\n=== Alternatives to all-at-once rehashing ===\\nSome hash table implementations, notably in real-time systems, cannot pay the price of enlarging the hash table all at once, because it may interrupt time-critical operations. If one cannot avoid dynamic resizing, a solution is to perform the resizing gradually.\\nDisk-based hash tables almost always use some alternative to all-at-once rehashing, since the cost of rebuilding the entire table on disk would be too high.\\n\\n\\n==== Incremental resizing ====\\nOne alternative to enlarging the table all at once is to perform the rehashing gradually:\\n\\nDuring the resize, allocate the new hash table, but keep the old table unchanged.\\nIn each lookup or delete operation, check both tables.\\nPerform insertion operations only in the new table.\\nAt each insertion also move r elements from the old table to the new table.\\nWhen all elements are removed from the old table, deallocate it.To ensure that the old table is completely copied over before the new table itself needs to be enlarged, it\\nis necessary to increase the size of the table by a factor of at least (r + 1)/r during resizing.\\n\\n\\n==== Monotonic keys ====\\nIf it is known that keys will be stored in monotonically increasing (or decreasing) order, then a variation of consistent hashing can be achieved.\\nGiven some initial key k1, a subsequent key ki partitions the key domain [k1, ∞) into the set {[k1, ki), [ki, ∞)}. In general, repeating this process gives a finer partition {[k1, ki0), [ki0, ki1), ..., [kin - 1, kin), [kin, ∞)} for some sequence of monotonically increasing keys (ki0, ..., kin), where n is the number of refinements. The same process applies, mutatis mutandis, to monotonically decreasing keys. By assigning to each subinterval of this partition a different hash function or hash table (or both), and by refining the partition whenever the hash table is resized, this approach guarantees that any key\\'s hash, once issued, will never change, even when the hash table is grown.\\nSince it is common to grow the overall number of entries by doubling, there will only be O(log(N)) subintervals to check, and binary search time for the redirection will be O(log(log(N))).\\n\\n\\n==== Linear hashing ====\\nLinear hashing is a hash table algorithm that permits incremental hash table expansion. It is implemented using a single hash table, but with two possible lookup functions.\\n\\n\\n==== Hashing for distributed hash tables ====\\nAnother way to decrease the cost of table resizing is to choose a hash function in such a way that the hashes of most values do not change when the table is resized. Such hash functions are prevalent in disk-based and distributed hash tables, where rehashing is prohibitively costly.\\nThe problem of designing a hash such that most values do not change when the table is resized is known as the distributed hash table problem.\\nThe four most popular approaches are rendezvous hashing, consistent hashing, the content addressable network algorithm, and Kademlia distance.\\n\\n\\n== Performance ==\\n\\n\\n=== Speed analysis ===\\nIn the simplest model, the hash function is completely unspecified and the table does not resize. With an ideal hash function, a table of size \\n  \\n    \\n      \\n        k\\n      \\n    \\n    {\\\\displaystyle k}\\n   with open addressing has no collisions and holds up to \\n  \\n    \\n      \\n        k\\n      \\n    \\n    {\\\\displaystyle k}\\n   elements with a single comparison for successful lookup, while a table of size \\n  \\n    \\n      \\n        k\\n      \\n    \\n    {\\\\displaystyle k}\\n   with chaining and \\n  \\n    \\n      \\n        n\\n      \\n    \\n    {\\\\displaystyle n}\\n   keys has the minimum \\n  \\n    \\n      \\n        max\\n        (\\n        0\\n        ,\\n        n\\n        −\\n        k\\n        )\\n      \\n    \\n    {\\\\displaystyle \\\\max(0,n-k)}\\n   collisions and \\n  \\n    \\n      \\n        Θ\\n        (\\n        \\n          \\n            n\\n            k\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle \\\\Theta ({\\\\frac {n}{k}})}\\n   comparisons for lookup. With the worst possible hash function, every insertion causes a collision, and hash tables degenerate to linear search, with \\n  \\n    \\n      \\n        Θ\\n        (\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle \\\\Theta (n)}\\n   amortized comparisons per insertion and up to \\n  \\n    \\n      \\n        n\\n      \\n    \\n    {\\\\displaystyle n}\\n   comparisons for a successful lookup.\\nAdding rehashing to this model is straightforward. As in a dynamic array, geometric resizing by a factor of \\n  \\n    \\n      \\n        b\\n      \\n    \\n    {\\\\displaystyle b}\\n   implies that only \\n  \\n    \\n      \\n        \\n          \\n            n\\n            \\n              b\\n              \\n                i\\n              \\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\frac {n}{b^{i}}}}\\n   keys are inserted \\n  \\n    \\n      \\n        i\\n      \\n    \\n    {\\\\displaystyle i}\\n   or more times, so that the total number of insertions is bounded above by \\n  \\n    \\n      \\n        \\n          \\n            \\n              b\\n              n\\n            \\n            \\n              b\\n              −\\n              1\\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\frac {bn}{b-1}}}\\n  , which is \\n  \\n    \\n      \\n        Θ\\n        (\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle \\\\Theta (n)}\\n  . By using rehashing to maintain \\n  \\n    \\n      \\n        n\\n        <\\n        k\\n      \\n    \\n    {\\\\displaystyle n<k}\\n  , tables using both chaining and open addressing can have unlimited elements and perform successful lookup in a single comparison for the best choice of hash function.\\nIn more realistic models, the hash function is a random variable over a probability distribution of hash functions, and performance is computed on average over the choice of hash function. When this distribution is uniform, the assumption is called \"simple uniform hashing\" and it can be shown that hashing with chaining requires \\n  \\n    \\n      \\n        Θ\\n        \\n          (\\n          \\n            1\\n            +\\n            \\n              \\n                n\\n                k\\n              \\n            \\n          \\n          )\\n        \\n      \\n    \\n    {\\\\displaystyle \\\\Theta \\\\left(1+{\\\\frac {n}{k}}\\\\right)}\\n   comparisons on average for an unsuccessful lookup, and hashing with open addressing requires \\n  \\n    \\n      \\n        Θ\\n        \\n          (\\n          \\n            \\n              1\\n              \\n                1\\n                −\\n                n\\n                \\n                  /\\n                \\n                k\\n              \\n            \\n          \\n          )\\n        \\n      \\n    \\n    {\\\\displaystyle \\\\Theta \\\\left({\\\\frac {1}{1-n/k}}\\\\right)}\\n  . Both these bounds are constant, if we maintain \\'\\n  \\n    \\n      \\n        \\n          \\n            n\\n            k\\n          \\n        \\n        <\\n        c\\n      \\n    \\n    {\\\\displaystyle {\\\\frac {n}{k}}<c}\\n   using table resizing, where \\n  \\n    \\n      \\n        c\\n      \\n    \\n    {\\\\displaystyle c}\\n   is a fixed constant less than 1.\\nTwo factors affect significantly the latency of operations on a hash table:\\nCache missing. With the increasing of load factor, the search and insertion performance of hash tables can be degraded a lot due to the rise of average cache missing.\\nCost of resizing. Resizing becomes an extreme time-consuming task when hash tables grow massive.In latency-sensitive programs, the time consumption of operations on both the average and the worst cases are required to be small, stable, and even predictable. The K hash table  is designed for a general scenario of low-latency applications, aiming to achieve cost-stable operations on a growing huge-sized table.\\n\\n\\n=== Memory utilization ===\\nSometimes the memory requirement for a table needs to be minimized.  One way to reduce memory usage in chaining methods is to eliminate some of the chaining pointers or to replace them with some form of abbreviated pointers.\\nAnother technique was introduced by Donald Knuth where it is called abbreviated keys.  (Bender et al. wrote that Knuth called it quotienting.)  For this discussion assume that the key, or a reversibly-hashed version of that key, is an integer m from {0, 1, 2, ..., M-1} and the number of buckets is N.  m is divided by N to produce a quotient q and a remainder r.  The remainder r is used to select the bucket; in the bucket only the quotient q need be stored.  This saves log2(N) bits per element, which can be significant in some applications.Quotienting works readily with chaining hash tables, or with simple cuckoo hash tables.  To apply the technique with ordinary open-addressing hash tables, John G. Cleary introduced a method where two bits (a virgin bit and a change bit) are included in each bucket to allow the original bucket index (r) to be reconstructed.\\nIn the scheme just described, log2(M/N) + 2 bits are used to store each key.  It is interesting to note that the theoretical minimum storage would be log2(M/N) + 1.4427 bits where 1.4427 = log2(e).\\n\\n\\n== Features ==\\n\\n\\n=== Advantages ===\\nThe main advantage of hash tables over other table data structures is speed. This advantage is more apparent when the number of entries is large. Hash tables are particularly efficient when the maximum number of entries can be predicted, so that the bucket array can be allocated once with the optimum size and never resized.\\nIf the set of key–value pairs is fixed and known ahead of time (so insertions and deletions are not allowed), one may reduce the average lookup cost by a careful choice of the hash function, bucket table size, and internal data structures. In particular, one may be able to devise a hash function that is collision-free, or even perfect. In this case the keys need not be stored in the table.\\n\\n\\n=== Drawbacks ===\\nAlthough operations on a hash table take constant time on average, the cost of a good hash function can be significantly higher than the inner loop of the lookup algorithm for a sequential list or search tree. Thus hash tables are not effective when the number of entries is very small. (However, in some cases the high cost of computing the hash function can be mitigated by saving the hash value together with the key.)\\nFor certain string processing applications, such as spell-checking, hash tables may be less efficient than tries, finite automata, or Judy arrays. Also, if there are not too many possible keys to store—that is, if each key can be represented by a small enough number of bits—then, instead of a hash table, one may use the key directly as the index into an array of values. Note that there are no collisions in this case.\\nThe entries stored in a hash table can be enumerated efficiently (at constant cost per entry), but only in some pseudo-random order. Therefore, there is no efficient way to locate an entry whose key is nearest to a given key. Listing all n entries in some specific order generally requires a separate sorting step, whose cost is proportional to log(n) per entry. In comparison, ordered search trees have lookup and insertion cost proportional to log(n), but allow finding the nearest key at about the same cost, and ordered enumeration of all entries at constant cost per entry. However, a LinkingHashMap can be made to create a hash table with a non-random sequence.\\nIf the keys are not stored (because the hash function is collision-free), there may be no easy way to enumerate the keys that are present in the table at any given moment.\\nAlthough the average cost per operation is constant and fairly small, the cost of a single operation may be quite high. In particular, if the hash table uses dynamic resizing, an insertion or deletion operation may occasionally take time proportional to the number of entries. This may be a serious drawback in real-time or interactive applications.\\nHash tables in general exhibit poor locality of reference—that is, the data to be accessed is distributed seemingly at random in memory. Because hash tables cause access patterns that jump around, this can trigger microprocessor cache misses that cause long delays. Compact data structures such as arrays searched with linear search may be faster, if the table is relatively small and keys are compact. The optimal performance point varies from system to system.\\nHash tables become quite inefficient when there are many collisions. While extremely uneven hash distributions are extremely unlikely to arise by chance, a malicious adversary with knowledge of the hash function may be able to supply information to a hash that creates worst-case behavior by causing excessive collisions, resulting in very poor performance, e.g., a denial of service attack. In critical applications, a data structure with better worst-case guarantees can be used; however, universal hashing or a keyed hash function, both of which prevents the attacker from predicting which inputs cause worst-case behavior, may be preferable.The hash function used by the hash table in the Linux routing table cache was changed with Linux version 2.4.2 as a countermeasure against such attacks. The ad hoc short-keyed hash construction was later updated to use a \"jhash\", and then SipHash.\\n\\n\\n== Uses ==\\n\\n\\n=== Associative arrays ===\\n\\nHash tables are commonly used to implement many types of in-memory tables. They are used to implement associative arrays (arrays whose indices are arbitrary strings or other complicated objects), especially in interpreted programming languages like Ruby, Python, and PHP.\\nWhen storing a new item into a multimap and a hash collision occurs, the multimap unconditionally stores both items.\\nWhen storing a new item into a typical associative array and a hash collision occurs, but the actual keys themselves are different, the associative array likewise stores both items. However, if the key of the new item exactly matches the key of an old item, the associative array typically erases the old item and overwrites it with the new item, so every item in the table has a unique key.\\n\\n\\n=== Database indexing ===\\nHash tables may also be used as disk-based data structures and database indices (such as in dbm) although B-trees are more popular in these applications. In multi-node database systems, hash tables are commonly used to distribute rows amongst nodes, reducing network traffic for hash joins.\\n\\n\\n=== Caches ===\\n\\nHash tables can be used to implement caches, auxiliary data tables that are used to speed up the access to data that is primarily stored in slower media. In this application, hash collisions can be handled by discarding one of the two colliding entries—usually erasing the old item that is currently stored in the table and overwriting it with the new item, so every item in the table has a unique hash value.\\n\\n\\n=== Sets ===\\nBesides recovering the entry that has a given key, many hash table implementations can also tell whether such an entry exists or not.\\nThose structures can therefore be used to implement a set data structure, which merely records whether a given key belongs to a specified set of keys. In this case, the structure can be simplified by eliminating all parts that have to do with the entry values. Hashing can be used to implement both static and dynamic sets.\\n\\n\\n=== Object representation ===\\nSeveral dynamic languages, such as Perl, Python, JavaScript, Lua, and Ruby, use hash tables to implement objects. In this representation, the keys are the names of the members and methods of the object, and the values are pointers to the corresponding member or method.\\n\\n\\n=== Unique data representation ===\\n\\nHash tables can be used by some programs to avoid creating multiple character strings with the same contents. For that purpose, all strings in use by the program are stored in a single string pool implemented as a hash table, which is checked whenever a new string has to be created. This technique was introduced in Lisp interpreters under the name hash consing, and can be used with many other kinds of data (expression trees in a symbolic algebra system, records in a database, files in a file system, binary decision diagrams, etc.).\\n\\n\\n=== Transposition table ===\\n\\nA transposition table to a complex Hash Table which stores information about each section that has been searched.\\n\\n\\n== Implementations ==\\n\\n\\n=== In programming languages ===\\nMany programming languages provide hash table functionality, either as built-in associative arrays or as standard library modules. In C++11, for example, the unordered_map class provides hash tables for keys and values of arbitrary type.\\nThe Java programming language (including the variant which is used on Android) includes the HashSet, HashMap, LinkedHashSet, and LinkedHashMap generic collections.In PHP 5 and 7, the Zend 2 engine and the Zend 3 engine (respectively) use one of the hash functions from Daniel J. Bernstein to generate the hash values used in managing the mappings of data pointers stored in a hash table. In the PHP source code, it is labelled as DJBX33A (Daniel J. Bernstein, Times 33 with Addition).\\nPython\\'s built-in hash table implementation, in the form of the dict type, as well as Perl\\'s hash type (%) are used internally to implement namespaces and therefore need to pay more attention to security, i.e., collision attacks.  Python sets also use hashes internally, for fast lookup (though they store only keys, not values). CPython 3.6+ uses an insertion-ordered variant of the hash table, implemented by splitting out the value storage into an array and having the vanilla hash table only store a set of indices.In the .NET Framework, support for hash tables is provided via the non-generic Hashtable and generic Dictionary classes, which store key–value pairs, and the generic HashSet class, which stores only values.\\nIn Ruby the hash table uses the open addressing model from Ruby 2.4 onwards.In Rust\\'s standard library, the generic HashMap and HashSet structs use linear probing with Robin Hood bucket stealing.\\nANSI Smalltalk defines the classes Set / IdentitySet and Dictionary / IdentityDictionary. All Smalltalk implementations provide additional (not yet standardized) versions of WeakSet, WeakKeyDictionary and WeakValueDictionary.\\nTcl array variables are hash tables, and Tcl dictionaries are immutable values based on hashes. The functionality is also available as C library functions Tcl_InitHashTable et al. (for generic hash tables) and Tcl_NewDictObj et al. (for dictionary values). The performance has been independently benchmarked as extremely competitive.The Wolfram Language supports hash tables since version 10. They are implemented under the name Association.\\nCommon Lisp provides the hash-table class for efficient mappings. In spite of its naming, the language standard does not mandate the actual adherence to any hashing technique for implementations.\\n\\n\\n== History ==\\nThe idea of hashing arose independently in different places. In January 1953, Hans Peter Luhn wrote an internal IBM memorandum that used hashing with chaining. Gene Amdahl, Elaine M. McGraw, Nathaniel Rochester, and Arthur Samuel implemented a program using hashing at about the same time. Open addressing with linear probing (relatively prime stepping) is credited to Amdahl, but Ershov (in Russia) had the same idea.\\n\\n\\n== See also ==\\nRabin–Karp string search algorithm\\nStable hashing\\nConsistent hashing\\nExtendible hashing\\nLazy deletion\\nPearson hashing\\nPhotoDNA\\nSearch data structure\\nConcurrent hash table\\nRecord (computer science)\\n\\n\\n=== Related data structures ===\\nThere are several data structures that use hash functions but cannot be considered special cases of hash tables:\\n\\nBloom filter, a memory-efficient data structure designed for constant-time approximate lookups; uses hash function(s) and can be seen as an approximate hash table.\\nDistributed hash table (DHT), a resilient dynamic table spread over several nodes of a network.\\nHash array mapped trie, a trie structure, similar to the array mapped trie, but where each key is hashed first.\\n\\n\\n== References ==\\n\\n\\n== Further reading ==\\nTamassia, Roberto; Goodrich, Michael T. (2006). \"Chapter Nine: Maps and Dictionaries\". Data structures and algorithms in Java : [updated for Java 5.0] (4th ed.). Hoboken, NJ: Wiley. pp. 369–418. ISBN 978-0-471-73884-8.\\nMcKenzie, B. J.; Harries, R.; Bell, T. (February 1990). \"Selecting a hashing algorithm\". Software Practice & Experience. 20 (2): 209–224. doi:10.1002/spe.4380200207. hdl:10092/9691. S2CID 12854386.\\n\\n\\n== External links ==\\nA Hash Function for Hash Table Lookup by Bob Jenkins.\\nHash functions by Paul Hsieh\\nDesign of Compact and Efficient Hash Tables for Java\\nNIST entry on hash tables\\nLecture on Hash Tables from Stanford\\'s CS106A\\nOpen Data Structures – Chapter 5 – Hash Tables, Pat Morin\\nMIT\\'s Introduction to Algorithms: Hashing 1 MIT OCW lecture Video\\nMIT\\'s Introduction to Algorithms: Hashing 2 MIT OCW lecture Video', 'In computer science, an array data structure, or simply an array, is a data structure consisting of a collection of elements (values or variables), each identified by at least one array index or key. An array is stored such that the position of each element can be computed from its index tuple by a mathematical formula. The simplest type of data structure is a linear array, also called one-dimensional array.\\nFor example, an array of 10 32-bit (4-byte) integer variables, with indices 0 through 9, may be stored as 10 words at memory addresses 2000, 2004, 2008, ..., 2036, (in hexadecimal: 0x7D0, 0x7D4, 0x7D8, ..., 0x7F4) so that the element with index i has the address 2000 + (i × 4).The memory address of the first element of an array is called first address, foundation address, or base address.\\nBecause the mathematical concept of a matrix can be represented as a two-dimensional grid, two-dimensional arrays are also sometimes called matrices. In some cases the term \"vector\" is used in computing to refer to an array, although tuples rather than vectors are the more mathematically correct equivalent. Tables are often implemented in the form of arrays, especially lookup tables; the word table is sometimes used as a synonym of array.\\nArrays are among the oldest and most important data structures, and are used by almost every program. They are also used to implement many other data structures, such as lists and strings. They effectively exploit the addressing logic of computers. In most modern computers and many external storage devices, the memory is a one-dimensional array of words, whose indices are their addresses. Processors, especially vector processors, are often optimized for array operations.\\nArrays are useful mostly because the element indices can be computed at run time. Among other things, this feature allows a single iterative statement to process arbitrarily many elements of an array. For that reason, the elements of an array data structure are required to have the same size and should use the same data representation. The set of valid index tuples and the addresses of the elements (and hence the element addressing formula) are usually, but not always, fixed while the array is in use.\\nThe term array is often used to mean array data type, a kind of data type provided by most high-level programming languages that consists of a collection of values or variables that can be selected by one or more indices computed at run-time. Array types are often implemented by array structures; however, in some languages they may be implemented by hash tables, linked lists, search trees, or other data structures.\\nThe term is also used, especially in the description of algorithms, to mean associative array or \"abstract array\", a theoretical computer science model (an abstract data type or ADT) intended to capture the essential properties of arrays.\\n\\n\\n== History ==\\nThe first digital computers used machine-language programming to set up and access array structures for data tables, vector and matrix computations, and for many other purposes. John von Neumann wrote the first array-sorting program (merge sort) in 1945, during the building of the first stored-program computer.p. 159 Array indexing was originally done by self-modifying code, and later using index registers and indirect addressing. Some mainframes designed in the 1960s, such as the Burroughs B5000 and its successors, used memory segmentation to perform index-bounds checking in hardware.Assembly languages generally have no special support for arrays, other than what the machine itself provides. The earliest high-level programming languages, including FORTRAN (1957), Lisp (1958), COBOL (1960), and ALGOL 60 (1960), had support for multi-dimensional arrays, and so has C (1972). In C++ (1983), class templates exist for multi-dimensional arrays whose dimension is fixed at runtime as well as for runtime-flexible arrays.\\n\\n\\n== Applications ==\\nArrays are used to implement mathematical vectors and matrices, as well as other kinds of rectangular tables. Many databases, small and large, consist of (or include) one-dimensional arrays whose elements are records.\\nArrays are used to implement other data structures, such as lists, heaps, hash tables, deques, queues, stacks, strings, and VLists. Array-based implementations of other data structures are frequently simple and space-efficient (implicit data structures), requiring little space overhead, but may have poor space complexity, particularly when modified, compared to tree-based data structures (compare a sorted array to a search tree).\\nOne or more large arrays are sometimes used to emulate in-program dynamic memory allocation, particularly memory pool allocation. Historically, this has sometimes been the only way to allocate \"dynamic memory\" portably.\\nArrays can be used to determine partial or complete control flow in programs, as a compact alternative to (otherwise repetitive) multiple IF statements. They are known in this context as control tables and are used in conjunction with a purpose built interpreter whose control flow is altered according to values contained in the array. The array may contain subroutine pointers (or relative subroutine numbers that can be acted upon by SWITCH statements) that direct the path of the execution.\\n\\n\\n== Element identifier and addressing formulas ==\\nWhen data objects are stored in an array, individual objects are selected by an index that is usually a non-negative scalar integer. Indexes are also called subscripts. An index maps the array value to a stored object.\\nThere are three ways in which the elements of an array can be indexed:\\n\\n0 (zero-based indexing)\\nThe first element of the array is indexed by subscript of 0.\\n1 (one-based indexing)\\nThe first element of the array is indexed by subscript of 1.\\nn (n-based indexing)\\nThe base index of an array can be freely chosen. Usually programming languages allowing n-based indexing also allow negative index values and other scalar data types like enumerations, or characters may be used as an array index.Using zero based indexing is the design choice of many influential programming languages, including C, Java and Lisp. This leads to simpler implementation where the subscript refers to an offset from the starting position of an array, so the first element has an offset of zero.\\nArrays can have multiple dimensions, thus it is not uncommon to access an array using multiple indices. For example, a two-dimensional array A with three rows and four columns might provide access to the element at the 2nd row and 4th column by the expression A[1][3] in the case of a zero-based indexing system. Thus two indices are used for a two-dimensional array, three for a three-dimensional array, and n for an n-dimensional array.\\nThe number of indices needed to specify an element is called the dimension, dimensionality, or rank of the array.\\nIn standard arrays, each index is restricted to a certain range of consecutive integers (or consecutive values of some enumerated type), and the address of an element is computed by a \"linear\" formula on the indices.\\n\\n\\n=== One-dimensional arrays ===\\nA one-dimensional array (or single dimension array) is a type of linear array. Accessing its elements involves a single subscript which can either represent a row or column index.\\nAs an example consider the C declaration int anArrayName[10]; which declares a one-dimensional array of ten integers. Here, the array can store ten elements of type int . This array has indices starting from zero through nine. For example, the expressions anArrayName[0] and anArrayName[9] are the first and last elements respectively.\\nFor a vector with linear addressing, the element with index i is located at the address B + c × i, where B is a fixed base address and c a fixed constant, sometimes called the address increment or stride.\\nIf the valid element indices begin at 0, the constant B is simply the address of the first element of the array. For this reason, the C programming language specifies that array indices always begin at 0; and many programmers will call that element \"zeroth\" rather than \"first\".\\nHowever, one can choose the index of the first element by an appropriate choice of the base address B. For example, if the array has five elements, indexed 1 through 5, and the base address B is replaced by B + 30c, then the indices of those same elements will be 31 to 35. If the numbering does not start at 0, the constant B may not be the address of any element.\\n\\n\\n=== Multidimensional arrays ===\\nFor a multidimensional array, the element with indices i,j would have address B + c · i + d · j, where the coefficients c and d are the row and column address increments, respectively.\\nMore generally, in a k-dimensional array, the address of an element with indices i1, i2, ..., ik is\\n\\nB + c1 · i1 + c2 · i2 + … + ck · ik.For example: int a[2][3];\\nThis means that array a has 2 rows and 3 columns, and the array is of integer type. Here we can store 6 elements they will be stored linearly but starting from first row linear then continuing with second row. The above array will be stored as a11, a12, a13, a21, a22, a23.\\nThis formula requires only k multiplications and k additions, for any array that can fit in memory. Moreover, if any coefficient is a fixed power of 2, the multiplication can be replaced by bit shifting.\\nThe coefficients ck must be chosen so that every valid index tuple maps to the address of a distinct element.\\nIf the minimum legal value for every index is 0, then B is the address of the element whose indices are all zero. As in the one-dimensional case, the element indices may be changed by changing the base address B. Thus, if a two-dimensional array has rows and columns indexed from 1 to 10 and 1 to 20, respectively, then replacing B by B + c1 − 3c2 will cause them to be renumbered from 0 through 9 and 4 through 23, respectively. Taking advantage of this feature, some languages (like FORTRAN 77) specify that array indices begin at 1, as in mathematical tradition while other languages (like Fortran 90, Pascal and Algol) let the user choose the minimum value for each index.\\n\\n\\n=== Dope vectors ===\\nThe addressing formula is completely defined by the dimension d, the base address B, and the increments c1, c2, ..., ck. It is often useful to pack these parameters into a record called the array\\'s descriptor or stride vector or dope vector. The size of each element, and the minimum and maximum values allowed for each index may also be included in the dope vector. The dope vector is a complete handle for the array, and is a convenient way to pass arrays as arguments to procedures. Many useful array slicing operations (such as selecting a sub-array, swapping indices, or reversing the direction of the indices) can be performed very efficiently by manipulating the dope vector.\\n\\n\\n=== Compact layouts ===\\n\\nOften the coefficients are chosen so that the elements occupy a contiguous area of memory. However, that is not necessary. Even if arrays are always created with contiguous elements, some array slicing operations may create non-contiguous sub-arrays from them.\\n\\nThere are two systematic compact layouts for a two-dimensional array. For example, consider the matrix\\n\\n  \\n    \\n      \\n        A\\n        =\\n        \\n          \\n            [\\n            \\n              \\n                \\n                  1\\n                \\n                \\n                  2\\n                \\n                \\n                  3\\n                \\n              \\n              \\n                \\n                  4\\n                \\n                \\n                  5\\n                \\n                \\n                  6\\n                \\n              \\n              \\n                \\n                  7\\n                \\n                \\n                  8\\n                \\n                \\n                  9\\n                \\n              \\n            \\n            ]\\n          \\n        \\n        .\\n      \\n    \\n    {\\\\displaystyle A={\\\\begin{bmatrix}1&2&3\\\\\\\\4&5&6\\\\\\\\7&8&9\\\\end{bmatrix}}.}\\n  In the row-major order layout (adopted by C for statically declared arrays), the elements in each row are stored in consecutive positions and all of the elements of a row have a lower address than any of the elements of a consecutive row:\\n\\nIn column-major order (traditionally used by Fortran), the elements in each column are consecutive in memory and all of the elements of a column have a lower address than any of the elements of a consecutive column:\\n\\nFor arrays with three or more indices, \"row major order\" puts in consecutive positions any two elements whose index tuples differ only by one in the last index. \"Column major order\" is analogous with respect to the first index.\\nIn systems which use processor cache or virtual memory, scanning an array is much faster if successive elements are stored in consecutive positions in memory, rather than sparsely scattered. Many algorithms that use multidimensional arrays will scan them in a predictable order. A programmer (or a sophisticated compiler) may use this information to choose between row- or column-major layout for each array. For example, when computing the product A·B of two matrices, it would be best to have A stored in row-major order, and B in column-major order.\\n\\n\\n=== Resizing ===\\n\\nStatic arrays have a size that is fixed when they are created and consequently do not allow elements to be inserted or removed. However, by allocating a new array and copying the contents of the old array to it, it is possible to effectively implement a dynamic version of an array; see dynamic array. If this operation is done infrequently, insertions at the end of the array require only amortized constant time.\\nSome array data structures do not reallocate storage, but do store a count of the number of elements of the array in use, called the count or size. This effectively makes the array a dynamic array with a fixed maximum size or capacity; Pascal strings are examples of this.\\n\\n\\n=== Non-linear formulas ===\\nMore complicated (non-linear) formulas are occasionally used. For a compact two-dimensional triangular array, for instance, the addressing formula is a polynomial of degree 2.\\n\\n\\n== Efficiency ==\\nBoth store and select take (deterministic worst case) constant time. Arrays take linear (O(n)) space in the number of elements n that they hold.\\nIn an array with element size k and on a machine with a cache line size of B bytes, iterating through an array of n elements requires the minimum of ceiling(nk/B) cache misses, because its elements occupy contiguous memory locations. This is roughly a factor of B/k better than the number of cache misses needed to access n elements at random memory locations. As a consequence, sequential iteration over an array is noticeably faster in practice than iteration over many other data structures, a property called locality of reference (this does not mean however, that using a perfect hash or trivial hash within the same (local) array, will not be even faster - and achievable in constant time). Libraries provide low-level optimized facilities for copying ranges of memory (such as memcpy) which can be used to move contiguous blocks of array elements significantly faster than can be achieved through individual element access. The speedup of such optimized routines varies by array element size, architecture, and implementation.\\nMemory-wise, arrays are compact data structures with no per-element overhead. There may be a per-array overhead (e.g., to store index bounds) but this is language-dependent. It can also happen that elements stored in an array require less memory than the same elements stored in individual variables, because several array elements can be stored in a single word; such arrays are often called packed arrays. An extreme (but commonly used) case is the bit array, where every bit represents a single element. A single octet can thus hold up to 256 different combinations of up to 8 different conditions, in the most compact form.\\nArray accesses with statically predictable access patterns are a major source of data parallelism.\\n\\n\\n=== Comparison with other data structures ===\\nDynamic arrays or growable arrays are similar to arrays but add the ability to insert and delete elements; adding and deleting at the end is particularly efficient. However, they reserve linear (Θ(n)) additional storage, whereas arrays do not reserve additional storage.\\nAssociative arrays provide a mechanism for array-like functionality without huge storage overheads when the index values are sparse. For example, an array that contains values only at indexes 1 and 2 billion may benefit from using such a structure. Specialized associative arrays with integer keys include Patricia tries, Judy arrays, and van Emde Boas trees.\\nBalanced trees require O(log n) time for indexed access, but also permit inserting or deleting elements in O(log n) time, whereas growable arrays require linear (Θ(n)) time to insert or delete elements at an arbitrary position.\\nLinked lists allow constant time removal and insertion in the middle but take linear time for indexed access. Their memory use is typically worse than arrays, but is still linear.\\n\\nAn Iliffe vector is an alternative to a multidimensional array structure. It uses a one-dimensional array of references to arrays of one dimension less. For two dimensions, in particular, this alternative structure would be a vector of pointers to vectors, one for each row(pointer on c or c++). Thus an element in row i and column j of an array A would be accessed by double indexing (A[i][j] in typical notation). This alternative structure allows jagged arrays, where each row may have a different size—or, in general, where the valid range of each index depends on the values of all preceding indices. It also saves one multiplication (by the column address increment) replacing it by a bit shift (to index the vector of row pointers) and one extra memory access (fetching the row address), which may be worthwhile in some architectures.\\n\\n\\n== Dimension ==\\nThe dimension of an array is the number of indices needed to select an element. Thus, if the array is seen as a function on a set of possible index combinations, it is the dimension of the space of which its domain is a discrete subset. Thus a one-dimensional array is a list of data, a two-dimensional array is a rectangle of data, a three-dimensional array a block of data, etc.\\nThis should not be confused with the dimension of the set of all matrices with a given domain, that is, the number of elements in the array. For example, an array with 5 rows and 4 columns is two-dimensional, but such matrices form a 20-dimensional space. Similarly, a three-dimensional vector can be represented by a one-dimensional array of size three.\\n\\n\\n== See also ==\\n\\n\\n== References ==', 'This is a list of computing mascots. A mascot is any person, animal, or object thought to bring luck, or anything used to represent a group with a common public identity. In case of computing mascots, they either represent software, hardware, or any project or collective entity behind them.\\n\\n\\n== A ==\\nAdiumy, a cartoon duck, is the mascot of Adium, a free and open-source instant messaging client for macOS.\\nAmanda the Panda, a cartoon panda, is the mascot of Window Maker, a free and open-source window manager for the X Window System.\\n\\t\\t\\n\\n\\n== B ==\\nBlinky, a cartoon fish, is the mascot of FreeDOS, a free and open-source DOS implementation for IBM PC compatible computers.\\nThe BSD Daemon, a cartoon demon, is the mascot of BSD, a free and open-source Unix operating system derivative that also has many derivations out of itself.\\nBuggie, a cartoon anthropomorphic bug,  is the mascot of Bugzilla, a free and open-source web-based general-purpose bugtracker and testing tool.\\n\\t\\t\\n\\t\\t\\n\\n\\n== C ==\\nCamelia, a cartoon bug with butterfly-like wings, is the mascot of Raku.\\nCowDuck, a cartoon hybrid with the head of a cow and the body of a duck is the mascot of TerminusDB.\\n\\t\\t\\n\\n\\n== D ==\\nDotNet Bot (typically stylized as \"dotnet bot\" or \"dotnet-bot\") is the official community mascot of the .NET free and open source software framework.\\nDuke, a stylized, unspecified creature, is the mascot of Java, a system for developing application software and deploying it in a cross-platform computing environment.\\n\\t\\t\\n\\n\\n== E ==\\nelePHPant, a cartoon elephant, is the mascot of PHP, a server-side scripting language designed primarily for web development.\\neMule, a free and open-source peer-to-peer file sharing application for Microsoft Windows, is represented by a cartoon mule of the same name.\\n\\t\\t\\n\\n\\n== F ==\\nFreedo, a cartoon anthropomorphic penguin, is the mascot of Linux-libre, a free and open-source operating system kernel derived from Linux kernel, packaged by GNU to have all the proprietary components removed.\\nFerris, a crab, who is the unofficial mascot of the Rust language.\\n\\t\\t\\n\\n\\n== G ==\\nGavroche, a cartoon goblin, is the mascot of GNU MediaGoblin, a free and open-source decentralized server software for hosting and sharing digital media.\\nGeeko, a stylized chameleon, is the mascot of SUSE Linux, a Linux-based free and open-source computer operating system family.\\nGlenda, the Plan 9 Bunny, a cartoon rabbit, is the mascot of Plan 9 from Bell Labs, a free and open-source distributed operating system that manages all computing resources through its file system rather than specialized interfaces.\\nGNU – or just the drawing \"GNU head\", an anthropomorphic wildebeest head—is the mascot—or just the logo—of GNU, a free and open-source operating system and an extensive collection of computer software; it is also the mascot of GNU Project, a free-software, mass-collaboration project.\\nGooey, a cartoon octopus, is the mascot of WebGUI, a free and open-source content management system.\\nThe free and open-source Go programming language is represented by a gopher.\\n\\t\\t\\n\\t\\t\\n\\t\\t\\n\\t\\t\\n\\t\\t\\n\\n\\n== H ==\\nHexley, a cartoon platypus, is the mascot of Darwin.\\n\\n\\n== K ==\\nKandalf, a cartoon wizard, is the former mascot of KDE.\\nKate the Cyber Woodpecker, a cartoon robotic woodpecker, is the mascot of Kate, a free and open-source advanced text editor for software developers.\\nKiki the Cyber Squirrel, a cartoon anthropomorphic robotic squirrel, is the mascot of Krita, a free and open-source raster graphics editor designed for digital painting and animation.\\nKitty, a cartoon anthropomorphic cat, created by Eric W. Schwartz, is the mascot of AROS Research Operating System, a free and open-source multimedia centric implementation of the AmigaOS 3.1 APIs.\\nKonqi is the primary mascot of KDE, an international community that develops free and open-source software, and KDE Projects, software they have developed, including KDE Plasma workspace, KDE Frameworks, and the software foundation of other KDE Applications. A number of other dragons also exist, such as Katie, associated with KDE Women\\'s Project.\\n\\t\\t\\n\\t\\t\\n\\t\\t\\n\\t\\t\\n\\t\\t\\n\\t\\t\\n\\n\\n== L ==\\nLenny, a penguin with blue hair, who is the mascot for Lubuntu.\\nLarry, a hand-drawn cow, is one of Gentoo\\'s unofficial mascots.\\nThe Lisp mascot is an quadruped alien with more than four eyes and a single arm extending from the nose\\n\\t\\t\\n\\t\\t\\n\\n\\n== M ==\\nMoby Dock, a cartoon whale that hauls shipping containers on its back, is the mascot of Docker, a set of platform as a service (PaaS) products.\\nMozilla, a cartoon anthropomorphic lizard and later a stylized tyrannosaurus rex, is the retired mascot of Mozilla Foundation, a non-profit organization that supports and leads Mozilla, a free-software community that developed Firefox, a free and open-source web browser and many related projects.\\n\\t\\t\\n\\n\\n== O ==\\nOctocat, an anthropomorphized cat with five octopus-like arms is GitHub\\'s mascot.\\n\\n\\n== P ==\\nThe Apache Pig, an anthropomorphic pig, is the mascot of Apache Pig.\\nPuffy, a cartoon pufferfish, is the mascot of OpenBSD, a free and open-source Unix-like operating system descended from BSD, dedicated to security and stability features.\\nPurple Pidgin, a cartoon pigeon, is the mascot of Pidgin, a free and open-source multi-platform instant messaging client.\\n\\t\\t\\n\\n\\n== R ==\\nThe Raft consensus algorithm mascot is a log raft with a face. Created by Andrea Ruygt, and made a vector by Diego Ongaro\\nRocky Raccoon, a cartoon raccoon, is the mascot of MINIX 3, a free and open-source project to create a small, high availability, high functioning Unix-like operating system.\\n\\t\\t\\n\\n\\n== T ==\\nTux, a cartoon anthropomorphic penguin, is the mascot of Linux kernel, a free and open-source monolithic Unix-like computer operating system kernel that has been included in many OS distributions.\\n\\n\\n== W ==\\nWilber is the mascot of GIMP, a free and open-source raster graphics editor designed for image editing, drawing, image format conversion and others.\\nWombats are associated with DATATRIEVE, being adopted as the mascot of its product group.  References where included in the help system for the product, and a graphic demonstration using the \"PLOT WOMBAT\" command.\\n\\n\\n== X ==\\nXenia, a transgender anthropomorphic fox, is an unofficial mascot for Linux. Xenia was originally designed by Alan Mackey as an alternative to the official Linux mascot, Tux. Her character and design has undergone additional iterations between Mackey and other artists and developers in the community.\\nXue, a stylized mouse, is the mascot of Xfce, a free and open-source desktop environment for Unix-like operating systems that aims to be fast and lightweight, while still being visually appealing and easy to use.\\n\\n\\n== Z ==\\nZero the Ziguana and Ziggy the Ziguana are the two official mascots of the programming language Zig.\\nZnurt the Flying Saucer is one of Gentoo Linux\\'s unofficial mascots.\\n\\t\\t\\n\\n\\n== See also ==\\nList of video game mascots\\nOS-tan\\n\\n\\n== References ==', 'In computer science, a tree is a widely used abstract data type that simulates a hierarchical tree structure, with a root value and subtrees of children with a parent node, represented as a set of linked nodes.\\nA tree data structure can be defined recursively as a collection of nodes, where each node is a data structure consisting of a value and a list of references to nodes. The start of the tree is the \"root node\" and the reference nodes are the \"children\". No reference is duplicated and none points to the root.\\nAlternatively, a tree can be defined abstractly as a whole (globally) as an ordered tree, with a value assigned to each node. Both these perspectives are useful: while a tree can be analyzed mathematically as a whole, when actually represented as a data structure it is usually represented and worked with separately by node (rather than as a set of nodes and an adjacency list of edges between nodes, as one may represent a digraph, for instance). For example, looking at a tree as a whole, one can talk about \"the parent node\" of a given node, but in general, as a data structure, a given node only contains the list of its children but does not contain a reference to its parent (if any).\\n\\n\\n== Common uses ==\\nRepresenting hierarchical data such as:\\nFile systems used to store data on hard drives so that files can easily be found by name\\nExploded-view drawing used to identify the sub components of components used to construct larger components\\nSubroutine calls used to identify which subroutines in a program call other subroutines non recursively\\nEvolution which species evolved from which prior species (not restricted to biology, e.g. Linux evolution timeline)\\nAbstract syntax trees for computer languages\\nParse trees for human languages\\nDocument Object Models of XML and HTML documents\\nJSON and YAML documents being processed\\nSearch trees store data in a way that makes an efficient search algorithm possible via tree traversal\\nA binary search tree is a type of binary tree\\nRepresenting sorted lists of data\\nAs a workflow for compositing digital images for visual effects\\nStoring Barnes-Hut trees used to simulate galaxies\\n\\n\\n== Terminology ==\\nA node is a structure which may contain a value or condition, or represent a separate data structure (which could be a tree of its own). Each node in a tree has zero or more child nodes, which are below it in the tree (by convention, trees are drawn growing downwards). A node that has a child is called the child\\'s parent node (or superior). A node has at most one parent, but possibly many ancestor nodes, such as the parent\\'s parent.  Child nodes with the same parent are sibling nodes.\\nAn internal node (also known as an inner node, inode for short, or branch node) is any node of a tree that has child nodes. Similarly, an external node (also known as an outer node, leaf node, or terminal node) is any node that does not have child nodes.\\nThe topmost node in a tree is called the root node. Depending on the definition, a tree may be required to have a root node (in which case all trees are non-empty), or may be allowed to be empty, in which case it does not necessarily have a root node. Being the topmost node, the root node will not have a parent. It is the node at which algorithms on the tree begin, since as a data structure, one can only pass from parents to children. Note that some algorithms (such as post-order depth-first search) begin at the root, but first visit leaf nodes (access the value of leaf nodes), only to visit the root last (i.e., they first access the children of the root, but only access the value of the root last). All other nodes can be reached from it by following edges or links. (In the formal definition, each such path is also unique.) In diagrams, the root node is conventionally drawn at the top. In some trees, such as heaps, the root node has special properties. Every node in a tree can be seen as the root node of the subtree rooted at that node.\\nThe height of a node is the length of the longest downward path to a leaf from that node. The height of the root is the height of the tree. The depth of a node is the length of the path to its root (i.e., its root path). This is commonly needed in the manipulation of the various self-balancing trees, AVL Trees in particular. The root node has depth zero, leaf nodes have height zero, and a tree with only a single node (hence both a root and leaf) has depth and height zero. Conventionally, an empty tree (tree with no nodes, if such are allowed) has height −1.\\nA subtree of a tree T is a tree consisting of a node in T and all of its descendants in T. Nodes thus correspond to subtrees (each node corresponds to the subtree of itself and all its descendants) – the subtree corresponding to the root node is the entire tree, and each node is the root node of the subtree it determines; the subtree corresponding to any other node is called a proper subtree (by analogy to a proper subset).\\nOther terms used with trees:\\n\\nNeighbor Parent or child.\\nAncestor A node reachable by repeated proceeding from child to parent.\\nDescendant A node reachable by repeated proceeding from parent to child. Also known as subchild.\\nDegree For a given node, its number of children. A leaf has necessarily degree zero.\\nDegree of tree The degree of a tree is the maximum degree of a node in the tree.\\nDistance The number of edges along the shortest path between two nodes.\\nLevel The level of a node is the number of edges along the\\nunique path between it and the root node.\\nWidth The number of nodes in a level.\\nBreadth The number of leaves.\\nForest A set of n ≥ 0 disjoint trees.\\nOrdered tree A rooted tree in which an ordering is specified for the children of each vertex.\\nSize of a tree Number of nodes in the tree.\\n\\n\\n== Preliminary definition ==\\n\\nA tree is a nonlinear data structure, compared to arrays, linked lists, stacks and queues which are linear data structures. A tree can be empty with no nodes or a tree is a structure consisting of one node called the root and zero or one or more subtrees.\\n\\n\\n== Drawing trees ==\\nTrees are often drawn in the plane. Ordered trees can be represented essentially uniquely in the plane, and are hence called plane trees, as follows: if one fixes a conventional order (say, counterclockwise), and arranges the child nodes in that order (first incoming parent edge, then first child edge, etc.), this yields an embedding of the tree in the plane, unique up to ambient isotopy. Conversely, such an embedding determines an ordering of the child nodes.\\nIf one places the root at the top (parents above children, as in a family tree) and places all nodes that are a given distance from the root (in terms of number of edges: the \"level\" of a tree) on a given horizontal line, one obtains a standard drawing of the tree. Given a binary tree, the first child is on the left (the \"left node\"), and the second child is on the right (the \"right node\").\\n\\n\\n== Common operations ==\\nEnumerating all the items\\nEnumerating a section of a tree\\nSearching for an item\\nAdding a new item at a certain position on the tree\\nDeleting an item\\nPruning: Removing a whole section of a tree\\nGrafting: Adding a whole section to a tree\\nFinding the root for any node\\nFinding the lowest common ancestor of two nodes\\n\\n\\n=== Traversal and search methods ===\\n\\nStepping through the items of a tree, by means of the connections between parents and children, is called walking the tree, and the action is a walk of the tree. Often, an operation might be performed when a pointer arrives at a particular node. A walk in which each parent node is traversed before its children is called a pre-order walk; a walk in which the children are traversed before their respective parents are traversed is called a post-order walk; a walk in which a node\\'s left subtree, then the node itself, and finally its right subtree are traversed is called an in-order traversal. (This last scenario, referring to exactly two subtrees, a left subtree and a right subtree, assumes specifically a binary tree.) A level-order walk effectively performs a breadth-first search over the entirety of a tree; nodes are traversed level by level, where the root node is visited first, followed by its direct child nodes and their siblings, followed by its grandchild nodes and their siblings, etc., until all nodes in the tree have been traversed.\\n\\n\\n== Representations ==\\nThere are many different ways to represent trees; common representations represent the nodes as dynamically allocated records with pointers to their children, their parents, or both, or as items in an array, with relationships between them determined by their positions in the array (e.g., binary heap).\\nIndeed, a binary tree can be implemented as a list of lists (a list where the values are lists): the head of a list (the value of the first term) is the left child (subtree), while the tail (the list of second and subsequent terms) is the right child (subtree). This can be modified to allow values as well, as in Lisp S-expressions, where the head (value of first term) is the value of the node, the head of the tail (value of second term) is the left child, and the tail of the tail (list of third and subsequent terms) is the right child.\\nIn general a node in a tree will not have pointers to its parents, but this information can be included (expanding the data structure to also include a pointer to the parent) or stored separately. Alternatively, upward links can be included in the child node data, as in a threaded binary tree.\\n\\n\\n== Generalizations ==\\n\\n\\n=== Digraphs ===\\nIf edges (to child nodes) are thought of as references, then a tree is a special case of a digraph, and the tree data structure can be generalized to represent directed graphs by removing the constraints that a node may have at most one parent, and that no cycles are allowed. Edges are still abstractly considered as pairs of nodes, however, the terms parent and child are usually replaced by different terminology (for example, source and target). Different implementation strategies exist: a digraph can be represented by the same local data structure as a tree (node with value and list of children), assuming that \"list of children\" is a list of references, or globally by such structures as adjacency lists.\\nIn graph theory, a tree is a connected acyclic graph; unless stated otherwise, in graph theory trees and graphs are assumed undirected. There is no one-to-one correspondence between such trees and trees as data structure. We can take an arbitrary undirected tree, arbitrarily pick one of its vertices as the root, make all its edges directed by making them point away from the root node – producing an arborescence – and assign an order to all the nodes. The result corresponds to a tree data structure. Picking a different root or different ordering produces a different one.\\nGiven a node in a tree, its children define an ordered forest (the union of subtrees given by all the children, or equivalently taking the subtree given by the node itself and erasing the root). Just as subtrees are natural for recursion (as in a depth-first search), forests are natural for corecursion (as in a breadth-first search).\\nVia mutual recursion, a forest can be defined as a list of trees (represented by root nodes), where a node (of a tree) consists of a value and a forest (its children):\\n\\nf: [n[1], ..., n[k]]\\nn: v f\\n\\n\\n== Data type versus data structure ==\\nThere is a distinction between a tree as an abstract data type and as a concrete data structure, analogous to the distinction between a list and a linked list.\\nAs a data type, a tree has a value and children, and the children are themselves trees; the value and children of the tree are interpreted as the value of the root node and the subtrees of the children of the root node. To allow finite trees, one must either allow the list of children to be empty (in which case trees can be required to be non-empty, an \"empty tree\" instead being represented by a forest of zero trees), or allow trees to be empty, in which case the list of children can be of fixed size (branching factor, especially 2 or \"binary\"), if desired.\\nAs a data structure, a linked tree is a group of nodes, where each node has a value and a list of references to other nodes (its children).  There is also the requirement that no two \"downward\" references point to the same node. In practice, nodes in a tree commonly include other data as well, such as next/previous references, references to their parent nodes, or nearly anything.\\nOwing to the use of references to trees in the linked tree data structure, trees are often discussed implicitly assuming that they are being represented by references to the root node, as this is often how they are actually implemented. For example, rather than an empty tree, one may have a null reference: a tree is always non-empty, but a reference to a tree may be null.\\n\\n\\n=== Recursive ===\\nRecursively, as a data type a tree is defined as a value (of some data type, possibly empty), together with a list of trees (possibly an empty list), the subtrees of its children; symbolically:\\n\\nt: v [t[1], ..., t[k]](A tree t consists of a value v and a list of other trees.)\\nMore elegantly, via mutual recursion, of which a tree is one of the most basic examples, a tree can be defined in terms of forest (a list of trees), where a tree consists of a value and a forest (the subtrees of its children):\\n\\nf: [t[1], ..., t[k]]\\nt: v fNote that this definition is in terms of values, and is appropriate in functional languages (it assumes referential transparency); different trees have no connections, as they are simply lists of values.\\nAs a data structure, a tree is defined as a node (the root), which itself consists of a value (of some data type, possibly empty), together with a list of references to other nodes (list possibly empty, references possibly null); symbolically:\\n\\nn: v [&n[1], ..., &n[k]](A node n consists of a value v and a list of references to other nodes.)\\nThis data structure defines a directed graph, and for it to be a tree one must add a condition on its global structure (its topology), namely that at most one reference can point to any given node (a node has at most a single parent), and no node in the tree point to the root. In fact, every node (other than the root) must have exactly one parent, and the root must have no parents.\\nIndeed, given a list of nodes, and for each node a list of references to its children, one cannot tell if this structure is a tree or not without analyzing its global structure and that it is in fact topologically a tree, as defined below.\\n\\n\\n=== Type theory ===\\nAs an abstract data type, the abstract tree type T with values of some type E is defined, using the abstract forest type F (list of trees), by the functions:\\n\\nvalue: T → E\\nchildren: T → F\\nnil: () → F\\nnode: E × F → Twith the axioms:\\n\\nvalue(node(e, f)) = e\\nchildren(node(e, f)) = fIn terms of type theory, a tree is an inductive type defined by the constructors nil (empty forest) and node (tree with root node with given value and children).\\n\\n\\n=== Mathematical terminology ===\\nViewed as a whole, a tree data structure is an ordered tree, generally with values attached to each node. Concretely, it is (if required to be non-empty):\\n\\nA rooted tree with the \"away from root\" direction (a more narrow term is an \"arborescence\"), meaning:\\nA directed graph,\\nwhose underlying undirected graph is a tree (any two vertices are connected by exactly one simple path),\\nwith a distinguished root (one vertex is designated as the root),\\nwhich determines the direction on the edges (arrows point away from the root; given an edge, the node that the edge points from is called the parent and the node that the edge points to is called the child),together with:\\n\\nan ordering on the child nodes of a given node, and\\na value (of some data type) at each node.Often trees have a fixed (more properly, bounded) branching factor (outdegree), particularly always having two child nodes (possibly empty, hence at most two non-empty child nodes), hence a \"binary tree\".\\nAllowing empty trees makes some definitions simpler, some more complicated: a rooted tree must be non-empty, hence if empty trees are allowed the above definition instead becomes \"an empty tree or a rooted tree such that ...\". On the other hand, empty trees simplify defining fixed branching factor: with empty trees allowed, a binary tree is a tree such that every node has exactly two children, each of which is a tree (possibly empty). The complete sets of operations on the tree must include fork operation.\\n\\n\\n== Mathematical definition ==\\n\\n\\n=== Unordered tree ===\\nMathematically, an unordered tree (or \"algebraic tree\") can be defined as an algebraic structure (X, parent) where X is the non-empty carrier set of nodes and parent is a function on X which assigns each node x its \"parent\" node, parent(x). The structure is subject to the condition that every non-empty subalgebra must have the same fixed point. That is, there must be a unique \"root\" node r, such that parent(r) = r and for every node x, some iterative application parent(parent(⋯parent(x)⋯)) equals r.\\nThere are several equivalent definitions.\\nAs the closest alternative, one can define unordered trees as partial algebras (X, parent) which are obtained from the total algebras described above by letting parent(r) be undefined. That is, the root r is the only node on which the parent function is not defined and for every node x, the root is reachable from x in the directed graph (X, parent). This definition is in fact coincident with that of an anti-arborescence. The TAoCP book uses the term oriented tree.\\n\\nThe box on the right describes the partial algebra (X, parent) as a relational structure (X, ≺). If (1) is replaced by\\n\\n \\nthen condition (2) becomes redundant.\\nUsing this definition, dedicated terminology can be provided for generalizations of unordered trees that correspond to distinguished subsets of the listed conditions: \\n\\n(1,2,3) – directed pseudotree,\\n(3) – directed pseudoforest,\\n(3,4) – unordered forest (whose components are unordered trees),\\n(4) – directed acyclic graph, assumed that X is finite,\\n(1\\',4) – acyclic accessible pointed graph (then condition (2) holds implicitly).Another equivalent definition of an unordered tree is that of a set-theoretic tree that is singly-rooted and whose height is at most ω (a \"finite-ish\" tree). That is, the algebraic structures (X, parent) are equivalent to partial orders (X, ≤) that have a top element r and whose every principal upset (aka principal filter) is a finite chain. \\nTo be precise, we should speak about an inverse set-theoretic tree since the set-theoretic definition usually employs opposite ordering.\\nThe correspondence between (X, parent) and (X, ≤) is established via reflexive transitive closure / reduction, with the reduction resulting in the \"partial\" version without the root cycle.\\nThe definition of trees in descriptive set theory (DST) utilizes the \\nrepresentation of partial orders (X, ≥) as prefix orders between finite sequences. In turns out that up to isomorphism, there is a one-to-one correspondence between the (inverse of) DST trees and the tree structures defined so far.\\nWe can refer to the four equivalent characterizations as to tree as an algebra, tree as a partial algebra, tree as a partial order, and tree as a prefix order. There is also a fifth equivalent definition – that of a graph-theoretic rooted tree which is just a connected acyclic \\nrooted graph.\\n\\nThe expression of trees as (partial) algebras (also called functional graphs) (X, parent) follows directly the implementation of tree structures using parent pointers. Typically, the partial version is used in which the root node has no parent defined. However, in some implementations or models even the parent(r) = r circularity is established. Notable examples: \\n\\nThe Linux VFS where \"The root dentry has a d_parent that points to itself\".\\nThe concept of an instantiation treefrom object-oriented programming. In this case, the root node is the top metaclass – the only class that is a direct instance of itself.\\nNote that the above definition admits infinite trees. This allows for the description of infinite structures supported by some implementations via lazy evaluation. A notable example is the infinite regress of eigenclasses from the Ruby object model. In this model, the tree established via superclass links between non-terminal objects is infinite and has an infinite branch (a single infinite branch of \"helix\" objects – see the diagram).\\n\\n\\n==== Sibling sets ====\\nIn every unordered tree (X, parent) there is a distinguished partition of the set X of nodes into sibling sets. Two non-root nodes x, y belong to the same sibling set if parent(x) = parent(y). The root node r forms the singleton sibling set {r}. A tree is said to be locally finite or finitely branching if each of its sibling sets is finite.\\nEach pair of distinct siblings is incomparable in ≤. This is why the word unordered is used in the definition. Such a terminology might become misleading when all sibling sets are singletons, i.e. when the set X of all nodes is totally ordered (and thus well-ordered) by ≤ In such a case we might speak about a singly-branching tree instead.\\n\\n\\n==== Using set inclusion ====\\nAs with every partially ordered set, tree structures (X, ≤) can be represented by inclusion order – by set systems in which ≤ is coincident with ⊆, the induced inclusion order. Consider a structure (U, ℱ) such that U is a non-empty set, and ℱ is a set of subsets of U such that the following are satisfied (by Nested Set Collection definition):\\n\\n∅ ∉ ℱ. (That is, (U, ℱ) is a hypergraph.)\\nU ∈ ℱ.\\nFor every X, Y from ℱ, X ∩ Y ∈ {∅, X, Y}. (That is, ℱ is a laminar family.)\\nFor every X from ℱ, there are only finitely many Y from ℱ such that X ⊆ Y.Then the structure (ℱ, ⊆) is an unordered tree whose root equals U. Conversely, if (U, ≤) is an unordered tree, and ℱ is the set {↓x | x ∈ U} of all principal ideals of (U, ≤) then the set system (U, ℱ) satisfies the above properties.\\n\\nThe set-system view of tree structures provides the default semantic model – in the majority of most popular cases, tree data structures represent containment hierarchy. This also offers a justification for order direction with the root at the top: The root node is a greater container than any other node. Notable examples:\\n\\nDirectory structure of a file system. A directory contains its sub-directories.\\nDOM tree. The document parts correspondent to DOM nodes are in subpart relation according to the tree order.\\nSingle inheritance in object-oriented programming. An instance of a class is also an instance of a superclass.\\nHierarchical taxonomy such as the Dewey Decimal Classification with sections of increasing specificity.\\nBSP trees, quadtrees, octrees, R-trees and other tree data structures used for recursive space partitioning.\\n\\n\\n==== Well-founded trees ====\\nAn unordered tree (X, ≤) is well-founded if the strict partial order < is a well-founded relation. In particular, every finite tree is well-founded. \\nAssuming the axiom of dependent choice a tree is well-founded if and only if it has no infinite branch.\\nWell-founded trees can be defined recursively – by forming trees from a disjoint union of smaller trees. For the precise definition, suppose that X is a set of nodes. Using the reflexivity of partial orders, we can identify any tree (Y, ≤) on a subset of X with its partial order (≤) – a subset of X × X. The set ℛ of all relations R that form a well-founded tree (Y, R) on a subset Y of X is defined in stages ℛi, so that ℛ = ⋃{ℛi | i is ordinal}. For each ordinal number i, let R belong to the i-th stage ℛi if and only if R is equal to\\n\\n⋃ℱ ∪ ((dom(⋃ℱ) ∪ {x}) × {x})where ℱ is a subset of ⋃{ℛk | k < i} such that elements of ℱ are pairwise disjoint, and x is a node that does not belong to dom(⋃ℱ). (We use dom(S) to denote the domain of a relation S.) Observe that the lowest stage ℛ0 consists of single-node trees {(x,x)} since only empty ℱ is possible. In each stage, (possibly) new trees R are built by taking a forest ⋃ℱ with components ℱ from lower stages and attaching a new root x atop of ⋃ℱ.\\nIn contrast to the tree height which is at most ω, the rank of well-founded trees is unlimited, see the properties of \"unfolding\".\\n\\n\\n==== Using recursive pairs ====\\nIn computing, a common way to define well-founded trees is via recursive ordered pairs\\n(F, x): a tree is a forest F together with a \"fresh\" node x.  A forest F in turn is a possibly empty set of trees with pairwise disjoint sets of nodes. For the precise definition, proceed similarly as in the construction of names used in the set-theoretic technique of forcing. Let X be a set of nodes. In the superstructure over X, define sets T, ℱ of trees and forests, respectively, and a map  nodes : T → ℘(X) assigning each tree t its underlying set of nodes so that:\\n\\nCircularities in the above conditions can be eliminated by stratifying each of T, ℱ and nodes into stages like in the previous subsection. Subsequently, define a \"subtree\" relation ≤ on T as the reflexive transitive closure of the \"immediate subtree\" relation ≺ defined between trees by\\n\\nwhere π1(t) is the projection of t onto the first coordinate; i.e., it is the forest F such that t = (F, x) for some x ∈ X. It can be observed that (T, ≤) is a multitree: for every t ∈ T, the principal ideal ↓t ordered by ≤ is a well-founded tree as a partial order. Moreover, for every tree  t ∈ T, its \"nodes\"-order structure  (nodes(t), ≤t) is given by  x ≤t y if and only if there are forests F, G ∈ ℱ such that both (F, x) and (G, y) are subtrees of t and (F, x) ≤ (G, y).\\n\\n\\n==== Using arrows ====\\nAnother formalization as well as generalization of unordered trees can be obtained by reifying parent-child pairs of nodes. Each such ordered pair can be regarded as an abstract entity – an \"arrow\". This results in a multidigraph (X, A, s, t) where X is the set of nodes, A is the set of arrows, and s and t are functions from A to X assigning each arrow its source and target, respectively. The structure is subject to the following conditions:\\n\\n(1) (A, s ○ t–1) is an unordered tree, as a total algebra.\\n(2) The t map is a bijection between arrows and nodes.In (1), the composition symbol ○ is to be interpreted left-to-right. The condition says that inverse consecutivity of arrows is a total child-to-parent map. Let this parent map between arrows be denoted p, i.e. p = s ○ t−1. Then we also have s = p ○ t, thus a multidigraph satisfying (1,2) can also be axiomatized as (X, A, p, t), with the parent map p instead of s as a definitory constituent. Observe that the root arrow is necessarily a loop, i.e. its source and target coincide.\\n\\nAn important generalization of the above structure is established by allowing the target map t to be many-to-one. This means that (2) is weakened to\\n\\n(2\\') The t map is surjective – each node is the target of some arrow.Note that condition (1) asserts that only leaf arrows are allowed to have the same target. That is, the restriction of t to the range of p is still injective.\\nMultidigraphs satisfying (1,2\\') can be called \"arrow trees\" – their tree characteristics is imposed on arrows rather than nodes. These structures can be regarded as the most essential abstraction of the Linux VFS because they reflect the hard-link structure of filesystems. Nodes are called inodes, arrows are dentries (or hard links). The parent and target maps p and t are respectively represented by d_parent and d_inode fields in the dentry data structure. Each inode is assigned a fixed file type, of which the directory type plays a special role of \"designed parents\":\\n\\n(a) only directory inodes can appear as hard-link source, and\\n(b) a directory inode cannot appear as the target of more than one hard-link.Using dashed style for the first half of the root loop indicates that, similarly to the parent map, there is a partial version for the source map s in which the source of the root arrow is undefined. This variant is employed for further generalization, see #Using paths in a multidigraph.\\n\\n\\n==== Using paths in a digraph ====\\n\\nUnordered trees naturally arise by \"unfolding\" of accessible pointed graphs.Let ℛ = (X, R, r) be a pointed relational structure, i.e. such that X is the set of nodes, R is a relation between nodes (a subset of X × X), and r is a distinguished \"root\" node. Assume further that ℛ is accessible, which means that X equals the preimage of {r} under the reflexive transitive closure of R, and call such a structure an accessible pointed graph or apg for short. Then one can derive another apg ℛ\\' = (X\\', R\\', r\\') – the unfolding of ℛ – as follows:\\n\\nX\\' is the set of reversed paths to r, i.e. the set of non-empty finite sequences p of nodes (elements of X) such that (a) consecutive members of p are inversely R-related, and (b) the first member of p is the root r,\\nR\\' is a relation between paths from X\\' such that paths p and q are R\\'-related if and only if p = q ⁎ [x] for some node x (i.e. q is a maximal proper prefix of p, the \"popped\" p), and\\nr\\' is the one-element sequence [r].Apparently, the structure (X\\', R\\') is an unordered tree in the \"partial-algebra\" version: R\\' is a partial map that relates each non-root element of X\\' to its parent by path popping. The root element is obviously  r\\'. Moreover, the following properties are satisfied:\\n\\nℛ is isomorphic to its unfolding ℛ\\' if and only if ℛ is a tree. (In particular, unfolding is idempotent, up to isomorphism.)\\nUnfolding preserves well-foundedness: If R is well-founded then so is R\\'.\\nUnfolding preserves rank: If R is well-founded then the ranks of R and R\\' coincide.Notes:\\n\\n\\n==== Using paths in a multidigraph ====\\nAs shown on the example of hard-link structure of file systems, many data structures in computing allow multiple links between nodes. Therefore,  in order to properly exhibit the appearance of unordered trees among data structures it is necessary to generalize accessible pointed graphs to multidigraph setting. To simplify the terminology, we make use of the term quiver which is an established synonym for \"multidigraph\".\\n\\nLet an accessible pointed quiver or apq for short be defined as a structure\\n\\nℳ = (X, A, s, t)where\\nX is a set of nodes,\\nA is a set of arrows,\\ns is a partial function from A to X (the source map), and\\nt is a total function from A to X (the target map).\\nThus, ℳ is a \"partial multidigraph\".\\nThe structure is subject to the following conditions:\\n\\nThere is exactly one \"root\" arrow, ar, whose source s(ar) is undefined.\\nEvery node x ∈ X is reachable via a finite sequence of consecutive arrows starting with the root arrow ar.ℳ is said to be a tree if the target map t is a bijection between arrows and nodes.\\nThe unfolding of ℳ is formed by the sequences mentioned in (2) – which are the accessibility paths (cf. Path algebra). As an apq, the unfolding can be written as \\nℳ\\' = (X\\', A\\', s\\', t\\')\\nwhere \\nX\\' is the set of accessibility paths,\\nA\\' coincides with X\\',\\ns\\' coincides with path popping, and\\nt\\' is the identity on X\\'.\\nLike with apgs, unfolding is idempotent and always results in a tree.\\nThe underlying apg is obtained as the structure\\n(X, R, t(ar))\\nwhere\\nR = {(t(a),s(a)) | a ∈ A \\\\ {ar}}.\\nThe diagram above shows an example of an apq with 1 + 14 arrows. In JavaScript, Python or Ruby, the structure can be created by the following (exactly the same) code:\\n\\n\\n==== Using names ====\\nUnordered trees and their generalizations form the essence of naming systems.\\nThere are two prominent examples of naming systems: file systems and (nested) associative arrays. The multidigraph-based structures from previous subsections provided anonymous abstractions for both cases. To obtain naming capabilities, arrows are to be equipped with names as identifiers.\\nA name must be locally unique – within each sibling set of arrows there can be at most one arrow labelled by a given name.\\n\\nThis can be formalized as a structure\\n\\n ℰ = (X, Σ, A, s, σ, t)where \\nX is a set of nodes, \\nΣ is a set of names, \\nA is a set of arrows, \\ns is a partial function from A to X, \\nσ is a partial function from A to Σ, and\\nt is a total function from A to X.\\nFor an arrow a, constituents of the triple (s(a), σ(a), t(a)) are respectively a\\'s source, name and target.\\nThe structure is subject to the following conditions:\\n\\nThe reduct (X, A, s, t) is an accessible pointed quiver (apq) as defined previously.\\nThe name function σ is undefined just for the source-less root arrow.\\nThe name function σ is injective in the restriction to every sibling set of arrows, i.e. for every non-root arrows a, b, if s(a) = s(b) and σ(a) = σ(b) then a = b.This structure can be called a nested dictionary or named apq. In computing, such structures are ubiquitous. The table above shows that arrows can be considered \"un-reified\" as the set A\\' = {(s(a), σ(a), t(a)) | a ∈ A \\\\ {ar}} of source-name-target triples. This leads to a relational structure (X, Σ, A\\') which can be viewed as a relational database table. Underlines in source and name indicate primary key.\\nThe structure can be rephrased as a deterministic labelled transition system: X is a set of \"states\", Σ is a set of \"labels\", A\\' is a set of \"labelled transitions\". (Moreover, the root node r = t(ar) is an \"initial state\", and the accessibility condition means that every state is reachable from the initial state.)\\n\\nThe diagram on the right shows a nested dictionary ℰ that has the same underlying multidigraph as the example in the previous subsection. The structure can be created by the code below. Like before, exactly the same code applies for JavaScript, Python and Ruby.\\nFirst, a substructure, ℰ0, is created by a single assignment of a literal {...} to r. This structure, depicted by full lines, is an \"arrow tree\" (therefore, it is a spanning tree). The literal in turn appears to be a JSON serialization of ℰ0.\\nSubsequently, the remaining arrows are created by assignments of already existing nodes. Arrows that cause cycles are displayed in blue.\\n\\nIn the Linux VFS, the name function σ is represented by the d_name field in the dentry data structure. The ℰ0 structure above demonstrates a correspondence between JSON-representable structures and hard-link structures of file systems. In both cases, there is a fixed set of built-in types of \"nodes\" of which one type is a container type, except that in JSON, there are in fact two such types – Object and Array. If the latter one is ignored (as well as the distinction between individual primitive data types) then the provided abstractions of file-systems and JSON data are the same – both are arrow trees equipped with naming σ and a distinction of container nodes.\\nSee #Nested data for the formal description of tree structures (\"JSON-trees\") with distinction and bipartition of container nodes.\\n\\n\\n===== Pathnames =====\\nThe naming function σ of a nested dictionary ℰ naturally extends from arrows to arrow paths. Each sequence p = [a1, ..., an]  of consecutive arrows is implicitly assigned a pathname (cf. Pathname) – the sequence σ(p) = [σ(a1), ..., σ(an)]  of arrow names.\\nLocal uniqueness carries over to arrow paths: different sibling paths have different pathnames. In particular, the root-originating arrow paths are in one-to-one correspondence with their pathnames. This correspondence provides a \"symbolic\" representation of the unfolding of ℰ via pathnames – the nodes in ℰ are globally identified via a tree of pathnames.\\n\\n\\n=== Ordered tree ===\\nThe structures introduced in the previous subsection form just the core \"hierarchical\"  part of tree data structures that appear in computing. In most cases, there is also an additional \"horizontal\" ordering between siblings. In search trees the order is commonly established by the \"key\" or value associated with each sibling, but in many trees that is not the case. For example, XML documents, lists within JSON files, and many other structures have order that does not depend on the values in the nodes, but is itself data — sorting the paragraphs of a novel alphabetically would change its meaning.\\nThe correspondent expansion of the previously described tree structures (X, ≤) can be defined by endowing each sibling set with a linear order as follows.An alternative definition according to Kuboyama is presented in the next subsection.\\nAn ordered tree is a structure (X, ≤V, ≤S) where  X is a non-empty set of nodes and ≤V and ≤S are relations on X called vertical (or also hierarchical) order and sibling order, respectively. The structure is subject to the following conditions:\\n\\n(X, ≤V) is a partial order that is an unordered tree as defined in the previous subsection.\\n(X, ≤S)  is a partial order.\\nDistinct nodes are comparable in <S if and only if they are siblings:\\n(<S) ∪ (>S) = ((≺V) ○ (≻V)) ∖ idX.\\nEvery node has only finitely many preceding siblings, i.e. every principal ideal of (X, ≤S) is finite.  (This condition can be omitted in the case of finite trees.)Conditions (2) and (3) say that (X, ≤S) is a component-wise linear order, each component being a sibling set. Condition (4) asserts that if a sibling set S is infinite then (S, ≤S) is isomorphic to (\\n  \\n    \\n      \\n        \\n          N\\n        \\n      \\n    \\n    {\\\\displaystyle \\\\mathbb {N} }\\n  , ≤), the usual ordering of natural numbers.\\nGiven this, there are three (another) distinguished partial orders which are uniquely given by the following prescriptions:\\n\\nThis amounts to a \"V-S-H-L±\" system of five partial orders ≤V, ≤S, ≤H, ≤L+, ≤L- on the same set X of nodes, in which, except for the pair { ≤S, ≤H }, any two relations uniquely determine the other three, see the determinacy table.\\nNotes about notational conventions:\\n\\nThe relation composition symbol ○ used in this subsection is to be interpreted left-to-right (as \\n  \\n    \\n      \\n        \\n          ∘\\n          \\n            l\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle \\\\circ _{l}}\\n  ).\\nSymbols < and ≤ express the strict and non-strict versions of a partial order.\\nSymbols > and ≥ express the converse relations.\\nThe ≺ symbol is used for the covering relation of ≤ which is the immediate version of a partial order.This yields six versions ≺, <, ≤, ≻, >, ≥ for a single partial order relation. Except for ≺ and ≻, each version uniquely determines the others. Passing from ≺ to <requires that < be transitively reducible. This is always satisfied for all of <V, <S and <H but might not hold for <L+ or <L- if X is infinite.\\n\\nThe partial orders ≤V and ≤Hare complementary:\\n\\n(<V) ⊎ (>V) ⊎ (<H) ⊎ (>H) = X × X ∖ idX.As a consequence, the \"concordant\" linear order <L+ is a linear extension of <V. Similarly, <L- is a linear extension of >V.\\nThe covering relations ≺L- and ≺L+ correspond to pre-order traversal and post-order traversal, respectively. If x ≺L- y then, according to whether y has a previous sibling or not, the x node is either the \"rightmost\" non-strict descendant of the previous sibling of y or, in the latter case, x is the first child of y. Pairs \\n  \\n    \\n      \\n        (\\n        x\\n        ,\\n        y\\n        )\\n      \\n    \\n    {\\\\displaystyle (x,y)}\\n   of the latter case form the relation  (≺L-) ∖ (<H) which is a partial map that assigns each non-leaf node its first child node. Similarly, (≻L+) ∖ (>H) assigns each non-leaf node with finitely many children its last child node.\\n\\n\\n==== Definition using horizontal order ====\\nThe Kuboyama\\'s definition of \"rooted ordered trees\" makes use of the horizontal order ≤H as a definitory relation. (See also Suppes.)\\nUsing the notation and terminology introduced so far, the definition can be expressed as follows.\\nAn ordered tree is a structure (X, ≤V, ≤H) such that conditions (1–5) are satisfied:\\n\\n(X, ≤V) is a partial order that is an unordered tree.  (The vertical order.)\\n(X, ≤H) is a partial order. (The horizontal order.)\\nThe partial orders ≤V and ≤H are complementary: (<V) ⊎ (>V) ⊎ (<H) ⊎ (>H) = X × X ∖ idX.\\n(That is, pairs of nodes that are incomparable in (<V) are comparable in (<H) and vice versa.)\\nThe partial orders ≤V and ≤H are \"consistent\": (<H) = (≤V) ○ (<H) ○ (≥V).\\n(That is, for every nodes x, y such that x <H y, all descendants of x must precede all the descendants of y.)\\nEvery node has only finitely many preceding siblings. (That is, for every infinite sibling set S, (S, ≤H) has the order type of the natural numbers.)  (Like before, this condition can be omitted in the case of finite trees.)The sibling order (≤S) is obtained by (<S) = (<H) ∩ ((≺V) ○ (≻V)), i.e. two distinct nodes are in sibling order if and only if they are in horizontal order and are siblings.\\n\\n\\n==== Determinacy table ====\\nThe following table shows the determinacy of the \"V-S-H-L±\" system. Relational expressions in the table\\'s body are equal to one of <V, <S, <H, <L-, or <L+ according to the column. It follows that except for the pair { ≤S, ≤H }, an ordered tree (X, ...) is uniquely determined by any two of the five relations.\\n\\nIn the last two rows, infL-(Y) denotes the infimum of Y in (X, ≤L-), and supL+(Y) denotes the supremum of Y in (X, ≤L+). In both rows, (≤S) resp. (≥S) can be equivalently replaced by the sibling equivalence (≤S)○(≥S). In particular, the partition into sibling sets together with either of ≤L- or ≤L+ is also sufficient to determine the ordered tree. The first prescription for ≺V can be read as: the parent of a non-root node x equals the infimum of the set of all immediate predecessors of siblings of x, where the words \"infimum\" and \"predecessors\" are meant with regard to ≤L-. Similarly with the second prescription, just use \"supremum\", \"successors\" and ≤L+.\\nThe relations ≤S and ≤H obviously cannot form a definitory pair. For the simplest example, consider an ordered tree with exactly two nodes – then one cannot tell which of them is the root.\\n\\n\\n==== XPath axes ====\\n\\nThe table on the right shows a correspondence of introduced relations to XPath axes, which are used in structured document systems to access nodes that bear particular ordering relationships to a starting \"context\" node. For a context node x, its axis named by the specifier in the left column is the set of nodes that equals the\\nimage of {x} under the correspondent relation. As of XPath 2.0, the nodes are \"returned\" in document order, which is the \"discordant\" linear order ≤L-. A \"concordance\" would be achieved, if the vertical order ≤V was defined oppositely, with the bottom-up direction outwards the root like in set theory in accordance to natural trees.\\n\\n\\n==== Traversal maps ====\\nBelow is the list of partial maps that are typically used for ordered tree traversal. Each map is a distinguished functional subrelation of  ≤L- or of its opposite.\\n\\n≺V ... the parent-node partial map,\\n≻S ... the previous-sibling partial map,\\n≺S ... the next-sibling partial map,\\n(≺L-) ∖ (<H) ... the first-child partial map,\\n(≻L+) ∖ (>H) ... the last-child partial map,\\n≻L- ... the previous-node partial map,\\n≺L- ... the next-node partial map.\\n\\n\\n==== Generating structure ====\\nThe traversal maps constitute a partial unary algebra (X, parent, previousSibling, ..., nextNode) that forms a basis for representing trees as linked data structures. At least conceptually, there are parent links, sibling adjacency links, and first / last child links. This also applies to unordered trees in general, which can be observed on the dentry data structure in the Linux VFS.Similarly to the \"V-S-H-L±\" system of partial orders, there are pairs of traversal maps that uniquely determine the whole ordered tree structure. Naturally, one such generating structure is (X, ≺V, ≺S) which can be transcribed as (X, parent, nextSibling) – the structure of parent and next-sibling links. Another important generating structure is (X, firstChild, nextSibling) known as left-child right-sibling binary tree. This partial algebra establishes a one-to-one correspondence between binary trees and ordered trees.\\n\\n\\n==== Definition using binary trees ====\\nThe correspondence to binary trees provides a concise definition of ordered trees as partial algebras.\\nAn ordered tree is a structure \\n  \\n    \\n      \\n        (\\n        X\\n        ,\\n        l\\n        c\\n        ,\\n        r\\n        s\\n        )\\n      \\n    \\n    {\\\\displaystyle (X,lc,rs)}\\n   where X is a non-empty set of nodes, and lc, rs are partial maps on X called left-child and  right-sibling, respectively. The structure is subject to the following conditions:\\n\\nThe partial maps lc and rs are disjoint, i.e. (lc) ∩ (rs) = ∅ .\\nThe inverse of (lc) ∪ (rs) is a partial map p such that the partial algebra (X, p) is an unordered tree.The partial order structure (X, ≤V, ≤S) is obtained as follows:\\n\\n\\n==== Per-level ordering ====\\n\\nAs a possible expansion of the \"V-S-H-L±\" system, another distinguished relations between nodes can be defined, based on the tree\\'s level structure. First, let us denote by ∼E the equivalence relation defined by x ∼E y if and only if x and y have the same number of ancestors. This yields a partition of the set of nodes into levels L0, L1, ... (, Ln) – a coarsement of the partition into sibling sets. Then define relations <E, <B- and <B+ by\\n\\n  \\n    \\n      \\n        \\n          \\n            \\n              \\n                \\n                  (\\n                  \\n                    <\\n                    \\n                      \\n                        E\\n                      \\n                    \\n                  \\n                  )\\n                \\n              \\n              \\n                \\n                =\\n                \\n                  (\\n                  \\n                    <\\n                    \\n                      \\n                        H\\n                      \\n                    \\n                  \\n                  )\\n                \\n                ∩\\n                \\n                  (\\n                  \\n                    ∼\\n                    \\n                      \\n                        E\\n                      \\n                    \\n                  \\n                  )\\n                \\n              \\n              \\n                \\n                  ((per) level order),\\n                \\n              \\n            \\n            \\n              \\n                \\n                  (\\n                  \\n                    <\\n                    \\n                      \\n                        \\n                          B\\n                        \\n                        \\n                          −\\n                        \\n                      \\n                    \\n                  \\n                  )\\n                \\n              \\n              \\n                \\n                =\\n                \\n                  (\\n                  \\n                    <\\n                    \\n                      \\n                        E\\n                      \\n                    \\n                  \\n                  )\\n                \\n                ∪\\n                \\n                  (\\n                  \\n                    \\n                      (\\n                      \\n                        ∼\\n                        \\n                          \\n                            E\\n                          \\n                        \\n                      \\n                      )\\n                    \\n                    ∘\\n                    \\n                      (\\n                      \\n                        >\\n                        \\n                          \\n                            V\\n                          \\n                        \\n                      \\n                      )\\n                    \\n                  \\n                  )\\n                \\n              \\n              \\n                \\n                  (breadth-first order, BFS ordering),\\n                \\n              \\n            \\n            \\n              \\n                \\n                  (\\n                  \\n                    <\\n                    \\n                      \\n                        \\n                          B\\n                        \\n                        \\n                          +\\n                        \\n                      \\n                    \\n                  \\n                  )\\n                \\n              \\n              \\n                \\n                =\\n                \\n                  (\\n                  \\n                    <\\n                    \\n                      \\n                        E\\n                      \\n                    \\n                  \\n                  )\\n                \\n                ∪\\n                \\n                  (\\n                  \\n                    \\n                      (\\n                      \\n                        \\n                          <\\n                        \\n                        \\n                          \\n                            V\\n                          \\n                        \\n                      \\n                      )\\n                    \\n                    ∘\\n                    \\n                      (\\n                      \\n                        ∼\\n                        \\n                          \\n                            E\\n                          \\n                        \\n                      \\n                      )\\n                    \\n                  \\n                  )\\n                \\n              \\n              \\n                \\n                  (breadth-first post-order).\\n                \\n              \\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\begin{aligned}\\\\left(<_{\\\\mathrm {E} }\\\\right)&=\\\\left(<_{\\\\mathrm {H} }\\\\right)\\\\cap \\\\left(\\\\sim _{\\\\mathrm {E} }\\\\right)&{\\\\text{((per) level order),}}\\\\\\\\\\\\left(<_{\\\\mathrm {B} ^{-}}\\\\right)&=\\\\left(<_{\\\\mathrm {E} }\\\\right)\\\\cup \\\\left(\\\\left(\\\\sim _{\\\\mathrm {E} }\\\\right)\\\\circ \\\\left(>_{\\\\mathrm {V} }\\\\right)\\\\right)&{\\\\text{(breadth-first order, BFS ordering),}}\\\\\\\\\\\\left(<_{\\\\mathrm {B} ^{+}}\\\\right)&=\\\\left(<_{\\\\mathrm {E} }\\\\right)\\\\cup \\\\left(\\\\left({<}_{\\\\mathrm {V} }\\\\right)\\\\circ \\\\left(\\\\sim _{\\\\mathrm {E} }\\\\right)\\\\right)&{\\\\text{(breadth-first post-order).}}\\\\end{aligned}}}\\n  It can be observed that <E is a strict partial order and <B- and <B+ are strict total orders. Moreover, there is a similarity between the \"V-S-L±\" and  \"V-E-B±\" systems: <E is component-wise linear and orthogonal to <V, <B- is linear extension of <E and of >V, and <B+ is a linear extension of <E and of <V.\\n\\n\\n==== Encoding by sequences ====\\nOrdered trees can be naturally encoded by finite sequences of natural numbers. Denote \\n  \\n    \\n      \\n        \\n          N\\n        \\n      \\n    \\n    {\\\\displaystyle \\\\mathbb {N} }\\n  ⁎ the set of all finite sequences of natural numbers. A non-empty subset W of \\n  \\n    \\n      \\n        \\n          N\\n        \\n      \\n    \\n    {\\\\displaystyle \\\\mathbb {N} }\\n  ⁎ is called a tree domain\\nif for all u, v from \\n  \\n    \\n      \\n        \\n          N\\n        \\n      \\n    \\n    {\\\\displaystyle \\\\mathbb {N} }\\n  ⁎ and all i, j from \\n  \\n    \\n      \\n        \\n          N\\n        \\n      \\n    \\n    {\\\\displaystyle \\\\mathbb {N} }\\n   the following holds (⁎ is the concatenation operator):\\n\\nIf u ⁎ v ∈  W   then   u ∈  W.\\n  (W is prefix-closed.)\\n\\nIf u ⁎ [i] ∈  W and j < i   then   u ⁎ [j] ∈ W.\\n   (W is left-sibling-closed.)\\nThe induced structure on W gives rise to an ordered tree: take the prefix order for ≥V and the lexicographical order for ≤L-.Conversely, for an ordered tree T = (X, ≤V, ≤L-) assign each node x the sequence w(x) of sibling indices, i.e. the root is assigned the empty sequence and for every non-root node x, let w(x) = w(parent(x)) ⁎ [i] where i is the number of preceding siblings of x. Put W = {w(x) | x ∈ X} . Then W is a tree domain with its induced structure isomorphic to T.\\n\\n\\n==== Nested list ====\\n\\nOrdering of siblings can be naturally applied to multidigraph generalizations that have been introduced for unordered trees. Moveover, sibling indices can be viewed as special names. A tree domain is just a tree of pathnames. As a result, one obtains a nested list as a counterpart to a nested dictionary.\\nThe example shown on the right provides a nested-list version of the nested dictionary presented before.\\nLike before, there is an initial structure (an arrow tree, depicted by full lines) that is created by a single assignment of a literal [...] to r. The structure is subsequently modified by assignments that introduce additional links.\\nThe code is applicable to JavaScript and Ruby. In Python, the modifying assignments are disallowed because of the use of sibling indices that are not present in the initial structure.\\n\\n\\n=== Nested data ===\\nThe notions of a nested dictionary and a nested list (which are generalizations of unordered / ordered trees, respectively) can be combined into the unifying concept of nested data. Such structures are most popular in connection with the JSON data format. These structures are multidigraphs with a distinguished set of container nodes, each container being either a dictionary or a list. In the text below, the sets of dictionaries and lists are respectively denoted XO and XA. This is according to the JSON terminology in which the corresponding two types of containers are called Object and Array.\\nThe complementary set of non-container nodes represents \"primitive values\". JSON-specific formalizations\\nprovide further refinement according to the supported data types.\\nNested data can be formalized as a structure\\n ℳ = (X, Σ, A, XO, XA, s, t, σ, ι) where \\nX is a set of nodes, \\nΣ is a set of names, \\nA is a set of arrows, \\nXO and\\nXA are distinguished subsets of X,\\ns is a partial function from A to X (the source map), \\nt is a total function from A to X (the target map),\\nσ is a partial function from A to Σ assigning an arrow its name, and \\nι is a partial function from A to the set \\n  \\n    \\n      \\n        \\n          N\\n        \\n      \\n    \\n    {\\\\displaystyle \\\\mathbb {N} }\\n   of natural numbers assigning an arrow its sibling index.\\nLet  ℳ be called a nested data tree if the following conditions are satisfied:\\n\\nThere is exactly one \"root\" arrow, ar, whose source s(ar) is undefined.\\n\\nEvery node is reachable via a path of arrows starting with the root arrow ar.\\n\\nEvery node from XO ∪ XA is the target of\\nexactly one arrow.\\n\\nEvery node that is the source of an arrow is from XO ∪ XA.\\n\\nThe sets XO and XA are disjoint.\\n\\nThe arrow-naming function σ satisfies the following, for every arrows a, b.\\n\\nσ(a) is defined \\n  if and only if  \\ns(a) ∈ XO.\\n\\nIf σ(a) and σ(b) are both defined and equal\\n  then  \\neither\\na = b or\\ns(a) ≠ s(b).\\n\\nThe arrow-indexing function ι satisfies the following, for every arrows a, b.\\n\\nι(a) is defined \\n  if and only if  \\ns(a) ∈ XA.\\n\\nIf ι(a) and ι(b) are both defined and equal\\n  then  \\neither\\na = b or\\ns(a) ≠ s(b).\\n\\nIf ι(a) is defined and non-zero then \\nι(a) = ι(a′) + 1 for some arrow a′\\nsuch that s(a) = s(a′).\\nNote in particular that (1) and (2) define accessible pointed quivers (X, A, s, t).\\nConditions (1–4) provide axiomatization of arrow trees with distinction of containers XO ∪ XA. By (3) there are unique links to containers and by (4) non-containers must be leaf nodes (cf. conditions (b) and (a) for hard-links in file systems).\\nAn important consequence of (1–4) is the condition of acyclicity:\\n\\nThere are no circular paths of consecutive arrows.\\nThis condition provides a weak alternative to (3).\\n\\n\\n==== Total ordering ====\\nAlthough dictionaries have the semantics of unordered collections, in programming environments they are often equipped with some intrinsic ordering.\\nSuch ordering is supported in all of Python, Ruby and JavaScript.Thus, it is worthwhile to also consider ordered nested data trees, a refinement of nested data trees in which all sibling sets are ordered.\\n(Condition (7a) is altered to let ι(a) be defined for every non-root arrow a.)\\n\\n\\n==== Nomenclature ====\\nBy considering just particular subsets of the above conditions and/or particular constituents of  ℳ we obtain a nomenclature of tree data structures. Accessible pointed quivers (X,A,s,t) form the \"lowest common denominator\" – conditions (1) and (2) are always required.\\nConditions (4–7) are imposed appropriately to whether \\nXO, XA, σ or ι are defined.\\nThe \"tree\" attribution is established by condition (3).\\n\\n\\n== See also ==\\nTree structure\\nTree (graph theory)\\nTree (set theory)\\nCardinal tree and Ordinal tree\\nHierarchy (mathematics)\\nDialog tree\\nSingle inheritance\\nGenerative grammar\\nGenetic programming\\nHierarchical clustering\\nBinary space partition tree\\nRecursion\\nFenwick tree\\n\\n\\n=== Other trees ===\\nTrie\\nDay–Stout–Warren algorithm\\nEnfilade\\nLeft child-right sibling binary tree\\nHierarchical temporal memory\\nIntegral Tree \\n\\n\\n== Notes ==\\n\\n\\n== References ==\\n\\n\\n== Further reading ==\\nDonald Knuth. The Art of Computer Programming: Fundamental Algorithms, Third Edition. Addison-Wesley, 1997. ISBN 0-201-89683-4 . Section 2.3: Trees, pp. 308–423.\\nThomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest, and Clifford Stein. Introduction to Algorithms, Second Edition. MIT Press and McGraw-Hill, 2001. ISBN 0-262-03293-7 . Section 10.4: Representing rooted trees, pp. 214–217. Chapters 12–14 (Binary Search Trees, Red-Black Trees, Augmenting Data Structures), pp. 253–320.\\n\\n\\n== External links ==\\nData Trees as a Means of Presenting Complex Data Analysis by Sally Knipe on August 8, 2013\\nDescription from the Dictionary of Algorithms and Data Structures\\nCRAN - Package data.tree implementation of a tree data structure in the R programming language\\nWormWeb.org: Interactive Visualization of the C. elegans Cell Tree – Visualize the entire cell lineage tree of the nematode C. elegans (javascript)\\nBinary Trees by L. Allison', 'In computer programming, a string is traditionally a sequence of characters, either as a literal constant or as some kind of variable. The latter may allow its elements to be mutated and the length changed, or it may be fixed (after creation). A string is generally considered as a data type and is often implemented as an array data structure of bytes (or words) that stores a sequence of elements, typically characters, using some character encoding. String may also denote more general arrays or other sequence (or list) data types and structures.\\nDepending on the programming language and precise data type used, a variable declared to be a string may either cause storage in memory to be statically allocated for a predetermined maximum length or employ dynamic allocation to allow it to hold a variable number of elements.\\nWhen a string appears literally in source code, it is known as a string literal or an anonymous string.In formal languages, which are used in mathematical logic and theoretical computer science, a string is a finite sequence of symbols that are chosen from a set called an alphabet.\\n\\n\\n== String datatypes ==\\n\\nA string datatype is a datatype modeled on the idea of a formal string. Strings are such an important and useful datatype that they are implemented in nearly every programming language. In some languages they are available as primitive types and in others as composite types. The syntax of most high-level programming languages allows for a string, usually quoted in some way, to represent an instance of a string datatype; such a meta-string is called a literal or string literal.\\n\\n\\n=== String length ===\\nAlthough formal strings can have an arbitrary finite length, the length of strings in real languages is often constrained to an artificial maximum. In general, there are two types of string datatypes: fixed-length strings, which have a fixed maximum length to be determined at compile time and which use the same amount of memory whether this maximum is needed or not, and variable-length strings, whose length is not arbitrarily fixed and which can use varying amounts of memory depending on the actual requirements at run time (see Memory management). Most strings in modern programming languages are variable-length strings. Of course, even variable-length strings are limited in length – by the size of available computer memory. The string length can be stored as a separate integer (which may put another artificial limit on the length) or implicitly through a termination character, usually a character value with all bits zero such as in C programming language. See also \"Null-terminated\" below.\\n\\n\\n=== Character encoding ===\\nString datatypes have historically allocated one byte per character, and, although the exact character set varied by region, character encodings were similar enough that programmers could often get away with ignoring this, since characters a program treated specially (such as period and space and comma) were in the same place in all the encodings a program would encounter. These character sets were typically based on ASCII or EBCDIC. If text in one encoding was displayed on a system using a different encoding, text was often mangled, though often somewhat readable and some computer users learned to read the mangled text.\\nLogographic languages such as Chinese, Japanese, and Korean (known collectively as CJK) need far more than 256 characters (the limit of a one 8-bit byte per-character encoding) for reasonable representation. The normal solutions involved keeping single-byte representations for ASCII and using two-byte representations for CJK ideographs. Use of these with existing code led to problems with matching and cutting of strings, the severity of which depended on how the character encoding was designed. Some encodings such as the EUC family guarantee that a byte value in the ASCII range will represent only that ASCII character, making the encoding safe for systems that use those characters as field separators. Other encodings such as ISO-2022 and Shift-JIS do not make such guarantees, making matching on byte codes unsafe. These encodings also were not \"self-synchronizing\", so that locating character boundaries required backing up to the start of a string, and pasting two strings together could result in corruption of the second string.\\nUnicode has simplified the picture somewhat. Most programming languages now have a datatype for Unicode strings. Unicode\\'s preferred byte stream format UTF-8 is designed not to have the problems described above for older multibyte encodings. UTF-8, UTF-16 and UTF-32 require the programmer to know that the fixed-size code units are different than the \"characters\", the main difficulty currently is incorrectly designed APIs that attempt to hide this difference (UTF-32 does make code points fixed-sized, but these are not \"characters\" due to composing codes).\\n\\n\\n=== Implementations ===\\n\\nSome languages, such as C++ and Ruby, normally allow the contents of a string to be changed after it has been created; these are termed mutable strings.  In other languages, such as Java and Python, the value is fixed and a new string must be created if any alteration is to be made; these are termed immutable strings (some of these languages also provide another type that is mutable, such as Java and .NET StringBuilder, the thread-safe Java StringBuffer, and the Cocoa NSMutableString).\\nStrings are typically implemented as arrays of bytes, characters, or code units, in order to allow fast access to individual units or substrings—including characters when they have a fixed length.  A few languages such as Haskell implement them as linked lists instead.\\nSome languages, such as Prolog and Erlang, avoid implementing a dedicated string datatype at all, instead adopting the convention of representing strings as lists of character codes.\\n\\n\\n=== Representations ===\\nRepresentations of strings depend heavily on the choice of character repertoire and the method of character encoding. Older string implementations were designed to work with repertoire and encoding defined by ASCII, or more recent extensions like the ISO 8859 series. Modern implementations often use the extensive repertoire defined by Unicode along with a variety of complex encodings such as UTF-8 and UTF-16.\\nThe term byte string usually indicates a general-purpose string of bytes, rather than strings of only (readable) characters, strings of bits, or such. Byte strings often imply that bytes can take any value and any data can be stored as-is, meaning that there should be no value interpreted as a termination value.\\nMost string implementations are very similar to variable-length arrays with the entries storing the character codes of corresponding characters. The principal difference is that, with certain encodings, a single logical character may take up more than one entry in the array. This happens for example with UTF-8, where single codes (UCS code points) can take anywhere from one to four bytes, and single characters can take an arbitrary number of codes. In these cases, the logical length of the string (number of characters) differs from the physical length of the array (number of bytes in use). UTF-32 avoids the first part of the problem.\\n\\n\\n==== Null-terminated ====\\n\\nThe length of a string can be stored implicitly by using a special terminating character; often this is the null character (NUL), which has all bits zero, a convention used and perpetuated by the popular C programming language. Hence, this representation is commonly referred to as a C string. This representation of an n-character string takes n + 1 space (1 for the terminator), and is thus an implicit data structure.\\nIn terminated strings, the terminating code is not an allowable character in any string. Strings with length field do not have this limitation and can also store arbitrary binary data.\\nAn example of a null-terminated string stored in a 10-byte buffer, along with its ASCII (or more modern UTF-8) representation as 8-bit hexadecimal numbers is:\\n\\nThe length of the string in the above example, \"FRANK\", is 5 characters, but it occupies 6 bytes. Characters after the terminator do not form part of the representation; they may be either part of other data or just garbage. (Strings of this form are sometimes called ASCIZ strings, after the original assembly language directive used to declare them.)\\n\\n\\n==== Byte- and bit-terminated ====\\nUsing a special byte other than null for terminating strings has historically appeared in both hardware and software, though sometimes with a value that was also a printing character. $ was used by many assembler systems, : used by CDC systems (this character had a value of zero), and the ZX80 used \" since this was the string delimiter in its BASIC language.\\nSomewhat similar, \"data processing\" machines like the IBM 1401 used a special word mark bit to delimit strings at the left, where the operation would start at the right. This bit had to be clear in all other parts of the string. This meant that, while the IBM 1401 had a seven-bit word, almost no-one ever thought to use this as a feature, and override the assignment of the seventh bit to (for example) handle ASCII codes.\\nEarly microcomputer software relied upon the fact that ASCII codes do not use the high-order bit, and set it to indicate the end of a string. It must be reset to 0 prior to output.\\n\\n\\n==== Length-prefixed ====\\nThe length of a string can also be stored explicitly, for example by prefixing the string with the length as a byte value. This convention is used in many Pascal dialects; as a consequence, some people call such a string a Pascal string or P-string. Storing the string length as byte limits the maximum string length to 255. To avoid such limitations, improved implementations of P-strings use 16-, 32-, or 64-bit words to store the string length. When the length field covers the address space, strings are limited only by the available memory.\\nIf the length is bounded, then it can be encoded in constant space, typically a machine word, thus leading to an implicit data structure, taking n + k space, where k is the number of characters in a word (8 for 8-bit ASCII on a 64-bit machine, 1 for 32-bit UTF-32/UCS-4 on a 32-bit machine, etc.).\\nIf the length is not bounded, encoding a length n takes log(n) space (see fixed-length code), so length-prefixed strings are a succinct data structure, encoding a string of length n in log(n) + n space.\\nIn the latter case, the length-prefix field itself doesn\\'t have fixed length, therefore the actual string data needs to be moved when the string grows such that the length field needs to be increased.\\nHere is a Pascal string stored in a 10-byte buffer, along with its ASCII / UTF-8 representation:\\n\\n\\n==== Strings as records ====\\nMany languages, including object-oriented ones, implement strings as records with an internal structure like:\\n\\nHowever, since the implementation is usually hidden, the string must be accessed and modified through member functions. text is a pointer to a dynamically allocated memory area, which might be expanded as needed. See also string (C++).\\n\\n\\n==== Other representations ====\\nBoth character termination and length codes limit strings: For example, C character arrays that contain null (NUL) characters cannot be handled directly by C string library functions: Strings using a length code are limited to the maximum value of the length code.\\nBoth of these limitations can be overcome by clever programming.\\nIt is possible to create data structures and functions that manipulate them that do not have the problems associated with character termination and can in principle overcome length code bounds. It is also possible to optimize the string represented using techniques from run length encoding (replacing repeated characters by the character value and a length) and Hamming encoding.\\nWhile these representations are common, others are possible. Using ropes makes certain string operations, such as insertions, deletions, and concatenations more efficient.\\nThe core data structure in a text editor is the one that manages the string (sequence of characters) that represents the current state of the file being edited.\\nWhile that state could be stored in a single long consecutive array of characters, a typical text editor instead uses an alternative representation as its sequence data structure—a gap buffer, a linked list of lines, a piece table, or a rope—which makes certain string operations, such as insertions, deletions, and undoing previous edits, more efficient.\\n\\n\\n=== Security concerns ===\\nThe differing memory layout and storage requirements of strings can affect the security of the program accessing the string data. String representations requiring a terminating character are commonly susceptible to buffer overflow problems if the terminating character is not present, caused by a coding error or an attacker deliberately altering the data. String representations adopting a separate length field are also susceptible if the length can be manipulated. In such cases, program code accessing the string data requires bounds checking to ensure that it does not inadvertently access or change data outside of the string memory limits.\\nString data is frequently obtained from user input to a program. As such, it is the responsibility of the program to validate the string to ensure that it represents the expected format. Performing limited or no validation of user input can cause a program to be vulnerable to code injection attacks.\\n\\n\\n== Literal strings ==\\n\\nSometimes, strings need to be embedded inside a text file that is both human-readable and intended for consumption by a machine.  This is needed in, for example, source code of programming languages, or in configuration files. In this case, the NUL character doesn\\'t work well as a terminator since it is normally invisible (non-printable) and is difficult to input via a keyboard.  Storing the string length would also be inconvenient as manual computation and tracking of the length is tedious and error-prone.\\nTwo common representations are:\\n\\nSurrounded by quotation marks (ASCII 0x22 double quote or ASCII 0x27 single quote), used by most programming languages. To be able to include special characters such as the quotation mark itself, newline characters, or non-printable characters, escape sequences are often available, usually prefixed with the backslash character (ASCII 0x5C).\\nTerminated by a newline sequence, for example in Windows INI files.\\n\\n\\n== Non-text strings ==\\nWhile character strings are very common uses of strings, a string in computer science may refer generically to any sequence of homogeneously typed data. A bit string or byte string, for example, may be used to represent non-textual binary data retrieved from a communications medium. This data may or may not be represented by a string-specific datatype, depending on the needs of the application, the desire of the programmer, and the capabilities of the programming language being used. If the programming language\\'s string implementation is not 8-bit clean, data corruption may ensue.\\nC programmers draw a sharp distinction between a \"string\", aka a \"string of characters\", which by definition is always null terminated, vs. a \"byte string\" or \"pseudo string\" which may be stored in the same array but is often not null terminated.\\nUsing C string handling functions on such a \"byte string\" often seems to work, but later leads to security problems.\\n\\n\\n== String processing algorithms ==\\n\\nThere are many algorithms for processing strings, each with various trade-offs. Competing algorithms can be analyzed with respect to run time, storage requirements, and so forth.\\nSome categories of algorithms include:\\n\\nString searching algorithms for finding a given substring or pattern\\nString manipulation algorithms\\nSorting algorithms\\nRegular expression algorithms\\nParsing a string\\nSequence miningAdvanced string algorithms often employ complex mechanisms and data structures, among them suffix trees and finite-state machines.\\nThe name stringology was coined in 1984 by computer scientist Zvi Galil for the issue of algorithms and data structures used for string processing.\\n\\n\\n== Character string-oriented languages and utilities ==\\nCharacter strings are such a useful datatype that several languages have been designed in order to make string processing applications easy to write. Examples include the following languages:\\n\\nawk\\nIcon\\nMUMPS\\nPerl\\nRexx\\nRuby\\nsed\\nSNOBOL\\nTcl\\nTTMMany Unix utilities perform simple string manipulations and can be used to easily program some powerful string processing algorithms. Files and finite streams may be viewed as strings.\\nSome APIs like Multimedia Control Interface, embedded SQL or printf use strings to hold commands that will be interpreted.\\nRecent scripting programming languages, including Perl, Python, Ruby, and Tcl employ regular expressions to facilitate text operations. Perl is particularly noted for its regular expression use, and many other languages and applications implement Perl compatible regular expressions.\\nSome languages such as Perl and Ruby support string interpolation, which permits arbitrary expressions to be evaluated and included in string literals.\\n\\n\\n== Character string functions ==\\n\\nString functions are used to create strings or change the contents of a mutable string. They also are used to query information about a string. The set of functions and their names varies depending on the computer programming language.\\nThe most basic example of a string function is the string length function – the function that returns the length of a string (not counting any terminator characters or any of the string\\'s internal structural information) and does not modify the string. This function is often named length or len. For example, length(\"hello world\") would return 11. Another common function is concatenation, where a new string is created by appending two strings, often this is the + addition operator.\\nSome microprocessor\\'s instruction set architectures contain direct support for string operations, such as block copy (e.g. In intel x86m REPNZ MOVSB).\\n\\n\\n== Formal theory ==\\n\\nLet Σ be a finite set of symbols (alternatively called characters), called the alphabet. No assumption is made about the nature of the symbols. A string (or word) over Σ is any finite sequence of symbols from Σ. For example, if Σ = {0, 1}, then 01011 is a string over Σ.\\nThe length of a string s is the number of symbols in s (the length of the sequence) and can be any non-negative integer; it is often denoted as |s|.  The empty string is the unique string over Σ of length 0, and is denoted ε or λ.The set of all strings over Σ of length n is denoted Σn.  For example, if Σ = {0, 1}, then Σ2 = {00, 01, 10, 11}.  Note that Σ0 = {ε} for any alphabet Σ.\\nThe set of all strings over Σ of any length is the Kleene closure of Σ and is denoted Σ*.  In terms of Σn,\\n\\n  \\n    \\n      \\n        \\n          Σ\\n          \\n            ∗\\n          \\n        \\n        =\\n        \\n          ⋃\\n          \\n            n\\n            ∈\\n            \\n              N\\n            \\n            ∪\\n            {\\n            0\\n            }\\n          \\n        \\n        \\n          Σ\\n          \\n            n\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle \\\\Sigma ^{*}=\\\\bigcup _{n\\\\in \\\\mathbb {N} \\\\cup \\\\{0\\\\}}\\\\Sigma ^{n}}\\n  For example, if Σ = {0, 1}, then Σ* = {ε, 0, 1, 00, 01, 10, 11, 000, 001, 010, 011, ...}.  Although the set Σ* itself is countably infinite, each element of Σ* is a string of finite length.\\nA set of strings over Σ (i.e. any subset of Σ*) is called a formal language over Σ.  For example, if Σ = {0, 1}, the set of strings with an even number of zeros, {ε, 1, 00, 11, 001, 010, 100, 111, 0000, 0011, 0101, 0110, 1001, 1010, 1100, 1111, ...}, is a formal language over Σ.\\n\\n\\n=== Concatenation and substrings ===\\nConcatenation is an important binary operation on Σ*.  For any two strings s and t in Σ*, their concatenation is defined as the sequence of symbols in s followed by the sequence of characters in t, and is denoted st.  For example, if Σ = {a, b, ..., z}, s = bear, and t = hug, then st = bearhug and ts = hugbear.\\nString concatenation is an associative, but non-commutative operation. The empty string ε serves as the identity element; for any string s, εs = sε = s.  Therefore, the set Σ* and the concatenation operation form a monoid, the free monoid generated by Σ.  In addition, the length function defines a monoid homomorphism from Σ* to the non-negative integers (that is, a function \\n  \\n    \\n      \\n        L\\n        :\\n        \\n          Σ\\n          \\n            ∗\\n          \\n        \\n        ↦\\n        \\n          N\\n        \\n        ∪\\n        {\\n        0\\n        }\\n      \\n    \\n    {\\\\displaystyle L:\\\\Sigma ^{*}\\\\mapsto \\\\mathbb {N} \\\\cup \\\\{0\\\\}}\\n  , such that \\n  \\n    \\n      \\n        L\\n        (\\n        s\\n        t\\n        )\\n        =\\n        L\\n        (\\n        s\\n        )\\n        +\\n        L\\n        (\\n        t\\n        )\\n        \\n        ∀\\n        s\\n        ,\\n        t\\n        ∈\\n        \\n          Σ\\n          \\n            ∗\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle L(st)=L(s)+L(t)\\\\quad \\\\forall s,t\\\\in \\\\Sigma ^{*}}\\n  ).\\nA string s is said to be a substring or factor of t if there exist (possibly empty) strings u and v such that t = usv.  The relation \"is a substring of\" defines a partial order on Σ*, the least element of which is the empty string.\\n\\n\\n=== Prefixes and suffixes ===\\nA string s is said to be a prefix of t if there exists a string u such that t = su. If u is nonempty, s is said to be a proper prefix of t. Symmetrically, a string s is said to be a suffix of t if there exists a string u such that t = us. If u is nonempty, s is said to be a proper suffix of t. Suffixes and prefixes are substrings of t. Both the relations \"is a prefix of\" and \"is a suffix of\" are prefix orders.\\n\\n\\n=== Reversal ===\\nThe reverse of a string is a string with the same symbols but in reverse order. For example, if s = abc (where a, b, and c are symbols of the alphabet), then the reverse of s is cba. A string that is the reverse of itself (e.g., s = madam) is called a palindrome, which also includes the empty string and all strings of length 1.\\n\\n\\n=== Rotations ===\\nA string s = uv is said to be a rotation of t if t = vu. For example, if Σ = {0, 1} the string 0011001 is a rotation of 0100110, where u = 00110 and v = 01. As another example, the string abc has three different rotations, viz. abc itself (with u=abc, v=ε), bca (with u=bc, v=a), and cab (with u=c, v=ab).\\n\\n\\n=== Lexicographical ordering ===\\nIt is often useful to define an ordering on a set of strings. If the alphabet Σ has a total order (cf. alphabetical order) one can define a total order on Σ* called lexicographical order. For example, if Σ = {0, 1} and 0 < 1, then the lexicographical order on Σ* includes the relationships ε < 0 < 00 < 000 < ... < 0001 < 001 < 01 < 010 < 011 < 0110 < 01111 < 1 < 10 < 100 < 101 < 111 < 1111 < 11111 ... The lexicographical order is total if the alphabetical order is, but isn\\'t well-founded for any nontrivial alphabet, even if the alphabetical order is.\\nSee Shortlex for an alternative string ordering that preserves well-foundedness.\\n\\n\\n=== String operations ===\\nA number of additional operations on strings commonly occur in the formal theory. These are given in the article on string operations.\\n\\n\\n=== Topology ===\\n\\nStrings admit the following interpretation as nodes on a graph, where k is the number of symbols in Σ:\\n\\nFixed-length strings of length n can be viewed as the integer locations in an n-dimensional hypercube with sides of length k-1.\\nVariable-length strings (of finite length) can be viewed as nodes on a perfect k-ary tree.\\nInfinite strings (otherwise not considered here) can be viewed as infinite paths on a k-node complete graph.The natural topology on the set of fixed-length strings or variable-length strings is the discrete topology, but the natural topology on the set of infinite strings is the limit topology, viewing the set of infinite strings as the inverse limit of the sets of finite strings. This is the construction used for the p-adic numbers and some constructions of the Cantor set, and yields the same topology.\\nIsomorphisms between string representations of topologies can be found by normalizing according to the lexicographically minimal string rotation.\\n\\n\\n== See also ==\\nBinary-safe — a property of string manipulating functions treating their input as raw data stream\\nBit array — a string of binary digits\\nC string handling — overview of C string handling\\nC++ string handling — overview of C++ string handling\\nComparison of programming languages (string functions)\\nConnection string — passed to a driver to initiate a connection (e.g., to a database)\\nEmpty string — its properties and representation in programming languages\\nIncompressible string — a string that cannot be compressed by any algorithm\\nRope (data structure) — a data structure for efficiently manipulating long strings\\nString metric — notions of similarity between strings\\n\\n\\n== References ==', 'A Data Matrix is a two-dimensional code consisting of black and white \"cells\" or dots arranged in either a square or rectangular pattern, also known as a matrix. The information to be encoded can be text or numeric data. Usual data size is from a few bytes up to 1556 bytes. The length of the encoded data depends on the number of cells in the matrix. Error correction codes are often used to increase reliability: even if one or more cells are damaged so it is unreadable, the message can still be read. A Data Matrix symbol can store up to 2,335 alphanumeric characters.\\nData Matrix symbols are rectangular, usually square in shape and composed of square \"cells\" which represent bits. Depending on the coding used, a \"light\" cell represents a 0 and a \"dark\" cell is a 1, or vice versa. Every Data Matrix is composed of two solid adjacent borders in an \"L\" shape (called the \"finder pattern\") and two other borders consisting of alternating dark and light \"cells\" or modules (called the \"timing pattern\"). Within these borders are rows and columns of cells encoding information. The finder pattern is used to locate and orient the symbol while the timing pattern provides a count of the number of rows and columns in the symbol. As more data is encoded in the symbol, the number of cells (rows and columns) increases. Each code is unique.  Symbol sizes vary from 10×10 to 144×144 in the new version ECC 200, and from 9×9 to 49×49 in the old version ECC 000 – 140.\\n\\n\\n== Applications ==\\n\\nThe most popular application for Data Matrix is marking small items, due to the code\\'s ability to encode fifty characters in a symbol that is readable at 2 or 3 mm2 (0.003 or 0.005 sq in) and the fact that the code can be read with only a 20% contrast ratio.\\nA Data Matrix is scalable; commercial applications exist with images as small as 300 micrometres (0.012 in) (laser etched on a 600-micrometre (0.024 in) silicon device) and as large as a 1 metre (3 ft) square (painted on the roof of a boxcar). Fidelity of the marking and reading systems are the only limitation.\\nThe US Electronic Industries Alliance (EIA) recommends using Data Matrix for labeling small electronic components.Data Matrix codes are becoming common on printed media such as labels and letters. The code can be read quickly by a barcode reader which allows the media to be tracked, for example when a parcel has been dispatched to the recipient.\\n\\nFor industrial engineering purposes, Data Matrix codes can be marked directly onto components, ensuring that only the intended component is identified with the data-matrix-encoded data. The codes can be marked onto components with various methods, but within the aerospace industry these are commonly industrial ink-jet, dot-peen marking, laser marking, and electrolytic chemical etching (ECE). These methods give a permanent mark which can last up to the lifetime of the component.\\nData Matrix codes are usually verified using specialist camera equipment and software. This verification ensures the code conforms to the relevant standards, and ensures readability for the lifetime of the component. After  component enters service, the Data Matrix code can then be read by a reader camera, which decodes the Data Matrix data which can then be used for a number of purposes, such as movement tracking or inventory stock checks.\\n\\nData Matrix codes, along with other open-source codes such as 1D barcodes can also be read with mobile phones by downloading code specific mobile applications. Although many mobile devices are able to read 2D codes including Data Matrix Code,  few extend the decoding to enable mobile access and interaction, whereupon the codes can be used securely and across media; for example, in track and trace, anti-counterfeit, e.govt, and banking solutions.\\n\\n\\n=== Food industry ===\\nData Matrix codes are used in the food industry in autocoding systems to prevent food products being packaged and dated incorrectly. Codes are maintained internally on a food manufacturers database and associated with each unique product, e.g. ingredient variations. For each product run the unique code is supplied to the printer. Label artwork is required to allow the 2D Data Matrix to be positioned for optimal scanning. For black on white codes testing isn\\'t required unless print quality is an issue, but all color variations need to be tested before production to ensure they are readable.\\n\\n\\n=== Art ===\\nIn May 2006 a German computer programmer, Bernd Hopfengärtner, created a large Data Matrix in a wheat field (in a fashion similar to crop circles). The message read \"Hello, World!\". In June 2011 the Parisian tattoo artist K.A.R.L., as part of a promotion for Ballantine\\'s scotch whisky, created the world\\'s first animated tattoo utilizing a Data Matrix code in a collaborative process streamed live on Facebook.\\n\\n\\n== Technical specifications ==\\n\\nData Matrix symbols are made up of modules arranged within a perimeter finder and timing pattern. It can encode up to 3,116 characters from the entire ASCII character set (with extensions). The symbol consists of data regions which contain modules set out in a regular array. Large symbols contain several regions. Each data region is delimited by a finder pattern, and this is surrounded on all four sides by a quiet zone border (margin). (Note: The modules may be round or square- no specific shape is defined in the standard. For example, dot-peened cells are generally round.)\\n\\n\\n=== Data Matrix ECC 200 ===\\nECC 200, the newer version of Data Matrix, uses Reed–Solomon codes for error and erasure recovery. ECC 200 allows the routine reconstruction of the entire encoded data string when the symbol has sustained 30% damage, assuming the matrix can still be accurately located.   Data Matrix has an error rate of less than 1 in 10 million characters scanned.Symbols have an even number of rows and an even number of columns. Most of the symbols are square with sizes from 10 × 10 to 144 × 144. Some symbols however are rectangular with sizes from 8×18 to 16×48 (even values only). All symbols using the ECC 200 error correction can be recognized by the upper-right corner module being the same as the background color. (binary 0).\\nAdditional capabilities that differentiate ECC 200 symbols from the earlier standards include:\\n\\nInverse reading symbols (light images on a dark background)\\nSpecification of the character set (via Extended Channel Interpretations)\\nRectangular symbols\\nStructured append (linking of up to 16 symbols to encode larger amounts of data)\\n\\n\\n=== Data Matrix ECC 000–140 ===\\nOlder versions of Data Matrix include ECC 000, ECC 050, ECC 080, ECC 100, ECC 140. Instead of using Reed–Solomon codes like ECC 200, ECC 000–140 use a convolution-based error correction.  Each varies in the amount of error correction it offers, with ECC 000 offering none, and ECC 140 offering the greatest. For error detection at decode time, even in the case of ECC 000, each of these versions also encode a cyclic redundancy check (CRC) on the bit pattern.  As an added measure, the placement of each bit in the code is determined by bit-placement tables included in the specification.  These older versions always have an odd number of modules, and can be made in sizes ranging from 9 × 9 to 49 × 49. All symbols utilizing the ECC 000 through 140 error correction can be recognized by the upper-right corner module being the inverse of the background color. (binary 1).\\nAccording to ISO/IEC 16022, \"ECC 000–140 should only be used in closed applications where a single party controls both the production and reading of the symbols and is responsible for overall system performance.\"\\n\\n\\n== Standards ==\\nData Matrix was invented by International Data Matrix, Inc. (ID Matrix) which was merged into RVSI/Acuity CiMatrix, who were acquired by Siemens AG in October 2005 and Microscan Systems in September 2008. Data Matrix is covered today by several ISO/IEC standards and is in the public domain for many applications, which means it can be used free of any licensing or royalties.\\n\\nISO/IEC 16022:2006—Data Matrix bar code symbology specification\\nISO/IEC 15415—2-D Print quality standard\\nISO/IEC 15418:2016—Symbol data format semantics (GS1 application identifiers and ASC MH10 data identifiers and maintenance)\\nISO/IEC 15424:2008—Data Carrier Identifiers (including Symbology Identifiers) [IDs for distinguishing different barcode types]\\nISO/IEC 15434:2006—Syntax for high-capacity ADC media (format of data transferred from scanner to software, etc.)\\nISO/IEC 15459—Unique identifiers\\n\\n\\n== Encoding ==\\n\\nThe encoding process is described in the ISO/IEC standard 16022:2006.  Open-source software for encoding and decoding the ECC-200 variant of Data Matrix has been published.The diagrams below illustrate the placement of the message data within a Data Matrix symbol. The message is \"Wikipedia\", and it is arranged in a somewhat complicated diagonal pattern starting near the upper-left corner. Some characters are split in two pieces, such as the initial W, and the third \\'i\\' is in \"corner pattern 2\" rather than the usual L-shaped arrangement. Also shown are the end-of-message code (marked End), the padding (P) and error correction (E) bytes, and four modules of unused space (X).\\n\\nMultiple encoding modes are used to store different kinds of messages. The default mode stores one ASCII character per 8-bit codeword. Control codes are provided to switch between modes, as shown below.\\n\\n\\n=== Text modes ===\\nThe C40, Text and X12 modes are potentially more compact for storing text messages. They are similar to DEC Radix-50, using character codes in the range 0–39, and three of these codes are combined to make a number up to 403=64000, which is packed into two bytes (maximum value 65536) as follows:\\n\\nV = C1×1600 + C2×40 + C3 + 1\\nB1 = floor(V/256)\\nB2 = V mod 256The resulting value of B1 is in the range 0–250. The special value 254 is used to return to ASCII encoding mode.\\nCharacter code interpretations are shown in the table below. The C40 and Text modes have four separate sets. Set 0 is the default, and contains codes that temporarily select a different set for the next character.  The only difference is that they reverse upper-and lower-case letters.  C40 is primarily upper-case, with lower-case letters in set 3; Text is the other way around.  Set 1, containing ASCII control codes, and set 2, containing punctuation symbols are identical in C40 and Text mode.\\n\\n\\n=== EDIFACT mode ===\\nEDIFACT mode uses six bits per character, with four characters packed into three bytes. It can store digits, upper-case letters, and many punctuation marks, but has no support for lower-case letters.\\n\\n\\n=== Base 256 mode ===\\nBase 256 mode data starts with a length indicator, followed by a number of data bytes. A length of 1 to 249 is encoded as a single byte,\\nand longer lengths are stored as two bytes.\\n\\nL1 = floor(length / 250) + 249, L2 = length mod 250It is desirable to avoid long strings of zeros in the coded message, because they become large blank areas in the Data Matrix symbol, which may\\ncause a scanner to lose synchronization. (The default ASCII encoding does not use zero for this reason.) In order to make that less likely, the\\nlength and data bytes are obscured by adding a pseudorandom value R(n), where n is the position in the byte stream.\\n\\nR(n) = (149 × n) mod 255 + 1\\n\\n\\n== Patent issues ==\\nPrior to the expiration of U.S. Patent 5,612,524 in November 2007, intellectual property company Acacia Technologies claimed that Data Matrix was partially covered by its contents. As the patent owner, Acacia allegedly contacted Data Matrix users demanding license fees related to the patent.\\nCognex Corporation, a large manufacturer of 2D barcode devices, filed a declaratory judgment complaint on 13 March 2006 after receiving information that Acacia had contacted its customers demanding licensing fees. On 19 May 2008 Judge Joan N. Ericksen of the U.S. District Court in Minnesota ruled in favor of Cognex. The ruling held that the \\'524 patent, which claimed to cover a system for capturing and reading 2D symbology codes, is both invalid and unenforceable due to inequitable conduct by the defendants during the procurement of the patent.\\nWhile the ruling was delivered after the patent expired, it precluded claims for infringement based on use of Data Matrix prior to November 2007.\\nA German patent application DE 4107020 was filed in 1991, and published in 1992. This patent is not cited in the above US patent applications and might invalidate them.\\n\\n\\n== See also ==\\nPDF417\\nAztec Code\\nHigh Capacity Color Barcode\\nMaxiCode\\nNintendo e-Reader\\nQR Code\\nSemacode\\nSPARQCode\\nTrusted paper key\\n\\n\\n== References ==\\n\\n\\n== External links ==\\nGS1 DataMatrix Guideline: Overview and technical introduction to the use of GS1 DataMatrix\\nDatamatrix Code Generator - Online Tool', 'In computing, a database is an organized collection of data stored and accessed electronically from a computer system. Where databases are more complex they are often developed using formal design and modeling techniques.\\nThe database management system (DBMS) is the software that interacts with end users, applications, and the database itself to capture and analyze the data. The DBMS software additionally encompasses the core facilities provided to administer the database. The sum total of the database, the DBMS and the associated applications can be referred to as a \"database system\". Often the term \"database\" is also used loosely to refer to any of the DBMS, the database system or an application associated with the database.\\nComputer scientists may classify database-management systems according to the database models that they support. Relational databases became dominant in the 1980s. These model data as rows and columns in a series of tables, and the vast majority use SQL for writing and querying data. In the 2000s, non-relational databases became popular, referred to as NoSQL because they use different query languages.\\n\\n\\n== Terminology and overview ==\\nFormally, a \"database\" refers to a set of related data and the way it is organized. Access to this data is usually provided by a \"database management system\" (DBMS) consisting of an integrated set of computer software that allows users to interact with one or more databases and provides access to all of the data contained in the database (although restrictions may exist that limit access to particular data). The DBMS provides various functions that allow entry, storage and retrieval of large quantities of information and provides ways to manage how that information is organized.\\nBecause of the close relationship between them, the term \"database\" is often used casually to refer to both a database and the DBMS used to manipulate it.\\nOutside the world of professional information technology, the term database is often used to refer to any collection of related data (such as a spreadsheet or a card index) as size and usage requirements typically necessitate use of a database management system.Existing DBMSs provide various functions that allow management of a database and its data which can be classified into four main functional groups:\\n\\nData definition – Creation, modification and removal of definitions that define the organization of the data.\\nUpdate – Insertion, modification, and deletion of the actual data.\\nRetrieval – Providing information in a form directly usable or for further processing by other applications. The retrieved data may be made available in a form basically the same as it is stored in the database or in a new form obtained by altering or combining existing data from the database.\\nAdministration – Registering and monitoring users, enforcing data security, monitoring performance, maintaining data integrity, dealing with concurrency control, and recovering information that has been corrupted by some event such as an unexpected system failure.Both a database and its DBMS conform to the principles of a particular database model. \"Database system\" refers collectively to the database model, database management system, and database.Physically, database servers are dedicated computers that hold the actual databases and run only the DBMS and related software. Database servers are usually multiprocessor computers, with generous memory and RAID disk arrays used for stable storage. Hardware database accelerators, connected to one or more servers via a high-speed channel, are also used in large volume transaction processing environments. DBMSs are found at the heart of most database applications. DBMSs may be built around a custom multitasking kernel with built-in networking support, but modern DBMSs typically rely on a standard operating system to provide these functions.Since DBMSs comprise a significant market, computer and storage vendors often take into account DBMS requirements in their own development plans.Databases and DBMSs can be categorized according to the database model(s) that they support (such as relational or XML), the type(s) of computer they run on (from a server cluster to a mobile phone), the query language(s) used to access the database (such as SQL or XQuery), and their internal engineering, which affects performance, scalability, resilience, and security.\\n\\n\\n== History ==\\nThe sizes, capabilities, and performance of databases and their respective DBMSs have grown in orders of magnitude. These performance increases were enabled by the technology progress in the areas of processors, computer memory, computer storage, and computer networks. The concept of a database was made possible by the emergence of direct access storage media such as magnetic disks, which became widely available in the mid 1960s; earlier systems relied on sequential storage of data on magnetic tape. The subsequent development of database technology can be divided into three eras based on data model or structure: navigational, SQL/relational, and post-relational.\\nThe two main early navigational data models were the hierarchical model and the CODASYL model (network model). These were characterized by the use of pointers (often physical disk addresses) to follow relationships from one record to another.\\nThe relational model, first proposed in 1970 by Edgar F. Codd, departed from this tradition by insisting that applications should search for data by content, rather than by following links. The relational model employs sets of ledger-style tables, each used for a different type of entity. Only in the mid-1980s did computing hardware become powerful enough to allow the wide deployment of relational systems (DBMSs plus applications). By the early 1990s, however, relational systems dominated in all large-scale data processing applications, and as of 2018 they remain dominant: IBM DB2, Oracle, MySQL, and Microsoft SQL Server are the most searched DBMS. The dominant database language, standardised SQL for the relational model, has influenced database languages for other data models.Object databases were developed in the 1980s to overcome the inconvenience of object–relational impedance mismatch, which led to the coining of the term \"post-relational\" and also the development of hybrid object–relational databases.\\nThe next generation of post-relational databases in the late 2000s became known as NoSQL databases, introducing fast key–value stores and document-oriented databases. A competing \"next generation\" known as NewSQL databases attempted new implementations that retained the relational/SQL model while aiming to match the high performance of NoSQL compared to commercially available relational DBMSs.\\n\\n\\n=== 1960s, navigational DBMS ===\\n\\nThe introduction of the term database coincided with the availability of direct-access storage (disks and drums) from the mid-1960s onwards. The term represented a contrast with the tape-based systems of the past, allowing shared interactive use rather than daily batch processing. The Oxford English Dictionary cites a 1962 report by the System Development Corporation of California as the first to use the term \"data-base\" in a specific technical sense.As computers grew in speed and capability, a number of general-purpose database systems emerged; by the mid-1960s a number of such systems had come into commercial use. Interest in a standard began to grow, and Charles Bachman, author of one such product, the Integrated Data Store (IDS), founded the Database Task Group within CODASYL, the group responsible for the creation and standardization of COBOL. In 1971, the Database Task Group delivered their standard, which generally became known as the CODASYL approach, and soon a number of commercial products based on this approach entered the market.\\nThe CODASYL approach offered applications the ability to navigate around a linked data set which was formed into a large network. Applications could find records by one of three methods:\\n\\nUse of a primary key (known as a CALC key, typically implemented by hashing)\\nNavigating relationships (called sets) from one record to another\\nScanning all the records in a sequential orderLater systems added B-trees to provide alternate access paths. Many CODASYL databases also added a declarative query language for end users (as distinct from the navigational API). However CODASYL databases were complex and required significant training and effort to produce useful applications.\\nIBM also had their own DBMS in 1966, known as Information Management System (IMS). IMS was a development of software written for the Apollo program on the System/360. IMS was generally similar in concept to CODASYL, but used a strict hierarchy for its model of data navigation instead of CODASYL\\'s network model. Both concepts later became known as navigational databases due to the way data was accessed: the term was popularized by Bachman\\'s 1973 Turing Award presentation The Programmer as Navigator. IMS is classified by IBM as a hierarchical database. IDMS and Cincom Systems\\' TOTAL database are classified as network databases. IMS remains in use as of 2014.\\n\\n\\n=== 1970s, relational DBMS ===\\nEdgar F. Codd worked at IBM in San Jose, California, in one of their offshoot offices that was primarily involved in the development of hard disk systems. He was unhappy with the navigational model of the CODASYL approach, notably the lack of a \"search\" facility. In 1970, he wrote a number of papers that outlined a new approach to database construction that eventually culminated in the groundbreaking A Relational Model of Data for Large Shared Data Banks.In this paper, he described a new system for storing and working with large databases. Instead of records being stored in some sort of linked list of free-form records as in CODASYL, Codd\\'s idea was to organize the data as a number of \"tables\", each table being used for a different type of entity. Each table would contain a fixed number of columns containing the attributes of the entity. One or more columns of each table were designated as a  primary key by which the rows of the table could be uniquely identified; cross-references between tables always used these primary keys, rather than disk addresses, and queries would join tables based on these key relationships, using a set of operations based on the mathematical system of relational calculus (from which the model takes its name). Splitting the data into a set of normalized tables (or relations) aimed to ensure that each \"fact\" was only stored once, thus simplifying update operations. Virtual tables called views could present the data in different ways for different users, but views could not be directly updated.\\nCodd used mathematical terms to define the model: relations, tuples, and domains rather than tables, rows, and columns. The terminology that is now familiar came from early implementations. Codd would later criticize the tendency for practical implementations to depart from the mathematical foundations on which the model was based.\\n\\nThe use of primary keys (user-oriented identifiers) to represent cross-table relationships, rather than disk addresses, had two primary motivations. From an engineering perspective, it enabled tables to be relocated and resized without expensive database reorganization. But Codd was more interested in the difference in semantics: the use of explicit identifiers made it easier to define update operations with clean mathematical definitions, and it also enabled query operations to be defined in terms of the established discipline of first-order predicate calculus; because these operations have clean mathematical properties, it becomes possible to rewrite queries in provably correct ways, which is the basis of query optimization. There is no loss of expressiveness compared with the hierarchic or network models, though the connections between tables are no longer so explicit.\\nIn the hierarchic and network models, records were allowed to have a complex internal structure. For example, the salary history of an employee might be represented as a \"repeating group\" within the employee record. In the relational model, the process of normalization led to such internal structures being replaced by data held in multiple tables, connected only by logical keys.\\nFor instance, a common use of a database system is to track information about users, their name, login information, various addresses and phone numbers. In the navigational approach, all of this data would be placed in a single variable-length record. In the relational approach, the data would be normalized into a user table, an address table and a phone number table (for instance). Records would be created in these optional tables only if the address or phone numbers were actually provided.\\nAs well as identifying rows/records using logical identifiers rather than disk addresses, Codd changed the way in which applications assembled data from multiple records. Rather than requiring applications to gather data one record at a time by navigating the links, they would use a declarative query language that expressed what data was required, rather than the access path by which it should be found. Finding an efficient access path to the data became the responsibility of the database management system, rather than the application programmer. This process, called query optimization, depended on the fact that queries were expressed in terms of mathematical logic.\\nCodd\\'s paper was picked up by two people at Berkeley, Eugene Wong and Michael Stonebraker. They started a project known as INGRES using funding that had already been allocated for a geographical database project and student programmers to produce code. Beginning in 1973, INGRES delivered its first test products which were generally ready for widespread use in 1979. INGRES was similar to System R in a number of ways, including the use of a \"language\" for data access, known as QUEL. Over time, INGRES moved to the emerging SQL standard.\\nIBM itself did one test implementation of the relational model, PRTV, and a production one, Business System 12, both now discontinued. Honeywell wrote MRDS for Multics, and now there are two new implementations: Alphora Dataphor and Rel. Most other DBMS implementations usually called relational are actually SQL DBMSs.\\nIn 1970, the University of Michigan began development of the MICRO Information Management System based on D.L. Childs\\' Set-Theoretic Data model. MICRO was used to manage very large data sets by the US Department of Labor, the U.S. Environmental Protection Agency, and researchers from the University of Alberta, the University of Michigan, and Wayne State University. It ran on IBM mainframe computers using the Michigan Terminal System. The system remained in production until 1998.\\n\\n\\n=== Integrated approach ===\\n\\nIn the 1970s and 1980s, attempts were made to build database systems with integrated hardware and software. The underlying philosophy was that such integration would provide higher performance at a lower cost. Examples were IBM System/38, the early offering of Teradata, and the Britton Lee, Inc. database machine.\\nAnother approach to hardware support for database management was ICL\\'s CAFS accelerator, a hardware disk controller with programmable search capabilities. In the long term, these efforts were generally unsuccessful because specialized database machines could not keep pace with the rapid development and progress of general-purpose computers. Thus most database systems nowadays are software systems running on general-purpose hardware, using general-purpose computer data storage. However, this idea is still pursued for certain applications by some companies like Netezza and Oracle (Exadata).\\n\\n\\n=== Late 1970s, SQL DBMS ===\\nIBM started working on a prototype system loosely based on Codd\\'s concepts as System R in the early 1970s. The first version was ready in 1974/5, and work then started on multi-table systems in which the data could be split so that all of the data for a record (some of which is optional) did not have to be stored in a single large \"chunk\". Subsequent multi-user versions were tested by customers in 1978 and 1979, by which time a standardized query language – SQL – had been added. Codd\\'s ideas were establishing themselves as both workable and superior to CODASYL, pushing IBM to develop a true production version of System R, known as SQL/DS, and, later, Database 2 (DB2).\\nLarry Ellison\\'s Oracle Database (or more simply, Oracle) started from a different chain, based on IBM\\'s papers on System R. Though Oracle V1 implementations were completed in 1978, it wasn\\'t until Oracle Version 2 when Ellison beat IBM to market in 1979.Stonebraker went on to apply the lessons from INGRES to develop a new database, Postgres, which is now known as PostgreSQL. PostgreSQL is often used for global mission-critical applications (the .org and .info domain name registries use it as their primary data store, as do many large companies and financial institutions).\\nIn Sweden, Codd\\'s paper was also read and Mimer SQL was developed from the mid-1970s at Uppsala University. In 1984, this project was consolidated into an independent enterprise.\\nAnother data model, the entity–relationship model, emerged in 1976 and gained popularity for database design as it emphasized a more familiar description than the earlier relational model. Later on, entity–relationship constructs were retrofitted as a data modeling construct for the relational model, and the difference between the two have become irrelevant.\\n\\n\\n=== 1980s, on the desktop ===\\nThe 1980s ushered in the age of desktop computing. The new computers empowered their users with spreadsheets like Lotus 1-2-3 and database software like dBASE. The dBASE product was lightweight and easy for any computer user to understand out of the box. C. Wayne Ratliff, the creator of dBASE, stated: \"dBASE was different from programs like BASIC, C, FORTRAN, and COBOL in that a lot of the dirty work had already been done. The data manipulation is done by dBASE instead of by the user, so the user can concentrate on what he is doing, rather than having to mess with the dirty details of opening, reading, and closing files, and managing space allocation.\" dBASE was one of the top selling software titles in the 1980s and early 1990s.\\n\\n\\n=== 1990s, object-oriented ===\\nThe 1990s, along with a rise in object-oriented programming, saw a growth in how data in various databases were handled. Programmers and designers began to treat the data in their databases as objects. That is to say that if a person\\'s data were in a database, that person\\'s attributes, such as their address, phone number, and age, were now considered to belong to that person instead of being extraneous data. This allows for relations between data to be relations to objects and their attributes and not to individual fields. The term \"object–relational impedance mismatch\" described the inconvenience of translating between programmed objects and database tables. Object databases and object–relational databases attempt to solve this problem by providing an object-oriented language (sometimes as extensions to SQL) that programmers can use as alternative to purely relational SQL. On the programming side, libraries known as object–relational mappings (ORMs) attempt to solve the same problem.\\n\\n\\n=== 2000s, NoSQL and NewSQL ===\\n\\nXML databases are a type of structured document-oriented database that allows querying based on XML document attributes. XML databases are mostly used in applications where the data is conveniently viewed as a collection of documents, with a structure that can vary from the very flexible to the highly rigid: examples include scientific articles, patents, tax filings, and personnel records.\\nNoSQL databases are often very fast, do not require fixed table schemas, avoid join operations by storing denormalized data, and are designed to scale horizontally.\\nIn recent years, there has been a strong demand for massively distributed databases with high partition tolerance, but according to the CAP theorem it is impossible for a distributed system to simultaneously provide consistency, availability, and partition tolerance guarantees. A distributed system can satisfy any two of these guarantees at the same time, but not all three. For that reason, many NoSQL databases are using what is called eventual consistency to provide both availability and partition tolerance guarantees with a reduced level of data consistency.\\nNewSQL is a class of modern relational databases that aims to provide the same scalable performance of NoSQL systems for online transaction processing (read-write) workloads while still using SQL and maintaining the ACID guarantees of a traditional database system.\\n\\n\\n== Use cases ==\\nDatabases are used to support internal operations of organizations and to underpin online interactions with customers and suppliers (see Enterprise software).\\nDatabases are used to hold administrative information and more specialized data, such as engineering data or economic models. Examples include computerized library systems, flight reservation systems, computerized parts inventory systems, and many content management systems that store websites as collections of webpages in a database.\\n\\n\\n== Classification ==\\nOne way to classify databases involves the type of their contents, for example: bibliographic, document-text, statistical, or multimedia objects. Another way is by their application area, for example: accounting, music compositions, movies, banking, manufacturing, or insurance. A third way is by some technical aspect, such as the database structure or interface type. This section lists a few of the adjectives used to characterize different kinds of databases.\\n\\nAn in-memory database is a database that primarily resides in main memory, but is typically backed-up by non-volatile computer data storage. Main memory databases are faster than disk databases, and so are often used where response time is critical, such as in telecommunications network equipment.\\nAn active database includes an event-driven architecture which can respond to conditions both inside and outside the database. Possible uses include security monitoring, alerting, statistics gathering and authorization. Many databases provide active database features in the form of database triggers.\\nA cloud database relies on cloud technology. Both the database and most of its DBMS reside remotely, \"in the cloud\", while its applications are both developed by programmers and later maintained and used by end-users through a web browser and Open APIs.\\nData warehouses archive data from operational databases and often from external sources such as market research firms. The warehouse becomes the central source of data for use by managers and other end-users who may not have access to operational data. For example, sales data might be aggregated to weekly totals and converted from internal product codes to use UPCs so that they can be compared with ACNielsen data. Some basic and essential components of data warehousing include extracting, analyzing, and mining data, transforming, loading, and managing data so as to make them available for further use.\\nA deductive database combines logic programming with a relational database.\\nA distributed database is one in which both the data and the DBMS span multiple computers.\\nA document-oriented database is designed for storing, retrieving, and managing document-oriented, or semi structured, information. Document-oriented databases are one of the main categories of NoSQL databases.\\nAn embedded database system is a DBMS which is tightly integrated with an application software that requires access to stored data in such a way that the DBMS is hidden from the application\\'s end-users and requires little or no ongoing maintenance.\\nEnd-user databases consist of data developed by individual end-users. Examples of these are collections of documents, spreadsheets, presentations, multimedia, and other files. Several products exist to support such databases. Some of them are much simpler than full-fledged DBMSs, with more elementary DBMS functionality.\\nA federated database system comprises several distinct databases, each with its own DBMS. It is handled as a single database by a federated database management system (FDBMS), which transparently integrates multiple autonomous DBMSs, possibly of different types (in which case it would also be a heterogeneous database system), and provides them with an integrated conceptual view.\\nSometimes the term multi-database is used as a synonym to federated database, though it may refer to a less integrated (e.g., without an FDBMS and a managed integrated schema) group of databases that cooperate in a single application. In this case, typically middleware is used for distribution, which typically includes an atomic commit protocol (ACP), e.g., the two-phase commit protocol, to allow distributed (global) transactions across the participating databases.\\nA graph database is a kind of NoSQL database that uses graph structures with nodes, edges, and properties to represent and store information. General graph databases that can store any graph are distinct from specialized graph databases such as triplestores and network databases.\\nAn array DBMS is a kind of NoSQL DBMS that allows modeling, storage, and retrieval of (usually large) multi-dimensional arrays such as satellite images and climate simulation output.\\nIn a hypertext or hypermedia database, any word or a piece of text representing an object, e.g., another piece of text, an article, a picture, or a film, can be hyperlinked to that object. Hypertext databases are particularly useful for organizing large amounts of disparate information. For example, they are useful for organizing online encyclopedias, where users can conveniently jump around the text. The World Wide Web is thus a large distributed hypertext database.\\nA knowledge base (abbreviated KB, kb or Δ) is a special kind of database for knowledge management, providing the means for the computerized collection, organization, and retrieval of knowledge. Also a collection of data representing problems with their solutions and related experiences.A mobile database can be carried on or synchronized from a mobile computing device.\\nOperational databases store detailed data about the operations of an organization. They typically process relatively high volumes of updates using transactions. Examples include customer databases that record contact, credit, and demographic information about a business\\'s customers, personnel databases that hold information such as salary, benefits, skills data about employees, enterprise resource planning systems that record details about product components, parts inventory, and financial databases that keep track of the organization\\'s money, accounting and financial dealings.\\nA parallel database seeks to improve performance through parallelization for tasks such as loading data, building indexes and evaluating queries.The major parallel DBMS architectures which are induced by the underlying hardware architecture are:\\nShared memory architecture, where multiple processors share the main memory space, as well as other data storage.\\nShared disk architecture, where each processing unit (typically consisting of multiple processors) has its own main memory, but all units share the other storage.\\nShared-nothing architecture, where each processing unit has its own main memory and other storage.Probabilistic databases employ fuzzy logic to draw inferences from imprecise data.\\nReal-time databases process transactions fast enough for the result to come back and be acted on right away.\\nA spatial database can store the data with multidimensional features. The queries on such data include location-based queries, like \"Where is the closest hotel in my area?\".\\nA temporal database has built-in time aspects, for example a temporal data model and a temporal version of SQL. More specifically the temporal aspects usually include valid-time and transaction-time.\\nA terminology-oriented database builds upon an object-oriented database, often customized for a specific field.\\nAn unstructured data database is intended to store in a manageable and protected way diverse objects that do not fit naturally and conveniently in common databases. It may include email messages, documents, journals, multimedia objects, etc. The name may be misleading since some objects can be highly structured. However, the entire possible object collection does not fit into a predefined structured framework. Most established DBMSs now support unstructured data in various ways, and new dedicated DBMSs are emerging.\\n\\n\\n== Database management system ==\\nConnolly and Begg define database management system (DBMS) as a \"software system that enables users to define, create, maintain and control access to the database\". Examples of DBMS\\'s include MySQL, PostgreSQL, Microsoft SQL Server, Oracle Database, and Microsoft Access.\\nThe DBMS acronym is sometimes extended to indicate the underlying database model, with RDBMS for the relational, OODBMS for the object (oriented) and ORDBMS for the object–relational model. Other extensions can indicate some other characteristic, such as DDBMS for a distributed database management systems.\\nThe functionality provided by a DBMS can vary enormously. The core functionality is the storage, retrieval and update of data. Codd proposed the following functions and services a fully-fledged general purpose DBMS should provide:\\nData storage, retrieval and update\\nUser accessible catalog or data dictionary describing the metadata\\nSupport for transactions and concurrency\\nFacilities for recovering the database should it become damaged\\nSupport for authorization of access and update of data\\nAccess support from remote locations\\nEnforcing constraints to ensure data in the database abides by certain rulesIt is also generally to be expected the DBMS will provide a set of utilities for such purposes as may be necessary to administer the database effectively, including import, export, monitoring, defragmentation and analysis utilities. The core part of the DBMS interacting between the database and the application interface sometimes referred to as the database engine.\\nOften DBMSs will have configuration parameters that can be statically and dynamically tuned, for example the maximum amount of main memory on a server the database can use. The trend is to minimize the amount of manual configuration, and for cases such as embedded databases the need to target zero-administration is paramount.\\nThe large major enterprise DBMSs have tended to increase in size and functionality and can have involved thousands of human years of development effort through their lifetime.Early multi-user DBMS typically only allowed for the application to reside on the same computer with access via terminals or terminal emulation software. The client–server architecture was a development where the application resided on a client desktop and the database on a server allowing the processing to be distributed. This evolved into a multitier architecture incorporating application servers and web servers with the end user interface via a web browser with the database only directly connected to the adjacent tier.A general-purpose DBMS will provide public application programming interfaces (API) and optionally a processor for database languages such as SQL to allow applications to be written to interact with the database. A special purpose DBMS may use a private API and be specifically customized and linked to a single application. For example, an email system performing many of the functions of a general-purpose DBMS such as message insertion, message deletion, attachment handling, blocklist lookup, associating messages an email address and so forth however these functions are limited to what is required to handle email.\\n\\n\\n== Application ==\\n\\nExternal interaction with the database will be via an application program that interfaces with the DBMS. This can range from a database tool that allows users to execute SQL queries textually or graphically, to a web site that happens to use a database to store and search information.\\n\\n\\n=== Application program interface ===\\nA programmer will code interactions to the database (sometimes referred to as a datasource) via an application program interface (API) or via a database language. The particular API or language chosen will need to be supported by DBMS, possible indirectly via a preprocessor or a bridging API. Some API\\'s aim to be database independent, ODBC being a commonly known example. Other common API\\'s include JDBC and ADO.NET.\\n\\n\\n== Database languages ==\\nDatabase languages are special-purpose languages, which allow one or more of the following tasks, sometimes distinguished as sublanguages:\\n\\nData control language (DCL) – controls access to data;\\nData definition language (DDL) – defines data types such as creating, altering, or dropping tables and the relationships among them;\\nData manipulation language (DML) – performs tasks such as inserting, updating, or deleting data occurrences;\\nData query language (DQL) – allows searching for information and computing derived information.Database languages are specific to a particular data model. Notable examples include:\\n\\nSQL combines the roles of data definition, data manipulation, and query in a single language. It was one of the first commercial languages for the relational model, although it departs in some respects from the relational model as described by Codd (for example, the rows and columns of a table can be ordered). SQL became a standard of the American National Standards Institute (ANSI) in 1986, and of the International Organization for Standardization (ISO) in 1987. The standards have been regularly enhanced since and is supported (with varying degrees of conformance) by all mainstream commercial relational DBMSs.\\nOQL is an object model language standard (from the Object Data Management Group). It has influenced the design of some of the newer query languages like JDOQL and EJB QL.\\nXQuery is a standard XML query language implemented by XML database systems such as MarkLogic and eXist, by relational databases with XML capability such as Oracle and DB2, and also by in-memory XML processors such as Saxon.\\nSQL/XML combines XQuery with SQL.A database language may also incorporate features like:\\n\\nDBMS-specific configuration and storage engine management\\nComputations to modify query results, like counting, summing, averaging, sorting, grouping, and cross-referencing\\nConstraint enforcement (e.g. in an automotive database, only allowing one engine type per car)\\nApplication programming interface version of the query language, for programmer convenience\\n\\n\\n== Storage ==\\n\\nDatabase storage is the container of the physical materialization of a database. It comprises the internal (physical) level in the database architecture. It also contains all the information needed (e.g., metadata, \"data about the data\", and internal data structures) to reconstruct the conceptual level and external level from the internal level when needed. Putting data into permanent storage is generally the responsibility of the database engine a.k.a. \"storage engine\". Though typically accessed by a DBMS through the underlying operating system (and often using the operating systems\\' file systems as intermediates for storage layout), storage properties and configuration setting are extremely important for the efficient operation of the DBMS, and thus are closely maintained by database administrators. A DBMS, while in operation, always has its database residing in several types of storage (e.g., memory and external storage). The database data and the additional needed information, possibly in very large amounts, are coded into bits. Data typically reside in the storage in structures that look completely different from the way the data look in the conceptual and external levels, but in ways that attempt to optimize (the best possible) these levels\\' reconstruction when needed by users and programs, as well as for computing additional types of needed information from the data (e.g., when querying the database).\\nSome DBMSs support specifying which character encoding was used to store data, so multiple encodings can be used in the same database.\\nVarious low-level database storage structures are used by the storage engine to serialize the data model so it can be written to the medium of choice. Techniques such as indexing may be used to improve performance. Conventional storage is row-oriented, but there are also column-oriented and correlation databases.\\n\\n\\n=== Materialized views ===\\n\\nOften storage redundancy is employed to increase performance. A common example is storing materialized views, which consist of frequently needed external views or query results. Storing such views saves the expensive computing of them each time they are needed. The downsides of materialized views are the overhead incurred when updating them to keep them synchronized with their original updated database data, and the cost of storage redundancy.\\n\\n\\n=== Replication ===\\n\\nOccasionally a database employs storage redundancy by database objects replication (with one or more copies) to increase data availability (both to improve performance of simultaneous multiple end-user accesses to a same database object, and to provide resiliency in a case of partial failure of a distributed database). Updates of a replicated object need to be synchronized across the object copies. In many cases, the entire database is replicated.\\n\\n\\n== Security ==\\n\\nDatabase security deals with all various aspects of protecting the database content, its owners, and its users. It ranges from protection from intentional unauthorized database uses to unintentional database accesses by unauthorized entities (e.g., a person or a computer program).\\nDatabase access control deals with controlling who (a person or a certain computer program) is allowed to access what information in the database. The information may comprise specific database objects (e.g., record types, specific records, data structures), certain computations over certain objects (e.g., query types, or specific queries), or using specific access paths to the former (e.g., using specific indexes or other data structures to access information). Database access controls are set by special authorized (by the database owner) personnel that uses dedicated protected security DBMS interfaces.\\nThis may be managed directly on an individual basis, or by the assignment of individuals and privileges to groups, or (in the most elaborate models) through the assignment of individuals and groups to roles which are then granted entitlements. Data security prevents unauthorized users from viewing or updating the database. Using passwords, users are allowed access to the entire database or subsets of it called \"subschemas\". For example, an employee database can contain all the data about an individual employee, but one group of users may be authorized to view only payroll data, while others are allowed access to only work history and medical data. If the DBMS provides a way to interactively enter and update the database, as well as interrogate it, this capability allows for managing personal databases.\\nData security in general deals with protecting specific chunks of data, both physically (i.e., from corruption, or destruction, or removal; e.g., see physical security), or the interpretation of them, or parts of them to meaningful information (e.g., by looking at the strings of bits that they comprise, concluding specific valid credit-card numbers; e.g., see data encryption).\\nChange and access logging records who accessed which attributes, what was changed, and when it was changed. Logging services allow for a forensic database audit later by keeping a record of access occurrences and changes. Sometimes application-level code is used to record changes rather than leaving this to the database. Monitoring can be set up to attempt to detect security breaches.\\n\\n\\n== Transactions and concurrency ==\\n\\nDatabase transactions can be used to introduce some level of fault tolerance and data integrity after recovery from a crash. A database transaction is a unit of work, typically encapsulating a number of operations over a database (e.g., reading a database object, writing, acquiring or releasing a lock, etc.), an abstraction supported in database and also other systems. Each transaction has well defined boundaries in terms of which program/code executions are included in that transaction (determined by the transaction\\'s programmer via special transaction commands).\\nThe acronym ACID describes some ideal properties of a database transaction: atomicity, consistency, isolation, and durability.\\n\\n\\n== Migration ==\\n\\nA database built with one DBMS is not portable to another DBMS (i.e., the other DBMS cannot run it). However, in some situations, it is desirable to migrate a database from one DBMS to another. The reasons are primarily economical (different DBMSs may have different total costs of ownership or TCOs), functional, and operational (different DBMSs may have different capabilities). The migration involves the database\\'s transformation from one DBMS type to another. The transformation should maintain (if possible) the database related application (i.e., all related application programs) intact. Thus, the database\\'s conceptual and external architectural levels should be maintained in the transformation. It may be desired that also some aspects of the architecture internal level are maintained. A complex or large database migration may be a complicated and costly (one-time) project by itself, which should be factored into the decision to migrate. This in spite of the fact that tools may exist to help migration between specific DBMSs. Typically, a DBMS vendor provides tools to help importing databases from other popular DBMSs.\\n\\n\\n== Building, maintaining, and tuning ==\\n\\nAfter designing a database for an application, the next stage is building the database. Typically, an appropriate general-purpose DBMS can be selected to be used for this purpose. A DBMS provides the needed user interfaces to be used by database administrators to define the needed application\\'s data structures within the DBMS\\'s respective data model. Other user interfaces are used to select needed DBMS parameters (like security related, storage allocation parameters, etc.).\\nWhen the database is ready (all its data structures and other needed components are defined), it is typically populated with initial application\\'s data (database initialization, which is typically a distinct project; in many cases using specialized DBMS interfaces that support bulk insertion) before making it operational. In some cases, the database becomes operational while empty of application data, and data are accumulated during its operation.\\nAfter the database is created, initialized and populated it needs to be maintained. Various database parameters may need changing and the database may need to be tuned (tuning) for better performance; application\\'s data structures may be changed or added, new related application programs may be written to add to the application\\'s functionality, etc.\\n\\n\\n== Backup and restore ==\\n\\nSometimes it is desired to bring a database back to a previous state (for many reasons, e.g., cases when the database is found corrupted due to a software error, or if it has been updated with erroneous data). To achieve this, a backup operation is done occasionally or continuously, where each desired database state (i.e., the values of its data and their embedding in database\\'s data structures) is kept within dedicated backup files (many techniques exist to do this effectively). When it is decided by a database administrator to bring the database back to this state (e.g., by specifying this state by a desired point in time when the database was in this state), these files are used to restore that state.\\n\\n\\n== Static analysis ==\\nStatic analysis techniques for software verification can be applied also in the scenario of query languages. In particular, the *Abstract interpretation framework has been extended to the field of query languages for relational databases as a way to support sound approximation techniques. The semantics of query languages can be tuned according to suitable abstractions of the concrete domain of data. The abstraction of relational database system has many interesting applications, in particular, for security purposes, such as fine grained access control, watermarking, etc.\\n\\n\\n== Miscellaneous features ==\\nOther DBMS features might include:\\n\\nDatabase logs – This helps in keeping a history of the executed functions.\\nGraphics component for producing graphs and charts, especially in a data warehouse system.\\nQuery optimizer – Performs query optimization on every query to choose an efficient query plan (a partial order (tree) of operations) to be executed to compute the query result. May be specific to a particular storage engine.\\nTools or hooks for database design, application programming, application program maintenance, database performance analysis and monitoring, database configuration monitoring, DBMS hardware configuration (a DBMS and related database may span computers, networks, and storage units) and related database mapping (especially for a distributed DBMS), storage allocation and database layout monitoring, storage migration, etc.Increasingly, there are calls for a single system that incorporates all of these core functionalities into the same build, test, and deployment framework for database management and source control. Borrowing from other developments in the software industry, some market such offerings as \"DevOps for database\".\\n\\n\\n== Design and modeling ==\\n\\nThe first task of a database designer is to produce a conceptual data model that reflects the structure of the information to be held in the database. A common approach to this is to develop an entity–relationship model, often with the aid of drawing tools. Another popular approach is the Unified Modeling Language. A successful data model will accurately reflect the possible state of the external world being modeled: for example, if people can have more than one phone number, it will allow this information to be captured. Designing a good conceptual data model requires a good understanding of the application domain; it typically involves asking deep questions about the things of interest to an organization, like \"can a customer also be a supplier?\", or \"if a product is sold with two different forms of packaging, are those the same product or different products?\", or \"if a plane flies from New York to Dubai via Frankfurt, is that one flight or two (or maybe even three)?\". The answers to these questions establish definitions of the terminology used for entities (customers, products, flights, flight segments) and their relationships and attributes.\\nProducing the conceptual data model sometimes involves input from business processes, or the analysis of workflow in the organization. This can help to establish what information is needed in the database, and what can be left out. For example, it can help when deciding whether the database needs to hold historic data as well as current data.\\nHaving produced a conceptual data model that users are happy with, the next stage is to translate this into a schema that implements the relevant data structures within the database. This process is often called logical database design, and the output is a logical data model expressed in the form of a schema. Whereas the conceptual data model is (in theory at least) independent of the choice of database technology, the logical data model will be expressed in terms of a particular database model supported by the chosen DBMS. (The terms data model and database model are often used interchangeably, but in this article we use data model for the design of a specific database, and database model for the modeling notation used to express that design).\\nThe most popular database model for general-purpose databases is the relational model, or more precisely, the relational model as represented by the SQL language. The process of creating a logical database design using this model uses a methodical approach known as normalization. The goal of normalization is to ensure that each elementary \"fact\" is only recorded in one place, so that insertions, updates, and deletions automatically maintain consistency.\\nThe final stage of database design is to make the decisions that affect performance, scalability, recovery, security, and the like, which depend on the particular DBMS. This is often called physical database design, and the output is the physical data model. A key goal during this stage is data independence, meaning that the decisions made for performance optimization purposes should be invisible to end-users and applications. There are two types of data independence: Physical data independence and logical data independence. Physical design is driven mainly by performance requirements, and requires a good knowledge of the expected workload and access patterns, and a deep understanding of the features offered by the chosen DBMS.\\nAnother aspect of physical database design is security. It involves both defining access control to database objects as well as defining security levels and methods for the data itself.\\n\\n\\n=== Models ===\\n\\nA database model is a type of data model that determines the logical structure of a database and fundamentally determines in which manner data can be stored, organized, and manipulated. The most popular example of a database model is the relational model (or the SQL approximation of relational), which uses a table-based format.\\nCommon logical data models for databases include:\\n\\nNavigational databases\\nHierarchical database model\\nNetwork model\\nGraph database\\nRelational model\\nEntity–relationship model\\nEnhanced entity–relationship model\\nObject model\\nDocument model\\nEntity–attribute–value model\\nStar schemaAn object–relational database combines the two related structures.\\nPhysical data models include:\\n\\nInverted index\\nFlat fileOther models include:\\n\\nMultidimensional model\\nArray model\\nMultivalue modelSpecialized models are optimized for particular types of data:\\n\\nXML database\\nSemantic model\\nContent store\\nEvent store\\nTime series model\\n\\n\\n=== External, conceptual, and internal views ===\\n\\nA database management system provides three views of the database data:\\n\\nThe external level defines how each group of end-users sees the organization of data in the database. A single database can have any number of views at the external level.\\nThe conceptual level unifies the various external views into a compatible global view. It provides the synthesis of all the external views. It is out of the scope of the various database end-users, and is rather of interest to database application developers and database administrators.\\nThe internal level (or physical level) is the internal organization of data inside a DBMS. It is concerned with cost, performance, scalability and other operational matters. It deals with storage layout of the data, using storage structures such as indexes to enhance performance. Occasionally it stores data of individual views (materialized views), computed from generic data, if performance justification exists for such redundancy. It balances all the external views\\' performance requirements, possibly conflicting, in an attempt to optimize overall performance across all activities.While there is typically only one conceptual (or logical) and physical (or internal) view of the data, there can be any number of different external views. This allows users to see database information in a more business-related way rather than from a technical, processing viewpoint. For example, a financial department of a company needs the payment details of all employees as part of the company\\'s expenses, but does not need details about employees that are the interest of the human resources department. Thus different departments need different views of the company\\'s database.\\nThe three-level database architecture relates to the concept of data independence which was one of the major initial driving forces of the relational model. The idea is that changes made at a certain level do not affect the view at a higher level. For example, changes in the internal level do not affect application programs written using conceptual level interfaces, which reduces the impact of making physical changes to improve performance.\\nThe conceptual view provides a level of indirection between internal and external. On one hand it provides a common view of the database, independent of different external view structures, and on the other hand it abstracts away details of how the data are stored or managed (internal level). In principle every level, and even every external view, can be presented by a different data model. In practice usually a given DBMS uses the same data model for both the external and the conceptual levels (e.g., relational model). The internal level, which is hidden inside the DBMS and depends on its implementation, requires a different level of detail and uses its own types of data structure types.\\nSeparating the external, conceptual and internal levels was a major feature of the relational database model implementations that dominate 21st century databases.\\n\\n\\n== Research ==\\nDatabase technology has been an active research topic since the 1960s, both in academia and in the research and development groups of companies (for example IBM Research). Research activity includes theory and development of prototypes. Notable research topics have included models, the atomic transaction concept, and related concurrency control techniques, query languages and query optimization methods, RAID, and more.\\nThe database research area has several dedicated academic journals (for example, ACM Transactions on Database Systems-TODS, Data and Knowledge Engineering-DKE) and annual conferences (e.g., ACM SIGMOD, ACM PODS, VLDB, IEEE ICDE).\\n\\n\\n== See also ==\\n\\n\\n== Notes ==\\n\\n\\n== References ==\\n\\n\\n== Sources ==\\n\\n\\n== Further reading ==\\nLing Liu and Tamer M. Özsu (Eds.) (2009).  \"Encyclopedia of Database Systems, 4100 p. 60 illus. ISBN 978-0-387-49616-0.\\nGray, J. and Reuter, A. Transaction Processing: Concepts and Techniques, 1st edition,  Morgan Kaufmann Publishers, 1992.\\nKroenke, David M. and David J. Auer. Database Concepts. 3rd ed. New York: Prentice, 2007.\\nRaghu Ramakrishnan and Johannes Gehrke, Database Management Systems\\nAbraham Silberschatz, Henry F. Korth, S. Sudarshan, Database System Concepts\\nLightstone, S.; Teorey, T.; Nadeau, T. (2007). Physical Database Design: the database professional\\'s guide to exploiting indexes, views, storage, and more. Morgan Kaufmann Press. ISBN 978-0-12-369389-1.\\nTeorey, T.; Lightstone, S. and Nadeau, T. Database Modeling & Design: Logical Design, 4th edition, Morgan Kaufmann Press, 2005. ISBN 0-12-685352-5\\n\\n\\n== External links ==\\n\\nDB File extension – information about files with the DB extension', 'In computer science, imperative programming is a  programming paradigm that uses statements that change a program\\'s state. In much the same way that the imperative mood in natural languages expresses commands, an imperative program consists of commands for the computer to perform. Imperative programming focuses on describing how a program operates.\\nThe term is often used in contrast to declarative programming, which focuses on what the program should accomplish without specifying how the program should achieve the result.\\n\\n\\n== Imperative and procedural programming ==\\nProcedural programming is a type of imperative programming in which the program is built from one or more procedures (also termed subroutines or functions). The terms are often used as synonyms, but the use of procedures has a dramatic effect on how imperative programs appear and how they are constructed. Heavily procedural programming, in which state changes are localized to procedures or restricted to explicit arguments and returns from procedures, is a form of structured programming. From the 1960s onwards, structured programming and modular programming in general have been promoted as techniques to improve the maintainability and overall quality of imperative programs. The concepts behind object-oriented programming attempt to extend this approach.\\nProcedural programming could be considered a step toward declarative programming. A programmer can often tell, simply by looking at the names, arguments, and return types of procedures (and related comments), what a particular procedure is supposed to do, without necessarily looking at the details of how it achieves its result. At the same time, a complete program is still imperative since it fixes the statements to be executed and their order of execution to a large extent.\\n\\n\\n== Rationale and foundations of imperative programming ==\\nThe hardware implementation of almost all computers is imperative. Nearly all computer hardware is designed to execute machine code, which is native to the computer and is written in the imperative style. From this low-level perspective, the program state is defined by the contents of memory, and the statements are instructions in the native machine language of the computer. Higher-level imperative languages use variables and more complex statements, but still follow the same paradigm. Recipes and process checklists, while not computer programs, are also familiar concepts that are similar in style to imperative programming; each step is an instruction, and the physical world holds the state. Since the basic ideas of imperative programming are both conceptually familiar and directly embodied in the hardware, most computer languages are in the imperative style.\\nAssignment statements, in imperative paradigm, perform an operation on information located in memory and store the results in memory for later use. High-level imperative languages, in addition, permit the evaluation of complex expressions, which may consist of a combination of arithmetic operations and function evaluations, and the assignment of the resulting value to memory. Looping statements (as in while loops, do while loops, and for loops) allow a sequence of statements to be executed multiple times. Loops can either execute the statements they contain a predefined number of times, or they can execute them repeatedly until some condition changes. Conditional branching statements allow a sequence of statements to be executed only if some condition is met. Otherwise, the statements are skipped and the execution sequence continues from the statement following them. Unconditional branching statements allow an execution sequence to be transferred to another part of a program. These include the jump (called goto in many languages), switch, and the subprogram, subroutine, or procedure call (which usually returns to the next statement after the call).\\nEarly in the development of high-level programming languages, the introduction of the block enabled the construction of programs in which a group of statements and declarations could be treated as if they were one statement. This, alongside the introduction of subroutines, enabled complex structures to be expressed by hierarchical decomposition into simpler procedural structures.\\nMany imperative programming languages (such as Fortran, BASIC, and C) are abstractions of assembly language.\\n\\n\\n== History of imperative and object-oriented languages ==\\nThe earliest imperative languages were the machine languages of the original computers. In these languages, instructions were very simple, which made hardware implementation easier but hindered the creation of complex programs. FORTRAN, developed by John Backus at International Business Machines (IBM) starting in 1954, was the first major programming language to remove the obstacles presented by machine code in the creation of complex programs. FORTRAN was a compiled language that allowed named variables, complex expressions, subprograms, and many other features now common in imperative languages. The next two decades saw the development of many other major high-level imperative programming languages. In the late 1950s and 1960s, ALGOL was developed in order to allow mathematical algorithms to be more easily expressed and even served as the operating system\\'s target language for some computers. MUMPS (1966) carried the imperative paradigm to a logical extreme, by not having any statements at all, relying purely on commands, even to the extent of making the IF and ELSE commands independent of each other, connected only by an intrinsic variable named $TEST. COBOL (1960) and BASIC (1964) were both attempts to make programming syntax look more like English. In the 1970s, Pascal was developed by Niklaus Wirth, and C was created by Dennis Ritchie while he was working at Bell Laboratories. Wirth went on to design Modula-2 and Oberon. For the needs of the United States Department of Defense, Jean Ichbiah and a team at Honeywell began designing Ada in 1978, after a 4-year project to define the requirements for the language. The specification was first published in 1983, with revisions in 1995, 2005, and 2012.\\nThe 1980s saw a rapid growth in interest in object-oriented programming. These languages were imperative in style, but added features to support objects. The last two decades of the 20th century saw the development of many such languages. Smalltalk-80, originally conceived by Alan Kay in 1969, was released in 1980, by the Xerox Palo Alto Research Center (PARC). Drawing from concepts in another object-oriented language—Simula (which is considered the world\\'s first object-oriented programming language, developed in the 1960s)—Bjarne Stroustrup designed C++, an object-oriented language based on C. Design of C++ began in 1979 and the first implementation was completed in 1983. In the late 1980s and 1990s, the notable imperative languages drawing on object-oriented concepts were Perl, released by Larry Wall in 1987; Python, released by Guido van Rossum in 1990; Visual Basic and Visual C++ (which included Microsoft Foundation Class Library (MFC) 2.0), released by Microsoft in 1991 and 1993 respectively; PHP, released by Rasmus Lerdorf in 1994; Java, by James Gosling (Sun Microsystems) in 1995, JavaScript, by Brendan Eich (Netscape), and Ruby, by Yukihiro \"Matz\" Matsumoto, both released in 1995. Microsoft\\'s .NET Framework (2002) is imperative at its core, as are its main target languages, VB.NET and C# that run on it; however Microsoft\\'s F#, a functional language, also runs on it.\\n\\n\\n== See also ==\\nFunctional programming\\nComparison of programming paradigms\\nReactive programming\\nHistory of programming languages\\nList of imperative programming languages\\n\\n\\n== Notes ==\\n\\n\\n== References ==\\n\\nPratt, Terrence W. and Marvin V. Zelkowitz. Programming Languages: Design and Implementation, 3rd ed. Englewood Cliffs, N.J.: Prentice Hall, 1996.\\nSebesta, Robert W. Concepts of Programming Languages, 3rd ed. Reading, Mass.: Addison-Wesley Publishing Company, 1996.Originally based on the article \\'Imperative programming\\' by Stan Seibert, from Nupedia, licensed under the GNU Free Documentation License.', 'Procedural programming is a programming paradigm, derived from imperative programming, based on the concept of the procedure call.  Procedures (a type of routine or subroutine) simply contain a series of computational steps to be carried out.  Any given procedure might be called at any point during a program\\'s execution, including by other procedures or itself. The first major procedural programming languages appeared circa 1957–1964, including Fortran, ALGOL, COBOL, PL/I and BASIC.  Pascal and C were published circa 1970–1972.\\nComputer processors provide hardware support for procedural programming through a stack register and instructions for calling procedures and returning from them. Hardware support for other types of programming is possible, but no attempt was commercially successful (for example Lisp machines or Java processors).\\n\\n\\n== Procedures and modularity ==\\n\\nModularity is generally desirable, especially in large, complicated programs. Inputs are usually specified syntactically in the form of arguments and the outputs delivered as return values.\\nScoping is another technique that helps keep procedures modular.  It prevents the procedure from accessing the variables of other procedures (and vice versa), including previous instances of itself, without explicit authorization.\\nLess modular procedures, often used in small or quickly written programs, tend to interact with a large number of variables in the execution environment, which other procedures might also modify.\\nBecause of the ability to specify a simple interface, to be self-contained, and to be reused, procedures are a convenient vehicle for making pieces of code written by different people or different groups, including through programming libraries.\\n\\n\\n== Comparison with other programming paradigms ==\\n\\n\\n=== Imperative programming ===\\nProcedural programming languages are also imperative languages, because they make explicit references to the state of the execution environment. This could be anything from variables (which may correspond to processor registers) to something like the position of the \"turtle\" in the Logo programming language.\\nOften, the terms \"procedural programming\" and \"imperative programming\" are used synonymously. However, procedural programming relies heavily on blocks and scope, whereas imperative programming as a whole may or may not have such features. As such, procedural languages generally use reserved words that act on blocks, such as if, while, and for, to implement control flow, whereas non-structured imperative languages use goto statements and branch tables for the same purpose.\\n\\n\\n=== Object-oriented programming ===\\nThe focus of procedural programming is to break down a programming task into a collection of variables, data structures, and subroutines, whereas in object-oriented programming it is to break down a programming task into objects that expose behavior (methods) and data (members or attributes) using interfaces. The most important distinction is that while procedural programming uses procedures to operate on data structures, object-oriented programming bundles the two together, so an \"object\", which is an instance of a class, operates on its \"own\" data structure.Nomenclature varies between the two, although they have similar semantics:\\n\\n\\n=== Functional programming ===\\nThe principles of modularity and code reuse in practical functional languages are fundamentally the same as in procedural languages, since they both stem from structured programming. So for example:\\n\\nProcedures correspond to functions.  Both allow the reuse of the same code in various parts of the programs, and at various points of its execution.\\nBy the same token, procedure calls correspond to function application.\\nFunctions and their modularly separated from each other in the same manner, by the use of function arguments, return values and variable scopes.The main difference between the styles is that functional programming languages remove or at least deemphasize the imperative elements of procedural programming.  The feature set of functional languages is therefore designed to support writing programs as much as possible in terms of pure functions:\\n\\nWhereas procedural languages model execution of the program as a sequence of imperative commands that may implicitly alter shared state, functional programming languages model execution as the evaluation of complex expressions that only depend on each other in terms of arguments and return values.  For this reason, functional programs can have a free order of code execution, and the languages may offer little control over the order in which various parts of the program are executed.  (For example, the arguments to a procedure invocation in Scheme are executed in an arbitrary order.)\\nFunctional programming languages support (and heavily use) first-class functions, anonymous functions and closures, although these concepts are being included in newer procedural languages.\\nFunctional programming languages tend to rely on tail call optimization and higher-order functions instead of imperative looping constructs.Many functional languages, however, are in fact impurely functional and offer imperative/procedural constructs that allow the programmer to write programs in procedural style, or in a combination of both styles.  It is common for input/output code in functional languages to be written in a procedural style.\\nThere do exist a few esoteric functional languages (like Unlambda) that eschew structured programming precepts for the sake of being difficult to program in (and therefore challenging).  These languages are the exception to the common ground between procedural and functional languages.\\n\\n\\n=== Logic programming ===\\nIn logic programming, a program is a set of premises, and computation is performed by attempting to prove candidate theorems. From this point of view, logic programs are declarative, focusing on what the problem is, rather than on how to solve it.\\nHowever, the backward reasoning technique, implemented by SLD resolution, used to solve problems in logic programming languages such as Prolog, treats programs as goal-reduction procedures. Thus  clauses of the form:\\n\\nH :- B1, …, Bn.have a dual interpretation, both as procedures\\n\\nto show/solve H, show/solve B1 and … and Bnand as logical implications:\\n\\nB1 and … and Bn implies H.Experienced logic programmers use the procedural interpretation to write programs that are effective and efficient, and they use the declarative interpretation to help ensure that programs are correct.\\n\\n\\n== See also ==\\nComparison of programming paradigms\\nDeclarative programming\\nFunctional programming (contrast)\\nImperative programming\\nLogic programming\\nObject-oriented programming\\nProgramming paradigms\\nProgramming language\\nStructured programming\\nSQL procedural extensions\\n\\n\\n== References ==\\n\\n\\n== External links ==\\nProcedural Languages at Curlie', 'In computer science, functional programming is a programming paradigm where programs are constructed by applying and composing functions. It is a declarative programming paradigm in which function definitions are trees of expressions that map values to other values, rather than a sequence of imperative statements which update the running state of the program.\\nIn functional programming, functions are treated as first-class citizens, meaning that they can be bound to names (including local identifiers), passed as arguments, and returned from other functions, just as any other data type can. This allows programs to be written in a declarative and composable style, where small functions are combined in a modular manner.\\nFunctional programming is sometimes treated as synonymous with purely functional programming, a subset of functional programming which treats all functions as deterministic mathematical functions, or pure functions. When a pure function is called with some given arguments, it will always return the same result, and cannot be affected by any mutable state or other side effects. This is in contrast with impure procedures, common in imperative programming, which can have side effects (such as modifying the program\\'s state or taking input from a user). Proponents of purely functional programming claim that by restricting side effects, programs can have fewer bugs, be easier to debug and test, and be more suited to formal verification.Functional programming has its roots in academia, evolving from the lambda calculus, a formal system of computation based only on functions. Functional programming has historically been less popular than imperative programming, but many functional languages are seeing use today in industry and education, including Common Lisp, Scheme, Clojure, Wolfram Language, Racket, Erlang,  Elixir, OCaml, Haskell, and F#. Functional programming is also key to some languages that have found success in specific domains, like JavaScript in the Web, R in statistics, J, K and Q in financial analysis, and XQuery/XSLT for XML. Domain-specific declarative languages like SQL and Lex/Yacc use some elements of functional programming, such as not allowing mutable values. In addition, many other programming languages support programming in a functional style or have implemented features from functional programming, such as C++11, C#, Kotlin, Perl, PHP, Python, Go, Rust, Raku, Scala, and Java (since Java 8).\\n\\n\\n== History ==\\nThe lambda calculus, developed in the 1930s by Alonzo Church, is a formal system of computation built from function application. In 1937 Alan Turing proved that the lambda calculus and Turing machines are equivalent models of computation, showing that the lambda calculus is Turing complete. Lambda calculus forms the basis of all functional programming languages. An equivalent theoretical formulation, combinatory logic, was developed by Moses Schönfinkel and Haskell Curry in the 1920s and 1930s.Church later developed a weaker system, the simply-typed lambda calculus, which extended the lambda calculus by assigning a type to all terms. This forms the basis for statically-typed functional programming.\\nThe first functional programming language, LISP, was developed in the late 1950s for the IBM 700/7000 series of scientific computers by John McCarthy while at Massachusetts Institute of Technology (MIT). LISP functions were defined using Church\\'s lambda notation, extended with a label construct to allow recursive functions. Lisp first introduced many paradigmatic features of functional programming, though early Lisps were multi-paradigm languages, and incorporated support for numerous programming styles as new paradigms evolved. Later dialects, such as Scheme and Clojure, and offshoots such as Dylan and Julia, sought to simplify and rationalise Lisp around a cleanly functional core, while Common Lisp was designed to preserve and update the paradigmatic features of the numerous older dialects it replaced.Information Processing Language (IPL), 1956, is sometimes cited as the first computer-based functional programming language. It is an assembly-style language for manipulating lists of symbols. It does have a notion of generator, which amounts to a function that accepts a function as an argument, and, since it is an assembly-level language, code can be data, so IPL can be regarded as having higher-order functions. However, it relies heavily on the mutating list structure and similar imperative features.\\nKenneth E. Iverson developed APL in the early 1960s, described in his 1962 book A Programming Language (ISBN 9780471430148). APL was the primary influence on John Backus\\'s FP. In the early 1990s, Iverson and Roger Hui created J. In the mid-1990s, Arthur Whitney, who had previously worked with Iverson, created K, which is used commercially in financial industries along with its descendant Q.\\nJohn Backus presented FP in his 1977 Turing Award lecture \"Can Programming Be Liberated From the von Neumann Style? A Functional Style and its Algebra of Programs\". He defines functional programs as being built up in a hierarchical way by means of \"combining forms\" that allow an \"algebra of programs\"; in modern language, this means that functional programs follow the principle of compositionality. Backus\\'s paper popularized research into functional programming, though it emphasized function-level programming rather than the lambda-calculus style now associated with functional programming.\\nThe 1973 language ML was created by Robin Milner at the University of Edinburgh, and David Turner developed the language SASL at the University of St Andrews. Also in Edinburgh in the 1970s, Burstall and Darlington developed the functional language NPL. NPL was based on Kleene Recursion Equations and was first introduced in their work on program transformation. Burstall, MacQueen and Sannella then incorporated the polymorphic type checking from ML to produce the language Hope. ML eventually developed into several dialects, the most common of which are now OCaml and Standard ML.\\nIn the 1970s, Guy L. Steele and Gerald Jay Sussman developed Scheme, as described in the Lambda Papers and the 1985 textbook Structure and Interpretation of Computer Programs. Scheme was the first dialect of lisp to use lexical scoping and to require tail-call optimization, features that encourage functional programming.\\nIn the 1980s, Per Martin-Löf developed intuitionistic type theory (also called constructive type theory), which associated functional programs with constructive proofs expressed as dependent types. This led to new approaches to interactive theorem proving and has influenced the development of subsequent functional programming languages.The lazy functional language, Miranda, developed by David Turner, initially appeared in 1985 and had a strong influence on Haskell. With Miranda being proprietary, Haskell began with a consensus in 1987 to form an open standard for functional programming research; implementation releases have been ongoing since 1990.\\nMore recently it has found use in niches such as parametric CAD courtesy of the OpenSCAD language built on the CSG geometry framework, although its restriction on reassigning values (all values are treated as constants) has led to confusion among users who are unfamiliar with functional programming as a concept.Functional programming continues to be used in commercial settings.\\n\\n\\n== Concepts ==\\nA number of concepts and paradigms are specific to functional programming, and generally foreign to imperative programming (including object-oriented programming). However, programming languages often cater to several programming paradigms, so programmers using \"mostly imperative\" languages may have utilized some of these concepts.\\n\\n\\n=== First-class and higher-order functions ===\\n\\nHigher-order functions are functions that can either take other functions as arguments or return them as results. In calculus, an example of a higher-order function is the differential operator \\n  \\n    \\n      \\n        d\\n        \\n          /\\n        \\n        d\\n        x\\n      \\n    \\n    {\\\\displaystyle d/dx}\\n  , which returns the derivative of a function \\n  \\n    \\n      \\n        f\\n      \\n    \\n    {\\\\displaystyle f}\\n  .\\nHigher-order functions are closely related to first-class functions in that higher-order functions and first-class functions both allow functions as arguments and results of other functions. The distinction between the two is subtle: \"higher-order\" describes a mathematical concept of functions that operate on other functions, while \"first-class\" is a computer science term for programming language entities that have no restriction on their use (thus first-class functions can appear anywhere in the program that other first-class entities like numbers can, including as arguments to other functions and as their return values).\\nHigher-order functions enable partial application or currying, a technique that applies a function to its arguments one at a time, with each application returning a new function that accepts the next argument. This lets a programmer succinctly express, for example, the successor function as the addition operator partially applied to the natural number one.\\n\\n\\n=== Pure functions ===\\n\\nPure functions (or expressions) have no side effects (memory or I/O). This means that pure functions have several useful properties, many of which can be used to optimize the code:\\n\\nIf the result of a pure expression is not used, it can be removed without affecting other expressions.\\nIf a pure function is called with arguments that cause no side-effects, the result is constant with respect to that argument list (sometimes called referential transparency or idempotence), i.e., calling the pure function again with the same arguments returns the same result. (This can enable caching optimizations such as memoization.)\\nIf there is no data dependency between two pure expressions, their order can be reversed, or they can be performed in parallel and they cannot interfere with one another (in other terms, the evaluation of any pure expression is thread-safe).\\nIf the entire language does not allow side-effects, then any evaluation strategy can be used; this gives the compiler freedom to reorder or combine the evaluation of expressions in a program (for example, using deforestation).While most compilers for imperative programming languages detect pure functions and perform common-subexpression elimination for pure function calls, they cannot always do this for pre-compiled libraries, which generally do not expose this information, thus preventing optimizations that involve those external functions. Some compilers, such as gcc, add extra keywords for a programmer to explicitly mark external functions as pure, to enable such optimizations. Fortran 95 also lets functions be designated pure. C++11 added constexpr keyword with similar semantics.\\n\\n\\n=== Recursion ===\\n\\nIteration (looping) in functional languages is usually accomplished via recursion. Recursive functions invoke themselves, letting an operation be repeated until it reaches the base case. In general, recursion requires maintaining a stack, which consumes space in a linear amount to the depth of recursion. This could make recursion prohibitively expensive to use instead of imperative loops. However, a special form of recursion known as tail recursion can be recognized and optimized by a compiler into the same code used to implement iteration in imperative languages. Tail recursion optimization can be implemented by transforming the program into continuation passing style during compiling, among other approaches.\\nThe Scheme language standard requires implementations to support proper tail recursion, meaning they must allow an unbounded number of active tail calls. Proper tail recursion is not simply an optimization; it is a language feature that assures users that they can use recursion to express a loop and doing so would be safe-for-space. Moreover, contrary to its name, it accounts for all tail calls, not just tail recursion. While proper tail recursion is usually implemented by turning code into imperative loops, implementations might implement it in other ways. For example, CHICKEN intentionally maintains a stack and lets the stack overflow. However, when this happens, its garbage collector will claim space back, allowing an unbounded number of active tail calls even though it does not turn tail recursion into a loop.\\nCommon patterns of recursion can be abstracted away using higher-order functions, with catamorphisms and anamorphisms (or \"folds\" and \"unfolds\") being the most obvious examples. Such recursion schemes play a role analogous to built-in control structures such as loops in imperative languages.\\nMost general purpose functional programming languages allow unrestricted recursion and are Turing complete, which makes the halting problem undecidable, can cause unsoundness of equational reasoning, and generally requires the introduction of inconsistency into the logic expressed by the language\\'s type system. Some special purpose languages such as Coq allow only well-founded recursion and are strongly normalizing (nonterminating computations can be expressed only with infinite streams of values called codata). As a consequence, these languages fail to be Turing complete and expressing certain functions in them is impossible, but they can still express a wide class of interesting computations while avoiding the problems introduced by unrestricted recursion. Functional programming limited to well-founded recursion with a few other constraints is called total functional programming.\\n\\n\\n=== Strict versus non-strict evaluation ===\\n\\nFunctional languages can be categorized by whether they use strict (eager) or non-strict (lazy) evaluation, concepts that refer to how function arguments are processed when an expression is being evaluated. The technical difference is in the denotational semantics of expressions containing failing or divergent computations. Under strict evaluation, the evaluation of any term containing a failing subterm fails. For example, the expression:\\n\\nprint length([2+1, 3*2, 1/0, 5-4])\\n\\nfails under strict evaluation because of the division by zero in the third element of the list. Under lazy evaluation, the length function returns the value 4 (i.e., the number of items in the list), since evaluating it does not attempt to evaluate the terms making up the list. In brief, strict evaluation always fully evaluates function arguments before invoking the function. Lazy evaluation does not evaluate function arguments unless their values are required to evaluate the function call itself.\\nThe usual implementation strategy for lazy evaluation in functional languages is graph reduction. Lazy evaluation is used by default in several pure functional languages, including Miranda, Clean, and Haskell.\\nHughes 1984 argues for lazy evaluation as a mechanism for improving program modularity through separation of concerns, by easing independent implementation of producers and consumers of data streams. Launchbury 1993  describes some difficulties that lazy evaluation introduces, particularly in analyzing a program\\'s storage requirements, and proposes an operational semantics to aid in such analysis. Harper 2009 proposes including both strict and lazy evaluation in the same language, using the language\\'s type system to distinguish them.\\n\\n\\n=== Type systems ===\\n\\nEspecially since the development of Hindley–Milner type inference in the 1970s, functional programming languages have tended to use typed lambda calculus, rejecting all invalid programs at compilation time and risking false positive errors, as opposed to the untyped lambda calculus, that accepts all valid programs at compilation time and risks false negative errors, used in Lisp and its variants (such as Scheme), though they reject all invalid programs at runtime when the information is enough to not reject valid programs. The use of algebraic datatypes makes manipulation of complex data structures convenient; the presence of strong compile-time type checking makes programs more reliable in absence of other reliability techniques like test-driven development, while type inference frees the programmer from the need to manually declare types to the compiler in most cases.\\nSome research-oriented functional languages such as Coq, Agda, Cayenne, and Epigram are based on intuitionistic type theory, which lets types depend on terms. Such types are called dependent types. These type systems do not have decidable type inference and are difficult to understand and program with. But dependent types can express arbitrary propositions in higher-order logic. Through the Curry–Howard isomorphism, then, well-typed programs in these languages become a means of writing formal mathematical proofs from which a compiler can generate certified code. While these languages are mainly of interest in academic research (including in formalized mathematics), they have begun to be used in engineering as well. Compcert is a compiler for a subset of the C programming language that is written in Coq and formally verified.A limited form of dependent types called generalized algebraic data types (GADT\\'s) can be implemented in a way that provides some of the benefits of dependently typed programming while avoiding most of its inconvenience. GADT\\'s are available in the Glasgow Haskell Compiler, in OCaml and in Scala, and have been proposed as additions to other languages including Java and C#.\\n\\n\\n=== Referential transparency ===\\n\\nFunctional programs do not have assignment statements, that is, the value of a variable in a functional program never changes once defined. This eliminates any chances of side effects because any variable can be replaced with its actual value at any point of execution. So, functional programs are referentially transparent.Consider C assignment statement x = x * 10, this changes the value assigned to the variable x. Let us say that the initial value of x was 1, then two consecutive evaluations of the variable x yields 10 and 100 respectively. Clearly, replacing x = x * 10 with either 10 or 100 gives a program a different meaning, and so the expression is not referentially transparent. In fact, assignment statements are never referentially transparent.\\nNow, consider another function such as int plusone(int x) {return x+1;} is transparent, as it does not implicitly change the input x and thus has no such side effects.\\nFunctional programs exclusively use this type of function and are therefore referentially transparent.\\n\\n\\n=== Data structures ===\\n\\nPurely functional data structures are often represented in a different way than their imperative counterparts. For example, the array with constant access and update times is a basic component of most imperative languages, and many imperative data-structures, such as the hash table and binary heap,  are based on arrays. Arrays can be replaced by maps or random access lists, which admit purely functional implementation, but have logarithmic access and update times. Purely functional data structures have persistence, a property of keeping previous versions of the data structure unmodified. In Clojure, persistent data structures are used as functional alternatives to their imperative counterparts. Persistent vectors, for example, use trees for partial updating. Calling the insert method will result in some but not all nodes being created.\\n\\n\\n== Comparison to imperative programming ==\\nFunctional programming is very different from imperative programming. The most significant differences stem from the fact that functional programming avoids side effects, which are used in imperative programming to implement state and I/O. Pure functional programming completely prevents side-effects and provides referential transparency.\\nHigher-order functions are rarely used in older imperative programming. A traditional imperative program might use a loop to traverse and modify a list. A functional program, on the other hand, would probably use a higher-order “map” function that takes a function and a list, generating and returning a new list by applying the function to each list item.\\n\\n\\n=== Imperative vs. functional programming ===\\nThe following two examples (written in JavaScript) achieve the same effect: they multiply all even numbers in an array by 10 and add them all, storing the final sum in the variable \"result\".\\nTraditional Imperative Loop:\\n\\nFunctional Programming with higher-order functions:\\n\\n\\n=== Simulating state ===\\nThere are tasks (for example, maintaining a bank account balance) that often seem most naturally implemented with state. Pure functional programming performs these tasks, and I/O tasks such as accepting user input and printing to the screen, in a different way.\\nThe pure functional programming language Haskell implements them using monads, derived from category theory. Monads offer a way to abstract certain types of computational patterns, including (but not limited to) modeling of computations with mutable state (and other side effects such as I/O) in an imperative manner without losing purity. While existing monads may be easy to apply in a program, given appropriate templates and examples, many students find them difficult to understand conceptually, e.g., when asked to define new monads (which is sometimes needed for certain types of libraries).Functional languages also simulate states by passing around immutable states. This can be done by making a function accept the state as one of its parameters, and return a new state together with the result, leaving the old state unchanged.Impure functional languages usually include a more direct method of managing mutable state. Clojure, for example, uses managed references that can be updated by applying pure functions to the current state. This kind of approach enables mutability while still promoting the use of pure functions as the preferred way to express computations.Alternative methods such as Hoare logic and uniqueness have been developed to track side effects in programs. Some modern research languages use effect systems to make the presence of side effects explicit.\\n\\n\\n=== Efficiency issues ===\\nFunctional programming languages are typically less efficient in their use of CPU and memory than imperative languages such as C and Pascal.  This is related to the fact that some mutable data structures like arrays have a very straightforward implementation using present hardware. Flat arrays may be accessed very efficiently with deeply pipelined CPUs, prefetched efficiently through caches (with no complex pointer chasing), or handled with SIMD instructions.  It is also not easy to create their equally efficient general-purpose immutable counterparts. For purely functional languages, the worst-case slowdown is logarithmic in the number of memory cells used, because mutable memory can be represented by a purely functional data structure with logarithmic access time (such as a balanced tree). However, such slowdowns are not universal. For programs that perform intensive numerical computations, functional languages such as OCaml and Clean are only slightly slower than C according to The Computer Language Benchmarks Game. For programs that handle large matrices and multidimensional databases, array functional languages (such as J and K) were designed with speed optimizations.\\nImmutability of data can in many cases lead to execution efficiency by allowing the compiler to make assumptions that are unsafe in an imperative language, thus increasing opportunities for inline expansion.Lazy evaluation may also speed up the program, even asymptotically, whereas it may slow it down at most by a constant factor (however, it may introduce memory leaks if used improperly). Launchbury 1993 discusses theoretical issues related to memory leaks from lazy evaluation, and O\\'Sullivan et al. 2008 give some practical advice for analyzing and fixing them.\\nHowever, the most general implementations of lazy evaluation making extensive use of dereferenced code and data perform poorly on modern processors with deep pipelines and multi-level caches (where a cache miss may cost hundreds of cycles).\\n\\n\\n=== Functional programming in non-functional languages ===\\nIt is possible to use a functional style of programming in languages that are not traditionally considered functional languages. For example, both D and Fortran 95 explicitly support pure functions.\\nJavaScript, Lua, Python and Go had first class functions from their inception. Python had support for \"lambda\", \"map\", \"reduce\", and \"filter\" in 1994, as well as closures in Python 2.2, though Python 3 relegated  \"reduce\" to the functools standard library module. First-class functions have been introduced into other mainstream languages such as PHP 5.3, Visual Basic 9, C# 3.0, C++11, and Kotlin.In PHP, anonymous classes, closures and lambdas are fully supported. Libraries and language extensions for immutable data structures are being developed to aid programming in the functional style.\\nIn Java, anonymous classes can sometimes be used to simulate closures; however, anonymous classes are not always proper replacements to closures because they have more limited capabilities. Java 8 supports lambda expressions as a replacement for some anonymous classes.In C#, anonymous classes are not necessary, because closures and lambdas are fully supported. Libraries and language extensions for immutable data structures are being developed to aid programming in the functional style in C#.\\nMany object-oriented design patterns are expressible in functional programming terms: for example, the strategy pattern simply dictates use of a higher-order function, and the visitor pattern roughly corresponds to a catamorphism, or fold.\\nSimilarly, the idea of immutable data from functional programming is often included in imperative programming languages, for example the tuple in Python, which is an immutable array, and Object.freeze() in JavaScript.\\n\\n\\n== Applications ==\\n\\n\\n=== Spreadsheets ===\\nSpreadsheets can be considered a form of pure, zeroth-order, strict-evaluation functional programming system. However, spreadsheets generally lack higher-order functions as well as code reuse, and in some implementations, also lack recursion. Several extensions have been developed for spreadsheet programs to enable higher-order and reusable functions, but so far remain primarily academic in nature.\\n\\n\\n=== Academia ===\\nFunctional programming is an active area of research in the field of programming language theory. There are several peer-reviewed publication venues focusing on functional programming, including the International Conference on Functional Programming, the Journal of Functional Programming, and the Symposium on Trends in Functional Programming.\\n\\n\\n=== Industry ===\\nFunctional programming has seen use in a wide variety of industrial applications. For example, Erlang, which was developed by the Swedish company Ericsson in the late 1980s, was originally used to implement fault-tolerant telecommunications systems, but has since become popular for building a range of applications at companies such as Nortel, Facebook, Électricité de France and WhatsApp. Scheme, a dialect of Lisp, was used as the basis for several applications on early Apple Macintosh computers, and has been applied to problems such as training simulation software and telescope control. OCaml, which was introduced in the mid-1990s, has seen commercial use in areas such as financial analysis, driver verification, industrial robot programming, and static analysis of embedded software. Haskell, though initially intended as a research language, has also been applied by a range of companies, in areas such as aerospace systems, hardware design, and web programming.Other functional programming languages that have seen use in industry include Scala, F#, Wolfram Language, Lisp, Standard ML, and Clojure.Functional \"platforms\" have been popular in finance for risk analytics (particularly with the larger investment banks). Risk factors are coded as functions that form interdependent graphs (categories) to measure correlations in market shifts not unlike Gröbner basis optimizations but also for regulatory compliance such as Comprehensive Capital Analysis and Review. Given the use of OCAML or CAML variations in finance, these systems are sometimes considered related to a categorical abstract machine or CAM. Indeed, functional programming is heavily influenced by category theory.\\n\\n\\n=== Education ===\\nMany universities teach or have taught functional programming as part of their undergraduate Computer Science degrees. Some use it as their introduction to programming, while others teach it after teaching imperative programming.Outside of computer science, functional programming is being used as a method to teach problem solving, algebra and geometric concepts.\\nIt has also been used as a tool to teach classical mechanics in Structure and Interpretation of Classical Mechanics.\\n\\n\\n== See also ==\\n\\nPurely functional programming\\nComparison of programming paradigms\\nEager evaluation\\nList of functional programming topics\\nNested function\\nInductive functional programming\\nFunctional reactive programming\\n\\n\\n== References ==\\n\\n\\n== Further reading ==\\nAbelson, Hal; Sussman, Gerald Jay (1985). Structure and Interpretation of Computer Programs. MIT Press.\\nCousineau, Guy and Michel Mauny. The Functional Approach to Programming. Cambridge, UK: Cambridge University Press, 1998.\\nCurry, Haskell Brooks and Feys, Robert and Craig, William. Combinatory Logic. Volume I. North-Holland Publishing Company, Amsterdam, 1958.\\nCurry, Haskell B.; Hindley, J. Roger; Seldin, Jonathan P. (1972). Combinatory Logic. Vol. II. Amsterdam: North Holland. ISBN 978-0-7204-2208-5. \\nDominus, Mark Jason. Higher-Order Perl. Morgan Kaufmann. 2005.\\nFelleisen, Matthias; Findler, Robert; Flatt, Matthew; Krishnamurthi, Shriram (2001). How to Design Programs. MIT Press.\\nGraham, Paul. ANSI Common LISP. Englewood Cliffs, New Jersey: Prentice Hall, 1996.\\nMacLennan, Bruce J. Functional Programming: Practice and Theory. Addison-Wesley, 1990.\\nO\\'Sullivan, Brian; Stewart, Don; Goerzen, John (2008). Real World Haskell. O\\'Reilly.\\nPratt, Terrence, W. and Marvin V. Zelkowitz. Programming Languages: Design and Implementation. 3rd ed. Englewood Cliffs, New Jersey: Prentice Hall, 1996.\\nSalus, Peter H. Functional and Logic Programming Languages. Vol. 4 of Handbook of Programming Languages. Indianapolis, Indiana: Macmillan Technical Publishing, 1998.\\nThompson, Simon. Haskell: The Craft of Functional Programming. Harlow, England: Addison-Wesley Longman Limited, 1996.\\n\\n\\n== External links ==\\n\\nFord, Neal (2012-01-29). \"Functional thinking: Why functional programming is on the rise\". Retrieved 2013-02-24.\\nAkhmechet, Slava (2006-06-19). \"defmacro – Functional Programming For The Rest of Us\". Retrieved 2013-02-24. An introduction\\nFunctional programming in Python (by David Mertz): part 1, part 2, part 3', 'Logic programming is a programming paradigm which is largely based on formal logic. Any program written in a logic programming language is a set of sentences in logical form, expressing facts and rules about some problem domain.  Major logic programming language families include Prolog, answer set programming (ASP) and Datalog. In all of these languages, rules are written in the form of clauses:\\n\\nH :- B1, …, Bn.and are read declaratively as logical implications:\\n\\nH if B1 and … and Bn.H is called the head of the rule and B1, ..., Bn is called the body. Facts are rules that have no body, and are written in the simplified form:\\n\\nH.In the simplest case in which H, B1, ..., Bn are all atomic formulae, these clauses are called definite clauses or Horn clauses. However, there are many extensions of this simple case, the most important one being the case in which conditions in the body of a clause can also be negations of atomic formulas. Logic programming languages that include this extension have the knowledge representation capabilities of a non-monotonic logic.\\nIn ASP and Datalog, logic programs have only a declarative reading, and their execution is performed by means of a proof procedure or model generator whose behaviour is not meant to be controlled by the programmer. However, in the Prolog family of languages, logic programs also have a procedural interpretation as goal-reduction procedures:\\n\\nto solve H, solve B1, and ... and solve Bn.Consider the following clause as an example:\\n\\nfallible(X) :- human(X).based on an example used by Terry Winograd to illustrate the programming language Planner. As a clause in a logic program, it can be used both as a procedure to test whether X is fallible by testing whether X is human, and as a procedure to find an X which is fallible by finding an X which is human. Even facts have a procedural interpretation. For example, the clause:\\n\\nhuman(socrates).can be used both as a procedure to show that socrates is human, and as a procedure to find an X that is human by \"assigning\" socrates to X.\\nThe declarative reading of logic programs can be used by a programmer to verify their correctness. Moreover, logic-based program transformation techniques can also be used to transform logic programs into logically equivalent programs that are more efficient. In the Prolog family of logic programming languages, the programmer can also use the known problem-solving behaviour of the execution mechanism to improve the efficiency of programs.\\n\\n\\n== History ==\\nThe use of mathematical logic to represent and execute computer programs is also a feature of the lambda calculus, developed by Alonzo Church in the 1930s. However, the first proposal to use the clausal form of logic for representing computer programs was made by Cordell Green. This used an axiomatization of a subset of LISP, together with a representation of an input-output relation, to compute the relation by simulating the execution of the program in LISP. Foster and Elcock\\'s Absys, on the other hand, employed a combination of equations and lambda calculus in an assertional programming language which places no constraints on the order in which operations are performed.Logic programming in its present form can be traced back to debates in the late 1960s and early 1970s about declarative versus procedural representations of knowledge in artificial intelligence. Advocates of declarative representations were notably working at Stanford, associated with John McCarthy, Bertram Raphael and Cordell Green, and in Edinburgh, with John Alan Robinson (an academic visitor from Syracuse University), Pat Hayes, and Robert Kowalski. Advocates of procedural representations were mainly centered at MIT, under the leadership of Marvin Minsky and Seymour Papert.Although it was based on the proof methods of logic, Planner, developed at MIT, was the first language to emerge within this proceduralist paradigm. Planner featured pattern-directed invocation of procedural plans from goals (i.e. goal-reduction or backward chaining) and from assertions (i.e. forward chaining). The most influential implementation of Planner was the subset of Planner, called Micro-Planner, implemented by Gerry Sussman, Eugene Charniak and Terry Winograd. It was used to implement Winograd\\'s natural-language understanding program SHRDLU, which was a landmark at that time. To cope with the very limited memory systems at the time, Planner used a backtracking control structure so that only one possible computation path had to be stored at a time. Planner gave rise to the programming languages QA-4, Popler, Conniver, QLISP, and the concurrent language Ether.Hayes and Kowalski in Edinburgh tried to reconcile the logic-based declarative approach to knowledge representation with Planner\\'s procedural approach. Hayes (1973) developed an equational language, Golux, in which different procedures could be obtained by altering the behavior of the theorem prover. Kowalski, on the other hand, developed SLD resolution, a variant of SL-resolution, and showed how it treats implications as goal-reduction procedures. Kowalski collaborated with Colmerauer in Marseille, who developed these ideas in the design and implementation of the programming language Prolog.\\nThe Association for Logic Programming was founded to promote Logic Programming in 1986.\\nProlog gave rise to the programming languages ALF, Fril, Gödel, Mercury, Oz, Ciao, Visual Prolog, XSB, and λProlog, as well as a variety of  concurrent logic programming languages, constraint logic programming languages and Datalog.\\n\\n\\n== Concepts ==\\n\\n\\n=== Logic and control ===\\n\\nLogic programming can be viewed as controlled deduction. An important concept in logic programming is the separation of programs into their logic component and their control component. With pure logic programming languages, the logic component alone determines the solutions produced. The control component can be varied to provide alternative ways of executing a logic program. This notion is captured by the slogan\\n\\nAlgorithm = Logic + Controlwhere \"Logic\" represents a logic program and \"Control\" represents different theorem-proving strategies.\\n\\n\\n=== Problem solving ===\\nIn the simplified, propositional case in which a logic program and a top-level atomic goal contain no variables, backward reasoning determines an and-or tree, which constitutes the search space for solving the goal. The top-level goal is the root of the tree. Given any node in the tree and any clause whose head matches the node, there exists a set of child nodes corresponding to the sub-goals in the body of the clause. These child nodes are grouped together by an \"and\". The alternative sets of children corresponding to alternative ways of solving the node are grouped together by an \"or\".\\nAny search strategy can be used to search this space. Prolog uses a sequential, last-in-first-out, backtracking strategy, in which only one alternative and one sub-goal is considered at a time. Other search strategies, such as parallel search, intelligent backtracking, or best-first search to find an optimal solution, are also possible.\\nIn the more general case, where sub-goals share variables, other strategies can be used, such as choosing the subgoal that is most highly instantiated or that is sufficiently instantiated so that only one procedure applies. Such strategies are used, for example, in concurrent logic programming.\\n\\n\\n=== Negation as failure ===\\n\\nFor most practical applications, as well as for applications that require non-monotonic reasoning in artificial intelligence, Horn clause logic programs need to be extended to normal logic programs, with negative conditions. A clause in a normal logic program has the form:\\n\\nH :- A1, …, An, not B1, …, not Bn. and is read declaratively as a logical implication:\\n\\nH if A1 and … and An and not B1 and … and not Bn.where H and all the Ai and Bi are atomic formulas. The negation in the negative literals  not Bi is commonly referred to as \"negation as failure\", because in most implementations, a negative condition  not Bi is shown to hold by showing that the positive condition  Bi fails to hold. For example:\\n\\nGiven the goal of finding something that can fly:\\n\\nthere are two candidate solutions, which solve the first subgoal bird(X), namely X = john and X = mary. The second subgoal not abnormal(john) of the first candidate solution fails, because wounded(john) succeeds and therefore abnormal(john) succeeds. However, the second subgoal not abnormal(mary) of the second candidate solution succeeds, because wounded(mary) fails and therefore abnormal(mary) fails. Therefore, X = mary is the only solution of the goal.\\nMicro-Planner had a construct, called \"thnot\", which when applied to an expression returns the value true if (and only if) the evaluation of the expression fails. An equivalent operator typically exists in modern Prolog\\'s implementations. It is typically written as not(Goal) or \\\\+ Goal, where Goal is some goal (proposition) to be proved by the program. This operator differs from negation in first-order logic: a negation such as \\\\+ X == 1 fails when the variable X has been bound to the atom 1, but it succeeds in all other cases, including when X is unbound. This makes Prolog\\'s reasoning non-monotonic: X = 1, \\\\+ X == 1 always fails, while \\\\+ X == 1, X = 1 can succeed, binding X to 1, depending on whether X was initially bound (note that standard Prolog executes goals in left-to-right order).\\nThe logical status of negation as failure was unresolved until Keith Clark [1978] showed that, under certain natural conditions, it is a correct (and sometimes complete) implementation of classical negation with respect to the completion of the program. Completion amounts roughly to regarding the set of all the program clauses with the same predicate on the left hand side, say\\n\\nH :-  Body1.\\n      …\\nH :-  Bodyk.as a definition of the predicate\\n\\nH iff (Body1 or … or Bodyk)where \"iff\" means \"if and only if\". Writing the completion also requires explicit use of the equality predicate and the inclusion of a set of appropriate axioms for equality. However, the implementation of negation as failure needs only the if-halves of the definitions without the axioms of equality.\\nFor example, the completion of the program above is:\\n\\ncanfly(X) iff bird(X), not abnormal(X).\\nabnormal(X) iff wounded(X).\\n bird(X) iff X = john or X = mary.\\n X = X.\\n not john = mary.\\n not mary = john.The notion of completion is closely related to McCarthy\\'s circumscription semantics for default reasoning, and to the closed world assumption.\\nAs an alternative to the completion semantics, negation as failure can also be interpreted epistemically, as in the stable model semantics of answer set programming. In this interpretation not(Bi) means literally that Bi is not known or not believed. The epistemic interpretation has the advantage that it can be combined very simply with classical negation, as in \"extended logic programming\", to formalise such phrases as \"the contrary can not be shown\", where \"contrary\" is classical negation and \"can not be shown\" is the epistemic interpretation of negation as failure.\\n\\n\\n=== Knowledge representation ===\\nThe fact that Horn clauses can be given a procedural interpretation and, vice versa, that goal-reduction procedures can be understood as Horn clauses + backward reasoning means that logic programs combine declarative and procedural representations of knowledge. The inclusion of negation as failure means that logic programming is a kind of non-monotonic logic.\\nDespite its simplicity compared with classical logic, this combination of Horn clauses and negation as failure has proved to be surprisingly expressive. For example, it provides a natural representation for the common-sense laws of cause and effect, as formalised by both the situation calculus and event calculus. It has also been shown to correspond quite naturally to the semi-formal language of legislation. In particular, Prakken and Sartor credit the representation of the British Nationality Act as a logic program with being \"hugely influential for the development of computational representations of legislation, showing how logic programming enables intuitively appealing representations that can be directly deployed to generate automatic inferences\".\\n\\n\\n== Variants and extensions ==\\n\\n\\n=== Prolog ===\\n\\nThe programming language Prolog was developed in 1972 by Alain Colmerauer. It emerged from a collaboration between Colmerauer in Marseille and Robert Kowalski in Edinburgh. Colmerauer was working on natural-language understanding, using logic to represent semantics and using resolution for question-answering. During the summer of 1971, Colmerauer and Kowalski discovered that the clausal form of logic could be used to represent formal grammars and that resolution theorem provers could be used for parsing. They observed that some theorem provers, like hyper-resolution, behave as bottom-up parsers and others, like SL-resolution (1971), behave as top-down parsers.\\nIt was in the following summer of 1972, that Kowalski, again working with Colmerauer, developed the procedural interpretation of implications. This dual declarative/procedural interpretation later became formalised in the Prolog notation\\n\\nH :- B1, …, Bn.which can be read (and used) both declaratively and procedurally. It also became clear that such clauses could be restricted to definite clauses or Horn clauses, where H, B1, ..., Bn are all atomic predicate logic formulae, and that SL-resolution could be restricted (and generalised) to LUSH or SLD-resolution. Kowalski\\'s procedural interpretation and LUSH were described in a 1973 memo, published in 1974.Colmerauer, with Philippe Roussel, used this dual interpretation of clauses as the basis of Prolog, which was implemented in the summer and autumn of 1972. The first Prolog program, also written in 1972 and implemented in Marseille, was a French question-answering system. The use of Prolog as a practical programming language was given great momentum by the development of a compiler by David Warren in Edinburgh in 1977. Experiments demonstrated that Edinburgh Prolog could compete with the processing speed of other symbolic programming languages such as Lisp. Edinburgh Prolog became the de facto standard and strongly influenced the definition of ISO standard Prolog.\\n\\n\\n=== Abductive logic programming ===\\nAbductive logic programming is an extension of normal Logic Programming that allows some predicates, declared as abducible predicates, to be \"open\" or undefined. A clause in an abductive logic program has the form:\\n\\nH :- B1, …, Bn, A1, …, An.where H is an atomic formula that is not abducible, all the Bi are literals whose predicates are not abducible, and the Ai are atomic formulas whose predicates are abducible. The abducible predicates can be constrained by integrity constraints, which can have the form:\\n\\nfalse :- L1, …, Ln.where the Li are arbitrary literals (defined or abducible, and atomic or negated). For example:\\n\\nwhere the predicate normal is abducible.\\nProblem solving is achieved by deriving hypotheses expressed in terms of the abducible predicates as solutions of problems to be solved. These problems can be either observations that need to be explained (as in classical abductive reasoning) or goals to be solved (as in normal logic programming). For example, the hypothesis normal(mary) explains the observation canfly(mary). Moreover, the same hypothesis entails the only solution X = mary of the goal of finding something which can fly:\\n\\nAbductive logic programming has been used for fault diagnosis, planning, natural language processing and machine learning. It has also been used to interpret Negation as Failure as a form of abductive reasoning.\\n\\n\\n=== Metalogic programming ===\\nBecause mathematical logic has a long tradition of distinguishing between object language and metalanguage, logic programming also allows metalevel programming. The simplest metalogic program is the so-called \"vanilla\" meta-interpreter:\\n\\nwhere true represents an empty conjunction, and clause(A,B) means that there is an object-level clause of the form\\tA :- B.\\nMetalogic programming allows object-level and metalevel representations to be combined, as in natural language. It can also be used to implement any logic which is specified as inference rules. Metalogic is used in logic programming to implement metaprograms, which manipulate other programs, databases, knowledge bases or axiomatic theories as data.\\n\\n\\n=== Constraint logic programming ===\\n\\nConstraint logic programming combines Horn clause logic programming with constraint solving. It extends Horn clauses by allowing some predicates, declared as constraint predicates, to occur as literals in the body of clauses. A constraint logic program is a set of clauses of the form:\\n\\nH :- C1, …, Cn ◊ B1, …, Bn.where H and all the Bi are atomic formulas, and the Ci are constraints. Declaratively, such clauses are read as ordinary logical implications:\\n\\nH if C1 and … and Cn and B1 and … and Bn.However, whereas the predicates in the heads of clauses are defined by the constraint logic program, the predicates in the constraints are predefined by some domain-specific model-theoretic structure or theory.\\nProcedurally, subgoals whose predicates are defined by the program are solved by goal-reduction, as in ordinary logic programming, but constraints are checked for satisfiability by a domain-specific constraint-solver, which implements the semantics of the constraint predicates. An initial problem is solved by reducing it to a satisfiable conjunction of constraints.\\nThe following constraint logic program represents a toy temporal database of john\\'s history as a teacher:\\n\\nHere ≤ and < are constraint predicates, with their usual intended semantics. The following goal clause queries the database to find out when john both taught logic and was a professor:\\n\\n:- teaches(john, logic, T), rank(john, professor, T).The solution is 2010 ≤ T, T ≤ 2012.\\nConstraint logic programming has been used to solve problems in such fields as civil engineering, mechanical engineering, digital circuit verification, automated timetabling, air traffic control, and finance. It is closely related to abductive logic programming.\\n\\n\\n=== Concurrent logic programming ===\\n\\nConcurrent logic programming integrates concepts of logic programming with concurrent programming. Its development was given a big impetus in the 1980s by its choice for the systems programming language of the Japanese Fifth Generation Project (FGCS).A concurrent logic program is a set of guarded Horn clauses of the form:\\n\\nH :- G1, …, Gn | B1, …, Bn.The conjunction G1, ... , Gn is called the guard of the clause, and  |  is the commitment operator. Declaratively, guarded Horn clauses are read as ordinary logical implications:\\n\\nH if G1 and … and Gn and B1 and … and Bn.However, procedurally, when there are several clauses whose heads  H  match a given goal, then all of the clauses are executed in parallel, checking whether their guards G1, ... , Gn hold. If the guards of more than one clause hold, then a committed choice is made to one of the clauses, and execution proceeds with the subgoals  B1, ..., Bn of the chosen clause. These subgoals can also be executed in parallel. Thus concurrent logic programming implements a form of \"don\\'t care nondeterminism\", rather than \"don\\'t know nondeterminism\".\\nFor example, the following concurrent logic program defines a predicate  shuffle(Left, Right, Merge) , which can be used to shuffle two lists Left and Right, combining them into a single list Merge that preserves the ordering of the two lists Left and Right:\\n\\nHere, [] represents the empty list, and [Head | Tail] represents a list with first element Head followed by list Tail, as in Prolog. (Notice that the first occurrence of  |  in the second and third clauses is the list constructor, whereas the second occurrence of  |  is the commitment operator.)  The program can be used, for example, to shuffle the lists [ace, queen, king] and [1, 4, 2] by invoking the goal clause:\\n\\nThe program will non-deterministically generate a single solution, for example  Merge = [ace, queen, 1, king, 4, 2].\\nArguably, concurrent logic programming is based on message passing, so it is subject to the same indeterminacy as other concurrent message-passing systems, such as Actors (see Indeterminacy in concurrent computation). Carl Hewitt has argued that concurrent logic programming is not based on logic in his sense that computational steps cannot be logically deduced. However, in concurrent logic programming, any result of a terminating computation is a logical consequence of the program, and any partial result of a partial computation is a logical consequence of the program and the residual goal (process network). Thus the indeterminacy of computations implies that not all logical consequences of the program can be deduced.\\n\\n\\n=== Concurrent constraint logic programming ===\\n\\nConcurrent constraint logic programming combines concurrent logic programming and constraint logic programming, using constraints to control concurrency. A clause can contain a guard, which is a set of constraints that may block the applicability of the clause. When the guards of several clauses are satisfied, concurrent constraint logic programming makes a committed choice to use only one.\\n\\n\\n=== Inductive logic programming ===\\n\\nInductive logic programming is concerned with generalizing positive and negative examples in the context of background knowledge: machine learning of logic programs. Recent work in this area, combining logic programming, learning and probability, has given rise to the new field of statistical relational learning and probabilistic inductive logic programming.\\n\\n\\n=== Higher-order logic programming ===\\nSeveral researchers have extended logic programming with higher-order programming features derived from higher-order logic, such as predicate variables. Such languages include the Prolog extensions HiLog and λProlog.\\n\\n\\n=== Linear logic programming ===\\nBasing logic programming within linear logic has resulted in the design of logic programming languages which are considerably more expressive than those based on classical logic. Horn clause programs can only represent state change by the change in arguments to predicates. In linear logic programming, one can use the ambient linear logic to support state change. Some early designs of logic programming languages based on linear logic include LO [Andreoli & Pareschi, 1991], Lolli, ACL, and Forum [Miller, 1996]. Forum provides a goal-directed interpretation of all of linear logic.\\n\\n\\n=== Object-oriented logic programming ===\\nF-logic extends logic programming with objects and the frame syntax.\\nLogtalk extends the Prolog programming language with support for objects, protocols, and other OOP concepts. It supports most standard-compliant Prolog systems as backend compilers.\\n\\n\\n=== Transaction logic programming ===\\nTransaction logic is an extension of logic programming with a logical theory of state-modifying updates. It has both a model-theoretic semantics and a procedural one. An implementation of a subset of Transaction logic is available in the Flora-2 system. Other prototypes are also available.\\n\\n\\n== See also ==\\nAutomated theorem proving\\nConstraint logic programming\\nControl theory\\nDatalog\\nFril\\nFunctional programming\\nFuzzy logic\\nInductive logic programming\\nLogic in computer science (includes Formal methods)\\nLogic programming languages\\nProgrammable logic controller\\nR++\\nReasoning system\\nRule-based machine learning\\nSatisfiability\\nBoolean satisfiability problem\\nLinear logic\\n\\n\\n== Citations ==\\n\\n\\n== Sources ==\\n\\n\\n=== General introductions ===\\nBaral, C.; Gelfond, M. (1994). \"Logic programming and knowledge representation\" (PDF). The Journal of Logic Programming. 19–20: 73–148. doi:10.1016/0743-1066(94)90025-6.\\nKowalski, R. A. (1988). \"The early years of logic programming\" (PDF). Communications of the ACM. 31: 38–43. doi:10.1145/35043.35046. S2CID 12259230. [1]\\nLloyd, J. W. (1987). Foundations of Logic Programming. (2nd edition). Springer-Verlag.\\n\\n\\n=== Other sources ===\\nJohn McCarthy. \"Programs with common sense\". Symposium on Mechanization of Thought Processes. National Physical Laboratory. Teddington, England. 1958.\\nMiller, Dale; Nadathur, Gopalan; Pfenning, Frank; Scedrov, Andre (1991). \"Uniform proofs as a foundation for logic programming\". Annals of Pure and Applied Logic. 51 (1–2): 125–157. doi:10.1016/0168-0072(91)90068-W.\\nEhud Shapiro (Editor). Concurrent Prolog. MIT Press. 1987.\\nJames Slagle. \"Experiments with a Deductive Question-Answering Program\". CACM. December 1965.\\nGabbay, Dov M.; Hogger, Christopher John; Robinson, J.A., eds. (1993-1998). Handbook of Logic in Artificial Intelligence and Logic Programming.Vols. 1–5, Oxford University Press.\\n\\n\\n== Further reading ==\\nCarl Hewitt. \"Procedural Embedding of Knowledge in Planner\". IJCAI 1971.\\nCarl Hewitt. \"The Repeated Demise of Logic Programming and Why It Will Be Reincarnated\". AAAI Spring Symposium: What Went Wrong and Why: Lessons from AI Research and Applications 2006: 2–9.\\nEvgeny Dantsin, Thomas Eiter, Georg Gottlob, Andrei Voronkov: Complexity and expressive power of logic programming. ACM Comput. Surv. 33(3): 374–425 (2001)\\nUlf Nilsson and Jan Maluszynski, Logic, Programming and Prolog\\n\\n\\n== External links ==\\nLogic Programming Virtual Library entry\\nBibliographies on Logic Programming\\nAssociation for Logic Programming (ALP)\\nTheory and Practice of Logic Programming (journal)\\nLogic programming in C++ with Castor\\nLogic programming in Oz\\nProlog Development Center \\nRacklog: Logic Programming in Racket', 'Object-oriented programming (OOP) is a programming paradigm based on the concept of \"objects\", which can contain data and code: data in the form of fields (often known as attributes or properties), and code, in the form of procedures (often known as methods). \\nA feature of objects is that an object\\'s own procedures can access and often modify the data fields of itself (objects have a notion of this or self). In OOP, computer programs are designed by making them out of objects that interact with one another. OOP languages are diverse, but the most popular ones are class-based, meaning that objects are instances of classes, which also determine their types.\\nMany of the most widely used programming languages (such as C++, Java, Python, etc.) are multi-paradigm and they support object-oriented programming to a greater or lesser degree, typically in combination with imperative, procedural programming. Significant object-oriented languages include:\\nJava,\\nC++,\\nC#,\\nPython,\\nR,\\nPHP,\\nVisual Basic.NET,\\nJavaScript,\\nRuby,\\nPerl,\\nSIMSCRIPT,\\nObject Pascal,\\nObjective-C,\\nDart,\\nSwift,\\nScala,\\nKotlin,\\nCommon Lisp,\\nMATLAB,\\nand\\nSmalltalk.\\n\\n\\n== History ==\\n\\nTerminology invoking \"objects\" and \"oriented\" in the modern sense of object-oriented programming made its first appearance at MIT in the late 1950s and early 1960s. In the environment of the artificial intelligence group, as early as 1960, \"object\" could refer to identified items (LISP atoms) with properties (attributes);Alan Kay later cited a detailed understanding of LISP internals as a strong influence on his thinking in 1966.\\n\\nAnother early MIT example was Sketchpad created by Ivan Sutherland in 1960–1961; in the glossary of the 1963 technical report based on his dissertation about Sketchpad, Sutherland defined notions of \"object\" and \"instance\" (with the class concept covered by \"master\" or \"definition\"), albeit specialized to graphical interaction.\\nAlso, an MIT ALGOL version, AED-0, established a direct link between data structures (\"plexes\", in that dialect) and procedures, prefiguring what were later termed \"messages\", \"methods\", and \"member functions\".Simula introduced important concepts that are today an essential part of object-oriented programming, such as class and object, inheritance, and dynamic binding. \\nThe object-oriented Simula programming language was used mainly by researchers involved with physical modelling, such as models to study and improve the movement of ships and their content through cargo ports.In the 1970s, the first version of the Smalltalk programming language was developed at Xerox PARC by Alan Kay, Dan Ingalls and Adele Goldberg. Smaltalk-72 included a programming environment and was dynamically typed, and at first was interpreted, not compiled. Smalltalk became noted for its application of object orientation at the language-level and its graphical development environment. Smalltalk went through various versions and interest in the language grew. While Smalltalk was influenced by the ideas introduced in Simula 67 it was designed to be a fully dynamic system in which classes could be created and modified dynamically.In the 1970s, Smalltalk influenced the Lisp community to incorporate object-based techniques that were introduced to developers via the Lisp machine. Experimentation with various extensions to Lisp (such as LOOPS and Flavors introducing multiple inheritance and mixins) eventually led to the Common Lisp Object System, which integrates functional programming and object-oriented programming and allows extension via a Meta-object protocol. In the 1980s, there were a few attempts to design processor architectures that included hardware support for objects in memory but these were not successful. Examples include the Intel iAPX 432 and the Linn Smart Rekursiv.\\nIn 1981, Goldberg edited the August issue of Byte Magazine, introducing Smalltalk and object-oriented programming to a wider audience. In 1986, the Association for Computing Machinery organised the first Conference on Object-Oriented Programming, Systems, Languages, and Applications (OOPSLA), which was unexpectedly attended by 1,000 people. In the mid-1980s Objective-C was developed by Brad Cox, who had used Smalltalk at ITT Inc., and Bjarne Stroustrup, who had used Simula for his PhD thesis, eventually went to create the object-oriented C++. In 1985, Bertrand Meyer also produced the first design of the Eiffel language. Focused on software quality, Eiffel is a purely object-oriented programming language and a notation supporting the entire software lifecycle. Meyer described the Eiffel software development method, based on a small number of key ideas from software engineering and computer science, in Object-Oriented Software Construction. Essential to the quality focus of Eiffel is Meyer\\'s reliability mechanism, Design by Contract, which is an integral part of both the method and language.\\n\\nIn the early and mid-1990s object-oriented programming developed as the dominant programming paradigm when programming languages supporting the techniques became widely available. These included Visual FoxPro 3.0, C++, and Delphi. Its dominance was further enhanced by the rising popularity of graphical user interfaces, which rely heavily upon object-oriented programming techniques. An example of a closely related dynamic GUI library and OOP language can be found in the Cocoa frameworks on Mac OS X, written in Objective-C, an object-oriented, dynamic messaging extension to C based on Smalltalk. OOP toolkits also enhanced the popularity of event-driven programming (although this concept is not limited to OOP).\\nAt ETH Zürich, Niklaus Wirth and his colleagues had also been investigating such topics as data abstraction and modular programming (although this had been in common use in the 1960s or earlier). Modula-2 (1978) included both, and their succeeding design, Oberon, included a distinctive approach to object orientation, classes, and such.\\nObject-oriented features have been added to many previously existing languages, including Ada, BASIC, Fortran, Pascal, and COBOL. Adding these features to languages that were not initially designed for them often led to problems with compatibility and maintainability of code.\\nMore recently, a number of languages have emerged that are primarily object-oriented, but that are also compatible with procedural methodology. Two such languages are Python and Ruby. Probably the most commercially important recent object-oriented languages are Java, developed by Sun Microsystems, as well as C# and Visual Basic.NET (VB.NET), both designed for Microsoft\\'s .NET platform. Each of these two frameworks shows, in its own way, the benefit of using OOP by creating an abstraction from implementation. VB.NET and C# support cross-language inheritance, allowing classes defined in one language to subclass classes defined in the other language.\\n\\n\\n== Features ==\\nObject-oriented programming uses objects, but not all of the associated techniques and structures are supported directly in languages that claim to support OOP.  The features listed below are common among languages considered to be strongly class- and object-oriented (or multi-paradigm with OOP support), with notable exceptions mentioned.\\n\\n\\n=== Shared with non-OOP languages ===\\nVariables that can store information formatted in a small number of built-in data types like integers and alphanumeric characters.  This may include data structures like strings, lists, and hash tables that are either built-in or result from combining variables using memory pointers.\\nProcedures – also known as functions, methods, routines, or subroutines – that take input, generate output, and manipulate data.  Modern languages include structured programming constructs like loops and conditionals.Modular programming support provides the ability to group procedures into files and modules for organizational purposes.  Modules are namespaced so identifiers in one module will not conflict with a procedure or variable sharing the same name in another file or module.\\n\\n\\n=== Objects and classes ===\\nLanguages that support object-oriented programming (OOP) typically use inheritance for code reuse and extensibility in the form of either classes or prototypes. Those that use classes support two main concepts:\\n\\nClasses – the definitions for the data format and available procedures for a given type or class of object; may also contain data and procedures (known as class methods) themselves, i.e. classes contain the data members and member functions\\nObjects – instances of classesObjects sometimes correspond to things found in the real world. For example, a graphics program may have objects such as \"circle\", \"square\", \"menu\". An online shopping system might have objects such as \"shopping cart\", \"customer\", and \"product\". Sometimes objects represent more abstract entities, like an object that represents an open file, or an object that provides the service of translating measurements from U.S. customary to metric.\\n\\nEach object is said to be an instance of a particular class (for example, an object with its name field set to \"Mary\" might be an instance of class Employee).  Procedures in object-oriented programming are known as methods; variables are also known as fields, members, attributes, or properties.  This leads to the following terms:\\n\\nClass variables – belong to the class as a whole; there is only one copy of each one\\nInstance variables or attributes – data that belongs to individual objects; every object has its own copy of each one\\nMember variables – refers to both the class and instance variables that are defined by a particular class\\nClass methods – belong to the class as a whole and have access to only class variables and inputs from the procedure call\\nInstance methods – belong to individual objects, and have access to instance variables for the specific object they are called on, inputs, and class variablesObjects are accessed somewhat like variables with complex internal structure, and in many languages are effectively pointers, serving as actual references to a single instance of said object in memory within a heap or stack.  They provide a layer of abstraction which can be used to separate internal from external code. External code can use an object by calling a specific instance method with a certain set of input parameters, read an instance variable, or write to an instance variable. Objects are created by calling a special type of method in the class known as a constructor.  A program may create many instances of the same class as it runs, which operate independently.  This is an easy way for the same procedures to be used on different sets of data.\\nObject-oriented programming that uses classes is sometimes called class-based programming, while prototype-based programming does not typically use classes. As a result, significantly different yet analogous terminology is used to define the concepts of object and instance.\\nIn some languages classes and objects can be composed using other concepts like traits and mixins.\\n\\n\\n=== Class-based vs prototype-based ===\\nIn class-based languages the classes are defined beforehand and the objects are instantiated based on the classes. If two objects apple and orange are instantiated from the class Fruit, they are inherently fruits and it is guaranteed that you may handle them in the same way; e.g. a programmer can expect the existence of the same attributes such as color or sugar_content or is_ripe.\\nIn prototype-based languages the objects are the primary entities. No classes even exist. The prototype of an object is just another object to which the object is linked. Every object has one prototype link (and only one).  New objects can be created based on already existing objects chosen as their prototype. You may call two different objects apple and orange a fruit, if the object fruit exists, and both apple and orange have fruit as their prototype. The idea of the fruit class doesn\\'t exist explicitly, but as the equivalence class of the objects sharing the same prototype. The attributes and methods of the prototype are delegated to all the objects of the equivalence class defined by this prototype. The attributes and methods owned individually by the object may not be shared by other objects of the same equivalence class; e.g. the attribute sugar_content may be unexpectedly not present in apple. Only single inheritance can be implemented through the prototype.\\n\\n\\n=== Dynamic dispatch/message passing ===\\nIt is the responsibility of the object, not any external code, to select the procedural code to execute in response to a method call, typically by looking up the method at run time in a table associated with the object.  This feature is known as dynamic dispatch, and distinguishes an object from an abstract data type (or module), which has a fixed (static) implementation of the operations for all instances.  If the call variability relies on more than the single type of the object on which it is called (i.e. at least one other parameter object is involved in the method choice), one speaks of multiple dispatch.\\nA method call is also known as message passing.  It is conceptualized as a message (the name of the method and its input parameters) being passed to the object for dispatch.\\n\\n\\n=== Encapsulation ===\\nEncapsulation is an object-oriented programming concept that binds together the data and functions that manipulate the data, and that keeps both safe from outside interference and misuse. Data encapsulation led to the important OOP concept of data hiding.\\nIf a class does not allow calling code to access internal object data and permits access through methods only, this is a strong form of abstraction or information hiding known as encapsulation.  Some languages (Java, for example) let classes enforce access restrictions explicitly, for example denoting internal data with the private keyword and designating methods intended for use by code outside the class with the public keyword.  Methods may also be designed public, private, or intermediate levels such as protected (which allows access from the same class and its subclasses, but not objects of a different class).  In other languages (like Python) this is enforced only by convention (for example, private methods may have names that start with an underscore).  Encapsulation prevents external code from being concerned with the internal workings of an object.  This facilitates code refactoring, for example allowing the author of the class to change how objects of that class represent their data internally without changing any external code (as long as \"public\" method calls work the same way).  It also encourages programmers to put all the code that is concerned with a certain set of data in the same class, which organizes it for easy comprehension by other programmers.  Encapsulation is a technique that encourages decoupling.\\n\\n\\n=== Composition, inheritance, and delegation ===\\nObjects can contain other objects in their instance variables; this is known as object composition.  For example, an object in the Employee class might contain (either directly or through a pointer) an object in the Address class, in addition to its own instance variables like \"first_name\" and \"position\".  Object composition is used to represent \"has-a\" relationships: every employee has an address, so every Employee object has access to a place to store an Address object (either directly embedded within itself, or at a separate location addressed via a pointer).\\nLanguages that support classes almost always support inheritance.  This allows classes to be arranged in a hierarchy that represents \"is-a-type-of\" relationships.  For example, class Employee might inherit from class Person.  All the data and methods available to the parent class also appear in the child class with the same names.  For example, class Person might define variables \"first_name\" and \"last_name\" with method \"make_full_name()\".  These will also be available in class Employee, which might add the variables \"position\" and \"salary\".  This technique allows easy re-use of the same procedures and data definitions, in addition to potentially mirroring real-world relationships in an intuitive way. Rather than utilizing database tables and programming subroutines, the developer utilizes objects the user may be more familiar with: objects from their application domain.Subclasses can override the methods defined by superclasses. Multiple inheritance is allowed in some languages, though this can make resolving overrides complicated.  Some languages have special support for mixins, though in any language with multiple inheritance, a mixin is simply a class that does not represent an is-a-type-of relationship.  Mixins are typically used to add the same methods to multiple classes.  For example, class UnicodeConversionMixin might provide a method unicode_to_ascii() when included in class FileReader and class WebPageScraper, which don\\'t share a common parent.\\nAbstract classes cannot be instantiated into objects; they exist only for the purpose of inheritance into other \"concrete\" classes that can be instantiated.  In Java, the final keyword can be used to prevent a class from being subclassed.\\nThe doctrine of composition over inheritance advocates implementing has-a relationships using composition instead of inheritance.  For example, instead of inheriting from class Person, class Employee could give each Employee object an internal Person object, which it then has the opportunity to hide from external code even if class Person has many public attributes or methods.  Some languages, like Go do not support inheritance at all.\\nThe \"open/closed principle\" advocates that classes and functions \"should be open for extension, but closed for modification\".\\nDelegation is another language feature that can be used as an alternative to inheritance.\\n\\n\\n=== Polymorphism ===\\nSubtyping – a form of polymorphism – is when calling code can be agnostic as to which class in the supported hierarchy it is operating on – the parent class or one of its descendants.  Meanwhile, the same operation name among objects in an inheritance hierarchy may behave differently.\\nFor example, objects of type Circle and Square are derived from a common class called Shape.  The Draw function for each type of Shape implements what is necessary to draw itself while calling code can remain indifferent to the particular type of Shape being drawn.\\nThis is another type of abstraction that simplifies code external to the class hierarchy and enables strong separation of concerns.\\n\\n\\n=== Open recursion ===\\nIn languages that support open recursion, object methods can call other methods on the same object (including themselves), typically using a special variable or keyword called this or self.  This variable is late-bound; it allows a method defined in one class to invoke another method that is defined later, in some subclass thereof.\\n\\n\\n== OOP languages ==\\n\\nSimula (1967) is generally accepted as being the first language with the primary features of an object-oriented language. It was created for making simulation programs, in which what came to be called objects were the most important information representation. Smalltalk (1972 to 1980) is another early example, and the one with which much of the theory of OOP was developed. Concerning the degree of object orientation, the following distinctions can be made:\\n\\nLanguages called \"pure\" OO languages, because everything in them is treated consistently as an object, from primitives such as characters and punctuation, all the way up to whole classes, prototypes, blocks, modules, etc. They were designed specifically to facilitate, even enforce, OO methods. Examples: Ruby, Scala, Smalltalk, Eiffel, Emerald, JADE, Self, Raku.\\nLanguages designed mainly for OO programming, but with some procedural elements. Examples: Java, Python, C++, C#, Delphi/Object Pascal, VB.NET.\\nLanguages that are historically procedural languages, but have been extended with some OO features. Examples: PHP, Perl, Visual Basic (derived from BASIC), MATLAB, COBOL 2002, Fortran 2003, ABAP, Ada 95, Pascal.\\nLanguages with most of the features of objects (classes, methods, inheritance), but in a distinctly original form. Examples: Oberon (Oberon-1 or Oberon-2).\\nLanguages with abstract data type support which may be used to resemble OO programming, but without all features of object-orientation. This includes object-based and prototype-based languages. Examples: JavaScript, Lua, Modula-2, CLU.\\nChameleon languages that support multiple paradigms, including OO. Tcl stands out among these for TclOO, a hybrid object system that supports both prototype-based programming and class-based OO.\\n\\n\\n=== OOP in dynamic languages ===\\nIn recent years, object-oriented programming has become especially popular in dynamic programming languages. Python, PowerShell, Ruby and Groovy are dynamic languages built on OOP principles, while Perl and PHP have been adding object-oriented features since Perl 5 and PHP 4, and ColdFusion since version 6.\\nThe Document Object Model of HTML, XHTML, and XML documents on the Internet has bindings to the popular JavaScript/ECMAScript language. JavaScript is perhaps the best known prototype-based programming language, which employs cloning from prototypes rather than inheriting from a class (contrast to class-based programming). Another scripting language that takes this approach is Lua.\\n\\n\\n=== OOP in a network protocol ===\\nThe messages that flow between computers to request services in a client-server environment can be designed as the linearizations of objects defined by class objects known to both the client and the server.  For example, a simple linearized object would consist of a length field, a code point identifying the class, and a data value.  A more complex example would be a command consisting of the length and code point of the command and values consisting of linearized objects representing the command\\'s parameters.  Each such command must be directed by the server to an object whose class (or superclass) recognizes the command and is able to provide the requested service. Clients and servers are best modeled as complex object-oriented structures. Distributed Data Management Architecture (DDM) took this approach and used class objects to define objects at four levels of a formal hierarchy:\\n\\nFields defining the data values that form messages, such as their length, code point and data values.\\nObjects and collections of objects similar to what would be found in a Smalltalk program for messages and parameters.\\nManagers similar to IBM i Objects, such as a directory to files and files consisting of metadata and records. Managers conceptually provide memory and processing resources for their contained objects.\\nA client or server consisting of all the managers necessary to implement a full processing environment, supporting such aspects as directory services, security and concurrency control.The initial version of DDM defined distributed file services.  It was later extended to be the foundation of Distributed Relational Database Architecture (DRDA).\\n\\n\\n== Design patterns ==\\nChallenges of object-oriented design are addressed by several approaches. Most common is known as the design patterns codified by Gamma et al.. More broadly, the term \"design patterns\" can be used to refer to any general, repeatable, solution pattern to a commonly occurring problem in software design. Some of these commonly occurring problems have implications and solutions particular to object-oriented development.\\n\\n\\n=== Inheritance and behavioral subtyping ===\\n\\nIt is intuitive to assume that inheritance creates a semantic \"is a\" relationship, and thus to infer that objects instantiated from subclasses can always be safely used instead of those instantiated from the superclass. This intuition is unfortunately false in most OOP languages, in particular in all those that allow mutable objects. Subtype polymorphism as enforced by the type checker in OOP languages (with mutable objects) cannot guarantee behavioral subtyping in any context. Behavioral subtyping is undecidable in general, so it cannot be implemented by a program (compiler). Class or object hierarchies must be carefully designed, considering possible incorrect uses that cannot be detected syntactically. This issue is known as the Liskov substitution principle.\\n\\n\\n=== Gang of Four design patterns ===\\n\\nDesign Patterns: Elements of Reusable Object-Oriented Software is an influential book published in 1994 by Erich Gamma, Richard Helm, Ralph Johnson, and John Vlissides, often referred to humorously as the \"Gang of Four\". Along with exploring the capabilities and pitfalls of object-oriented programming, it describes 23 common programming problems and patterns for solving them.\\nAs of April 2007, the book was in its 36th printing.\\nThe book describes the following patterns:\\n\\nCreational patterns (5): Factory method pattern, Abstract factory pattern, Singleton pattern, Builder pattern, Prototype pattern\\nStructural patterns (7): Adapter pattern, Bridge pattern, Composite pattern, Decorator pattern, Facade pattern, Flyweight pattern, Proxy pattern\\nBehavioral patterns (11): Chain-of-responsibility pattern, Command pattern, Interpreter pattern, Iterator pattern, Mediator pattern, Memento pattern, Observer pattern, State pattern, Strategy pattern, Template method pattern, Visitor pattern\\n\\n\\n=== Object-orientation and databases ===\\n\\nBoth object-oriented programming and relational database management systems (RDBMSs) are extremely common in software today. Since relational databases don\\'t store objects directly (though some RDBMSs have object-oriented features to approximate this), there is a general need to bridge the two worlds. The problem of bridging object-oriented programming accesses and data patterns with relational databases is known as object-relational impedance mismatch. There are a number of approaches to cope with this problem, but no general solution without downsides. One of the most common approaches is object-relational mapping, as found in IDE languages such as Visual FoxPro and libraries such as Java Data Objects and Ruby on Rails\\' ActiveRecord.\\nThere are also object databases that can be used to replace RDBMSs, but these have not been as technically and commercially successful as RDBMSs.\\n\\n\\n=== Real-world modeling and relationships ===\\nOOP can be used to associate real-world objects and processes with digital counterparts. However, not everyone agrees that OOP facilitates direct real-world mapping (see Criticism section) or that real-world mapping is even a worthy goal; Bertrand Meyer argues in Object-Oriented Software Construction that a program is not a model of the world but a model of some part of the world; \"Reality is a cousin twice removed\". At the same time, some principal limitations of OOP have been noted.\\nFor example, the circle-ellipse problem is difficult to handle using OOP\\'s concept of inheritance.\\nHowever, Niklaus Wirth (who popularized the adage now known as Wirth\\'s law: \"Software is getting slower more rapidly than hardware becomes faster\") said of OOP in his paper, \"Good Ideas through the Looking Glass\", \"This paradigm closely reflects the structure of systems \\'in the real world\\', and it is therefore well suited to model complex systems with complex behaviours\" (contrast KISS principle).\\nSteve Yegge and others noted that natural languages lack the OOP approach of strictly prioritizing things (objects/nouns) before actions (methods/verbs). This problem may cause OOP to suffer more convoluted solutions than procedural programming.\\n\\n\\n=== OOP and control flow ===\\nOOP was developed to increase the reusability and maintainability of source code. Transparent representation of the control flow had no priority and was meant to be handled by a compiler. With the increasing relevance of parallel hardware and multithreaded coding, developing transparent control flow becomes more important, something hard to achieve with OOP.\\n\\n\\n=== Responsibility- vs. data-driven design ===\\nResponsibility-driven design defines classes in terms of a contract, that is, a class should be defined around a responsibility and the information that it shares. This is contrasted by Wirfs-Brock and Wilkerson with data-driven design, where classes are defined around the data-structures that must be held. The authors hold that responsibility-driven design is preferable.\\n\\n\\n=== SOLID and GRASP guidelines ===\\nSOLID is a mnemonic invented by Michael Feathers that stands for and advocates five programming practices:\\n\\nSingle responsibility principle\\nOpen/closed principle\\nLiskov substitution principle\\nInterface segregation principle\\nDependency inversion principleGRASP (General Responsibility Assignment Software Patterns) is another set of guidelines advocated by Craig Larman.\\n\\n\\n== Criticism ==\\nThe OOP paradigm has been criticised for a number of reasons, including not meeting its stated goals of reusability and modularity, and for overemphasizing one aspect of software design and modeling (data/objects) at the expense of other important aspects (computation/algorithms).Luca Cardelli has claimed that OOP code is \"intrinsically less efficient\" than procedural code, that OOP can take longer to compile, and that OOP languages have \"extremely poor modularity properties with respect to class extension and modification\", and tend to be extremely complex. The latter point is reiterated by Joe Armstrong, the principal inventor of Erlang, who is quoted as saying:\\nThe problem with object-oriented languages is they\\'ve got all this implicit environment that they carry around with them. You wanted a banana but what you got was a gorilla holding the banana and the entire jungle.\\nA study by Potok et al. has shown no significant difference in productivity between OOP and procedural approaches.Christopher J. Date stated that critical comparison of OOP to other technologies, relational in particular, is difficult because of lack of an agreed-upon and rigorous definition of OOP; however, Date and Darwen have proposed a theoretical foundation on OOP that uses OOP as a kind of customizable type system to support RDBMS.In an article Lawrence Krubner claimed that compared to other languages (LISP dialects, functional languages, etc.) OOP languages have no unique strengths, and inflict a heavy burden of unneeded complexity.Alexander Stepanov compares object orientation unfavourably to generic programming:\\nI find OOP technically unsound. It attempts to decompose the world in terms of interfaces that vary on a single type. To deal with the real problems you need multisorted algebras — families of interfaces that span multiple types. I find OOP philosophically unsound. It claims that everything is an object. Even if it is true it is not very interesting — saying that everything is an object is saying nothing at all.\\nPaul Graham has suggested that OOP\\'s popularity within large companies is due to \"large (and frequently changing) groups of mediocre programmers\". According to Graham, the discipline imposed by OOP prevents any one programmer from \"doing too much damage\".Leo Brodie has suggested a connection between the standalone nature of objects and a tendency to duplicate code in violation of the don\\'t repeat yourself principle of software development.\\nSteve Yegge noted that, as opposed to functional programming:\\nObject Oriented Programming puts the Nouns first and foremost. Why would you go to such lengths to put one part of speech on a pedestal? Why should one kind of concept take precedence over another? It\\'s not as if OOP has suddenly made verbs less important in the way we actually think. It\\'s a strangely skewed perspective.\\nRich Hickey, creator of Clojure, described object systems as overly simplistic models of the real world. He emphasized the inability of OOP to model time properly, which is getting increasingly problematic as software systems become more concurrent.Eric S. Raymond, a Unix programmer and open-source software advocate, has been critical of claims that present object-oriented programming as the \"One True Solution\", and has written that object-oriented programming languages tend to encourage thickly layered programs that destroy transparency. Raymond compares this unfavourably to the approach taken with Unix and the C programming language.Rob Pike, a programmer involved in the creation of UTF-8 and Go, has called object-oriented programming \"the Roman numerals of computing\" and has said that OOP languages frequently shift the focus from data structures and algorithms to types. Furthermore, he cites an instance of a Java professor whose \"idiomatic\" solution to a problem was to create six new classes, rather than to simply use a lookup table.\\n\\n\\n== Formal semantics ==\\n\\nObjects are the run-time entities in an object-oriented system. They may represent a person, a place, a bank account, a table of data, or any item that the program has to handle.\\nThere have been several attempts at formalizing the concepts used in object-oriented programming. The following concepts and constructs have been used as interpretations of OOP concepts:\\n\\nco algebraic data types\\nabstract data types (which have existential types) allow the definition of modules but these do not support dynamic dispatch\\nrecursive types\\nencapsulated state\\ninheritance\\nrecords are basis for understanding objects if function literals can be stored in fields (like in functional-programming languages), but the actual calculi need be considerably more complex to incorporate essential features of OOP. Several extensions of System F<: that deal with mutable objects have been studied; these allow both subtype polymorphism and parametric polymorphism (generics)Attempts to find a consensus definition or theory behind objects have not proven very successful (however, see Abadi & Cardelli, A Theory of Objects for formal definitions of many OOP concepts and constructs), and often diverge widely. For example, some definitions focus on mental activities, and some on program structuring. One of the simpler definitions is that OOP is the act of using \"map\" data structures or arrays that can contain functions and pointers to other maps, all with some syntactic and scoping sugar on top. Inheritance can be performed by cloning the maps (sometimes called \"prototyping\").\\n\\n\\n== See also ==\\n\\nComparison of programming languages (object-oriented programming)\\nComparison of programming paradigms\\nComponent-based software engineering\\nDesign by contract\\nObject association\\nObject database\\nObject model reference\\nObject modeling language\\nObject-oriented analysis and design\\nObject-relational impedance mismatch (and The Third Manifesto)\\nObject-relational mapping\\n\\n\\n=== Systems ===\\nCADES\\nCommon Object Request Broker Architecture (CORBA)\\nDistributed Component Object Model\\nDistributed Data Management Architecture\\nJeroo\\n\\n\\n=== Modeling languages ===\\nIDEF4\\nInterface description language\\nLepus3\\nUML\\n\\n\\n== References ==\\n\\n\\n== Further reading ==\\nAbadi, Martin; Luca Cardelli (1998). A Theory of Objects. Springer Verlag. ISBN 978-0-387-94775-4.\\nAbelson, Harold; Gerald Jay Sussman (1997). Structure and Interpretation of Computer Programs. MIT Press. ISBN 978-0-262-01153-2.\\nArmstrong, Deborah J. (February 2006). \"The Quarks of Object-Oriented Development\". Communications of the ACM. 49 (2): 123–128. doi:10.1145/1113034.1113040. ISSN 0001-0782. S2CID 11485502.\\nBooch, Grady (1997). Object-Oriented Analysis and Design with Applications. Addison-Wesley. ISBN 978-0-8053-5340-2.\\nEeles, Peter; Oliver Sims (1998). Building Business Objects. John Wiley & Sons. ISBN 978-0-471-19176-6.\\nGamma, Erich; Richard Helm; Ralph Johnson; John Vlissides (1995). Design Patterns: Elements of Reusable Object Oriented Software. Addison-Wesley. Bibcode:1995dper.book.....G. ISBN 978-0-201-63361-0.\\nHarmon, Paul; William Morrissey (1996). The Object Technology Casebook – Lessons from Award-Winning Business Applications. John Wiley & Sons. ISBN 978-0-471-14717-6.\\nJacobson, Ivar (1992). Object-Oriented Software Engineering: A Use Case-Driven Approach. Addison-Wesley. Bibcode:1992oose.book.....J. ISBN 978-0-201-54435-0.\\nKay, Alan. The Early History of Smalltalk. Archived from the original on 4 April 2005. Retrieved 18 April 2005.\\nMeyer, Bertrand (1997). Object-Oriented Software Construction. Prentice Hall. ISBN 978-0-13-629155-8.\\nPecinovsky, Rudolf (2013). OOP – Learn Object Oriented Thinking & Programming. Bruckner Publishing. ISBN 978-80-904661-8-0.\\nRumbaugh, James; Michael Blaha; William Premerlani; Frederick Eddy; William Lorensen (1991). Object-Oriented Modeling and Design. Prentice Hall. ISBN 978-0-13-629841-0.\\nSchach, Stephen (2006). Object-Oriented and Classical Software Engineering, Seventh Edition. McGraw-Hill. ISBN 978-0-07-319126-3.\\nSchreiner, Axel-Tobias (1993). Object oriented programming with ANSI-C. Hanser. hdl:1850/8544. ISBN 978-3-446-17426-9.\\nTaylor, David A. (1992). Object-Oriented Information Systems – Planning and Implementation. John Wiley & Sons. ISBN 978-0-471-54364-0.\\nWeisfeld, Matt (2009). The Object-Oriented Thought Process, Third Edition. Addison-Wesley. ISBN 978-0-672-33016-2.\\nWest, David (2004). Object Thinking (Developer Reference). Microsoft Press. ISBN 978-0-7356-1965-4.\\n\\n\\n== External links ==\\nIntroduction to Object Oriented Programming Concepts (OOP) and More by L.W.C. Nirosh\\nDiscussion about the flaws of OOD\\nOOP Concepts (Java Tutorials)', 'Computer science is the study of algorithmic processes, computational machines and computation itself. As a discipline, computer science spans a range of topics from theoretical studies of algorithms, computation and information to the practical issues of implementing computational systems in hardware and software.Its fields can be divided into theoretical and practical disciplines. For example, the theory of computation concerns abstract models of computation and general classes of problems that can be solved using them, while computer graphics or computational geometry emphasize more specific applications. Algorithms and data structures have been called the heart of computer science. Programming language theory considers approaches to the description of computational processes, while computer programming involves the use of them to create complex systems. Computer architecture describes construction of computer components and computer-operated equipment. Artificial intelligence aims to synthesize goal-orientated processes such as problem-solving, decision-making, environmental adaptation, planning and learning found in humans and animals. A digital computer is capable of simulating various information processes. The fundamental concern of computer science is determining what can and cannot be automated. Computer scientists usually focus on academic research. The Turing Award is generally recognized as the highest distinction in computer sciences.\\n\\n\\n== History ==\\n\\nThe earliest foundations of what would become computer science predate the invention of the modern digital computer. Machines for calculating fixed numerical tasks such as the abacus have existed since antiquity, aiding in computations such as multiplication and division. Algorithms for performing computations have existed since antiquity, even before the development of sophisticated computing equipment. \\nWilhelm Schickard designed and constructed the first working mechanical calculator in 1623. In 1673, Gottfried Leibniz demonstrated a digital mechanical calculator, called the Stepped Reckoner. Leibniz may be considered the first computer scientist and information theorist, for, among other reasons, documenting the binary number system. In 1820, Thomas de Colmar launched the mechanical calculator industry when he invented his simplified arithmometer, the first calculating machine strong enough and reliable enough to be used daily in an office environment. Charles Babbage started the design of the first automatic mechanical calculator, his Difference Engine, in 1822, which eventually gave him the idea of the first programmable mechanical calculator, his Analytical Engine. He started developing this machine in 1834, and \"in less than two years, he had sketched out many of the salient features of the modern computer\". \"A crucial step was the adoption of a punched card system derived from the Jacquard loom\" making it infinitely programmable. In 1843, during the translation of a French article on the Analytical Engine, Ada Lovelace wrote, in one of the many notes she included, an algorithm to compute the Bernoulli numbers, which is considered to be the first published algorithm ever specifically tailored for implementation on a computer. Around 1885, Herman Hollerith invented the tabulator, which used punched cards to process statistical information; eventually his company became part of IBM. Following Babbage, although unaware of his earlier work, Percy Ludgate in 1909 published the 2nd of the only two designs for mechanical analytical engines in history. In 1937, one hundred years after Babbage\\'s impossible dream, Howard Aiken convinced IBM, which was making all kinds of punched card equipment and was also in the calculator business to develop his giant programmable calculator, the ASCC/Harvard Mark I, based on Babbage\\'s Analytical Engine, which itself used cards and a central computing unit. When the machine was finished, some hailed it as \"Babbage\\'s dream come true\".\\nDuring the 1940s, with the development of new and more powerful computing machines such as the Atanasoff–Berry computer and ENIAC, the term computer came to refer to the machines rather than their human predecessors. As it became clear that computers could be used for more than just mathematical calculations, the field of computer science broadened to study computation in general. In 1945, IBM founded the Watson Scientific Computing Laboratory at Columbia University in New York City. The renovated fraternity house on Manhattan\\'s West Side was IBM\\'s first laboratory devoted to pure science. The lab is the forerunner of IBM\\'s Research Division, which today operates research facilities around the world. Ultimately, the close relationship between IBM and the university was instrumental in the emergence of a new scientific discipline, with Columbia offering one of the first academic-credit courses in computer science in 1946. Computer science began to be established as a distinct academic discipline in the 1950s and early 1960s. The world\\'s first computer science degree program, the Cambridge Diploma in Computer Science, began at the University of Cambridge Computer Laboratory in 1953. The first computer science department in the United States was formed at Purdue University in 1962. Since practical computers became available, many applications of computing have become distinct areas of study in their own rights.\\n\\n\\n== Etymology ==\\n\\nAlthough first proposed in 1956, the term \"computer science\" appears in a 1959 article in Communications of the ACM,\\nin which Louis Fein argues for the creation of a Graduate School in Computer Sciences analogous to the creation of Harvard Business School in 1921, justifying the name by arguing that, like management science, the subject is applied and interdisciplinary in nature, while having the characteristics typical of an academic discipline.\\nHis efforts, and those of others such as numerical analyst George Forsythe, were rewarded: universities went on to create such departments, starting with Purdue in 1962. Despite its name, a significant amount of computer science does not involve the study of computers themselves. Because of this, several alternative names have been proposed. Certain departments of major universities prefer the term computing science, to emphasize precisely that difference. Danish scientist Peter Naur suggested the term datalogy, to reflect the fact that the scientific discipline revolves around data and data treatment, while not necessarily involving computers. The first scientific institution to use the term was the Department of Datalogy at the University of Copenhagen, founded in 1969, with Peter Naur being the first professor in datalogy. The term is used mainly in the Scandinavian countries. An alternative term, also proposed by Naur, is data science; this is now used for a multi-disciplinary field of data analysis, including statistics and databases.\\nIn the early days of computing, a number of terms for the practitioners of the field of computing were suggested in the Communications of the ACM—turingineer, turologist, flow-charts-man, applied meta-mathematician, and applied epistemologist. Three months later in the same journal, comptologist was suggested, followed next year by hypologist. The term computics has also been suggested. In Europe, terms derived from contracted translations of the expression \"automatic information\" (e.g. \"informazione automatica\" in Italian) or \"information and mathematics\" are often used, e.g. informatique (French), Informatik (German), informatica (Italian, Dutch), informática (Spanish, Portuguese), informatika (Slavic languages and Hungarian) or pliroforiki (πληροφορική, which means informatics) in Greek. Similar words have also been adopted in the UK (as in the School of Informatics, University of Edinburgh). \"In the U.S., however, informatics is linked with applied computing, or computing in the context of another domain.\"A folkloric quotation, often attributed to—but almost certainly not first formulated by—Edsger Dijkstra, states that \"computer science is no more about computers than astronomy is about telescopes.\" The design and deployment of computers and computer systems is generally considered the province of disciplines other than computer science. For example, the study of computer hardware is usually considered part of computer engineering, while the study of commercial computer systems and their deployment is often called information technology or information systems. However, there has been much cross-fertilization of ideas between the various computer-related disciplines. Computer science research also often intersects other disciplines, such as philosophy, cognitive science, linguistics, mathematics, physics, biology, Earth science, statistics, and logic.\\nComputer science is considered by some to have a much closer relationship with mathematics than many scientific disciplines, with some observers saying that computing is a mathematical science. Early computer science was strongly influenced by the work of mathematicians such as Kurt Gödel, Alan Turing, John von Neumann, Rózsa Péter and Alonzo Church and there continues to be a useful interchange of ideas between the two fields in areas such as mathematical logic, category theory, domain theory, and algebra.The relationship between Computer Science and Software Engineering is a contentious issue, which is further muddied by disputes over what the term \"Software Engineering\" means, and how computer science is defined. David Parnas, taking a cue from the relationship between other engineering and science disciplines, has claimed that the principal focus of computer science is studying the properties of computation in general, while the principal focus of software engineering is the design of specific computations to achieve practical goals, making the two separate but complementary disciplines.The academic, political, and funding aspects of computer science tend to depend on whether a department is formed with a mathematical emphasis or with an engineering emphasis. Computer science departments with a mathematics emphasis and with a numerical orientation consider alignment with computational science. Both types of departments tend to make efforts to bridge the field educationally if not across all research.\\n\\n\\n== Philosophy ==\\n\\n\\n=== Epistemology of computer science ===\\nDespite the word \"science\" in its name, there is debate over whether or not computer science is a discipline of science, mathematics, or engineering. Allen Newell and Herbert A. Simon argued in 1975, Computer science is an empirical discipline. We would have called it an experimental science, but like astronomy, economics, and geology, some of its unique forms of observation and experience do not fit a narrow stereotype of the experimental method. Nonetheless, they are experiments. Each new machine that is built is an experiment. Actually constructing the machine poses a question to nature; and we listen for the answer by observing the machine in operation and analyzing it by all analytical and measurement means available. It has since been argued that computer science can be classified as an empirical science since it makes use of empirical testing to evaluate the correctness of programs, but a problem remains in defining the laws and theorems of computer science (if any exist) and defining the nature of experiments in computer science. Proponents of classifying computer science as an engineering discipline argue that the reliability of computational systems is investigated in the same way as bridges in civil engineering and airplanes in aerospace engineering. They also argue that while empirical sciences observe what presently exists, computer science observes what is possible to exist and while scientists discover laws from observation, no proper laws have been found in computer science and it is instead concerned with creating phenomena.Proponents of classifying computer science as a mathematical discipline argue that computer programs are physical realizations of mathematical entities and programs can be deductively reasoned through mathematical formal methods. Computer scientists Edsger W. Dijkstra and Tony Hoare regard instructions for computer programs as mathematical sentences and interpret formal semantics for programming languages as mathematical axiomatic systems.\\n\\n\\n=== Paradigms of computer science ===\\nA number of computer scientists have argued for the distinction of three separate paradigms in computer science. Peter Wegner argued that those paradigms are science, technology, and mathematics. Peter Denning\\'s working group argued that they are theory, abstraction (modeling), and design. Amnon H. Eden described them as the \"rationalist paradigm\" (which treats computer science as a branch of mathematics, which is prevalent in theoretical computer science, and mainly employs deductive reasoning), the \"technocratic paradigm\" (which might be found in engineering approaches, most prominently in software engineering), and the \"scientific paradigm\" (which approaches computer-related artifacts from the empirical perspective of natural sciences, identifiable in some branches of artificial intelligence).\\nComputer science focuses on methods involved in design, specification, programming, verification, implementation and testing of human-made computing systems.\\n\\n\\n== Fields ==\\nComputer science is no more about computers than astronomy is about telescopes.\\n\\nAs a discipline, computer science spans a range of topics from theoretical studies of algorithms and the limits of computation to the practical issues of implementing computing systems in hardware and software.CSAB, formerly called Computing Sciences Accreditation Board—which is made up of representatives of the Association for Computing Machinery (ACM), and the IEEE Computer Society (IEEE CS)—identifies four areas that it considers crucial to the discipline of computer science: theory of computation, algorithms and data structures, programming methodology and languages, and computer elements and architecture. In addition to these four areas, CSAB also identifies fields such as software engineering, artificial intelligence, computer networking and communication, database systems, parallel computation, distributed computation, human–computer interaction, computer graphics, operating systems, and numerical and symbolic computation as being important areas of computer science.\\n\\n\\n=== Theoretical computer science ===\\n\\nTheoretical Computer Science is mathematical and abstract in spirit, but it derives its motivation from the practical and everyday computation. Its aim is to understand the nature of computation and, as a consequence of this understanding, provide more efficient methodologies.\\n\\n\\n==== Theory of computation ====\\n\\nAccording to Peter Denning, the fundamental question underlying computer science is, \"What can be automated?\" Theory of computation is focused on answering fundamental questions about what can be computed and what amount of resources are required to perform those computations. In an effort to answer the first question, computability theory examines which computational problems are solvable on various theoretical models of computation. The second question is addressed by computational complexity theory, which studies the time and space costs associated with different approaches to solving a multitude of computational problems.\\nThe famous P = NP? problem, one of the Millennium Prize Problems, is an open problem in the theory of computation.\\n\\n\\n==== Information and coding theory ====\\n\\nInformation theory, closely related to probability and statistics, is related to the quantification of information. This was developed by Claude Shannon to find fundamental limits on signal processing operations such as compressing data and on reliably storing and communicating data.\\nCoding theory is the study of the properties of codes (systems for converting information from one form to another) and their fitness for a specific application. Codes are used for data compression, cryptography, error detection and correction, and more recently also for network coding. Codes are studied for the purpose of designing efficient and reliable data transmission methods.\\n\\n\\n==== Data structures and algorithms ====\\nData structures and algorithms are the studies of commonly used computational methods and their computational efficiency.\\n\\n\\n==== Programming language theory and formal methods ====\\n\\nProgramming language theory is a branch of computer science that deals with the design, implementation, analysis, characterization, and classification of programming languages and their individual features. It falls within the discipline of computer science, both depending on and affecting mathematics, software engineering, and linguistics. It is an active research area, with numerous dedicated academic journals.\\nFormal methods are a particular kind of mathematically based technique for the specification, development and verification of software and hardware systems. The use of formal methods for software and hardware design is motivated by the expectation that, as in other engineering disciplines, performing appropriate mathematical analysis can contribute to the reliability and robustness of a design. They form an important theoretical underpinning for software engineering, especially where safety or security is involved. Formal methods are a useful adjunct to software testing since they help avoid errors and can also give a framework for testing. For industrial use, tool support is required. However, the high cost of using formal methods means that they are usually only used in the development of high-integrity and life-critical systems, where safety or security is of utmost importance. Formal methods are best described as the application of a fairly broad variety of theoretical computer science fundamentals, in particular logic calculi, formal languages, automata theory, and program semantics, but also type systems and algebraic data types to problems in software and hardware specification and verification.\\n\\n\\n=== Computer systems and computational processes ===\\n\\n\\n==== Artificial intelligence ====\\n\\nArtificial intelligence (AI) aims to or is required to synthesize goal-orientated processes such as problem-solving, decision-making, environmental adaptation, learning, and communication found in humans and animals. From its origins in cybernetics and in the Dartmouth Conference (1956), artificial intelligence research has been necessarily cross-disciplinary, drawing on areas of expertise such as applied mathematics, symbolic logic, semiotics, electrical engineering, philosophy of mind, neurophysiology, and social intelligence. AI is associated in the popular mind with robotic development, but the main field of practical application has been as an embedded component in areas of software development, which require computational understanding. The starting point in the late 1940s was Alan Turing\\'s question \"Can computers think?\", and the question remains effectively unanswered, although the Turing test is still used to assess computer output on the scale of human intelligence. But the automation of evaluative and predictive tasks has been increasingly successful as a substitute for human monitoring and intervention in domains of computer application involving complex real-world data.\\n\\n\\n==== Computer architecture and organization ====\\n\\nComputer architecture, or digital computer organization, is the conceptual design and fundamental operational structure of a computer system. It focuses largely on the way by which the central processing unit performs internally and accesses addresses in memory. Computer engineers study computational logic and design of computer hardware, from individual processor components, microcontrollers, personal computers to supercomputers and embedded systems. The term “architecture” in computer literature can be traced to the work of Lyle R. Johnson and Frederick P. Brooks, Jr., members of the Machine Organization department in IBM\\'s main research center in 1959.\\n\\n\\n==== Concurrent, parallel and distributed computing ====\\n\\nConcurrency is a property of systems in which several computations are executing simultaneously, and potentially interacting with each other. A number of mathematical models have been developed for general concurrent computation including Petri nets, process calculi and the Parallel Random Access Machine model. When multiple computers are connected in a network while using concurrency, this is known as a distributed system. Computers within that distributed system have their own private memory, and information can be exchanged to achieve common goals.\\n\\n\\n==== Computer networks ====\\n\\nThis branch of computer science aims to manage networks between computers worldwide.\\n\\n\\n==== Computer security and cryptography ====\\n\\nComputer security is a branch of computer technology with the objective of protecting information from unauthorized access, disruption, or modification while maintaining the accessibility and usability of the system for its intended users. Cryptography is the practice and study of hiding (encryption) and therefore deciphering (decryption) information. Modern cryptography is largely related to computer science, for many encryption and decryption algorithms are based on their computational complexity.\\n\\n\\n==== Databases and data mining ====\\n\\nA database is intended to organize, store, and retrieve large amounts of data easily. Digital databases are managed using database management systems to store, create, maintain, and search data, through database models and query languages. Data mining is a process of discovering patterns in large data sets.\\n\\n\\n==== Computer graphics and visualization ====\\n\\nComputer graphics is the study of digital visual contents and involves the synthesis and manipulation of image data. The study is connected to many other fields in computer science, including computer vision, image processing, and computational geometry, and is heavily applied in the fields of special effects and video games.\\n\\n\\n==== Image and sound processing ====\\n\\nInformation can take the form of images, sound, video or other multimedia. Bits of information can be streamed via signals. Its processing is the central notion of informatics, the European view on computing, which studies information processing algorithms independently of the type of information carrier - whether it is electrical, mechanical or biological. This field plays important role in information theory, telecommunications, information engineering and has applications in medical image computing and speech synthesis, among others. What is the lower bound on the complexity of fast Fourier transform algorithms? is one of unsolved problems in theoretical computer science.\\n\\n\\n=== Applied computer science ===\\n\\n\\n==== Computational science, finance and engineering ====\\n\\nScientific computing (or computational science) is the field of study concerned with constructing mathematical models and quantitative analysis techniques and using computers to analyze and solve scientific problems. A major usage of scientific computing is simulation of various processes, including computational fluid dynamics, physical, electrical, and electronic systems and circuits, as well as societies and social situations (notably war games) along with their habitats, among many others. Modern computers enable optimization of such designs as complete aircraft. Notable in electrical and electronic circuit design are SPICE, as well as software for physical realization of new (or modified) designs. The latter includes essential design software for integrated circuits.\\n\\n\\n==== Social computing and human–computer interaction ====\\n\\nSocial computing is an area that is concerned with the intersection of social behavior and computational systems. Human–computer interaction research develops theories, principles, and guidelines for user interface designers.\\n\\n\\n==== Software engineering ====\\n\\nSoftware engineering is the study of designing, implementing, and modifying the software in order to ensure it is of high quality, affordable, maintainable, and fast to build. It is a systematic approach to software design, involving the application of engineering practices to software. Software engineering deals with the organizing and analyzing of software—it doesn\\'t just deal with the creation or manufacture of new software, but its internal arrangement and maintenance. For example software testing, systems engineering, technical debt and software development processes.\\n\\n\\n== Discoveries ==\\nThe philosopher of computing Bill Rapaport noted three Great Insights of Computer Science:\\nGottfried Wilhelm Leibniz\\'s, George Boole\\'s, Alan Turing\\'s, Claude Shannon\\'s, and Samuel Morse\\'s insight: there are only two objects that a computer has to deal with in order to represent \"anything\".All the information about any computable problem can be represented using only 0 and 1 (or any other bistable pair that can flip-flop between two easily distinguishable states, such as \"on/off\", \"magnetized/de-magnetized\", \"high-voltage/low-voltage\", etc.).\\nAlan Turing\\'s insight: there are only five actions that a computer has to perform in order to do \"anything\".Every algorithm can be expressed in a language for a computer consisting of only five basic instructions:move left one location;\\nmove right one location;\\nread symbol at current location;\\nprint 0 at current location;\\nprint 1 at current location.\\nCorrado Böhm and Giuseppe Jacopini\\'s insight: there are only three ways of combining these actions (into more complex ones) that are needed in order for a computer to do \"anything\".Only three rules are needed to combine any set of basic instructions into more complex ones:\\nsequence: first do this, then do that;\\n selection: IF such-and-such is the case, THEN do this, ELSE do that;\\nrepetition: WHILE such-and-such is the case, DO this.\\nNote that the three rules of Boehm\\'s and Jacopini\\'s insight can be further simplified with the use of goto (which means it is more elementary than structured programming).\\n\\n\\n== Programming paradigms ==\\n\\nProgramming languages can be used to accomplish different tasks in different ways. Common programming paradigms include:\\n\\nFunctional programming, a style of building the structure and elements of computer programs that treats computation as the evaluation of mathematical functions and avoids state and mutable data. It is a declarative programming paradigm, which means programming is done with expressions or declarations instead of statements.\\nImperative programming, a programming paradigm that uses statements that change a program\\'s state. In much the same way that the imperative mood in natural languages expresses commands, an imperative program consists of commands for the computer to perform. Imperative programming focuses on describing how a program operates.\\nObject-oriented programming, a programming paradigm based on the concept of \"objects\", which may contain data, in the form of fields, often known as attributes; and code, in the form of procedures, often known as methods. A feature of objects is that an object\\'s procedures can access and often modify the data fields of the object with which they are associated. Thus object-oriented computer programs are made out of objects that interact with one another.\\nService-oriented programming, a programming paradigm that uses \"services\" as the unit of computer work, to design and implement integrated business applications and mission critical software programsMany languages offer support for multiple paradigms, making the distinction more a matter of style than of technical capabilities.\\n\\n\\n== Academia ==\\n\\nConferences are important events for computer science research. During these conferences, researchers from the public and private sectors present their recent work and meet. Unlike in most other academic fields, in computer science, the prestige of conference papers is greater than that of journal publications. One proposed explanation for this is the quick development of this relatively new field requires rapid review and distribution of results, a task better handled by conferences than by journals.\\n\\n\\n== Education ==\\n\\nComputer Science, known by its near synonyms, Computing, Computer Studies, has been taught in UK schools since the days of batch processing, mark sensitive cards and paper tape but usually to a select few students. In 1981, the BBC produced a micro-computer and classroom network and Computer Studies became common for GCE O level students (11–16-year-old), and Computer Science to A level students. Its importance was recognised, and it became a compulsory part of the National Curriculum, for Key Stage 3 & 4. In September 2014 it became an entitlement for all pupils over the age of 4.In the US, with 14,000 school districts deciding the curriculum, provision was fractured. According to a 2010 report by the Association for Computing Machinery (ACM) and Computer Science Teachers Association (CSTA), only 14 out of 50 states have adopted significant education standards for high school computer science.Israel, New Zealand, and South Korea have included computer science in their national secondary education curricula, and several others are following.\\n\\n\\n== See also ==\\n\\n\\n== Notes ==\\n\\n\\n== References ==\\n\\n\\n== Further reading ==\\n\\n\\n== External links ==\\n\\nComputer science at Curlie\\nScholarly Societies in Computer Science\\nWhat is Computer Science?\\nBest Papers Awards in Computer Science since 1996\\nPhotographs of computer scientists by Bertrand Meyer\\nEECS.berkeley.edu\\n\\n\\n=== Bibliography and academic search engines ===\\nCiteSeerx (article): search engine, digital library and repository for scientific and academic papers with a focus on computer and information science.\\nDBLP Computer Science Bibliography (article): computer science bibliography website hosted at Universität Trier, in Germany.\\nThe Collection of Computer Science Bibliographies (Collection of Computer Science Bibliographies)\\n\\n\\n=== Professional organizations ===\\nAssociation for Computing Machinery\\nIEEE Computer Society\\nInformatics Europe\\nAAAI\\nAAAS Computer Science\\n\\n\\n=== Misc ===\\nComputer Science—Stack Exchange: a community-run question-and-answer site for computer science\\nWhat is computer science\\nIs computer science science?\\nComputer Science (Software) Must be Considered as an Independent Discipline.', 'In object-oriented programming, inheritance is the mechanism of basing an object or class upon another object (prototype-based inheritance) or class (class-based inheritance), retaining similar implementation. Also defined as deriving new classes (sub classes) from existing ones such as super class or base class and then forming them into a hierarchy of classes. In most class-based object-oriented languages, an object created through inheritance, a \"child object\", acquires all the properties and behaviors of the \"parent object\" , with the exception of: constructors, destructor, overloaded operators and friend functions of the base class. Inheritance allows programmers to create classes that are built upon existing classes, to specify a new implementation while maintaining the same behaviors (realizing an interface), to reuse code and to independently extend original software via public classes and interfaces. The relationships of objects or classes through inheritance give rise to a directed graph.\\nInheritance was invented in 1969 for Simula and is now used throughout many object-oriented programming languages such as Java, C++, PHP and Python.\\nAn inherited class is called a subclass of its parent class or super class. The term \"inheritance\" is loosely used for both class-based and prototype-based programming, but in narrow use the term is reserved for class-based programming (one class inherits from another), with the corresponding technique in prototype-based programming being instead called delegation (one object delegates to another).\\nInheritance should not be confused with subtyping. In some languages inheritance and subtyping agree, whereas in others they differ; in general, subtyping establishes an is-a relationship, whereas inheritance only reuses implementation and establishes a syntactic relationship, not necessarily a semantic relationship (inheritance does not ensure behavioral subtyping). To distinguish these concepts, subtyping is sometimes referred to as interface inheritance (without acknowledging that the specialization of type variables also induces a subtyping relation), whereas inheritance as defined here is known as implementation inheritance or code inheritance. Still, inheritance is a commonly used mechanism for establishing subtype relationships.Inheritance is contrasted with object composition, where one object contains another object (or objects of one class contain objects of another class); see composition over inheritance. Composition implements a has-a relationship, in contrast to the is-a relationship of subtyping.\\n\\n\\n== Types ==\\n\\nThere are various types of inheritance, based on paradigm and specific language.\\nSingle inheritance\\nwhere subclasses inherit the features of one superclass. A class acquires the properties of another class.\\nMultiple inheritance\\nwhere one class can have more than one superclass and inherit features from all parent classes.\\n\"Multiple inheritance ... was widely supposed to be very difficult to implement efficiently. For example, in a summary of C++ in his book on Objective C, Brad Cox actually claimed that adding multiple inheritance to C++ was impossible. Thus, multiple inheritance seemed more of a challenge. Since I had considered multiple inheritance as early as 1982 and found a simple and efficient implementation technique in 1984, I couldn\\'t resist the challenge. I suspect this to be the only case in which fashion affected the sequence of events.\"\\nMultilevel inheritance\\nwhere a subclass is inherited from another subclass. It is not uncommon that a class is derived from another derived class as shown in the figure \"Multilevel inheritance\".\\n\\nThe class A serves as a base class for the derived class B, which in turn serves as a base class for the derived class C. The class B is known as intermediate base class because it provides a link for the inheritance between A and C. The chain ABC is known as inheritance path.\\nA derived class with multilevel inheritance is declared as follows:\\n\\nThis process can be extended to any number of levels.\\nHierarchical inheritance\\nThis is where one class serves as a superclass (base class) for more than one sub class. For example, a parent class, A, can have two subclasses B and C. Both B and C\\'s parent class is A, but B and C are two separate subclasses.\\nHybrid inheritance\\nHybrid inheritance is when a mix of two or more of the above types of inheritance occurs. An example of this is when class A has a subclass B which has two subclasses, C and D. This is a mixture of both multilevel inheritance and hierarchal inheritance.\\n\\n\\n== Subclasses and superclasses ==\\nSubclasses, derived classes, heir classes, or child classes are modular derivative classes that inherits one or more language entities from one or more other classes (called superclass, base classes, or parent classes). The semantics of class inheritance vary from language to language, but commonly the subclass automatically inherits the instance variables and member functions of its superclasses. \\nThe general form of defining a derived class is:\\n\\nThe colon indicates that the subclass inherits from the superclass. The visibility is optional and, if present, may be either private or public. The default visibility is private. Visibility specifies whether the features of the base class are privately derived or publicly derived.Some languages support also the inheritance of other constructs. For example, in Eiffel, contracts that define the specification of a class are also inherited by heirs. The superclass establishes a common interface and foundational functionality, which specialized subclasses can inherit, modify, and supplement. The software inherited by a subclass is considered reused in the subclass. A reference to an instance of a class may actually be referring to one of its subclasses. The actual class of the object being referenced is impossible to predict at compile-time. A uniform interface is used to invoke the member functions of objects of a number of different classes. Subclasses may replace superclass functions with entirely new functions that must share the same method signature.\\n\\n\\n=== Non-subclassable classes ===\\nIn some languages a class may be declared as  non-subclassable by adding certain class modifiers to the class declaration. Examples include the final keyword in Java and C++11 onwards or the sealed keyword in C#. Such modifiers are added to the class declaration before the class keyword and the class identifier declaration. Such non-subclassable classes restrict reusability, particularly when developers only have access to precompiled binaries and not source code.\\nA non-subclassable class has no subclasses, so it can be easily deduced at compile time that references or pointers to objects of that class are actually referencing instances of that class and not instances of subclasses (they don\\'t exist) or instances of superclasses (upcasting a reference type violates the type system). Because the exact type of the object being referenced is known before execution, early binding (also called static dispatch) can be used instead of late binding (also called dynamic dispatch), which requires one or more virtual method table lookups depending on whether multiple inheritance or only single inheritance are supported in the programming language that is being used.\\n\\n\\n=== Non-overridable methods ===\\nJust as classes may be non-subclassable, method declarations may contain method modifiers that prevent the method from being overridden (i.e. replaced with a new function with the same name and type signature in a subclass). A private method is un-overridable simply because it is not accessible by classes other than the class it is a member function of (this is not true for C++, though). A final method in Java, a sealed method in C# or a frozen feature in Eiffel cannot be overridden.\\n\\n\\n=== Virtual methods ===\\nIf the superclass method is a virtual method, then invocations of the superclass method will be dynamically dispatched. Some languages require that methods be specifically declared as virtual (e.g. C++), and in others, all methods are virtual (e.g. Java). An invocation of a non-virtual method will always be statically dispatched (i.e. the address of the function call is determined at compile-time). Static dispatch is faster than dynamic dispatch and allows optimizations such as inline expansion.\\n\\n\\n== Visibility of inherited members ==\\nThe following table shows which variables and functions get inherited dependent on the visibility given when deriving the class.\\n\\n\\n== Applications ==\\nInheritance is used to co-relate two or more classes to each other. \\n\\n\\n=== Overriding ===\\n\\nMany object-oriented programming languages permit a class or object to replace the implementation of an aspect—typically a behavior—that it has inherited. This process is called overriding. Overriding introduces a complication: which version of the behavior does an instance of the inherited class use—the one that is part of its own class, or the one from the parent (base) class? The answer varies between programming languages, and some languages provide the ability to indicate that a particular behavior is not to be overridden and should behave as defined by the base class. For instance, in C#, the base method or property can only be overridden in a subclass if it is marked with the virtual, abstract, or override modifier, while in programming languages such as Java, different methods can be called to override other methods. An alternative to overriding is hiding the inherited code.\\n\\n\\n=== Code reuse ===\\nImplementation inheritance is the mechanism whereby a subclass re-uses code in a base class. By default the subclass retains all of the operations of the base class, but the subclass may override some or all operations, replacing the base-class implementation with its own.\\nIn the following Python example, subclasses SquareSumComputer and CubeSumComputer override the transform() method of the base class SumComputer. The base class comprises operations to compute the sum of the squares between two integers. The subclass re-uses all of the functionality of the base class with the exception of the operation that transforms a number into its square, replacing it with an operation that transforms a number into its square and cube respectively. The subclasses therefore compute the sum of the squares/cubes between two integers.\\nBelow is an example of Python.\\n\\nIn most quarters, class inheritance for the sole purpose of code reuse has fallen out of favor. The primary concern is that implementation inheritance does not provide any assurance of polymorphic substitutability—an instance of the reusing class cannot necessarily be substituted for an instance of the inherited class. An alternative technique, explicit delegation, requires more programming effort, but avoids the substitutability issue. In C++ private inheritance can be used as a form of implementation inheritance without substitutability. Whereas public inheritance represents an \"is-a\" relationship and delegation represents a \"has-a\" relationship, private (and protected) inheritance can be thought of as an \"is implemented in terms of\" relationship.Another frequent use of inheritance is to guarantee that classes maintain a certain common interface; that is, they implement the same methods. The parent class can be a combination of implemented operations and operations that are to be implemented in the child classes. Often, there is no interface change between the supertype and subtype- the child implements the behavior described instead of its parent class.\\n\\n\\n== Inheritance vs subtyping ==\\n\\nInheritance is similar to but distinct from subtyping.\\nSubtyping enables a given type to be substituted for another type or abstraction, and is said to establish an is-a relationship between the subtype and some existing abstraction, either implicitly or explicitly, depending on language support. The relationship can be expressed explicitly via inheritance in languages that support inheritance as a subtyping mechanism. For example, the following C++ code establishes an explicit inheritance relationship between classes B and A, where B is both a subclass and a subtype of A, and can be used as an A wherever a B is specified (via a reference, a pointer or the object itself).\\n\\nIn programming languages that do not support inheritance as a subtyping mechanism, the relationship between a base class and a derived class is only a relationship between implementations (a mechanism for code reuse), as compared to a relationship between types. Inheritance, even in programming languages that support inheritance as a subtyping mechanism, does not necessarily entail behavioral subtyping. It is entirely possible to derive a class whose object will behave incorrectly when used in a context where the parent class is expected; see the Liskov substitution principle.\\n (Compare connotation/denotation.) In some OOP languages, the notions of code reuse and subtyping coincide because the only way to declare a subtype is to define a new class that inherits the implementation of another.\\n\\n\\n=== Design constraints ===\\nUsing inheritance extensively in designing a program imposes certain constraints.\\nFor example, consider a class Person that contains a person\\'s name, date of birth, address and phone number. We can define a subclass of Person called Student that contains the person\\'s grade point average and classes taken, and another subclass of Person called Employee that contains the person\\'s job-title, employer, and salary.\\nIn defining this inheritance hierarchy we have already defined certain restrictions, not all of which are desirable:\\n\\nSingleness\\nUsing single inheritance, a subclass can inherit from only one superclass. Continuing the example given above, Person can be either a Student or an Employee, but not both. Using multiple inheritance partially solves this problem, as one can then define a StudentEmployee class that inherits from both Student and Employee. However, in most implementations, it can still inherit from each superclass only once, and thus, does not support cases in which a student has two jobs or attends two institutions. The inheritance model available in Eiffel makes this possible through support for repeated inheritance.\\nStatic\\nThe inheritance hierarchy of an object is fixed at instantiation when the object\\'s type is selected and does not change with time. For example, the inheritance graph does not allow a Student object to become a Employee object while retaining the state of its Person superclass. (This kind of behavior, however, can be achieved with the decorator pattern.) Some have criticized inheritance, contending that it locks developers into their original design standards.\\nVisibility\\nWhenever client code has access to an object, it generally has access to all the object\\'s superclass data. Even if the superclass has not been declared public, the client can still cast the object to its superclass type. For example, there is no way to give a function a pointer to a Student\\'s grade point average and transcript without also giving that function access to all of the personal data stored in the student\\'s Person superclass. Many modern languages, including C++ and Java, provide a \"protected\" access modifier that allows subclasses to access the data, without allowing any code outside the chain of inheritance to access it.The composite reuse principle is an alternative to inheritance. This technique supports polymorphism and code reuse by separating behaviors from the primary class hierarchy and including specific behavior classes as required in any business domain class. This approach avoids the static nature of a class hierarchy by allowing behavior modifications at run time and allows one class to implement behaviors buffet-style, instead of being restricted to the behaviors of its ancestor classes.\\n\\n\\n== Issues and alternatives ==\\nImplementation inheritance is controversial among programmers and theoreticians of object-oriented programming since at least the 1990s. Among them are the authors of Design Patterns, who advocate interface inheritance instead, and favor composition over inheritance. For example, the decorator pattern (as mentioned above) has been proposed to overcome the static nature of inheritance between classes. As a more fundamental solution to the same problem, role-oriented programming introduces a distinct relationship, played-by, combining properties of inheritance and composition into a new concept.According to Allen Holub, the main problem with implementation inheritance is that it introduces unnecessary coupling in the form of the \"fragile base class problem\": modifications to the base class implementation can cause inadvertent behavioral changes in subclasses. Using interfaces avoids this problem because no implementation is shared, only the API. Another way of stating this is that \"inheritance breaks encapsulation\". The problem surfaces clearly in open object-oriented systems such as frameworks, where client code is expected to inherit from system-supplied classes and then substituted for the system\\'s classes in its algorithms.Reportedly, Java inventor James Gosling has spoken against implementation inheritance, stating that he would not include it if he were to redesign Java. Language designs that decouple inheritance from subtyping (interface inheritance) appeared as early as 1990; a modern example of this is the Go programming language.\\nComplex inheritance, or inheritance used within an insufficiently mature design, may lead to the yo-yo problem.  When inheritance was used as a primary approach to structure code in a system in the late 1990s, developers naturally started to break code into multiple layers of inheritance as the system functionality grew.  If a development team combined multiple layers of inheritance with the single responsibility principle it created many super thin layers of code, many which would only have 1 or 2 lines of code in each layer.  Too many layers make debugging a significant challenge, as it becomes hard to determine which layer needs to be debugged.\\nAnother issue with inheritance is that subclasses must be defined in code, which means that program users cannot add new subclasses at runtime. Other design patterns (such as Entity–component–system) allow program users to define variations of an entity at runtime.\\n\\n\\n== See also ==\\nArchetype pattern\\nCircle–ellipse problem\\nDefeasible reasoning – Reasoning that is rationally compelling, though not deductively valid\\nInterface (computing) – Concept of computer science; point of interaction between two things\\nMethod overriding\\nMixin\\nPolymorphism (computer science)\\nProtocol\\nRole-oriented programming – Programming paradigm based on conceptual understanding of objects\\nThe Third Manifesto\\nTrait (computer programming) – Concept used in object-oriented programming\\nVirtual inheritance\\n\\n\\n== Notes ==\\n\\n\\n== References ==\\n\\n\\n== Further reading ==\\nMeyer, Bertrand (1997). \"24. Using Inheritance Well\". Object-Oriented Software Construction (2nd ed.). Prentice Hall. ISBN 0-13-629155-4.\\nSamokhin, Vadim (2017). \"Implementation Inheritance Is Evil\". HackerNoon. Medium.', 'In computer science, an object can be a variable, a data structure, a function, or a method, and as such, is a value in memory referenced by an identifier.\\nIn the object-oriented programming paradigm, object can be a combination of variables, functions, and data structures; in particular in class-based variations of the paradigm it refers to a particular instance of a class.\\nIn the relational model of database management, an object can be a table or column, or an association between data and a database entity (such as relating a person\\'s age to a specific person).\\n\\n\\n== Object-based languages ==\\n\\nAn important distinction in programming languages is the difference between an object-oriented language and an object-based language. A language is usually considered object-based if it includes the basic capabilities for an object: identity, properties, and attributes. A language is considered object-oriented if it is object-based and also has the capability of polymorphism, inheritance, encapsulation, and, possibly, composition. Polymorphism refers to the ability to overload the name of a function with multiple behaviors based on which object(s) are passed to it. Conventional message passing discriminates only on the first object and considers that to be \"sending a message\" to that object. However, some OOP languages such as Flavors and the Common Lisp Object System (CLOS) enable discriminating on more than the first parameter of the function. Inheritance is the ability to subclass an object class, to create a new class that is a subclass of an existing one and inherits all the data constraints and behaviors of its parents but also adds new and/or changes one or more of them.\\n\\n\\n== Object-oriented programming ==\\n\\nObject-oriented programming is an approach to designing modular reusable software systems. The object-oriented approach is an evolution of good design practices that go back to the very beginning of computer programming. Object-orientation is simply the logical extension of older techniques such as structured programming and abstract data types. An object is an abstract data type with the addition of polymorphism and inheritance.\\nRather than structure programs as code and data, an object-oriented system integrates the two using the concept of an \"object\". An object has state (data) and behavior (code). Objects can correspond to things found in the real world. So for example, a graphics program will have objects such as circle, square, menu. An online shopping system will have objects such as shopping cart, customer, product. The shopping system will support behaviors such as place order, make payment, and offer discount. The objects are designed as class hierarchies. So for example with the shopping system there might be high level classes such as electronics product, kitchen product, and book. There may be further refinements for example under electronic products: CD Player, DVD player, etc. These classes and subclasses correspond to sets and subsets in mathematical logic.\\n\\n\\n== Specialized objects ==\\nAn important concept for objects is the design pattern. A design pattern provides a reusable template to address a common problem. The following object descriptions are examples of some of the most common design patterns for objects.\\nFunction object: an object with a single method (in C++, this method would be the function operator, \"operator()\") that acts much like a function (like a C/C++ pointer to a function).\\nImmutable object: an object set up with a fixed state at creation time and which does not change afterward.\\nFirst-class object: an object that can be used without restriction.\\nContainer object: an object that can contain other objects.\\nFactory object: an object whose purpose is to create other objects.\\nMetaobject: an object from which other objects can be created (compare with a class, which is not necessarily an object).\\nPrototype object: a specialized metaobject from which other objects can be created by copying\\nGod object: an object that knows or does too much (it is an example of an anti-pattern).\\nSingleton object: an object that is the only instance of its class during the lifetime of the program.\\nFilter object: an object that receives a stream of data as its input and transforms it into the object\\'s output. Often the input and output streams are streams of characters, but these also may be streams of arbitrary objects. These are generally used in wrappers since they conceal the existing implementation with the abstraction required at the developer side.\\n\\n\\n== Distributed objects ==\\n\\nThe object-oriented approach is not just a programming model. It can be used equally well as an interface definition language for distributed systems. The objects in a distributed computing model tend to be larger grained, longer lasting, and more service-oriented than programming objects.\\nA standard method to package distributed objects is via an Interface Definition Language (IDL). An IDL shields the client of all of the details of the distributed server object. Details such as which computer the object resides on, what programming language it uses, what operating system, and other platform-specific issues. The IDL is also usually part of a distributed environment that provides services such as transactions and persistence to all objects in a uniform manner. Two of the most popular standards for distributed objects are the Object Management Group\\'s CORBA standard and Microsoft\\'s DCOM.In addition to distributed objects, a number of other extensions to the basic concept of an object have been proposed to enable distributed computing:\\n\\nProtocol objects are components of a protocol stack that enclose network communication within an object-oriented interface.\\nReplicated objects are groups of distributed objects (called replicas) that run a distributed multi-party protocol to achieve high consistency between their internal states, and that respond to requests in a coordinated way. Examples include fault-tolerant CORBA objects.\\nLive distributed objects (or simply live objects) generalize the replicated object concept to groups of replicas that might internally use any distributed protocol, perhaps resulting in only a weak consistency between their local states.Some of these extensions, such as distributed objects and protocol objects, are domain-specific terms for special types of \"ordinary\" objects used in a certain context (such as remote method invocation or protocol composition). Others, such as replicated objects and live distributed objects, are more non-standard, in that they abandon the usual case that an object resides in a single location at a time, and apply the concept to groups of entities (replicas) that might span across multiple locations, might have only weakly consistent state, and whose membership might dynamically change.\\n\\n\\n== The Semantic Web ==\\nThe Semantic Web is essentially a distributed-objects framework. Two key technologies in the Semantic Web are the Web Ontology Language (OWL) and the Resource Description Framework (RDF). RDF provides the capability to define basic objects—names, properties, attributes, relations—that are accessible via the Internet. OWL adds a richer object model, based on set theory, that provides additional modeling capabilities such as multiple inheritance.\\nOWL objects are not like standard large-grained distributed objects accessed via an Interface Definition Language. Such an approach would not be appropriate for the Internet because the Internet is constantly evolving and standardization on one set of interfaces is difficult to achieve. OWL objects tend to be similar to the kinds of objects used to define application domain models in programming languages such as Java and C++.\\nHowever, there are important distinctions between OWL objects and traditional object-oriented programming objects. Traditional objects get compiled into static hierarchies usually with single inheritance, but OWL objects are dynamic. An OWL object can change its structure at run time and can become an instance of new or different classes.\\nAnother critical difference is the way the model treats information that is currently not in the system. Programming objects and most database systems use the \"closed-world assumption\". If a fact is not known to the system that fact is assumed to be false. Semantic Web objects use the open-world assumption, a statement is only considered false if there is actual relevant information that it is false, otherwise it is assumed to be unknown, neither true nor false.\\nOWL objects are actually most like objects in artificial intelligence frame languages such as KL-ONE and Loom.\\nThe following table contrasts traditional objects from Object-Oriented programming languages such as Java or C++ with Semantic Web Objects:\\n\\n\\n== See also ==\\nObject lifetime\\nObject copy\\nDesign pattern (computer science)\\nBusiness object (computer science)\\nActor model\\n\\n\\n== References ==\\n\\n\\n== External links ==\\nWhat Is an Object? from The Java Tutorials\\nHow to merge two or more php objects', 'In software engineering and computer science, abstraction is:\\n\\nthe process of removing physical, spatial, or temporal details or attributes in the study of objects or systems to focus attention on details of greater importance; it is similar in nature to the process of generalization;\\nthe creation of abstract concept-objects by mirroring common features or attributes of various non-abstract objects or systems of study – the result of the process of abstraction.Abstraction, in general, is a fundamental concept in computer science and software development. The process of abstraction can also be referred to as modeling and is closely related to the concepts of theory and design. Models can also be considered types of abstractions per their generalization of aspects of reality.\\nAbstraction in computer science is closely related to abstraction in mathematics due to their common focus on building abstractions as objects, but is also related to other notions of abstraction used in other fields such as art.Abstractions may also refer to real-world objects and systems, rules of computational systems or rules of programming languages that carry or utilize features of abstraction itself, such as:\\n\\nthe usage of data types to perform data abstraction to separate usage from working representations of data structures within programs;\\nthe concept of procedures, functions, or subroutines which represent a specific of implementing control flow in programs;\\nthe rules commonly named \"abstraction\" that generalize expressions using free and bound variables in the various versions of lambda calculus;\\nthe usage of S-expressions as an abstraction of data structures and programs in the Lisp programming language;\\nthe process of reorganizing common behavior from non-abstract classes into \"abstract classes\" using inheritance to abstract over sub-classes as seen in the object-oriented C++ and Java programming languages.\\n\\n\\n== Rationale ==\\nComputing mostly operates independently of the concrete world. The hardware implements a model of computation that is interchangeable with others. The software is structured in architectures to enable humans to create the enormous systems by concentrating on a few issues at a time. These architectures are made of specific choices of abstractions. Greenspun\\'s Tenth Rule is an aphorism on how such an architecture is both inevitable and complex.\\nA central form of abstraction in computing is language abstraction: new artificial languages are developed to express specific aspects of a system. Modeling languages help in planning. Computer languages can be processed with a computer. An example of this abstraction process is the generational development of programming languages from the machine language to the assembly language and the high-level language. Each stage can be used as a stepping stone for the next stage. The language abstraction continues for example in scripting languages and domain-specific programming languages.\\nWithin a programming language, some features let the programmer create new abstractions. These include subroutines, modules, polymorphism, and software components. Some other abstractions such as software design patterns and architectural styles remain invisible to a translator and operate only in the design of a system.\\nSome abstractions try to limit the range of concepts a programmer needs to be aware of, by completely hiding the abstractions that they in turn are built on. The software engineer and writer Joel Spolsky has criticised these efforts by claiming that all abstractions are leaky – that they can never completely hide the details below; however, this does not negate the usefulness of abstraction.\\nSome abstractions are designed to inter-operate with other abstractions – for example, a programming language may contain a foreign function interface for making calls to the lower-level language.\\n\\n\\n== Abstraction features ==\\n\\n\\n=== Programming languages ===\\n\\nDifferent programming languages provide different types of abstraction, depending on the intended applications for the language. For example:\\n\\nIn object-oriented programming languages such as C++, Object Pascal, or Java, the concept of abstraction has itself become a declarative statement – using the syntax function(parameters) = 0; (in C++) or the keywords abstract and interface (in Java). After such a declaration, it is the responsibility of the programmer to implement a class to instantiate the object of the declaration.\\nFunctional programming languages commonly exhibit abstractions related to functions, such as lambda abstractions (making a term into a function of some variable) and higher-order functions (parameters are functions).\\nModern members of the Lisp programming language family such as Clojure, Scheme and Common Lisp support macro systems to allow syntactic abstraction. Other programming languages such as Scala also have macros, or very similar metaprogramming features (for example, Haskell has Template Haskell, and OCaml has MetaOCaml). These can allow a programmer to eliminate boilerplate code, abstract away tedious function call sequences, implement new control flow structures, and implement Domain Specific Languages (DSLs), which allow domain-specific concepts to be expressed in concise and elegant ways. All of these, when used correctly, improve both the programmer\\'s efficiency and the clarity of the code by making the intended purpose more explicit. A consequence of syntactic abstraction is also that any Lisp dialect and in fact almost any programming language can, in principle, be implemented in any modern Lisp with significantly reduced (but still non-trivial in some cases) effort when compared to \"more traditional\" programming languages such as Python, C or Java.\\n\\n\\n=== Specification methods ===\\n\\nAnalysts have developed various methods to formally specify software systems.  Some known methods include:\\n\\nAbstract-model based method (VDM, Z);\\nAlgebraic techniques (Larch, CLEAR, OBJ, ACT ONE, CASL);\\nProcess-based techniques (LOTOS, SDL, Estelle);\\nTrace-based techniques (SPECIAL, TAM);\\nKnowledge-based techniques (Refine, Gist).\\n\\n\\n=== Specification languages ===\\n\\nSpecification languages generally rely on abstractions of one kind or another, since specifications are typically defined earlier in a project, (and at a more abstract level) than an eventual implementation. The UML specification language, for example, allows the definition of abstract classes, which in a waterfall project, remain abstract during the architecture and specification phase of the project.\\n\\n\\n== Control abstraction ==\\n\\nProgramming languages offer control abstraction as one of the main purposes of their use. Computer machines understand operations at the very low level such as moving some bits from one location of the memory to another location and producing the sum of two sequences of bits. Programming languages allow this to be done in the higher level. For example, consider this statement written in a Pascal-like fashion:\\n\\na := (1 + 2) * 5To a human, this seems a fairly simple and obvious calculation (\"one plus two is three, times five is fifteen\"). However, the low-level steps necessary to carry out this evaluation, and return the value \"15\", and then assign that value to the variable \"a\", are actually quite subtle and complex. The values need to be converted to binary representation (often a much more complicated task than one would think) and the calculations decomposed (by the compiler or interpreter) into assembly instructions (again, which are much less intuitive to the programmer: operations such as shifting a binary register left, or adding the binary complement of the contents of one register to another, are simply not how humans think about the abstract arithmetical operations of addition or multiplication). Finally, assigning the resulting value of \"15\" to the variable labeled \"a\", so that \"a\" can be used later, involves additional \\'behind-the-scenes\\' steps of looking up a variable\\'s label and the resultant location in physical or virtual memory, storing the binary representation of \"15\" to that memory location, etc.\\nWithout control abstraction, a programmer would need to specify all the register/binary-level steps each time they simply wanted to add or multiply a couple of numbers and assign the result to a variable. Such duplication of effort has two serious negative consequences:\\n\\nit forces the programmer to constantly repeat fairly common tasks every time a similar operation is needed\\nit forces the programmer to program for the particular hardware and instruction set\\n\\n\\n=== Structured programming ===\\n\\nStructured programming involves the splitting of complex program tasks into smaller pieces with clear flow-control and interfaces between components, with a reduction of the complexity potential for side-effects.\\nIn a simple program, this may aim to ensure that loops have single or obvious exit points and (where possible) to have single exit points from functions and procedures.\\nIn a larger system, it may involve breaking down complex tasks into many different modules. Consider a system which handles payroll on ships and at shore offices:\\n\\nThe uppermost level may feature a menu of typical end-user operations.\\nWithin that could be standalone executables or libraries for tasks such as signing on and off employees or printing checks.\\nWithin each of those standalone components there could be many different source files, each containing the program code to handle a part of the problem, with only selected interfaces available to other parts of the program. A sign on program could have source files for each data entry screen and the database interface (which may itself be a standalone third party library or a statically linked set of library routines).\\nEither the database or the payroll application also has to initiate the process of exchanging data with between ship and shore, and that data transfer task will often contain many other components.These layers produce the effect of isolating the implementation details of one component and its assorted internal methods from the others. Object-oriented programming embraces and extends this concept.\\n\\n\\n== Data abstraction ==\\n\\nData abstraction enforces a clear separation between the abstract properties of a data type and the concrete details of its implementation. The abstract properties are those that are visible to client code that makes use of the data type—the interface to the data type—while the concrete implementation is kept entirely private, and indeed can change, for example to incorporate efficiency improvements over time. The idea is that such changes are not supposed to have any impact on client code, since they involve no difference in the abstract behaviour.\\nFor example, one could define an abstract data type called lookup table which uniquely associates keys with values, and in which values may be retrieved by specifying their corresponding keys. Such a lookup table may be implemented in various ways: as a hash table, a binary search tree, or even a simple linear list of (key:value) pairs. As far as client code is concerned, the abstract properties of the type are the same in each case.\\nOf course, this all relies on getting the details of the interface right in the first place, since any changes there can have major impacts on client code. As one way to look at this: the interface forms a contract on agreed behaviour between the data type and client code; anything not spelled out in the contract is subject to change without notice.\\n\\n\\n== Manual data abstraction ==\\nWhile much of data abstraction occurs through computer science and automation, there are times when this process is done manually and without programming intervention. One way this can be understood is through data abstraction within the process of conducting a systematic review of the literature. In this methodology, data is abstracted by one or several abstractors when conducting a meta-analysis, with errors reduced through dual data abstraction followed by independent checking, known as adjudication.\\n\\n\\n== Abstraction in object oriented programming ==\\n\\nIn object-oriented programming theory, abstraction involves the facility to define objects that represent abstract \"actors\" that can perform work, report on and change their state, and \"communicate\" with other objects in the system. The term encapsulation refers to the hiding of state details, but extending the concept of data type from earlier programming languages to associate behavior most strongly with the data, and standardizing the way that different data types interact, is the beginning of abstraction.  When abstraction proceeds into the operations defined, enabling objects of different types to be substituted, it is called polymorphism. When it proceeds in the opposite direction, inside the types or classes, structuring them to simplify a complex set of relationships, it is called delegation or inheritance.\\nVarious object-oriented programming languages offer similar facilities for abstraction, all to support a general strategy of polymorphism in object-oriented programming, which includes the substitution of one type for another in the same or similar role. Although not as generally supported, a configuration or image or package may predetermine a great many of these bindings at compile-time, link-time, or loadtime. This would leave only a minimum of such bindings to change at run-time.\\nCommon Lisp Object System or Self, for example, feature less of a class-instance distinction and more use of delegation for polymorphism. Individual objects and functions are abstracted more flexibly to better fit with a shared functional heritage from Lisp.\\nC++ exemplifies another extreme: it relies heavily on templates and overloading and other static bindings at compile-time, which in turn has certain flexibility problems.\\nAlthough these examples offer alternate strategies for achieving the same abstraction, they do not fundamentally alter the need to support abstract nouns in code – all programming relies on an ability to abstract verbs as functions, nouns as data structures, and either as processes.\\nConsider for example a sample Java fragment to represent some common farm \"animals\" to a level of abstraction suitable to model simple aspects of their hunger and feeding. It defines an Animal class to represent both the state of the animal and its functions:\\n\\nWith the above definition, one could create objects of type Animal and call their methods like this:\\n\\nIn the above example, the class Animal is an abstraction used in place of an actual animal, LivingThing is a further abstraction (in this case a generalisation) of Animal.\\nIf one requires a more differentiated hierarchy of animals – to differentiate, say, those who provide milk from those who provide nothing except meat at the end of their lives – that is an intermediary level of abstraction, probably DairyAnimal (cows, goats) who would eat foods suitable to giving good milk, and MeatAnimal (pigs, steers) who would eat foods to give the best meat-quality.\\nSuch an abstraction could remove the need for the application coder to specify the type of food, so they could concentrate instead on the feeding schedule. The two classes could be related using inheritance or stand alone, and the programmer could define varying degrees of polymorphism between the two types. These facilities tend to vary drastically between languages, but in general each can achieve anything that is possible with any of the others. A great many operation overloads, data type by data type, can have the same effect at compile-time as any degree of inheritance or other means to achieve polymorphism. The class notation is simply a coder\\'s convenience.\\n\\n\\n=== Object-oriented design ===\\n\\nDecisions regarding what to abstract and what to keep under the control of the coder become the major concern of object-oriented design and domain analysis—actually determining the relevant relationships in the real world is the concern of object-oriented analysis or legacy analysis.\\nIn general, to determine appropriate abstraction, one must make many small decisions about scope (domain analysis), determine what other systems one must cooperate with (legacy analysis), then perform a detailed object-oriented analysis which is expressed within project time and budget constraints as an object-oriented design. In our simple example, the domain is the barnyard, the live pigs and cows and their eating habits are the legacy constraints, the detailed analysis is that coders must have the flexibility to feed the animals what is available and thus there is no reason to code the type of food into the class itself, and the design is a single simple Animal class of which pigs and cows are instances with the same functions. A decision to differentiate DairyAnimal would change the detailed analysis but the domain and legacy analysis would be unchanged—thus it is entirely under the control of the programmer, and it is called an abstraction in object-oriented programming as distinct from abstraction in domain or legacy analysis.\\n\\n\\n== Considerations ==\\nWhen discussing formal semantics of programming languages, formal methods or abstract interpretation, abstraction refers to the act of considering a less detailed, but safe, definition of the observed program behaviors. For instance, one may observe only the final result of program executions instead of considering all the intermediate steps of executions. Abstraction is defined to a concrete (more precise) model of execution.\\nAbstraction may be exact or faithful with respect to a property if one can answer a question about the property equally well on the concrete or abstract model. For instance, if one wishes to know what the result of the evaluation of a mathematical expression involving only integers +, -, ×, is worth modulo n, then one needs only perform all operations modulo n (a familiar form of this abstraction is casting out nines).\\nAbstractions, however, though not necessarily exact, should be sound. That is, it should be possible to get sound answers from them—even though the abstraction may simply yield a result of undecidability. For instance, students in a class may be abstracted by their minimal and maximal ages; if one asks whether a certain person belongs to that class, one may simply compare that person\\'s age with the minimal and maximal ages; if his age lies outside the range, one may safely answer that the person does not belong to the class; if it does not, one may only answer \"I don\\'t know\".\\nThe level of abstraction included in a programming language can influence its overall usability. The Cognitive dimensions framework includes the concept of abstraction gradient in a formalism. This framework allows the designer of a programming language to study the trade-offs between abstraction and other characteristics of the design, and how changes in abstraction influence the language usability.\\nAbstractions can prove useful when dealing with computer programs, because non-trivial properties of computer programs are essentially undecidable (see Rice\\'s theorem). As a consequence, automatic methods for deriving information on the behavior of computer programs either have to drop termination (on some occasions, they may fail, crash or never yield out a result), soundness (they may provide false information), or precision (they may answer \"I don\\'t know\" to some questions).\\nAbstraction is the core concept of abstract interpretation. Model checking generally takes place on abstract versions of the studied systems.\\n\\n\\n== Levels of abstraction ==\\n\\nComputer science commonly presents levels (or, less commonly, layers) of abstraction, wherein each level represents a different model of the same information and processes, but with varying amounts of detail. Each level uses a system of expression involving a unique set of objects and compositions that apply only to a particular domain.\\n\\nEach relatively abstract, \"higher\" level builds on a relatively concrete, \"lower\" level, which tends to provide an increasingly \"granular\" representation. For example, gates build on electronic circuits, binary on gates, machine language on binary, programming language on machine language, applications and operating systems on programming languages. Each level is embodied, but not determined, by the level beneath it, making it a language of description that is somewhat self-contained.\\n\\n\\n=== Database systems ===\\n\\nSince many users of database systems lack in-depth familiarity with computer data-structures, database developers often hide complexity through the following levels:\\n\\nPhysical level: The lowest level of abstraction describes how a system actually stores data. The physical level describes complex low-level data structures in detail.\\nLogical level: The next higher level of abstraction describes what data the database stores, and what relationships exist among those data. The logical level thus describes an entire database in terms of a small number of relatively simple structures. Although implementation of the simple structures at the logical level may involve complex physical level structures, the user of the logical level does not need to be aware of this complexity. This is referred to as physical data independence. Database administrators, who must decide what information to keep in a database, use the logical level of abstraction.\\nView level: The highest level of abstraction describes only part of the entire database. Even though the logical level uses simpler structures, complexity remains because of the variety of information stored in a large database. Many users of a database system do not need all this information; instead, they need to access only a part of the database. The view level of abstraction exists to simplify their interaction with the system. The system may provide many views for the same database.\\n\\n\\n=== Layered architecture ===\\n\\nThe ability to provide a design of different levels of abstraction can\\n\\nsimplify the design considerably\\nenable different role players to effectively work at various levels of abstraction\\nsupport the portability of software artifacts (model-based ideally)Systems design and business process design can both use this. Some design processes specifically generate designs that contain various levels of abstraction.\\nLayered architecture partitions the concerns of the application into stacked groups (layers).\\nIt is a technique used in designing computer software, hardware, and communications in which system or network components are isolated in layers so that changes can be made in one layer without affecting the others.\\n\\n\\n== See also ==\\nAbstraction principle (computer programming)\\nAbstraction inversion for an anti-pattern of one danger in abstraction\\nAbstract data type for an abstract description of a set of data\\nAlgorithm for an abstract description of a computational procedure\\nBracket abstraction for making a term into a function of a variable\\nData modeling for structuring data independent of the processes that use it\\nEncapsulation for abstractions that hide implementation details\\nGreenspun\\'s Tenth Rule for an aphorism about an (the?) optimum point in the space of abstractions\\nHigher-order function for abstraction where functions produce or consume other functions\\nLambda abstraction for making a term into a function of some variable\\nList of abstractions (computer science)\\nRefinement for the opposite of abstraction in computing\\nInteger (computer science)\\nHeuristic (computer science)\\n\\n\\n== References ==\\n\\n\\n== Further reading ==\\n\\n\\n== External links ==\\nSimArch example of layered architecture for distributed simulation systems.', 'Big O notation is a mathematical notation that describes the limiting behavior of a function when the argument tends towards a particular value or infinity. Big O is a member of a family of notations invented by Paul Bachmann, Edmund Landau, and others, collectively called Bachmann–Landau notation or asymptotic notation.\\nIn computer science, big O notation is used to classify algorithms according to how their run time or space requirements grow as the input size grows.  In analytic number theory, big O notation is often used to express a bound on the difference between an arithmetical function and a better understood approximation; a famous example of such a difference is the remainder term in the prime number theorem. Big O notation is also used in many other fields to provide similar estimates.\\nBig O notation characterizes functions according to their growth rates: different functions with the same growth rate may be represented using the same O notation. The letter O is used because the growth rate of a function is also referred to as the order of the function.  A description of a function in terms of big O notation usually only provides an upper bound on the growth rate of the function. \\nAssociated with big O notation are several related notations, using the symbols o, Ω, ω, and Θ, to describe other kinds of bounds on asymptotic growth rates. \\n\\n\\n== Formal definition ==\\nLet f be a real or complex valued function and g a real valued function. Let both functions be defined on some unbounded subset of the positive real numbers, and \\n  \\n    \\n      \\n        g\\n        (\\n        x\\n        )\\n      \\n    \\n    {\\\\displaystyle g(x)}\\n   be strictly positive for all large enough values of x. One writes\\n\\n  \\n    \\n      \\n        f\\n        (\\n        x\\n        )\\n        =\\n        O\\n        \\n          \\n            (\\n          \\n        \\n        g\\n        (\\n        x\\n        )\\n        \\n          \\n            )\\n          \\n        \\n        \\n        \\n           as \\n        \\n        x\\n        →\\n        ∞\\n      \\n    \\n    {\\\\displaystyle f(x)=O{\\\\bigl (}g(x){\\\\bigr )}\\\\quad {\\\\text{ as }}x\\\\to \\\\infty }\\n  if the absolute value of \\n  \\n    \\n      \\n        f\\n        (\\n        x\\n        )\\n      \\n    \\n    {\\\\displaystyle f(x)}\\n   is at most a positive constant multiple of \\n  \\n    \\n      \\n        g\\n        (\\n        x\\n        )\\n      \\n    \\n    {\\\\displaystyle g(x)}\\n   for all sufficiently large values of x. That is, \\n  \\n    \\n      \\n        f\\n        (\\n        x\\n        )\\n        =\\n        O\\n        \\n          \\n            (\\n          \\n        \\n        g\\n        (\\n        x\\n        )\\n        \\n          \\n            )\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle f(x)=O{\\\\bigl (}g(x){\\\\bigr )}}\\n   if there exists a positive real number M and a real number x0 such that\\n\\n  \\n    \\n      \\n        \\n          |\\n        \\n        f\\n        (\\n        x\\n        )\\n        \\n          |\\n        \\n        ≤\\n        M\\n        g\\n        (\\n        x\\n        )\\n        \\n        \\n           for all \\n        \\n        x\\n        ≥\\n        \\n          x\\n          \\n            0\\n          \\n        \\n        .\\n      \\n    \\n    {\\\\displaystyle |f(x)|\\\\leq Mg(x)\\\\quad {\\\\text{ for all }}x\\\\geq x_{0}.}\\n  In many contexts, the assumption that we are interested in the growth rate as the variable x goes to infinity is left unstated, and one writes more simply that\\n\\n  \\n    \\n      \\n        f\\n        (\\n        x\\n        )\\n        =\\n        O\\n        \\n          \\n            (\\n          \\n        \\n        g\\n        (\\n        x\\n        )\\n        \\n          \\n            )\\n          \\n        \\n        .\\n      \\n    \\n    {\\\\displaystyle f(x)=O{\\\\bigl (}g(x){\\\\bigr )}.}\\n  The notation can also be used to describe the behavior of f near some real number a (often, a = 0): we say\\n\\n  \\n    \\n      \\n        f\\n        (\\n        x\\n        )\\n        =\\n        O\\n        \\n          \\n            (\\n          \\n        \\n        g\\n        (\\n        x\\n        )\\n        \\n          \\n            )\\n          \\n        \\n        \\n        \\n           as \\n        \\n        x\\n        →\\n        a\\n      \\n    \\n    {\\\\displaystyle f(x)=O{\\\\bigl (}g(x){\\\\bigr )}\\\\quad {\\\\text{ as }}x\\\\to a}\\n  if there exist positive numbers \\n  \\n    \\n      \\n        δ\\n      \\n    \\n    {\\\\displaystyle \\\\delta }\\n   and M such that for all x with \\n  \\n    \\n      \\n        0\\n        <\\n        \\n          |\\n        \\n        x\\n        −\\n        a\\n        \\n          |\\n        \\n        <\\n        δ\\n      \\n    \\n    {\\\\displaystyle 0<|x-a|<\\\\delta }\\n  ,\\n\\n  \\n    \\n      \\n        \\n          |\\n        \\n        f\\n        (\\n        x\\n        )\\n        \\n          |\\n        \\n        ≤\\n        M\\n        g\\n        (\\n        x\\n        )\\n        .\\n      \\n    \\n    {\\\\displaystyle |f(x)|\\\\leq Mg(x).}\\n  As g(x) is chosen to be non-zero for values of x sufficiently close to a, both of these definitions can be unified using the limit superior:\\n\\n  \\n    \\n      \\n        f\\n        (\\n        x\\n        )\\n        =\\n        O\\n        \\n          \\n            (\\n          \\n        \\n        g\\n        (\\n        x\\n        )\\n        \\n          \\n            )\\n          \\n        \\n        \\n        \\n           as \\n        \\n        x\\n        →\\n        a\\n      \\n    \\n    {\\\\displaystyle f(x)=O{\\\\bigl (}g(x){\\\\bigr )}\\\\quad {\\\\text{ as }}x\\\\to a}\\n  if\\n\\n  \\n    \\n      \\n        \\n          lim\\u2006sup\\n          \\n            x\\n            →\\n            a\\n          \\n        \\n        \\n          \\n            \\n              |\\n              \\n                f\\n                (\\n                x\\n                )\\n              \\n              |\\n            \\n            \\n              g\\n              (\\n              x\\n              )\\n            \\n          \\n        \\n        <\\n        ∞\\n        .\\n      \\n    \\n    {\\\\displaystyle \\\\limsup _{x\\\\to a}{\\\\frac {\\\\left|f(x)\\\\right|}{g(x)}}<\\\\infty .}\\n  In computer science, a slightly more restrictive definition is common: \\n  \\n    \\n      \\n        f\\n      \\n    \\n    {\\\\displaystyle f}\\n   and \\n  \\n    \\n      \\n        g\\n      \\n    \\n    {\\\\displaystyle g}\\n   are both required to be functions from the positive integers to the nonnegative real numbers; \\n  \\n    \\n      \\n        f\\n        (\\n        x\\n        )\\n        =\\n        O\\n        \\n          \\n            (\\n          \\n        \\n        g\\n        (\\n        x\\n        )\\n        \\n          \\n            )\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle f(x)=O{\\\\bigl (}g(x){\\\\bigr )}}\\n   if there exist positive integer numbers M and n0 such that \\n  \\n    \\n      \\n        f\\n        (\\n        n\\n        )\\n        ≤\\n        M\\n        g\\n        (\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle f(n)\\\\leq Mg(n)}\\n   for all \\n  \\n    \\n      \\n        n\\n        ≥\\n        \\n          n\\n          \\n            0\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle n\\\\geq n_{0}}\\n  . Where necessary, finite ranges are (tacitly) excluded from \\n  \\n    \\n      \\n        f\\n      \\n    \\n    {\\\\displaystyle f}\\n  \\'s and \\n  \\n    \\n      \\n        g\\n      \\n    \\n    {\\\\displaystyle g}\\n  \\'s domain by choosing n0 sufficiently large. (For example, \\n  \\n    \\n      \\n        log\\n        \\u2061\\n        (\\n        n\\n        )\\n      \\n    \\n    {\\\\displaystyle \\\\log(n)}\\n   is undefined at \\n  \\n    \\n      \\n        n\\n        =\\n        0\\n      \\n    \\n    {\\\\displaystyle n=0}\\n  .)\\n\\n\\n== Example ==\\nIn typical usage the O notation is asymptotical, that is, it refers to very large x.  In this setting, the contribution of the terms that grow \"most quickly\" will eventually make the other ones irrelevant. As a result, the following simplification rules can be applied:\\n\\nIf f(x) is a sum of several terms, if there is one with largest growth rate, it can be kept, and all others omitted.\\nIf f(x) is a product of several factors, any constants (terms in the product that do not depend on x) can be omitted.For example, let f(x) = 6x4 − 2x3 + 5, and suppose we wish to simplify this function, using O notation, to describe its growth rate as x approaches infinity. This function is the sum of three terms: 6x4, −2x3, and 5. Of these three terms, the one with the highest growth rate is the one with the largest exponent as a function of x, namely 6x4. Now one may apply the second rule: 6x4 is a product of 6 and x4 in which the first factor does not depend on x. Omitting this factor results in the simplified form x4. Thus, we say that f(x) is a \"big O\" of x4. Mathematically, we can write f(x) = O(x4). One may confirm this calculation using the formal definition: let f(x) = 6x4 − 2x3 + 5 and g(x) = x4. Applying the formal definition from above, the statement that f(x) = O(x4) is equivalent to its expansion,\\n\\n  \\n    \\n      \\n        \\n          |\\n        \\n        f\\n        (\\n        x\\n        )\\n        \\n          |\\n        \\n        ≤\\n        M\\n        \\n          x\\n          \\n            4\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle |f(x)|\\\\leq Mx^{4}}\\n  for some suitable choice of x0 and M and for all x > x0. To prove this, let x0 = 1 and M = 13. Then, for all x > x0:\\n\\n  \\n    \\n      \\n        \\n          \\n            \\n              \\n                \\n                  |\\n                \\n                6\\n                \\n                  x\\n                  \\n                    4\\n                  \\n                \\n                −\\n                2\\n                \\n                  x\\n                  \\n                    3\\n                  \\n                \\n                +\\n                5\\n                \\n                  |\\n                \\n              \\n              \\n                \\n                ≤\\n                6\\n                \\n                  x\\n                  \\n                    4\\n                  \\n                \\n                +\\n                \\n                  |\\n                \\n                2\\n                \\n                  x\\n                  \\n                    3\\n                  \\n                \\n                \\n                  |\\n                \\n                +\\n                5\\n              \\n            \\n            \\n              \\n              \\n                \\n                ≤\\n                6\\n                \\n                  x\\n                  \\n                    4\\n                  \\n                \\n                +\\n                2\\n                \\n                  x\\n                  \\n                    4\\n                  \\n                \\n                +\\n                5\\n                \\n                  x\\n                  \\n                    4\\n                  \\n                \\n              \\n            \\n            \\n              \\n              \\n                \\n                =\\n                13\\n                \\n                  x\\n                  \\n                    4\\n                  \\n                \\n              \\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\begin{aligned}|6x^{4}-2x^{3}+5|&\\\\leq 6x^{4}+|2x^{3}|+5\\\\\\\\&\\\\leq 6x^{4}+2x^{4}+5x^{4}\\\\\\\\&=13x^{4}\\\\end{aligned}}}\\n  so\\n\\n  \\n    \\n      \\n        \\n          |\\n        \\n        6\\n        \\n          x\\n          \\n            4\\n          \\n        \\n        −\\n        2\\n        \\n          x\\n          \\n            3\\n          \\n        \\n        +\\n        5\\n        \\n          |\\n        \\n        ≤\\n        13\\n        \\n          x\\n          \\n            4\\n          \\n        \\n        .\\n      \\n    \\n    {\\\\displaystyle |6x^{4}-2x^{3}+5|\\\\leq 13x^{4}.}\\n  \\n\\n\\n== Usage ==\\nBig O notation has two main areas of application:\\n\\nIn mathematics, it is commonly used to describe how closely a finite series approximates a given function, especially in the case of a truncated Taylor series or asymptotic expansion\\nIn computer science, it is useful in the analysis of algorithmsIn both applications, the function g(x) appearing within the O(·) is typically chosen to be as simple as possible, omitting constant factors and lower order terms.\\nThere are two formally close, but noticeably different, usages of this notation:\\ninfinite asymptotics\\ninfinitesimal asymptotics.This distinction is only in application and not in principle, however—the formal definition for the \"big O\" is the same for both cases, only with different limits for the function argument.\\n\\n\\n=== Infinite asymptotics ===\\n\\nBig O notation is useful when analyzing algorithms for efficiency. For example, the time (or the number of steps) it takes to complete a problem of size n might be found to be T(n) = 4n2 − 2n + 2. As n grows large, the n2 term will come to dominate, so that all other terms can be neglected—for instance when n = 500, the term 4n2 is 1000 times as large as the 2n term. Ignoring the latter would have negligible effect on the expression\\'s value for most purposes. Further, the coefficients become irrelevant if we compare to any other order of expression, such as an expression containing a term n3 or n4. Even if T(n) = 1,000,000n2, if U(n) = n3, the latter will always exceed the former once n grows larger than 1,000,000 (T(1,000,000) = 1,000,0003 = U(1,000,000)). Additionally, the number of steps depends on the details of the machine model on which the algorithm runs, but different types of machines typically vary by only a constant factor in the number of steps needed to execute an algorithm. So the big O notation captures what remains: we write either\\n\\n  \\n    \\n      \\n        T\\n        (\\n        n\\n        )\\n        =\\n        O\\n        (\\n        \\n          n\\n          \\n            2\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle T(n)=O(n^{2})}\\n  or\\n\\n  \\n    \\n      \\n        T\\n        (\\n        n\\n        )\\n        ∈\\n        O\\n        (\\n        \\n          n\\n          \\n            2\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle T(n)\\\\in O(n^{2})}\\n  and say that the algorithm has order of n2 time complexity. The sign \"=\" is not meant to express \"is equal to\" in its normal mathematical sense, but rather a more colloquial \"is\", so the second expression is sometimes considered more accurate (see the \"Equals sign\" discussion below) while the first is considered by some as an abuse of notation.\\n\\n\\n=== Infinitesimal asymptotics ===\\nBig O can also be used to describe the error term in an approximation to a mathematical function. The most significant terms are written explicitly, and then the least-significant terms are summarized in a single big O term.  Consider, for example, the exponential series and two expressions of it that are valid when x is small:\\n\\n  \\n    \\n      \\n        \\n          \\n            \\n              \\n                \\n                  e\\n                  \\n                    x\\n                  \\n                \\n              \\n              \\n                \\n                =\\n                1\\n                +\\n                x\\n                +\\n                \\n                  \\n                    \\n                      x\\n                      \\n                        2\\n                      \\n                    \\n                    \\n                      2\\n                      !\\n                    \\n                  \\n                \\n                +\\n                \\n                  \\n                    \\n                      x\\n                      \\n                        3\\n                      \\n                    \\n                    \\n                      3\\n                      !\\n                    \\n                  \\n                \\n                +\\n                \\n                  \\n                    \\n                      x\\n                      \\n                        4\\n                      \\n                    \\n                    \\n                      4\\n                      !\\n                    \\n                  \\n                \\n                +\\n                ⋯\\n              \\n              \\n                \\n                  for all \\n                \\n                x\\n              \\n            \\n            \\n              \\n              \\n                \\n                =\\n                1\\n                +\\n                x\\n                +\\n                \\n                  \\n                    \\n                      x\\n                      \\n                        2\\n                      \\n                    \\n                    2\\n                  \\n                \\n                +\\n                O\\n                (\\n                \\n                  x\\n                  \\n                    3\\n                  \\n                \\n                )\\n              \\n              \\n                \\n                  as \\n                \\n                x\\n                →\\n                0\\n              \\n            \\n            \\n              \\n              \\n                \\n                =\\n                1\\n                +\\n                x\\n                +\\n                O\\n                (\\n                \\n                  x\\n                  \\n                    2\\n                  \\n                \\n                )\\n              \\n              \\n                \\n                  as \\n                \\n                x\\n                →\\n                0\\n              \\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\begin{aligned}e^{x}&=1+x+{\\\\frac {x^{2}}{2!}}+{\\\\frac {x^{3}}{3!}}+{\\\\frac {x^{4}}{4!}}+\\\\dotsb &{\\\\text{for all }}x\\\\\\\\[4pt]&=1+x+{\\\\frac {x^{2}}{2}}+O(x^{3})&{\\\\text{as }}x\\\\to 0\\\\\\\\[4pt]&=1+x+O(x^{2})&{\\\\text{as }}x\\\\to 0\\\\end{aligned}}}\\n  The second expression (the one with O(x3)) means the absolute-value of the error ex − (1 + x + x2/2) is at most some constant times |x3| when x is close enough to 0.\\n\\n\\n== Properties ==\\nIf the function f can be written as a finite sum of other functions, then the fastest growing one determines the order of f(n). For example,\\n\\n  \\n    \\n      \\n        f\\n        (\\n        n\\n        )\\n        =\\n        9\\n        log\\n        \\u2061\\n        n\\n        +\\n        5\\n        (\\n        log\\n        \\u2061\\n        n\\n        \\n          )\\n          \\n            4\\n          \\n        \\n        +\\n        3\\n        \\n          n\\n          \\n            2\\n          \\n        \\n        +\\n        2\\n        \\n          n\\n          \\n            3\\n          \\n        \\n        =\\n        O\\n        (\\n        \\n          n\\n          \\n            3\\n          \\n        \\n        )\\n        \\n        \\n          as \\n        \\n        n\\n        →\\n        ∞\\n        .\\n      \\n    \\n    {\\\\displaystyle f(n)=9\\\\log n+5(\\\\log n)^{4}+3n^{2}+2n^{3}=O(n^{3})\\\\qquad {\\\\text{as }}n\\\\to \\\\infty .}\\n  In particular, if a function may be bounded by a polynomial in n, then as n tends to infinity, one may disregard lower-order terms of the polynomial. The sets O(nc) and O(cn) are very different. If c is greater than one, then the latter grows much faster. A function that grows faster than nc for any c  is called superpolynomial.  One that grows more slowly than any exponential function of the form cn is called subexponential. An algorithm can require time that is both superpolynomial and subexponential; examples of this include the fastest known algorithms for integer factorization and the function nlog n.\\nWe may ignore any powers of n inside of the logarithms. The set O(log n) is exactly the same as O(log(nc)). The logarithms differ only by a constant factor (since log(nc) = c log n) and thus the big O notation ignores that. Similarly, logs with different constant bases are equivalent. On the other hand, exponentials with different bases are not of the same order. For example, 2n and 3n are not of the same order.\\nChanging units may or may not affect the order of the resulting algorithm. Changing units is equivalent to multiplying the appropriate variable by a constant wherever it appears. For example, if an algorithm runs in the order of n2, replacing n by cn means the algorithm runs in the order of c2n2, and the big O notation ignores the constant c2. This can be written as c2n2 = O(n2). If, however, an algorithm runs in the order of 2n, replacing n with cn gives 2cn = (2c)n. This is not equivalent to 2n in general. Changing variables may also affect the order of the resulting algorithm. For example, if an algorithm\\'s run time is O(n) when measured in terms of the number n of digits of an input number x, then its run time is O(log x) when measured as a function of the input number x itself, because n = O(log x).\\n\\n\\n=== Product ===\\n\\n  \\n    \\n      \\n        \\n          f\\n          \\n            1\\n          \\n        \\n        =\\n        O\\n        (\\n        \\n          g\\n          \\n            1\\n          \\n        \\n        )\\n        \\n           and \\n        \\n        \\n          f\\n          \\n            2\\n          \\n        \\n        =\\n        O\\n        (\\n        \\n          g\\n          \\n            2\\n          \\n        \\n        )\\n        ⇒\\n        \\n          f\\n          \\n            1\\n          \\n        \\n        \\n          f\\n          \\n            2\\n          \\n        \\n        =\\n        O\\n        (\\n        \\n          g\\n          \\n            1\\n          \\n        \\n        \\n          g\\n          \\n            2\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle f_{1}=O(g_{1}){\\\\text{ and }}f_{2}=O(g_{2})\\\\Rightarrow f_{1}f_{2}=O(g_{1}g_{2})}\\n  \\n\\n  \\n    \\n      \\n        f\\n        ⋅\\n        O\\n        (\\n        g\\n        )\\n        =\\n        O\\n        (\\n        f\\n        g\\n        )\\n      \\n    \\n    {\\\\displaystyle f\\\\cdot O(g)=O(fg)}\\n  \\n\\n\\n=== Sum ===\\nIf \\n  \\n    \\n      \\n        \\n          f\\n          \\n            1\\n          \\n        \\n        =\\n        O\\n        (\\n        \\n          g\\n          \\n            1\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle f_{1}=O(g_{1})}\\n   and \\n  \\n    \\n      \\n        \\n          f\\n          \\n            2\\n          \\n        \\n        =\\n        O\\n        (\\n        \\n          g\\n          \\n            2\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle f_{2}=O(g_{2})}\\n   then \\n  \\n    \\n      \\n        \\n          f\\n          \\n            1\\n          \\n        \\n        +\\n        \\n          f\\n          \\n            2\\n          \\n        \\n        =\\n        O\\n        (\\n        max\\n        (\\n        \\n          g\\n          \\n            1\\n          \\n        \\n        ,\\n        \\n          g\\n          \\n            2\\n          \\n        \\n        )\\n        )\\n      \\n    \\n    {\\\\displaystyle f_{1}+f_{2}=O(\\\\max(g_{1},g_{2}))}\\n  . It follows that if \\n  \\n    \\n      \\n        \\n          f\\n          \\n            1\\n          \\n        \\n        =\\n        O\\n        (\\n        g\\n        )\\n      \\n    \\n    {\\\\displaystyle f_{1}=O(g)}\\n   and \\n  \\n    \\n      \\n        \\n          f\\n          \\n            2\\n          \\n        \\n        =\\n        O\\n        (\\n        g\\n        )\\n      \\n    \\n    {\\\\displaystyle f_{2}=O(g)}\\n   then \\n  \\n    \\n      \\n        \\n          f\\n          \\n            1\\n          \\n        \\n        +\\n        \\n          f\\n          \\n            2\\n          \\n        \\n        ∈\\n        O\\n        (\\n        g\\n        )\\n      \\n    \\n    {\\\\displaystyle f_{1}+f_{2}\\\\in O(g)}\\n  .  In other words, this second statement says that \\n  \\n    \\n      \\n        O\\n        (\\n        g\\n        )\\n      \\n    \\n    {\\\\displaystyle O(g)}\\n   is a convex cone.\\n\\n\\n=== Multiplication by a constant ===\\nLet k be a nonzero constant. Then \\n  \\n    \\n      \\n        O\\n        (\\n        \\n          |\\n        \\n        k\\n        \\n          |\\n        \\n        ⋅\\n        g\\n        )\\n        =\\n        O\\n        (\\n        g\\n        )\\n      \\n    \\n    {\\\\displaystyle O(|k|\\\\cdot g)=O(g)}\\n  .  In other words, if \\n  \\n    \\n      \\n        f\\n        =\\n        O\\n        (\\n        g\\n        )\\n      \\n    \\n    {\\\\displaystyle f=O(g)}\\n  , then  \\n  \\n    \\n      \\n        k\\n        ⋅\\n        f\\n        =\\n        O\\n        (\\n        g\\n        )\\n        .\\n      \\n    \\n    {\\\\displaystyle k\\\\cdot f=O(g).}\\n  \\n\\n\\n== Multiple variables ==\\nBig O (and little o, Ω, etc.) can also be used with multiple variables. To define big O formally for multiple variables, suppose \\n  \\n    \\n      \\n        f\\n      \\n    \\n    {\\\\displaystyle f}\\n   and \\n  \\n    \\n      \\n        g\\n      \\n    \\n    {\\\\displaystyle g}\\n   are two functions defined on some subset of \\n  \\n    \\n      \\n        \\n          \\n            R\\n          \\n          \\n            n\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle \\\\mathbb {R} ^{n}}\\n  . We say\\n\\n  \\n    \\n      \\n        f\\n        (\\n        \\n          x\\n        \\n        )\\n        \\n           is \\n        \\n        O\\n        (\\n        g\\n        (\\n        \\n          x\\n        \\n        )\\n        )\\n        \\n        \\n           as \\n        \\n        \\n          x\\n        \\n        →\\n        ∞\\n      \\n    \\n    {\\\\displaystyle f(\\\\mathbf {x} ){\\\\text{ is }}O(g(\\\\mathbf {x} ))\\\\quad {\\\\text{ as }}\\\\mathbf {x} \\\\to \\\\infty }\\n  if and only if\\n\\n  \\n    \\n      \\n        ∃\\n        M\\n        ∃\\n        C\\n        >\\n        0\\n         \\n        \\n           such that for all \\n        \\n         \\n        \\n          x\\n        \\n         \\n        \\n           with \\n        \\n         \\n        \\n          x\\n          \\n            i\\n          \\n        \\n        ≥\\n        M\\n         \\n        \\n           for some \\n        \\n         \\n        i\\n        ,\\n        \\n          |\\n        \\n        f\\n        (\\n        \\n          x\\n        \\n        )\\n        \\n          |\\n        \\n        ≤\\n        C\\n        \\n          |\\n        \\n        g\\n        (\\n        \\n          x\\n        \\n        )\\n        \\n          |\\n        \\n         \\n        .\\n      \\n    \\n    {\\\\displaystyle \\\\exists M\\\\exists C>0~{\\\\text{ such that for all }}~\\\\mathbf {x} ~{\\\\text{ with }}~x_{i}\\\\geq M~{\\\\text{ for some }}~i,|f(\\\\mathbf {x} )|\\\\leq C|g(\\\\mathbf {x} )|~.}\\n  Equivalently, the condition that \\n  \\n    \\n      \\n        \\n          x\\n          \\n            i\\n          \\n        \\n        ≥\\n        M\\n      \\n    \\n    {\\\\displaystyle x_{i}\\\\geq M}\\n   for some \\n  \\n    \\n      \\n        i\\n      \\n    \\n    {\\\\displaystyle i}\\n   can be replaced with the condition that \\n  \\n    \\n      \\n        ‖\\n        \\n          x\\n        \\n        \\n          ‖\\n          \\n            ∞\\n          \\n        \\n        ≥\\n        M\\n      \\n    \\n    {\\\\displaystyle \\\\|\\\\mathbf {x} \\\\|_{\\\\infty }\\\\geq M}\\n  , where \\n  \\n    \\n      \\n        ‖\\n        \\n          x\\n        \\n        \\n          ‖\\n          \\n            ∞\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle \\\\|\\\\mathbf {x} \\\\|_{\\\\infty }}\\n   denotes the Chebyshev norm. For example, the statement\\n\\n  \\n    \\n      \\n        f\\n        (\\n        n\\n        ,\\n        m\\n        )\\n        =\\n        \\n          n\\n          \\n            2\\n          \\n        \\n        +\\n        \\n          m\\n          \\n            3\\n          \\n        \\n        +\\n        O\\n        (\\n        n\\n        +\\n        m\\n        )\\n        \\n        \\n           as \\n        \\n        n\\n        ,\\n        m\\n        →\\n        ∞\\n      \\n    \\n    {\\\\displaystyle f(n,m)=n^{2}+m^{3}+O(n+m)\\\\quad {\\\\text{ as }}n,m\\\\to \\\\infty }\\n  asserts that there exist constants C and M such that\\n\\n  \\n    \\n      \\n        ∀\\n        ‖\\n        (\\n        n\\n        ,\\n        m\\n        )\\n        \\n          ‖\\n          \\n            ∞\\n          \\n        \\n        ≥\\n        M\\n        :\\n        \\n        \\n          |\\n        \\n        g\\n        (\\n        n\\n        ,\\n        m\\n        )\\n        \\n          |\\n        \\n        ≤\\n        C\\n        \\n          |\\n        \\n        n\\n        +\\n        m\\n        \\n          |\\n        \\n      \\n    \\n    {\\\\displaystyle \\\\forall \\\\|(n,m)\\\\|_{\\\\infty }\\\\geq M:\\\\quad |g(n,m)|\\\\leq C|n+m|}\\n  where g(n,m) is defined by\\n\\n  \\n    \\n      \\n        f\\n        (\\n        n\\n        ,\\n        m\\n        )\\n        =\\n        \\n          n\\n          \\n            2\\n          \\n        \\n        +\\n        \\n          m\\n          \\n            3\\n          \\n        \\n        +\\n        g\\n        (\\n        n\\n        ,\\n        m\\n        )\\n         \\n        .\\n      \\n    \\n    {\\\\displaystyle f(n,m)=n^{2}+m^{3}+g(n,m)~.}\\n  This definition allows all of the coordinates of \\n  \\n    \\n      \\n        \\n          x\\n        \\n      \\n    \\n    {\\\\displaystyle \\\\mathbf {x} }\\n   to increase to infinity. In particular, the statement\\n\\n  \\n    \\n      \\n        f\\n        (\\n        n\\n        ,\\n        m\\n        )\\n        =\\n        O\\n        (\\n        \\n          n\\n          \\n            m\\n          \\n        \\n        )\\n        \\n        \\n           as \\n        \\n        n\\n        ,\\n        m\\n        →\\n        ∞\\n      \\n    \\n    {\\\\displaystyle f(n,m)=O(n^{m})\\\\quad {\\\\text{ as }}n,m\\\\to \\\\infty }\\n  (i.e., \\n  \\n    \\n      \\n        ∃\\n        C\\n        \\n        ∃\\n        M\\n        \\n        ∀\\n        n\\n        \\n        ∀\\n        m\\n        \\n        ⋯\\n      \\n    \\n    {\\\\displaystyle \\\\exists C\\\\,\\\\exists M\\\\,\\\\forall n\\\\,\\\\forall m\\\\,\\\\cdots }\\n  ) is quite different from\\n\\n  \\n    \\n      \\n        ∀\\n        m\\n        :\\n         \\n        f\\n        (\\n        n\\n        ,\\n        m\\n        )\\n        =\\n        O\\n        (\\n        \\n          n\\n          \\n            m\\n          \\n        \\n        )\\n        \\n        \\n           as \\n        \\n        n\\n        →\\n        ∞\\n      \\n    \\n    {\\\\displaystyle \\\\forall m\\\\colon ~f(n,m)=O(n^{m})\\\\quad {\\\\text{ as }}n\\\\to \\\\infty }\\n  (i.e., \\n  \\n    \\n      \\n        ∀\\n        m\\n        \\n        ∃\\n        C\\n        \\n        ∃\\n        M\\n        \\n        ∀\\n        n\\n        \\n        ⋯\\n      \\n    \\n    {\\\\displaystyle \\\\forall m\\\\,\\\\exists C\\\\,\\\\exists M\\\\,\\\\forall n\\\\,\\\\cdots }\\n  ).\\nUnder this definition, the subset on which a function is defined is significant when generalizing statements from the univariate setting to the multivariate setting. For example, if \\n  \\n    \\n      \\n        f\\n        (\\n        n\\n        ,\\n        m\\n        )\\n        =\\n        1\\n      \\n    \\n    {\\\\displaystyle f(n,m)=1}\\n   and \\n  \\n    \\n      \\n        g\\n        (\\n        n\\n        ,\\n        m\\n        )\\n        =\\n        n\\n      \\n    \\n    {\\\\displaystyle g(n,m)=n}\\n  , then \\n  \\n    \\n      \\n        f\\n        (\\n        n\\n        ,\\n        m\\n        )\\n        =\\n        O\\n        (\\n        g\\n        (\\n        n\\n        ,\\n        m\\n        )\\n        )\\n      \\n    \\n    {\\\\displaystyle f(n,m)=O(g(n,m))}\\n   if we restrict \\n  \\n    \\n      \\n        f\\n      \\n    \\n    {\\\\displaystyle f}\\n   and \\n  \\n    \\n      \\n        g\\n      \\n    \\n    {\\\\displaystyle g}\\n   to \\n  \\n    \\n      \\n        [\\n        1\\n        ,\\n        ∞\\n        \\n          )\\n          \\n            2\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle [1,\\\\infty )^{2}}\\n  , but not if they are defined on \\n  \\n    \\n      \\n        [\\n        0\\n        ,\\n        ∞\\n        \\n          )\\n          \\n            2\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle [0,\\\\infty )^{2}}\\n  .\\nThis is not the only generalization of big O to multivariate functions, and in practice, there is some inconsistency in the choice of definition.\\n\\n\\n== Matters of notation ==\\n\\n\\n=== Equals sign ===\\nThe statement \"f(x) is O(g(x))\" as defined above is usually written as f(x) = O(g(x)). Some consider this to be an abuse of notation, since the use of the equals sign could be misleading as it suggests a symmetry that this statement does not have. As de Bruijn says, O(x) = O(x2) is true but O(x2) = O(x) is not. Knuth describes such statements as \"one-way equalities\", since if the sides could be reversed, \"we could deduce ridiculous things like n = n2 from the identities n = O(n2) and n2 = O(n2).\" In another letter, Knuth also pointed out that \"the equality sign is not symmetric with respect to such notations\", as, in this notation, \"mathematicians customarily use the = sign as they use the word “is” in English: Aristotle is a man, but a man isn’t necessarily Aristotle\".For these reasons, it would be more precise to use set notation and write f(x) ∈ O(g(x)) (read as: \"f(x) is an element of O(g(x))\", or \"f(x) is in the set O(g(x))\"), thinking of O(g(x)) as the class of all functions h(x) such that |h(x)| ≤ C|g(x)| for some constant C. However, the use of the equals sign is customary.\\n\\n\\n=== Other arithmetic operators ===\\nBig O notation can also be used in conjunction with other arithmetic operators in more complicated equations. For example, h(x) + O(f(x)) denotes the collection of functions having the growth of h(x) plus a part whose growth is limited to that of f(x). Thus,\\n\\n  \\n    \\n      \\n        g\\n        (\\n        x\\n        )\\n        =\\n        h\\n        (\\n        x\\n        )\\n        +\\n        O\\n        (\\n        f\\n        (\\n        x\\n        )\\n        )\\n      \\n    \\n    {\\\\displaystyle g(x)=h(x)+O(f(x))}\\n  expresses the same as\\n\\n  \\n    \\n      \\n        g\\n        (\\n        x\\n        )\\n        −\\n        h\\n        (\\n        x\\n        )\\n        =\\n        O\\n        (\\n        f\\n        (\\n        x\\n        )\\n        )\\n        .\\n      \\n    \\n    {\\\\displaystyle g(x)-h(x)=O(f(x)).}\\n  \\n\\n\\n==== Example ====\\nSuppose an algorithm is being developed to operate on a set of n elements. Its developers are interested in finding a function T(n)  that will express how long the algorithm will take to run (in some arbitrary measurement of time) in terms of the number of elements in the input set. The algorithm works by first calling a subroutine to sort the elements in the set and then perform its own operations. The sort has a known time complexity of O(n2), and after the subroutine runs the algorithm must take an additional 55n3 + 2n + 10 steps before it terminates.  Thus the overall time complexity of the algorithm can be expressed as T(n) = 55n3 + O(n2). Here the terms 2n + 10 are subsumed within the faster-growing O(n2).  Again, this usage disregards some of the formal meaning of the \"=\" symbol, but it does allow one to use the big O notation as a kind of convenient placeholder.\\n\\n\\n=== Multiple uses ===\\nIn more complicated usage, O(·) can appear in different places in an equation, even several times on each side. For example, the following are true for \\n  \\n    \\n      \\n        n\\n        →\\n        ∞\\n      \\n    \\n    {\\\\displaystyle n\\\\to \\\\infty }\\n  :\\n\\nThe meaning of such statements is as follows: for any functions which satisfy each O(·) on the left side, there are some functions satisfying each O(·) on the right side, such that substituting all these functions into the equation makes the two sides equal. For example, the third equation above means: \"For any function f(n) = O(1), there is some function g(n)  = O(en) such that nf(n) = g(n).\" In terms of the \"set notation\" above, the meaning is that the class of functions represented by the left side is a subset of the class of functions represented by the right side. In this use the \"=\" is a formal symbol that unlike the usual use of \"=\" is not a symmetric relation. Thus for example nO(1) = O(en) does not imply the false statement O(en) = nO(1)\\n\\n\\n=== Typesetting ===\\nBig O is typeset as an italicized uppercase \"O\", as in the following example: \\n  \\n    \\n      \\n        O\\n        (\\n        \\n          n\\n          \\n            2\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle O(n^{2})}\\n  .  In TeX, it is produced by simply typing O inside math mode.  Unlike Greek-named Bachmann–Landau notations, it needs no special symbol. Yet, some authors use the calligraphic variant \\n  \\n    \\n      \\n        \\n          \\n            O\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle {\\\\mathcal {O}}}\\n   instead.\\n\\n\\n== Orders of common functions ==\\n\\nHere is a list of classes of functions that are commonly encountered when analyzing the running time of an algorithm.  In each case, c is a positive constant and n increases without bound. The slower-growing functions are generally listed first.\\n\\nThe statement \\n  \\n    \\n      \\n        f\\n        (\\n        n\\n        )\\n        =\\n        O\\n        (\\n        n\\n        !\\n        )\\n      \\n    \\n    {\\\\displaystyle f(n)=O(n!)}\\n   is sometimes weakened to \\n  \\n    \\n      \\n        f\\n        (\\n        n\\n        )\\n        =\\n        O\\n        \\n          (\\n          \\n            n\\n            \\n              n\\n            \\n          \\n          )\\n        \\n      \\n    \\n    {\\\\displaystyle f(n)=O\\\\left(n^{n}\\\\right)}\\n   to derive simpler formulas for asymptotic complexity. For any \\n  \\n    \\n      \\n        k\\n        >\\n        0\\n      \\n    \\n    {\\\\displaystyle k>0}\\n   and \\n  \\n    \\n      \\n        c\\n        >\\n        0\\n      \\n    \\n    {\\\\displaystyle c>0}\\n  , \\n  \\n    \\n      \\n        O\\n        (\\n        \\n          n\\n          \\n            c\\n          \\n        \\n        (\\n        log\\n        \\u2061\\n        n\\n        \\n          )\\n          \\n            k\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle O(n^{c}(\\\\log n)^{k})}\\n   is a subset of \\n  \\n    \\n      \\n        O\\n        (\\n        \\n          n\\n          \\n            c\\n            +\\n            ε\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle O(n^{c+\\\\varepsilon })}\\n   for any \\n  \\n    \\n      \\n        ε\\n        >\\n        0\\n      \\n    \\n    {\\\\displaystyle \\\\varepsilon >0}\\n  , so may be considered as a polynomial with some bigger order.\\n\\n\\n== Related asymptotic notations ==\\nBig O is widely used in computer science. Together with some other related notations it forms the family of Bachmann–Landau notations.\\n\\n\\n=== Little-o notation ===\\n\\nIntuitively, the assertion \"f(x) is o(g(x))\" (read \"f(x) is little-o of g(x)\") means that g(x) grows much faster than f(x). Let as before f be a real or complex valued function and g a real valued function, both defined on some unbounded subset of the positive real numbers, such that g(x) is strictly positive for all large enough values of x. One writes\\n\\n  \\n    \\n      \\n        f\\n        (\\n        x\\n        )\\n        =\\n        o\\n        (\\n        g\\n        (\\n        x\\n        )\\n        )\\n        \\n        \\n           as \\n        \\n        x\\n        →\\n        ∞\\n      \\n    \\n    {\\\\displaystyle f(x)=o(g(x))\\\\quad {\\\\text{ as }}x\\\\to \\\\infty }\\n  if for every positive constant ε there exists a constant N such that\\n\\n  \\n    \\n      \\n        \\n          |\\n        \\n        f\\n        (\\n        x\\n        )\\n        \\n          |\\n        \\n        ≤\\n        ε\\n        g\\n        (\\n        x\\n        )\\n        \\n        \\n           for all \\n        \\n        x\\n        ≥\\n        N\\n        .\\n      \\n    \\n    {\\\\displaystyle |f(x)|\\\\leq \\\\varepsilon g(x)\\\\quad {\\\\text{ for all }}x\\\\geq N.}\\n  For example, one has\\n\\n  \\n    \\n      \\n        2\\n        x\\n        =\\n        o\\n        (\\n        \\n          x\\n          \\n            2\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle 2x=o(x^{2})}\\n   and \\n  \\n    \\n      \\n        1\\n        \\n          /\\n        \\n        x\\n        =\\n        o\\n        (\\n        1\\n        )\\n        .\\n      \\n    \\n    {\\\\displaystyle 1/x=o(1).}\\n  The difference between the earlier definition for the big-O notation and the present definition of little-o is that while the former has to be true for at least one constant M, the latter must hold for every positive constant ε, however small. In this way, little-o notation makes a stronger statement than the corresponding big-O notation: every function that is little-o of g is also big-O of g, but not every function that is big-O of g is also little-o of g. For example, \\n  \\n    \\n      \\n        2\\n        \\n          x\\n          \\n            2\\n          \\n        \\n        =\\n        O\\n        (\\n        \\n          x\\n          \\n            2\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle 2x^{2}=O(x^{2})}\\n   but \\n  \\n    \\n      \\n        2\\n        \\n          x\\n          \\n            2\\n          \\n        \\n        ≠\\n        o\\n        (\\n        \\n          x\\n          \\n            2\\n          \\n        \\n        )\\n      \\n    \\n    {\\\\displaystyle 2x^{2}\\\\neq o(x^{2})}\\n  .\\nAs g(x) is nonzero, or at least becomes nonzero beyond a certain point, the relation \\n  \\n    \\n      \\n        f\\n        (\\n        x\\n        )\\n        =\\n        o\\n        (\\n        g\\n        (\\n        x\\n        )\\n        )\\n      \\n    \\n    {\\\\displaystyle f(x)=o(g(x))}\\n   is equivalent to\\n\\n  \\n    \\n      \\n        \\n          lim\\n          \\n            x\\n            →\\n            ∞\\n          \\n        \\n        \\n          \\n            \\n              f\\n              (\\n              x\\n              )\\n            \\n            \\n              g\\n              (\\n              x\\n              )\\n            \\n          \\n        \\n        =\\n        0\\n      \\n    \\n    {\\\\displaystyle \\\\lim _{x\\\\to \\\\infty }{\\\\frac {f(x)}{g(x)}}=0}\\n   (and this is in fact how Landau originally defined the little-o notation).Little-o respects a number of arithmetic operations.  For example,\\n\\nif c is a nonzero constant and \\n  \\n    \\n      \\n        f\\n        =\\n        o\\n        (\\n        g\\n        )\\n      \\n    \\n    {\\\\displaystyle f=o(g)}\\n   then \\n  \\n    \\n      \\n        c\\n        ⋅\\n        f\\n        =\\n        o\\n        (\\n        g\\n        )\\n      \\n    \\n    {\\\\displaystyle c\\\\cdot f=o(g)}\\n  , and\\nif \\n  \\n    \\n      \\n        f\\n        =\\n        o\\n        (\\n        F\\n        )\\n      \\n    \\n    {\\\\displaystyle f=o(F)}\\n   and \\n  \\n    \\n      \\n        g\\n        =\\n        o\\n        (\\n        G\\n        )\\n      \\n    \\n    {\\\\displaystyle g=o(G)}\\n   then \\n  \\n    \\n      \\n        f\\n        ⋅\\n        g\\n        =\\n        o\\n        (\\n        F\\n        ⋅\\n        G\\n        )\\n        .\\n      \\n    \\n    {\\\\displaystyle f\\\\cdot g=o(F\\\\cdot G).}\\n  It also satisfies a transitivity relation:\\n\\nif \\n  \\n    \\n      \\n        f\\n        =\\n        o\\n        (\\n        g\\n        )\\n      \\n    \\n    {\\\\displaystyle f=o(g)}\\n   and \\n  \\n    \\n      \\n        g\\n        =\\n        o\\n        (\\n        h\\n        )\\n      \\n    \\n    {\\\\displaystyle g=o(h)}\\n   then \\n  \\n    \\n      \\n        f\\n        =\\n        o\\n        (\\n        h\\n        )\\n        .\\n      \\n    \\n    {\\\\displaystyle f=o(h).}\\n  \\n\\n\\n=== Big Omega notation ===\\nAnother asymptotic notation is \\n  \\n    \\n      \\n        Ω\\n      \\n    \\n    {\\\\displaystyle \\\\Omega }\\n  , read \"big omega\". There are two widespread and incompatible definitions of the statement\\n\\n  \\n    \\n      \\n        f\\n        (\\n        x\\n        )\\n        =\\n        Ω\\n        (\\n        g\\n        (\\n        x\\n        )\\n        )\\n      \\n    \\n    {\\\\displaystyle f(x)=\\\\Omega (g(x))}\\n   as \\n  \\n    \\n      \\n        x\\n        →\\n        a\\n      \\n    \\n    {\\\\displaystyle x\\\\to a}\\n  ,where a is some real number, ∞,  or −∞, where f and g are real functions defined in a neighbourhood of a, and where g is positive in this neighbourhood.\\nThe Hardy–Littlewood definition is used mainly in analytic number theory, and the Knuth definition mainly in computational complexity theory; the definitions are not equivalent.\\n\\n\\n==== The Hardy–Littlewood definition ====\\nIn 1914 Godfrey Harold Hardy and John Edensor Littlewood introduced the new symbol \\n  \\n    \\n      \\n        Ω\\n      \\n    \\n    {\\\\displaystyle \\\\Omega }\\n  , which is defined as follows:\\n\\n  \\n    \\n      \\n        f\\n        (\\n        x\\n        )\\n        =\\n        Ω\\n        (\\n        g\\n        (\\n        x\\n        )\\n        )\\n      \\n    \\n    {\\\\displaystyle f(x)=\\\\Omega (g(x))}\\n   as \\n  \\n    \\n      \\n        x\\n        →\\n        ∞\\n      \\n    \\n    {\\\\displaystyle x\\\\to \\\\infty }\\n   if \\n  \\n    \\n      \\n        \\n          lim\\u2006sup\\n          \\n            x\\n            →\\n            ∞\\n          \\n        \\n        \\n          |\\n          \\n            \\n              \\n                f\\n                (\\n                x\\n                )\\n              \\n              \\n                g\\n                (\\n                x\\n                )\\n              \\n            \\n          \\n          |\\n        \\n        >\\n        0.\\n      \\n    \\n    {\\\\displaystyle \\\\limsup _{x\\\\to \\\\infty }\\\\left|{\\\\frac {f(x)}{g(x)}}\\\\right|>0.}\\n  Thus \\n  \\n    \\n      \\n        f\\n        (\\n        x\\n        )\\n        =\\n        Ω\\n        (\\n        g\\n        (\\n        x\\n        )\\n        )\\n      \\n    \\n    {\\\\displaystyle f(x)=\\\\Omega (g(x))}\\n   is the negation of \\n  \\n    \\n      \\n        f\\n        (\\n        x\\n        )\\n        =\\n        o\\n        (\\n        g\\n        (\\n        x\\n        )\\n        )\\n      \\n    \\n    {\\\\displaystyle f(x)=o(g(x))}\\n  .\\nIn 1916 the same authors introduced the two new symbols \\n  \\n    \\n      \\n        \\n          Ω\\n          \\n            R\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle \\\\Omega _{R}}\\n   and \\n  \\n    \\n      \\n        \\n          Ω\\n          \\n            L\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle \\\\Omega _{L}}\\n  , defined as:\\n\\n  \\n    \\n      \\n        f\\n        (\\n        x\\n        )\\n        =\\n        \\n          Ω\\n          \\n            R\\n          \\n        \\n        (\\n        g\\n        (\\n        x\\n        )\\n        )\\n      \\n    \\n    {\\\\displaystyle f(x)=\\\\Omega _{R}(g(x))}\\n   as \\n  \\n    \\n      \\n        x\\n        →\\n        ∞\\n      \\n    \\n    {\\\\displaystyle x\\\\to \\\\infty }\\n   if \\n  \\n    \\n      \\n        \\n          lim\\u2006sup\\n          \\n            x\\n            →\\n            ∞\\n          \\n        \\n        \\n          \\n            \\n              f\\n              (\\n              x\\n              )\\n            \\n            \\n              g\\n              (\\n              x\\n              )\\n            \\n          \\n        \\n        >\\n        0\\n      \\n    \\n    {\\\\displaystyle \\\\limsup _{x\\\\to \\\\infty }{\\\\frac {f(x)}{g(x)}}>0}\\n  ;\\n  \\n    \\n      \\n        f\\n        (\\n        x\\n        )\\n        =\\n        \\n          Ω\\n          \\n            L\\n          \\n        \\n        (\\n        g\\n        (\\n        x\\n        )\\n        )\\n      \\n    \\n    {\\\\displaystyle f(x)=\\\\Omega _{L}(g(x))}\\n   as \\n  \\n    \\n      \\n        x\\n        →\\n        ∞\\n      \\n    \\n    {\\\\displaystyle x\\\\to \\\\infty }\\n   if \\n  \\n    \\n      \\n        \\n          lim\\u2006inf\\n          \\n            x\\n            →\\n            ∞\\n          \\n        \\n        \\n          \\n            \\n              f\\n              (\\n              x\\n              )\\n            \\n            \\n              g\\n              (\\n              x\\n              )\\n            \\n          \\n        \\n        <\\n        0.\\n      \\n    \\n    {\\\\displaystyle \\\\liminf _{x\\\\to \\\\infty }{\\\\frac {f(x)}{g(x)}}<0.}\\n  These symbols were used by Edmund Landau, with the same meanings, in 1924.  After Landau, the notations were never used again exactly thus; \\n  \\n    \\n      \\n        \\n          Ω\\n          \\n            R\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle \\\\Omega _{R}}\\n   became \\n  \\n    \\n      \\n        \\n          Ω\\n          \\n            +\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle \\\\Omega _{+}}\\n   and \\n  \\n    \\n      \\n        \\n          Ω\\n          \\n            L\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle \\\\Omega _{L}}\\n   became \\n  \\n    \\n      \\n        \\n          Ω\\n          \\n            −\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle \\\\Omega _{-}}\\n  .These three symbols \\n  \\n    \\n      \\n        Ω\\n        ,\\n        \\n          Ω\\n          \\n            +\\n          \\n        \\n        ,\\n        \\n          Ω\\n          \\n            −\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle \\\\Omega ,\\\\Omega _{+},\\\\Omega _{-}}\\n  , as well as \\n  \\n    \\n      \\n        f\\n        (\\n        x\\n        )\\n        =\\n        \\n          Ω\\n          \\n            ±\\n          \\n        \\n        (\\n        g\\n        (\\n        x\\n        )\\n        )\\n      \\n    \\n    {\\\\displaystyle f(x)=\\\\Omega _{\\\\pm }(g(x))}\\n   (meaning that \\n  \\n    \\n      \\n        f\\n        (\\n        x\\n        )\\n        =\\n        \\n          Ω\\n          \\n            +\\n          \\n        \\n        (\\n        g\\n        (\\n        x\\n        )\\n        )\\n      \\n    \\n    {\\\\displaystyle f(x)=\\\\Omega _{+}(g(x))}\\n   and \\n  \\n    \\n      \\n        f\\n        (\\n        x\\n        )\\n        =\\n        \\n          Ω\\n          \\n            −\\n          \\n        \\n        (\\n        g\\n        (\\n        x\\n        )\\n        )\\n      \\n    \\n    {\\\\displaystyle f(x)=\\\\Omega _{-}(g(x))}\\n   are both satisfied), are now currently used in analytic number theory.\\n\\n\\n===== Simple examples =====\\nWe have\\n\\n  \\n    \\n      \\n        sin\\n        \\u2061\\n        x\\n        =\\n        Ω\\n        (\\n        1\\n        )\\n      \\n    \\n    {\\\\displaystyle \\\\sin x=\\\\Omega (1)}\\n   as \\n  \\n    \\n      \\n        x\\n        →\\n        ∞\\n        ,\\n      \\n    \\n    {\\\\displaystyle x\\\\to \\\\infty ,}\\n  and more precisely\\n\\n  \\n    \\n      \\n        sin\\n        \\u2061\\n        x\\n        =\\n        \\n          Ω\\n          \\n            ±\\n          \\n        \\n        (\\n        1\\n        )\\n      \\n    \\n    {\\\\displaystyle \\\\sin x=\\\\Omega _{\\\\pm }(1)}\\n   as \\n  \\n    \\n      \\n        x\\n        →\\n        ∞\\n        .\\n      \\n    \\n    {\\\\displaystyle x\\\\to \\\\infty .}\\n  We have\\n\\n  \\n    \\n      \\n        sin\\n        \\u2061\\n        x\\n        +\\n        1\\n        =\\n        Ω\\n        (\\n        1\\n        )\\n      \\n    \\n    {\\\\displaystyle \\\\sin x+1=\\\\Omega (1)}\\n   as \\n  \\n    \\n      \\n        x\\n        →\\n        ∞\\n        ,\\n      \\n    \\n    {\\\\displaystyle x\\\\to \\\\infty ,}\\n  and more precisely\\n\\n  \\n    \\n      \\n        sin\\n        \\u2061\\n        x\\n        +\\n        1\\n        =\\n        \\n          Ω\\n          \\n            +\\n          \\n        \\n        (\\n        1\\n        )\\n      \\n    \\n    {\\\\displaystyle \\\\sin x+1=\\\\Omega _{+}(1)}\\n   as \\n  \\n    \\n      \\n        x\\n        →\\n        ∞\\n        ;\\n      \\n    \\n    {\\\\displaystyle x\\\\to \\\\infty ;}\\n  however\\n\\n  \\n    \\n      \\n        sin\\n        \\u2061\\n        x\\n        +\\n        1\\n        ≠\\n        \\n          Ω\\n          \\n            −\\n          \\n        \\n        (\\n        1\\n        )\\n      \\n    \\n    {\\\\displaystyle \\\\sin x+1\\\\not =\\\\Omega _{-}(1)}\\n   as \\n  \\n    \\n      \\n        x\\n        →\\n        ∞\\n        .\\n      \\n    \\n    {\\\\displaystyle x\\\\to \\\\infty .}\\n  \\n\\n\\n==== The Knuth definition ====\\nIn 1976 Donald Knuth published a paper to justify his use of the \\n  \\n    \\n      \\n        Ω\\n      \\n    \\n    {\\\\displaystyle \\\\Omega }\\n  -symbol to describe a stronger property. Knuth wrote: \"For all the applications I have seen so far in computer science, a stronger requirement ... is much more appropriate\". He defined\\n\\n  \\n    \\n      \\n        f\\n        (\\n        x\\n        )\\n        =\\n        Ω\\n        (\\n        g\\n        (\\n        x\\n        )\\n        )\\n        ⇔\\n        g\\n        (\\n        x\\n        )\\n        =\\n        O\\n        (\\n        f\\n        (\\n        x\\n        )\\n        )\\n      \\n    \\n    {\\\\displaystyle f(x)=\\\\Omega (g(x))\\\\Leftrightarrow g(x)=O(f(x))}\\n  with the comment: \"Although I have changed Hardy and Littlewood\\'s definition of \\n  \\n    \\n      \\n        Ω\\n      \\n    \\n    {\\\\displaystyle \\\\Omega }\\n  , I feel justified in doing so because their definition is by no means in wide use, and because there are other ways to say what they want to say in the comparatively rare cases when their definition applies.\"\\n\\n\\n=== Family of Bachmann–Landau notations ===\\nThe limit definitions assume \\n  \\n    \\n      \\n        g\\n        (\\n        n\\n        )\\n        >\\n        0\\n      \\n    \\n    {\\\\displaystyle g(n)>0}\\n   for sufficiently large \\n  \\n    \\n      \\n        n\\n      \\n    \\n    {\\\\displaystyle n}\\n  . The table is (partly) sorted from smallest to largest, in the sense that \\n  \\n    \\n      \\n        o\\n        ,\\n        O\\n        ,\\n        Θ\\n        ,\\n        ∼\\n        ,\\n      \\n    \\n    {\\\\displaystyle o,O,\\\\Theta ,\\\\sim ,}\\n   (Knuth\\'s version of) \\n  \\n    \\n      \\n        Ω\\n        ,\\n        ω\\n      \\n    \\n    {\\\\displaystyle \\\\Omega ,\\\\omega }\\n   on functions correspond to \\n  \\n    \\n      \\n        <\\n        ,\\n        ≤\\n        ,\\n        ≈\\n        ,\\n        =\\n        ,\\n      \\n    \\n    {\\\\displaystyle <,\\\\leq ,\\\\approx ,=,}\\n  \\n  \\n    \\n      \\n        ≥\\n        ,\\n        >\\n      \\n    \\n    {\\\\displaystyle \\\\geq ,>}\\n   on the real line (the Hardy-Littlewood version of \\n  \\n    \\n      \\n        Ω\\n      \\n    \\n    {\\\\displaystyle \\\\Omega }\\n  , however, doesn\\'t correspond to any such description).\\nComputer science uses the big \\n  \\n    \\n      \\n        O\\n      \\n    \\n    {\\\\displaystyle O}\\n  , big Theta \\n  \\n    \\n      \\n        Θ\\n      \\n    \\n    {\\\\displaystyle \\\\Theta }\\n  , little \\n  \\n    \\n      \\n        o\\n      \\n    \\n    {\\\\displaystyle o}\\n  , little omega \\n  \\n    \\n      \\n        ω\\n      \\n    \\n    {\\\\displaystyle \\\\omega }\\n   and Knuth\\'s big Omega \\n  \\n    \\n      \\n        Ω\\n      \\n    \\n    {\\\\displaystyle \\\\Omega }\\n   notations. Analytic number theory often uses the big \\n  \\n    \\n      \\n        O\\n      \\n    \\n    {\\\\displaystyle O}\\n  , small \\n  \\n    \\n      \\n        o\\n      \\n    \\n    {\\\\displaystyle o}\\n  , Hardy–Littlewood\\'s big Omega \\n  \\n    \\n      \\n        Ω\\n      \\n    \\n    {\\\\displaystyle \\\\Omega }\\n   (with or without the +, − or ± subscripts) and \\n  \\n    \\n      \\n        ∼\\n      \\n    \\n    {\\\\displaystyle \\\\sim }\\n   notations. The small omega \\n  \\n    \\n      \\n        ω\\n      \\n    \\n    {\\\\displaystyle \\\\omega }\\n   notation is not used as often in analysis.\\n\\n\\n=== Use in computer science ===\\n\\nInformally, especially in computer science, the big O notation often can be used somewhat differently to describe an asymptotic tight bound where using big Theta Θ notation might be more factually appropriate in a given context. For example, when considering a function T(n) = 73n3 + 22n2 + 58, all of the following are generally acceptable, but tighter bounds (such as numbers 2 and 3 below) are usually strongly preferred over looser bounds (such as number 1 below).\\n\\nT(n) = O(n100)\\nT(n) = O(n3)\\nT(n) = Θ(n3)The equivalent English statements are respectively:\\n\\nT(n) grows asymptotically no faster than n100\\nT(n) grows asymptotically no faster than n3\\nT(n) grows asymptotically as fast as n3.So while all three statements are true, progressively more information is contained in each. In some fields, however, the big O notation (number 2 in the lists above) would be used more commonly than the big Theta notation (items numbered 3 in the lists above). For example, if T(n) represents the running time of a newly developed algorithm for input size n, the inventors and users of the algorithm might be more inclined to put an upper asymptotic bound on how long it will take to run without making an explicit statement about the lower asymptotic bound.\\n\\n\\n=== Other notation ===\\nIn their book Introduction to Algorithms, Cormen, Leiserson, Rivest and Stein consider the set of functions f which satisfy\\n\\n  \\n    \\n      \\n        f\\n        (\\n        n\\n        )\\n        =\\n        O\\n        (\\n        g\\n        (\\n        n\\n        )\\n        )\\n        \\n        (\\n        n\\n        →\\n        ∞\\n        )\\n         \\n        .\\n      \\n    \\n    {\\\\displaystyle f(n)=O(g(n))\\\\quad (n\\\\to \\\\infty )~.}\\n  In a correct notation this set can, for instance, be called O(g), where\\n\\n  \\n    \\n      \\n        O\\n        (\\n        g\\n        )\\n        =\\n        {\\n        f\\n        :\\n        \\n          there exist positive constants\\n        \\n         \\n        c\\n         \\n        \\n          and\\n        \\n         \\n        \\n          n\\n          \\n            0\\n          \\n        \\n         \\n        \\n          such that\\n        \\n         \\n        0\\n        ≤\\n        f\\n        (\\n        n\\n        )\\n        ≤\\n        c\\n        g\\n        (\\n        n\\n        )\\n        \\n           for all \\n        \\n        n\\n        ≥\\n        \\n          n\\n          \\n            0\\n          \\n        \\n        }\\n        .\\n      \\n    \\n    {\\\\displaystyle O(g)=\\\\{f:{\\\\text{there exist positive constants}}~c~{\\\\text{and}}~n_{0}~{\\\\text{such that}}~0\\\\leq f(n)\\\\leq cg(n){\\\\text{ for all }}n\\\\geq n_{0}\\\\}.}\\n  The authors state that the use of equality operator (=) to denote set membership rather than the set membership operator (∈) is an abuse of notation, but that doing so has advantages. Inside an equation or inequality, the use of asymptotic notation stands for an anonymous function in the set O(g), which eliminates lower-order terms, and helps to reduce inessential clutter in equations, for example:\\n\\n  \\n    \\n      \\n        2\\n        \\n          n\\n          \\n            2\\n          \\n        \\n        +\\n        3\\n        n\\n        +\\n        1\\n        =\\n        2\\n        \\n          n\\n          \\n            2\\n          \\n        \\n        +\\n        O\\n        (\\n        n\\n        )\\n        .\\n      \\n    \\n    {\\\\displaystyle 2n^{2}+3n+1=2n^{2}+O(n).}\\n  \\n\\n\\n=== Extensions to the Bachmann–Landau notations ===\\nAnother notation sometimes used in computer science is Õ (read soft-O): f(n) = Õ(g(n)) is shorthand for f(n) = O(g(n) logk g(n)) for some k. Essentially, it is big O notation, ignoring logarithmic factors because the growth-rate effects of some other super-logarithmic function indicate a growth-rate explosion for large-sized input parameters that is more important to predicting bad run-time performance than the finer-point effects contributed by the logarithmic-growth factor(s). This notation is often used to obviate the \"nitpicking\" within growth-rates that are stated as too tightly bounded for the matters at hand (since logk n is always o(nε) for any constant k and any ε > 0).\\nAlso the L notation, defined as\\n\\n  \\n    \\n      \\n        \\n          L\\n          \\n            n\\n          \\n        \\n        [\\n        α\\n        ,\\n        c\\n        ]\\n        =\\n        \\n          e\\n          \\n            (\\n            c\\n            +\\n            o\\n            (\\n            1\\n            )\\n            )\\n            (\\n            ln\\n            \\u2061\\n            n\\n            \\n              )\\n              \\n                α\\n              \\n            \\n            (\\n            ln\\n            \\u2061\\n            ln\\n            \\u2061\\n            n\\n            \\n              )\\n              \\n                1\\n                −\\n                α\\n              \\n            \\n          \\n        \\n      \\n    \\n    {\\\\displaystyle L_{n}[\\\\alpha ,c]=e^{(c+o(1))(\\\\ln n)^{\\\\alpha }(\\\\ln \\\\ln n)^{1-\\\\alpha }}}\\n  is convenient for functions that are between polynomial and exponential in terms of \\n  \\n    \\n      \\n        ln\\n        \\u2061\\n        n\\n      \\n    \\n    {\\\\displaystyle \\\\ln n}\\n  .\\n\\n\\n== Generalizations and related usages ==\\nThe generalization to functions taking values in any normed vector space is straightforward (replacing absolute values by norms), where f and g need not take their values in the same space. A generalization to functions g taking values in any topological group is also possible.\\nThe \"limiting process\" x → xo can also be generalized by introducing an arbitrary filter base, i.e. to directed nets f and g. The o notation can be used to define derivatives and differentiability in quite general spaces, and also (asymptotical) equivalence of functions,\\n\\n  \\n    \\n      \\n        f\\n        ∼\\n        g\\n        \\n        ⟺\\n        \\n        (\\n        f\\n        −\\n        g\\n        )\\n        ∈\\n        o\\n        (\\n        g\\n        )\\n      \\n    \\n    {\\\\displaystyle f\\\\sim g\\\\iff (f-g)\\\\in o(g)}\\n  which is an equivalence relation and a more restrictive notion than the relationship \"f is Θ(g)\" from above. (It reduces to lim f / g = 1 if f and g are positive real valued functions.)  For example, 2x is Θ(x), but 2x − x is not o(x).\\n\\n\\n== History (Bachmann–Landau, Hardy, and Vinogradov notations) ==\\nThe symbol O was first introduced by number theorist Paul Bachmann in 1894, in the second volume of his book Analytische Zahlentheorie (\"analytic number theory\"). The number theorist Edmund Landau adopted it, and was thus inspired to introduce in 1909 the notation o; hence both are now called Landau symbols. These notations were used in applied mathematics during the 1950s for asymptotic analysis.\\nThe symbol \\n  \\n    \\n      \\n        Ω\\n      \\n    \\n    {\\\\displaystyle \\\\Omega }\\n   (in the sense \"is not an o of\") was introduced in 1914 by Hardy and Littlewood. Hardy and Littlewood also introduced in 1916 the symbols \\n  \\n    \\n      \\n        \\n          Ω\\n          \\n            R\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle \\\\Omega _{R}}\\n   (\"right\") and \\n  \\n    \\n      \\n        \\n          Ω\\n          \\n            L\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle \\\\Omega _{L}}\\n   (\"left\"),  precursors of the modern symbols \\n  \\n    \\n      \\n        \\n          Ω\\n          \\n            +\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle \\\\Omega _{+}}\\n   (\"is not smaller than a small o of\") and \\n  \\n    \\n      \\n        \\n          Ω\\n          \\n            −\\n          \\n        \\n      \\n    \\n    {\\\\displaystyle \\\\Omega _{-}}\\n   (\"is not larger than a small o of\"). Thus the Omega symbols (with their original meanings) are sometimes also referred to as \"Landau symbols\". This notation \\n  \\n    \\n      \\n        Ω\\n      \\n    \\n    {\\\\displaystyle \\\\Omega }\\n   became commonly used in number theory at least since the 1950s.\\nIn the 1970s the big O was popularized in computer science by Donald Knuth, who introduced the related Theta notation, and proposed a different definition for the Omega notation.Landau never used the big Theta and small omega symbols.\\nHardy\\'s symbols were (in terms of the modern O notation)\\n\\n  \\n    \\n      \\n        f\\n        ≼\\n        g\\n        \\n        ⟺\\n        \\n        f\\n        ∈\\n        O\\n        (\\n        g\\n        )\\n      \\n    \\n    {\\\\displaystyle f\\\\preccurlyeq g\\\\iff f\\\\in O(g)}\\n     and   \\n  \\n    \\n      \\n        f\\n        ≺\\n        g\\n        \\n        ⟺\\n        \\n        f\\n        ∈\\n        o\\n        (\\n        g\\n        )\\n        ;\\n      \\n    \\n    {\\\\displaystyle f\\\\prec g\\\\iff f\\\\in o(g);}\\n  (Hardy however never defined or used the notation \\n  \\n    \\n      \\n        ≺\\n        \\n        \\n        ≺\\n      \\n    \\n    {\\\\displaystyle \\\\prec \\\\!\\\\!\\\\prec }\\n  , nor \\n  \\n    \\n      \\n        ≪\\n      \\n    \\n    {\\\\displaystyle \\\\ll }\\n  , as it has been sometimes reported).\\nHardy introduced the symbols \\n  \\n    \\n      \\n        ≼\\n      \\n    \\n    {\\\\displaystyle \\\\preccurlyeq }\\n   and \\n  \\n    \\n      \\n        ≺\\n      \\n    \\n    {\\\\displaystyle \\\\prec }\\n   (as well as some other symbols) in his 1910 tract \"Orders of Infinity\", and made use of them only in three papers (1910–1913). In his nearly 400 remaining papers and books he consistently used the Landau symbols O and o.\\nHardy\\'s notation is not used anymore. On the other hand, in the 1930s, the Russian number theorist  Ivan Matveyevich Vinogradov  introduced his notation\\t\\n  \\n    \\n      \\n        ≪\\n      \\n    \\n    {\\\\displaystyle \\\\ll }\\n  , which  has been increasingly used in number theory instead of  the \\n  \\n    \\n      \\n        O\\n      \\n    \\n    {\\\\displaystyle O}\\n   notation. We have\\n\\n  \\n    \\n      \\n        f\\n        ≪\\n        g\\n        \\n        ⟺\\n        \\n        f\\n        ∈\\n        O\\n        (\\n        g\\n        )\\n        ,\\n      \\n    \\n    {\\\\displaystyle f\\\\ll g\\\\iff f\\\\in O(g),}\\n  and frequently both notations are used in the same paper.\\nThe big-O originally stands for \"order of\" (\"Ordnung\", Bachmann 1894), and is thus a Latin letter. Neither Bachmann nor Landau ever call it \"Omicron\". The symbol was much later on (1976) viewed by Knuth as a capital omicron, probably in reference to his definition of the symbol Omega. The digit zero should not be used.\\n\\n\\n== See also ==\\nAsymptotic expansion: Approximation of functions generalizing Taylor\\'s formula\\nAsymptotically optimal algorithm: A phrase frequently used to describe an algorithm that has an upper bound asymptotically within a constant of a lower bound for the problem\\nBig O in probability notation: Op, op\\nLimit superior and limit inferior: An explanation of some of the limit notation used in this article\\nMaster theorem (analysis of algorithms): For analyzing divide-and-conquer recursive algorithms using Big O notation\\nNachbin\\'s theorem: A precise method of bounding complex analytic functions so that the domain of convergence of integral transforms can be stated\\nOrders of approximation\\nComputational complexity of mathematical operations\\n\\n\\n== References and notes ==\\n\\n\\n== Further reading ==\\nHardy, G. H. (1910). Orders of Infinity: The \\'Infinitärcalcül\\' of Paul du Bois-Reymond. Cambridge University Press.\\nKnuth, Donald (1997). \"1.2.11: Asymptotic Representations\". Fundamental Algorithms. The Art of Computer Programming. 1 (3rd ed.). Addison-Wesley. ISBN 978-0-201-89683-1.\\nCormen, Thomas H.; Leiserson, Charles E.; Rivest, Ronald L.; Stein, Clifford (2001). \"3.1: Asymptotic notation\". Introduction to Algorithms (2nd ed.). MIT Press and McGraw-Hill. ISBN 978-0-262-03293-3.\\nSipser, Michael (1997). Introduction to the Theory of Computation. PWS Publishing. pp. 226–228. ISBN 978-0-534-94728-6.\\nAvigad, Jeremy; Donnelly, Kevin (2004). Formalizing O notation in Isabelle/HOL (PDF). International Joint Conference on Automated Reasoning. doi:10.1007/978-3-540-25984-8_27.\\nBlack, Paul E. (11 March 2005).  Black, Paul E. (ed.). \"big-O notation\". Dictionary of Algorithms and Data Structures. U.S. National Institute of Standards and Technology. Retrieved December 16, 2006.\\nBlack, Paul E. (17 December 2004).  Black, Paul E. (ed.). \"little-o notation\". Dictionary of Algorithms and Data Structures. U.S. National Institute of Standards and Technology. Retrieved December 16, 2006.\\nBlack, Paul E. (17 December 2004).  Black, Paul E. (ed.). \"Ω\". Dictionary of Algorithms and Data Structures. U.S. National Institute of Standards and Technology. Retrieved December 16, 2006.\\nBlack, Paul E. (17 December 2004).  Black, Paul E. (ed.). \"ω\". Dictionary of Algorithms and Data Structures. U.S. National Institute of Standards and Technology. Retrieved December 16, 2006.\\nBlack, Paul E. (17 December 2004).  Black, Paul E. (ed.). \"Θ\". Dictionary of Algorithms and Data Structures. U.S. National Institute of Standards and Technology. Retrieved December 16, 2006.\\n\\n\\n== External links ==\\nGrowth of sequences — OEIS (Online Encyclopedia of Integer Sequences) Wiki\\nIntroduction to Asymptotic Notations\\nLandau Symbols\\nBig-O Notation – What is it good for\\nBig O Notation explained in plain english\\nAn example of Big O in accuracy of central divided difference scheme for first derivative\\nA Gentle Introduction to Algorithm Complexity Analysis', 'In programming languages, a closure, also lexical closure or function closure, is a technique for implementing lexically scoped name binding in a language with first-class functions. Operationally, a closure is a record storing a function together with an environment. The environment is a mapping associating each free variable of the function (variables that are used locally, but defined in an enclosing scope) with the value or reference to which the name was bound when the closure was created. Unlike a plain function, a closure allows the function to access those captured variables through the closure\\'s copies of their values or references, even when the function is invoked outside their scope.\\n\\n\\n== History and etymology ==\\nThe concept of closures was developed in the 1960s for the mechanical evaluation of expressions in the λ-calculus and was first fully implemented in 1970 as a language feature in the PAL programming language to support lexically scoped first-class functions.Peter J. Landin defined the term closure in 1964 as having an environment part and a control part as used by his SECD machine for evaluating expressions. Joel Moses credits Landin with introducing the term closure to refer to a lambda expression whose open bindings (free variables) have been closed by (or bound in) the lexical environment, resulting in a closed expression, or closure. This usage was subsequently adopted by Sussman and Steele when they defined Scheme in 1975, a lexically scoped variant of Lisp, and became widespread.\\nSussman and Abelson also use the term closure in the 1980s with a second, unrelated meaning: the property of an operator that adds data to a data structure to also be able to add nested data structures. This usage of the term comes from the mathematics usage rather than the prior usage in computer science. The authors consider this overlap in terminology to be \"unfortunate.\"\\n\\n\\n== Anonymous functions ==\\n\\nThe term closure is often used as a synonym for anonymous function, though strictly, an anonymous function is a function literal without a name, while a closure is an instance of a function, a value, whose non-local variables have been bound either to values or to storage locations (depending on the language; see the lexical environment section below).\\nFor example, in the following Python code:\\n\\nthe values of a and b are closures, in both cases produced by returning a nested function with a free variable from the enclosing function, so that the free variable binds to the value of parameter x of the enclosing function. The closures in a and b are functionally identical. The only difference in implementation is that in the first case we used a nested function with a name, g, while in the second case we used an anonymous nested function (using the Python keyword lambda for creating an anonymous function). The original name, if any, used in defining them is irrelevant.\\nA closure is a value like any other value. It does not need to be assigned to a variable and can instead be used directly, as shown in the last two lines of the example. This usage may be deemed an \"anonymous closure\".\\nThe nested function definitions are not themselves closures: they have a free variable which is not yet bound. Only once the enclosing function is evaluated with a value for the parameter is the free variable of the nested function bound, creating a closure, which is then returned from the enclosing function.\\nLastly, a closure is only distinct from a function with free variables when outside of the scope of the non-local variables, otherwise the defining environment and the execution environment coincide and there is nothing to distinguish these (static and dynamic binding cannot be distinguished because the names resolve to the same values). For example, in the below program, functions with a free variable x (bound to the non-local variable x with global scope) are executed in the same environment where x is defined, so it is immaterial whether these are actually closures:\\n\\nThis is most often achieved by a function return, since the function must be defined within the scope of the non-local variables, in which case typically its own scope will be smaller.\\nThis can also be achieved by variable shadowing (which reduces the scope of the non-local variable), though this is less common in practice, as it is less useful and shadowing is discouraged. In this example f can be seen to be a closure because x in the body of f is bound to the x in the global namespace, not the x local to g:\\n\\n\\n== Applications ==\\nThe use of closures is associated with languages where functions are first-class objects, in which functions can be returned as results from higher-order functions, or passed as arguments to other function calls; if functions with free variables are first-class, then returning one creates a closure. This includes functional programming languages such as Lisp and ML, as well as many modern, multi-paradigm languages, such as Python and Rust. Closures are also frequently used with callbacks, particularly for event handlers, such as in JavaScript, where they are used for interactions with a dynamic web page.\\nClosures can also be used in a continuation-passing style to hide state. Constructs such as objects and control structures can thus be implemented with closures. In some languages, a closure may occur when a function is defined within another function, and the inner function refers to local variables of the outer function. At run-time, when the outer function executes, a closure is formed, consisting of the inner function\\'s code and references (the upvalues) to any variables of the outer function required by the closure.\\n\\n\\n=== First-class functions ===\\n\\nClosures typically appear in languages with first-class functions—in other words, such languages enable functions to be passed as arguments, returned from function calls, bound to variable names, etc., just like simpler types such as strings and integers. For example, consider the following Scheme function:\\n\\nIn this example, the lambda expression (lambda (book) (>= (book-sales book) threshold)) appears within the function best-selling-books.  When the lambda expression is evaluated, Scheme creates a closure consisting of the code for the lambda expression and a reference to the threshold variable, which is a free variable inside the lambda expression.\\nThe closure is then passed to the filter function, which calls it repeatedly to determine which books are to be added to the result list and which are to be discarded. Because the closure itself has a reference to threshold, it can use that variable each time filter calls it. The function filter itself might be defined in a completely separate file.\\nHere is the same example rewritten in JavaScript, another popular language with support for closures:\\n\\nThe function keyword is used here instead of lambda, and an Array.filter method instead of a global filter function, but otherwise the structure and the effect of the code are the same.\\nA function may create a closure and return it, as in the following example:\\n\\nBecause the closure in this case outlives the execution of the function that creates it, the variables f and dx live on after the function derivative returns, even though execution has left their scope and they are no longer visible. In languages without closures, the lifetime of an automatic local variable coincides with the execution of the stack frame where that variable is declared. In languages with closures, variables must continue to exist as long as any existing closures have references to them. This is most commonly implemented using some form of garbage collection.\\n\\n\\n=== State representation ===\\nA closure can be used to associate a function with a set of \"private\" variables, which persist over several invocations of the function. The scope of the variable encompasses only the closed-over function, so it cannot be accessed from other program code. These are analogous to private variables in object-oriented programming, and in fact closures are analogous to a type of object, specifically function objects, with a single public method (function call), and possibly many private variables (the closed-over variables).\\nIn stateful languages, closures can thus be used to implement paradigms for state representation and information hiding, since the closure\\'s upvalues (its closed-over variables) are of indefinite extent, so a value established in one invocation remains available in the next.  Closures used in this way no longer have referential transparency, and are thus no longer pure functions; nevertheless, they are commonly used in impure functional languages such as Scheme.\\n\\n\\n=== Other uses ===\\nClosures have many uses:\\n\\nBecause closures delay evaluation—i.e., they do not \"do\" anything until they are called—they can be used to define control structures.  For example, all of Smalltalk\\'s standard control structures, including branches (if/then/else) and loops (while and for), are defined using objects whose methods accept closures.  Users can easily define their own control structures also.\\nIn languages which implement assignment, multiple functions can be produced that close over the same environment, enabling them to communicate privately by altering that environment. In Scheme:\\nClosures can be used to implement object systems.Note: Some speakers call any data structure that binds a lexical environment a closure, but the term usually refers specifically to functions.\\n\\n\\n== Implementation and theory ==\\nClosures are typically implemented with a special data structure that contains a pointer to the function code, plus a representation of the function\\'s lexical environment (i.e., the set of available variables) at the time when the closure was created. The referencing environment binds the non-local names to the corresponding variables in the lexical environment at the time the closure is created, additionally extending their lifetime to at least as long as the lifetime of the closure itself. When the closure is entered at a later time, possibly with a different lexical environment, the function is executed with its non-local variables referring to the ones captured by the closure, not the current environment.\\nA language implementation cannot easily support full closures if its run-time memory model allocates all automatic variables on a linear stack. In such languages, a function\\'s automatic local variables are deallocated when the function returns. However, a closure requires that the free variables it references survive the enclosing function\\'s execution. Therefore, those variables must be allocated so that they persist until no longer needed, typically via heap allocation, rather than on the stack, and their lifetime must be managed so they survive until all closures referencing them are no longer in use.\\nThis explains why, typically, languages that natively support closures also use garbage collection. The alternatives are manual memory management of non-local variables (explicitly allocating on the heap and freeing when done), or, if using stack allocation, for the language to accept that certain use cases will lead to undefined behaviour, due to dangling pointers to freed automatic variables, as in lambda expressions in C++11 or nested functions in GNU C. The funarg problem (or \"functional argument\" problem) describes the difficulty of implementing functions as first class objects in a stack-based programming language such as C or C++. Similarly in D version 1, it is assumed that the programmer knows what to do with delegates and automatic local variables, as their references will be invalid after return from its definition scope (automatic local variables are on the stack) – this still permits many useful functional patterns, but for complex cases needs explicit heap allocation for variables. D version 2 solved this by detecting which variables must be stored on the heap, and performs automatic allocation. Because D uses garbage collection, in both versions, there is no need to track usage of variables as they are passed.\\nIn strict functional languages with immutable data (e.g. Erlang), it is very easy to implement automatic memory management (garbage collection), as there are no possible cycles in variables\\' references. For example, in Erlang, all arguments and variables are allocated on the heap, but references to them are additionally stored on the stack. After a function returns, references are still valid. Heap cleaning is done by incremental garbage collector.\\nIn ML, local variables are lexically scoped, and hence define a stack-like model, but since they are bound to values and not to objects, an implementation is free to copy these values into the closure\\'s data structure in a way that is invisible to the programmer.\\nScheme, which has an ALGOL-like lexical scope system with dynamic variables and garbage collection, lacks a stack programming model and does not suffer from the limitations of stack-based languages. Closures are expressed naturally in Scheme. The lambda form encloses the code, and the free variables of its environment persist within the program as long as they can possibly be accessed, and so they can be used as freely as any other Scheme expression.Closures are closely related to Actors in the Actor model of concurrent computation where the values in the function\\'s lexical environment are called acquaintances. An important issue for closures in concurrent programming languages is whether the variables in a closure can be updated and, if so, how these updates can be synchronized. Actors provide one solution.Closures are closely related to function objects; the transformation from the former to the latter is known as defunctionalization or lambda lifting; see also closure conversion.\\n\\n\\n== Differences in semantics ==\\n\\n\\n=== Lexical environment ===\\nAs different languages do not always have a common definition of the lexical environment, their definitions of closure may vary also. The commonly held minimalist definition of the lexical environment defines it as a set of all bindings of variables in the scope, and that is also what closures in any language have to capture. However the meaning of a variable binding also differs. In imperative languages, variables bind to relative locations in memory that can store values.  Although the relative location of a binding does not change at runtime, the value in the bound location can. In such languages, since closure captures the binding, any operation on the variable, whether done from the closure or not, are performed on the same relative memory location. This is often called capturing the variable \"by reference\". Here is an example illustrating the concept in ECMAScript, which is one such language:\\n\\nFunction foo and the closures referred to by variables f and g all use the same relative memory location signified by local variable x.\\nIn some instances the above behaviour may be undesirable, and it is necessary to bind a different lexical closure. Again in ECMAScript, this would be done using the Function.bind().\\n\\n\\n=== Example 1: Reference to an unbound variable ===\\n\\n\\n=== Example 2: Accidental reference to a bound variable ===\\nFor this example the expected behaviour would be that each link should emit its id when clicked; but because the variable \\'e\\' is bound the scope above, and lazy evaluated on click, what actually happens is that each on click event emits the id of the last element in \\'elements\\' bound at the end of the for loop.\\n\\nAgain here variable e would need to be bound by the scope of the block using handle.bind(this) or the let keyword.\\nOn the other hand, many functional languages, such as ML, bind variables directly to values. In this case, since there is no way to change the value of the variable once it is bound, there is no need to share the state between closures—they just use the same values. This is often called capturing the variable \"by value\". Java\\'s local and anonymous classes also fall into this category—they require captured local variables to be final, which also means there is no need to share state.\\nSome languages enable you to choose between capturing the value of a variable or its location. For example, in C++11, captured variables are either declared with [&], which means captured by reference, or with [=], which means captured by value.\\nYet another subset, lazy functional languages such as Haskell, bind variables to results of future computations rather than values. Consider this example in Haskell:\\n\\nThe binding of r captured by the closure defined within function foo is to the computation (x / y)—which in this case results in division by zero. However, since it is the computation that is captured, and not the value, the error only manifests itself when the closure is invoked, and actually attempts to use the captured binding.\\n\\n\\n=== Closure leaving ===\\nYet more differences manifest themselves in the behavior of other lexically scoped constructs, such as return, break and continue statements. Such constructs can, in general, be considered in terms of invoking an escape continuation established by an enclosing control statement (in case of break and continue, such interpretation requires looping constructs to be considered in terms of recursive function calls). In some languages, such as ECMAScript, return refers to the continuation established by the closure lexically innermost with respect to the statement—thus, a return within a closure transfers control to the code that called it. However, in Smalltalk, the superficially similar operator ^ invokes the escape continuation established for the method invocation, ignoring the escape continuations of any intervening nested closures. The escape continuation of a particular closure can only be invoked in Smalltalk implicitly by reaching the end of the closure\\'s code. The following examples in ECMAScript and Smalltalk highlight the difference:\\n\\nThe above code snippets will behave differently because the Smalltalk ^ operator and the JavaScript return operator are not analogous.  In the ECMAScript example, return x will leave the inner closure to begin a new iteration of the forEach loop, whereas in the Smalltalk example, ^x will abort the loop and return from the method foo.\\nCommon Lisp provides a construct that can express either of the above actions: Lisp (return-from foo x) behaves as Smalltalk ^x, while Lisp (return-from nil x) behaves as JavaScript return x. Hence, Smalltalk makes it possible for a captured escape continuation to outlive the extent in which it can be successfully invoked. Consider:\\n\\nWhen the closure returned by the method foo is invoked, it attempts to return a value from the invocation of foo that created the closure. Since that call has already returned and the Smalltalk method invocation model does not follow the spaghetti stack discipline to facilitate multiple returns, this operation results in an error.\\nSome languages, such as Ruby, enable the programmer to choose the way return is captured. An example in Ruby:\\n\\nBoth Proc.new and lambda in this example are ways to create a closure, but semantics of the closures thus created are different with respect to the return statement.\\nIn Scheme, definition and scope of the return control statement is explicit (and only arbitrarily named \\'return\\' for the sake of the example). The following is a direct translation of the Ruby sample.\\n\\n\\n== Closure-like constructs ==\\nSome languages have features which simulate the behavior of closures. In languages such as Java, C++, Objective-C, C#, VB.NET, and D, these features are the result of the language\\'s object-oriented paradigm.\\n\\n\\n=== Callbacks (C) ===\\nSome C libraries support \\ncallbacks.  This is \\nsometimes implemented by providing two values when \\nregistering the callback with the library: a function \\npointer and a separate void* pointer to \\narbitrary data of the user\\'s choice. When the library \\nexecutes the callback function, it passes along the data \\npointer. This enables the callback to maintain state and \\nto refer to information captured at the time it was \\nregistered with the library. The idiom is similar to \\nclosures in functionality, but not in syntax. The \\nvoid* pointer is not type safe so this C\\nidiom differs from type-safe closures in C#, Haskell or ML.\\nCallbacks are extensively used in GUI Widget toolkits to\\nimplement Event-driven programming by associating general\\nfunctions of graphical widgets (menus, buttons, check boxes,\\nsliders, spinners, etc.) with application-specific functions\\nimplementing the specific desired behavior for the application.\\n\\n\\n==== Nested function and function pointer (C) ====\\nWith a gcc extension, a nested function can be used and a function pointer can emulate closures, providing the function does not exit the containing scope.\\nThe following example is invalid because adder is a top-level definition (depending by compiler version, it could produce a correct result if compiled without optimization, i.e. at -O0):\\n\\nBut moving adder (and, optionally, the typedef) in main makes it valid:\\n\\nIf executed this now prints 11 as expected.\\n\\n\\n=== Local classes and lambda functions (Java) ===\\nJava enables classes to be defined inside methods.  These are called local classes.  When such classes are not named, they are known as anonymous classes (or anonymous inner classes).  A local class (either named or anonymous) may refer to names in lexically enclosing classes, or read-only variables (marked as final) in the lexically enclosing method.\\n\\nThe capturing of final variables enables you to capture variables by value. Even if the variable you want to capture is non-final, you can always copy it to a temporary final variable just before the class.\\nCapturing of variables by reference can be emulated by using a final reference to a mutable container, for example, a single-element array. The local class will not be able to change the value of the container reference itself, but it will be able to change the contents of the container.\\nWith the advent of Java 8\\'s lambda expressions, the closure causes the above code to be executed as:\\n\\nLocal classes are one of the types of inner class that are declared within the body of a method.  Java also supports inner classes that are declared as non-static members of an enclosing class. They are normally referred to just as \"inner classes\". These are defined in the body of the enclosing class and have full access to instance variables of the enclosing class. Due to their binding to these instance variables, an inner class may only be instantiated with an explicit binding to an instance of the enclosing class using a special syntax.\\n\\nUpon execution, this will print the integers from 0 to 9. Beware to not confuse this type of class with the nested class, which is declared in the same way with an accompanied usage of the \"static\" modifier; those have not the desired effect but are instead just classes with no special binding defined in an enclosing class.\\nAs of Java 8, Java supports functions as first class objects. Lambda expressions of this form are considered of type Function<T,U> with T being the domain and U the image type. The expression can be called with its .apply(T t) method, but not with a standard method call.\\n\\n\\n=== Blocks (C, C++, Objective-C 2.0) ===\\n\\nApple introduced blocks, a form of closure, as a nonstandard extension into C, C++, Objective-C 2.0 and in Mac OS X 10.6 \"Snow Leopard\" and iOS 4.0. Apple made their implementation available for the GCC and clang compilers.\\nPointers to block and block literals are marked with ^. Normal local variables are captured by value when the block is created, and are read-only inside the block. Variables to be captured by reference are marked with __block. Blocks that need to persist outside of the scope they are created in may need to be copied.\\n\\n\\n=== Delegates (C#, VB.NET, D) ===\\nC# anonymous methods and lambda expressions support closure:\\n\\nVisual Basic .NET, which has many language features similar to those of C#, also supports lambda expressions with closures:\\n\\nIn D, closures are implemented by delegates, a function pointer paired with a context pointer (e.g. a class instance, or a stack frame on the heap in the case of closures).\\n\\nD version 1, has limited closure support. For example, the above code will not work correctly, because the variable a is on the stack, and after returning from test(), it is no longer valid to use it (most probably calling foo via dg(), will return a \\'random\\' integer). This can be solved by explicitly allocating the variable \\'a\\' on heap, or using structs or class to store all needed closed variables and construct a delegate from a method implementing the same code. Closures can be passed to other functions, as long as they are only used while the referenced values are still valid (for example calling another function with a closure as a callback parameter), and are useful for writing generic data processing code, so this limitation, in practice, is often not an issue.\\nThis limitation was fixed in D version 2 - the variable \\'a\\' will be automatically allocated on the heap because it is used in the inner function, and a delegate of that function can escape the current scope (via assignment to dg or return). Any other local variables (or arguments) that are not referenced by delegates or that are only referenced by delegates that do not escape the current scope, remain on the stack, which is simpler and faster than heap allocation. The same is true for inner\\'s class methods that reference a function\\'s variables.\\n\\n\\n=== Function objects (C++) ===\\nC++ enables defining function objects by overloading operator(). These objects behave somewhat like functions in a functional programming language. They may be created at runtime and may contain state, but they do not implicitly capture local variables as closures do. As of the 2011 revision, the C++ language also supports closures, which are a type of function object constructed automatically from a special language construct called lambda-expression. A C++ closure may capture its context either by storing copies of the accessed variables as members of the closure object or by reference. In the latter case, if the closure object escapes the scope of a referenced object, invoking its operator() causes undefined behavior since C++ closures do not extend the lifetime of their context.\\n\\n\\n=== Inline agents (Eiffel) ===\\nEiffel includes inline agents defining closures. An inline agent is an object representing a routine, defined by giving the code of the routine in-line. For example, in\\n\\nthe argument to subscribe is an agent, representing a procedure with two arguments; the procedure finds the country at the corresponding coordinates and displays it. The whole agent is \"subscribed\" to the event type click_event for a\\ncertain button, so that whenever an instance of the event type occurs on that button – because a user has clicked the button – the procedure will be executed with the mouse coordinates being passed as arguments for x and y.\\nThe main limitation of Eiffel agents, which distinguishes them from closures in other languages, is that they cannot reference local variables from the enclosing scope. This design decision helps in avoiding ambiguity when talking about a local variable value in a closure - should it be the latest value of the variable or the value captured when the agent is created? Only Current (a reference to current object, analogous to this in Java), its features, and arguments of the agent itself can be accessed from within the agent body. The values of the outer local variables can be passed by providing additional closed operands to the agent.\\n\\n\\n=== C++Builder __closure reserved word ===\\nEmbarcadero C++Builder provides the reserve word __closure to provide a pointer to a method with a similar syntax to a function pointer.\\nIn standard C you could write a typedef for a pointer to a function type using the following syntax:In a similar way you can declare a typedef for a pointer to a method using the following syntax:\\n\\n\\n== See also ==\\nAnonymous function\\nBlocks (C language extension)\\nCommand pattern\\nContinuation\\nCurrying\\nFunarg problem\\nLambda calculus\\nLazy evaluation\\nPartial application\\nSpaghetti stack\\nSyntactic closure\\nValue-level programming\\n\\n\\n== Notes ==\\n\\n\\n== References ==\\n\\n\\n== External links ==\\nOriginal \"Lambda Papers\": A classic series of papers by Guy Steele and Gerald Sussman discussing, among other things, the versatility of closures in the context of Scheme (where they appear as lambda expressions).\\nNeal Gafter (28 January 2007). \"A Definition of Closures\".\\nGilad Bracha, Neal Gafter, James Gosling, Peter von der Ahé. \"Closures for the Java Programming Language (v0.5)\".CS1 maint: multiple names: authors list (link)\\nClosures: An article about closures in dynamically typed imperative languages, by Martin Fowler.\\nCollection closure methods: An example of a technical domain where using closures is convenient, by Martin Fowler.', 'Cognitive science is the interdisciplinary, scientific study of the mind and its processes. It examines the nature, the tasks, and the functions of cognition (in a broad sense). Cognitive scientists study intelligence and behavior, with a focus on how nervous systems represent, process, and transform information. Mental faculties of concern to cognitive scientists include language, perception, memory, attention, reasoning, and emotion; to understand these faculties, cognitive scientists borrow from fields such as linguistics, psychology, artificial intelligence, philosophy, neuroscience, and anthropology. The typical analysis of cognitive science spans many levels of organization, from learning and decision to logic and planning; from neural circuitry to modular brain organization. One of the fundamental concepts of cognitive science is that \"thinking can best be understood in terms of representational structures in the mind and computational procedures that operate on those structures.\"The goal of cognitive science is to understand the principles of intelligence with the hope that this will lead to a better comprehension of the mind and of learning and to develop intelligent devices.\\nThe cognitive sciences began as an intellectual movement in the 1950s often referred to as the cognitive revolution.\\n\\n\\n== History ==\\nThe cognitive sciences began as an intellectual movement in the 1950s, called the cognitive revolution. Cognitive science has a prehistory traceable back to ancient Greek philosophical texts (see Plato\\'s Meno and Aristotle\\'s De Anima); and includes writers such as Descartes, David Hume, Immanuel Kant, Benedict de Spinoza, Nicolas Malebranche, Pierre Cabanis, Leibniz and John Locke.  However, although these early writers contributed greatly to the philosophical discovery of mind and this would ultimately lead to the development of psychology, they were working with an entirely different set of tools and core concepts than those of the cognitive scientist.\\nThe modern culture of cognitive science can be traced back to the early cyberneticists in the 1930s and 1940s, such as Warren McCulloch and Walter Pitts, who sought to understand the organizing principles of the mind. McCulloch and Pitts developed the first variants of what are now known as artificial neural networks, models of computation inspired by the structure of biological neural networks.\\nAnother precursor was the early development of the theory of computation and the digital computer in the 1940s and 1950s. Kurt Gödel, Alonzo Church, Alan Turing, and John von Neumann were instrumental in these developments. The modern computer, or Von Neumann machine, would play a central role in cognitive science, both as a metaphor for the mind, and as a tool for investigation.\\nThe first instance of cognitive science experiments being done at an academic institution took place at MIT Sloan School of Management, established by J.C.R. Licklider working within the psychology department and conducting experiments using computer memory as models for human cognition.In 1959, Noam Chomsky published a scathing review of B. F. Skinner\\'s book Verbal Behavior. At the time, Skinner\\'s behaviorist paradigm dominated the field of psychology within the United States. Most psychologists focused on functional relations between stimulus and response, without positing internal representations. Chomsky argued that in order to explain language, we needed a theory like generative grammar, which not only attributed internal representations but characterized their underlying order.\\nThe term cognitive science was coined by Christopher Longuet-Higgins in his 1973 commentary on the Lighthill report, which concerned the then-current state of Artificial Intelligence research. In the same decade, the journal Cognitive Science and the Cognitive Science Society were founded. The founding meeting of the Cognitive Science Society was held at the University of California, San Diego in 1979, which resulted in cognitive science becoming an internationally visible enterprise. In 1972, Hampshire College started the first undergraduate education program in Cognitive Science, led by Neil Stillings.  In 1982, with assistance from Professor Stillings, Vassar College became the first institution in the world to grant an undergraduate degree in Cognitive Science. In 1986, the first Cognitive Science Department in the world was founded at the University of California, San Diego.In the 1970s and early 1980s, as access to computers increased, artificial intelligence research expanded. Researchers such as Marvin Minsky would write computer programs in languages such as LISP to attempt to formally characterize the steps that human beings went through, for instance, in making decisions and solving problems, in the hope of better understanding human thought, and also in the hope of creating artificial minds. This approach is known as \"symbolic AI\".\\nEventually the limits of the symbolic AI research program became apparent. For instance, it seemed to be unrealistic to comprehensively list human knowledge in a form usable by a symbolic computer program. The late 80s and 90s saw the rise of neural networks and connectionism as a research paradigm. Under this point of view, often attributed to James McClelland and David Rumelhart, the mind could be characterized as a set of complex associations, represented as a layered network.  Critics argue that there are some phenomena which are better captured by symbolic models, and that connectionist models are often so complex as to have little explanatory power. Recently symbolic and connectionist models have been combined, making it possible to take advantage of both forms of explanation. While both connectionism and symbolic approaches have proven useful for testing various hypotheses and exploring approaches to understanding aspects of cognition and lower level brain functions, neither are biologically realistic and therefore, both suffer from a lack of neuroscientific plausibility. Connectionism has proven useful for exploring computationally how cognition emerges in development and occurs in the human brain, and has provided alternatives to strictly domain-specific / domain general approaches. For example, scientists such as Jeff Elman, Liz Bates, and Annette Karmiloff-Smith have posited that networks in the brain emerge from the dynamic interaction between them and environmental input.\\n\\n\\n== Principles ==\\n\\n\\n=== Levels of analysis ===\\n\\nA central tenet of cognitive science is that a complete understanding of the mind/brain cannot be attained by studying only a single level (an assumption also held in the field of cognitive modelling and cognitive architectures). An example would be the problem of remembering a phone number and recalling it later. One approach to understanding this process would be to study behavior through direct observation, or naturalistic observation. A person could be presented with a phone number and be asked to recall it after some delay of time; then the accuracy of the response could be measured. Another approach to measure cognitive ability would be to study the firings of individual neurons while a person is trying to remember the phone number. Neither of these experiments on its own would fully explain how the process of remembering a phone number works. Even if the technology to map out every neuron in the brain in real-time were available and it were known when each neuron fired it would still be impossible to know how a particular firing of neurons translates into the observed behavior. Thus an understanding of how these two levels relate to each other is imperative. The Embodied Mind: Cognitive Science and Human Experience says \"the new sciences of the mind need to enlarge their horizon to encompass both lived human experience and the possibilities for transformation inherent in human experience\". This can be provided by a functional level account of the process. Studying a particular phenomenon from multiple levels creates a better understanding of the processes that occur in the brain to give rise to a particular behavior.\\nMarr gave a famous description of three levels of analysis:\\n\\nThe computational theory, specifying the goals of the computation;\\nRepresentation and algorithms, giving a representation of the inputs and outputs and the algorithms which transform one into the other; and\\nThe hardware implementation, or how algorithm and representation may be physically realized.\\n\\n\\n=== Interdisciplinary nature ===\\nCognitive science is an interdisciplinary field with contributors from various fields, including psychology, neuroscience, linguistics, philosophy of mind, computer science, anthropology and biology.  Cognitive scientists work collectively in hope of understanding the mind and its interactions with the surrounding world much like other sciences do. The field regards itself as compatible with the physical sciences and uses the scientific method as well as simulation or modeling, often comparing the output of models with aspects of human cognition. Similarly to the field of psychology, there is some doubt whether there is a unified cognitive science, which have led some researchers to prefer \\'cognitive sciences\\' in plural.Many, but not all, who consider themselves cognitive scientists hold a functionalist view of the mind—the view that mental states and processes should be explained by their function – what they do. According to the multiple realizability account of functionalism, even non-human systems such as robots and computers can be ascribed as having cognition.\\n\\n\\n=== Cognitive science: the term ===\\nThe term \"cognitive\" in \"cognitive science\" is used for \"any kind of mental operation or structure that can be studied in precise terms\" (Lakoff and Johnson, 1999). This conceptualization is very broad, and should not be confused with how \"cognitive\" is used in some traditions of analytic philosophy, where \"cognitive\" has to do only with formal rules and truth conditional semantics.\\nThe earliest entries for the word \"cognitive\" in the OED take it to mean roughly \"pertaining to the action or process of knowing\". The first entry, from 1586, shows the word was at one time used in the context of discussions of Platonic theories of knowledge. Most in cognitive science, however, presumably do not believe their field is the study of anything as certain as the knowledge sought by Plato.\\n\\n\\n== Scope ==\\nCognitive science is a large field, and covers a wide array of topics on cognition. However, it should be recognized that cognitive science has not always been equally concerned with every topic that might bear relevance to the nature and operation of minds. Among philosophers, classical cognitivists have largely de-emphasized or avoided social and cultural factors, emotion, consciousness, animal cognition, and comparative and evolutionary psychologies. However, with the decline of behaviorism, internal states such as affects and emotions, as well as awareness and covert attention became approachable again. For example, situated and embodied cognition theories take into account the current state of the environment as well as the role of the body in cognition. With the newfound emphasis on information processing, observable behavior was no longer the hallmark of psychological theory, but the modeling or recording of mental states.\\nBelow are some of the main topics that cognitive science is concerned with. This is not an exhaustive list. See List of cognitive science topics for a list of various aspects of the field.\\n\\n\\n=== Artificial intelligence ===\\n\\nArtificial intelligence (AI) involves the study of cognitive phenomena in machines. One of the practical goals of AI is to implement aspects of human intelligence in computers. Computers are also widely used as a tool with which to study cognitive phenomena. Computational modeling uses simulations to study how human intelligence may be structured. (See § Computational modeling.)\\nThere is some debate in the field as to whether the mind is best viewed as a huge array of small but individually feeble elements (i.e. neurons), or as a collection of higher-level structures such as symbols, schemes, plans, and rules. The former view uses connectionism to study the mind, whereas the latter emphasizes symbolic artificial intelligence. One way to view the issue is whether it is possible to accurately simulate a human brain on a computer without accurately simulating the neurons that make up the human brain.\\n\\n\\n=== Attention ===\\n\\nAttention is the selection of important information.  The human mind is bombarded with millions of stimuli and it must have a way of deciding which of this information to process.  Attention is sometimes seen as a spotlight, meaning one can only shine the light on a particular set of information.  Experiments that support this metaphor include the dichotic listening task (Cherry, 1957) and studies of inattentional blindness (Mack and Rock, 1998).  In the dichotic listening task, subjects are bombarded with two different messages, one in each ear, and told to focus on only one of the messages.  At the end of the experiment, when asked about the content of the unattended message, subjects cannot report it.\\n\\n\\n=== Knowledge and processing of language ===\\n\\nThe ability to learn and understand language is an extremely complex process. Language is acquired within the first few years of life, and all humans under normal circumstances are able to acquire language proficiently. A major driving force in the theoretical linguistic field is discovering the nature that language must have in the abstract in order to be learned in such a fashion. Some of the driving research questions in studying how the brain itself processes language include: (1) To what extent is linguistic knowledge innate or learned?, (2) Why is it more difficult for adults to acquire a second-language than it is for infants to acquire their first-language?, and (3) How are humans able to understand novel sentences?\\nThe study of language processing ranges from the investigation of the sound patterns of speech to the meaning of words and whole sentences. Linguistics often divides language processing into orthography, phonetics, phonology, morphology, syntax, semantics, and pragmatics. Many aspects of language can be studied from each of these components and from their interaction.The study of language processing in cognitive science is closely tied to the field of linguistics. Linguistics was traditionally studied as a part of the humanities, including studies of history, art and literature. In the last fifty years or so, more and more researchers have studied knowledge and use of language as a cognitive phenomenon, the main problems being how knowledge of language can be acquired and used, and what precisely it consists of. Linguists have found that, while humans form sentences in ways apparently governed by very complex systems, they are remarkably unaware of the rules that govern their own speech. Thus linguists must resort to indirect methods to determine what those rules might be, if indeed rules as such exist. In any event, if speech is indeed governed by rules, they appear to be opaque to any conscious consideration.\\n\\n\\n=== Learning and development ===\\n\\nLearning and development are the processes by which we acquire knowledge and information over time. Infants are born with little or no knowledge (depending on how knowledge is defined), yet they rapidly acquire the ability to use language, walk, and recognize people and objects. Research in learning and development aims to explain the mechanisms by which these processes might take place.\\nA major question in the study of cognitive development is the extent to which certain abilities are innate or learned. This is often framed in terms of the nature and nurture debate. The nativist view emphasizes that certain features are innate to an organism and are determined by its genetic endowment. The empiricist view, on the other hand, emphasizes that certain abilities are learned from the environment. Although clearly both genetic and environmental input is needed for a child to develop normally, considerable debate remains about how genetic information might guide cognitive development. In the area of language acquisition, for example, some (such as Steven Pinker) have argued that specific information containing universal grammatical rules must be contained in the genes, whereas others (such as Jeffrey Elman and colleagues in Rethinking Innateness) have argued that Pinker\\'s claims are biologically unrealistic. They argue that genes determine the architecture of a learning system, but that specific \"facts\" about how grammar works can only be learned as a result of experience.\\n\\n\\n=== Memory ===\\n\\nMemory allows us to store information for later retrieval. Memory is often thought of as consisting of both a long-term and short-term store. Long-term memory allows us to store information over prolonged periods (days, weeks, years). We do not yet know the practical limit of long-term memory capacity. Short-term memory allows us to store information over short time scales (seconds or minutes).\\nMemory is also often grouped into declarative and procedural forms. Declarative memory—grouped into subsets of semantic and episodic forms of memory—refers to our memory for facts and specific knowledge, specific meanings, and specific experiences (e.g. \"Are apples food?\", or \"What did I eat for breakfast four days ago?\"). Procedural memory allows us to remember actions and motor sequences (e.g. how to ride a bicycle) and is often dubbed implicit knowledge or memory .\\nCognitive scientists study memory just as psychologists do, but tend to focus more on how memory bears on cognitive processes, and the interrelationship between cognition and memory. One example of this could be, what mental processes does a person go through to retrieve a long-lost memory? Or, what differentiates between the cognitive process of recognition (seeing hints of something before remembering it, or memory in context) and recall (retrieving a memory, as in \"fill-in-the-blank\")?\\n\\n\\n=== Perception and action ===\\n\\nPerception is the ability to take in information via the senses, and process it in some way. Vision and hearing are two dominant senses that allow us to perceive the environment. Some questions in the study of visual perception, for example, include: (1) How are we able to recognize objects?, (2) Why do we perceive a continuous visual environment, even though we only see small bits of it at any one time? One tool for studying visual perception is by looking at how people process optical illusions. The image on the right of a Necker cube is an example of a bistable percept, that is, the cube can be interpreted as being oriented in two different directions.\\nThe study of haptic (tactile), olfactory, and gustatory stimuli also fall into the domain of perception.\\nAction is taken to refer to the output of a system. In humans, this is accomplished through motor responses. Spatial planning and movement, speech production, and complex motor movements are all aspects of action.\\n\\n\\n=== Consciousness ===\\n\\nConsciousness is the awareness whether something is an external object or something within oneself. \\nThis helps the mind with having the ability to experience or feel a sense of self.\\n\\n\\n== Research methods ==\\nMany different methodologies are used to study cognitive science. As the field is highly interdisciplinary, research often cuts across multiple areas of study, drawing on research methods from psychology, neuroscience, computer science and systems theory.\\n\\n\\n=== Behavioral experiments ===\\nIn order to have a description of what constitutes intelligent behavior, one must study behavior itself. This type of research is closely tied to that in cognitive psychology and psychophysics. By measuring behavioral responses to different stimuli, one can understand something about how those stimuli are processed. Lewandowski & Strohmetz (2009) reviewed a collection of innovative uses of behavioral measurement in psychology including behavioral traces, behavioral observations, and behavioral choice. Behavioral traces are pieces of evidence that indicate behavior occurred, but the actor is not present (e.g., litter in a parking lot or readings on an electric meter). Behavioral observations involve the direct witnessing of the actor engaging in the behavior (e.g., watching how close a person sits next to another person). Behavioral choices are when a person selects between two or more options (e.g., voting behavior, choice of a punishment for another participant).\\n\\nReaction time. The time between the presentation of a stimulus and an appropriate response can indicate differences between two cognitive processes, and can indicate some things about their nature. For example, if in a search task the reaction times vary proportionally with the number of elements, then it is evident that this cognitive process of searching involves serial instead of parallel processing.\\nPsychophysical responses. Psychophysical experiments are an old psychological technique, which has been adopted by cognitive psychology. They typically involve making judgments of some physical property, e.g. the loudness of a sound. Correlation of subjective scales between individuals can show cognitive or sensory biases as compared to actual physical measurements. Some examples include:\\nsameness judgments for colors, tones, textures, etc.\\nthreshold differences for colors, tones, textures, etc.\\nEye tracking. This methodology is used to study a variety of cognitive processes, most notably visual perception and language processing. The fixation point of the eyes is linked to an individual\\'s focus of attention. Thus, by monitoring eye movements, we can study what information is being processed at a given time. Eye tracking allows us to study cognitive processes on extremely short time scales. Eye movements reflect online decision making during a task, and they provide us with some insight into the ways in which those decisions may be processed.\\n\\n\\n=== Brain imaging ===\\n\\nBrain imaging involves analyzing activity within the brain while performing various tasks. This allows us to link behavior and brain function to help understand how information is processed. Different types of imaging techniques vary in their temporal (time-based) and spatial (location-based) resolution. Brain imaging is often used in cognitive neuroscience.\\n\\nSingle-photon emission computed tomography and Positron emission tomography. SPECT and PET use radioactive isotopes, which are injected into the subject\\'s bloodstream and taken up by the brain. By observing which areas of the brain take up the radioactive isotope, we can see which areas of the brain are more active than other areas. PET has similar spatial resolution to fMRI, but it has extremely poor temporal resolution.\\nElectroencephalography. EEG measures the electrical fields generated by large populations of neurons in the cortex by placing a series of electrodes on the scalp of the subject. This technique has an extremely high temporal resolution, but a relatively poor spatial resolution.\\nFunctional magnetic resonance imaging. fMRI measures the relative amount of oxygenated blood flowing to different parts of the brain. More oxygenated blood in a particular region is assumed to correlate with an increase in neural activity in that part of the brain. This allows us to localize particular functions within different brain regions. fMRI has moderate spatial and temporal resolution.\\nOptical imaging. This technique uses infrared transmitters and receivers to measure the amount of light reflectance by blood near different areas of the brain. Since oxygenated and deoxygenated blood reflects light by different amounts, we can study which areas are more active (i.e., those that have more oxygenated blood). Optical imaging has moderate temporal resolution, but poor spatial resolution. It also has the advantage that it is extremely safe and can be used to study infants\\' brains.\\nMagnetoencephalography. MEG measures magnetic fields resulting from cortical activity. It is similar to EEG, except that it has improved spatial resolution since the magnetic fields it measures are not as blurred or attenuated by the scalp, meninges and so forth as the electrical activity measured in EEG is. MEG uses SQUID sensors to detect tiny magnetic fields.\\n\\n\\n=== Computational modeling ===\\n\\nComputational models require a mathematically and logically formal representation of a problem. Computer models are used in the simulation and experimental verification of different specific and general properties of intelligence.  Computational modeling can help us understand the functional organization of a particular cognitive phenomenon.\\nApproaches to cognitive modeling can be categorized as: (1) symbolic, on abstract mental functions of an intelligent mind by means of symbols; (2) subsymbolic, on the neural and associative properties of the human brain; and (3) across the symbolic–subsymbolic border, including hybrid.\\n\\nSymbolic modeling evolved from the computer science paradigms using the technologies of knowledge-based systems, as well as a philosophical perspective (e.g. \"Good Old-Fashioned Artificial Intelligence\" (GOFAI)). They were developed by the first cognitive researchers and later used in information engineering for expert systems. Since the early 1990s it was generalized in systemics for the investigation of functional human-like intelligence models, such as personoids, and, in parallel, developed as the SOAR environment. Recently, especially in the context of cognitive decision-making, symbolic cognitive modeling has been extended to the socio-cognitive approach, including social and organizational cognition, interrelated with a sub-symbolic non-conscious layer.\\nSubsymbolic modeling includes connectionist/neural network models. Connectionism relies on the idea that the mind/brain is composed of simple nodes and its problem-solving capacity derives from the connections between them. Neural nets are textbook implementations of this approach. Some critics of this approach feel that while these models approach biological reality as a representation of how the system works, these models lack explanatory powers because, even in systems endowed with simple connection rules, the emerging high complexity makes them less interpretable at the connection-level than they apparently are at the macroscopic level.\\nOther approaches gaining in popularity include (1) dynamical systems theory, (2) mapping symbolic models onto connectionist models (Neural-symbolic integration or hybrid intelligent systems), and (3) and Bayesian models, which are often drawn from machine learning.All the above approaches tend to be generalized to the form of integrated computational models of a synthetic/abstract intelligence in order to be applied to the explanation and improvement of individual and social/organizational decision-making and reasoning.\\n\\n\\n=== Neurobiological methods ===\\nResearch methods borrowed directly from neuroscience and neuropsychology can also help us to understand aspects of intelligence. These methods allow us to understand how intelligent behavior is implemented in a physical system.\\n\\nSingle-unit recording\\nDirect brain stimulation\\nAnimal models\\nPostmortem studiesThe only textbook that encompasses all these areas and which has gone through 3 editions is \\n\\n\\n== Key findings ==\\nCognitive science has given rise to models of human cognitive bias and risk perception, and has been influential in the development of behavioral finance, part of economics. It has also given rise to a new theory of the philosophy of mathematics (related to denotational mathematics), and many theories of artificial intelligence, persuasion and coercion. It has made its presence known in the philosophy of language and epistemology as well as constituting a substantial wing of modern linguistics. Fields of cognitive science have been influential in understanding the brain\\'s particular functional systems (and functional deficits) ranging from speech production to auditory processing and visual perception. It has made progress in understanding how damage to particular areas of the brain affect cognition, and it has helped to uncover the root causes and results of specific dysfunction, such as dyslexia, anopia, and hemispatial neglect.\\n\\n\\n== Criticism ==\\nSee Criticism of cognitive psychology.\\n\\n\\n== Notable researchers ==\\n\\nSome of the more recognized names in cognitive science are usually either the most controversial or the most cited. Within philosophy, some familiar names include Daniel Dennett, who writes from a computational systems perspective, John Searle, known for his controversial Chinese room argument, and Jerry Fodor, who advocates functionalism.Others include David Chalmers, who advocates Dualism and is also known for articulating the hard problem of consciousness, and Douglas Hofstadter, famous for writing Gödel, Escher, Bach, which questions the nature of words and thought.\\nIn the realm of linguistics, Noam Chomsky and George Lakoff have been influential (both have also become notable as political commentators). In artificial intelligence, Marvin Minsky, Herbert A. Simon, and Allen Newell are prominent.\\nPopular names in the discipline of psychology include George A. Miller, James McClelland, Philip Johnson-Laird, Lawrence Barsalou, Vittorio Guidano, Howard Gardner and Steven Pinker. Anthropologists Dan Sperber, Edwin Hutchins, Bradd Shore, James Wertsch and Scott Atran, have been involved in collaborative projects with cognitive and social psychologists, political scientists and evolutionary biologists in attempts to develop general theories of culture formation, religion, and political association.\\nComputational theories (with models and simulations) have also been developed, by David Rumelhart, James McClelland and Philip Johnson-Laird.\\n\\n\\n== Epistemics ==\\nEpistemics is a term coined in 1969 by the University of Edinburgh with the foundation of its School of Epistemics. Epistemics is to be distinguished from epistemology in that epistemology is the philosophical theory of knowledge, whereas epistemics signifies the scientific study of knowledge.\\nChristopher Longuet-Higgins has defined it as \"the construction of formal models of the processes (perceptual, intellectual, and linguistic) by which knowledge and understanding are achieved and communicated.\"\\nIn his 1978 essay \"Epistemics: The Regulative Theory of Cognition\", Alvin I. Goldman claims to have coined the term \"epistemics\" to describe a reorientation of epistemology. Goldman maintains that his epistemics is continuous with traditional epistemology and the new term is only to avoid opposition. Epistemics, in Goldman\\'s version, differs only slightly from traditional epistemology in its alliance with the psychology of cognition; epistemics stresses the detailed study of mental processes and information-processing mechanisms that lead to knowledge or beliefs.\\nIn the mid-1980s, the School of Epistemics was renamed as The Centre for Cognitive Science (CCS).  In 1998, CCS was incorporated into the University of Edinburgh\\'s School of Informatics.\\n\\n\\n== See also ==\\n\\nOutlinesOutline of human intelligence – topic tree presenting the traits, capacities, models, and research fields of human intelligence, and more.\\nOutline of thought – topic tree that identifies many types of thoughts, types of thinking, aspects of thought, related fields, and more.\\n\\n\\n== References ==\\n\\n\\n== External links ==\\n Media related to Cognitive science at Wikimedia Commons\\n Quotations related to Cognitive science at Wikiquote\\n Learning materials related to Cognitive science at Wikiversity\\n\"Cognitive Science\" on the Stanford Encyclopedia of Philosophy\\nCognitive Science Society\\nCognitive Science Movie Index: A broad list of movies showcasing themes in the Cognitive Sciences\\nList of leading thinkers in cognitive science', 'Earl Simmons (December 18, 1970 – April 9, 2021), known by his stage name DMX (\"Dark Man X\"), was an American rapper and actor. He began rapping in the early 1990s and released his debut album It\\'s Dark and Hell Is Hot in 1998, to both critical acclaim and commercial success, selling 251,000 copies within its first week of release. DMX released his best-selling album, ... And Then There Was X, in 1999, which included the hit single \"Party Up (Up in Here)\". His 2003 singles \"Where the Hood At?\" and \"X Gon\\' Give It to Ya\" were also commercially successful. He was the first artist to debut an album at No. 1 five times in a row on the Billboard 200 charts. Overall, DMX has sold over 74 million records worldwide.DMX was featured in films such as Belly, Romeo Must Die, Exit Wounds, Cradle 2 the Grave, and Last Hour. In 2006, he starred in the reality television series DMX: Soul of a Man, which was primarily aired on the BET cable television network. In 2003, he published a book of his memoirs entitled, E.A.R.L.: The Autobiography of DMX.In April 2021, DMX was hospitalized due to a cocaine-induced heart attack. He never regained consciousness and died of multiple organ failure a week later.\\n\\n\\n== Early life ==\\nEarl Simmons was born on December 18, 1970, with various accounts giving his birthplace as either Baltimore, Maryland, or Mount Vernon, New York. He was the son of 19-year-old Arnett Simmons and 18-year-old Joe Barker. Earl was Simmons\\' second child; she had given birth to a daughter, Bonita, two years prior, and later gave birth to one daughter, Shayla, and two stillborn sons. His father, Barker, was an artist who painted watercolor paintings of street scenes to sell at local fairs. Barker moved to Philadelphia and was largely absent from his life.As a child, Simmons suffered greatly from bronchial asthma, being taken to the emergency room almost nightly due to him waking up unable to breathe. He was raised as a Jehovah\\'s Witness but became disillusioned with the faith after an incident where he was hit by a drunk driver while crossing the street. A month later, an insurance representative went to his house to try and reach an agreement to prevent his family from suing. Simmons claims he was told that his family could have been awarded a settlement of $10,000 and possibly even more for the injuries he sustained but that his mother rejected the settlement as she claimed that Jehovah\\'s Witnesses are taught to be self-sufficient although the group\\'s official doctrine at the time did not prohibit suing or receiving settlements.Simmons went through a disjointed childhood that included being beaten by his mother and her various boyfriends so badly that he lost teeth and sustained numerous bruises and cuts on his face. Due to poverty, he slept on the floor with roaches and mice crawling over him in the night. When Simmons was five years old, his family settled into the School Street Projects in\\nYonkers, New York. When he was six years old, his mother knocked out two of his teeth with a broom after he innocently erased something in her notebook. At school, he threw chairs at teachers and stabbed another child in the face with a pencil. When he was seven, an aunt got him drunk on vodka. The same year, he was jailed for stealing cakes from a market. One summer, his mother locked him in his bedroom, allowing him to only exit for trips to the bathroom. At the end of the fifth grade, at age 10, Simmons was expelled from school and sent to the Julia Dyckman Andrus Children\\'s Home for 18 months. In what he described as a defining moment of betrayal, his mother tricked him by telling him they were just visiting the home, then she enrolled him there. A few months later, he was arrested for arson in an attempt to burn the school down. He nearly killed his co-conspirator.When he was 14, Simmons began living on the streets of Yonkers to escape his mother\\'s abuse, sleeping in Salvation Army clothing bins and  befriending stray dogs.Shortly after he began doing this, his mother once again sent him to a group home. During his stay, Simmons bonded with other students from New York over their shared love of hip hop music. After performing for his friends, they encouraged Simmons to continue writing music at the behest of his teacher. When he returned home, Simmons met Ready Ron, a local rapper, who was impressed with Simmons\\' beatboxing skills and asked him to become his partner. Simmons chose the name \"DMX\", which came from an instrument he had used at the boys\\' home, the Oberheim DMX drum machine. It later was also interpreted as \"Dark Man X\".As a freshman at Yonkers Middle High School, DMX was the second-fastest on the track and field varsity team. However, he had bad grades and a sparse attendance record. He turned to robbery as a way to get out of poverty: his first was a purse snatch theft in Yonkers that netted him $1,000 (equivalent to $2,500 in 2020) which he used to buy a new leather dog collar and dog harness for his dog, and a pair of Timberland boots for himself. By the end of the year, he attended school just to rob people and was robbing 3 people per day. He then turned to carjacking.\\n\\n\\n== Musical career ==\\n\\n\\n=== 1991–1996: Career beginnings ===\\nDMX got his start in the music industry at age 14, in 1984, when he beatboxed for Ready Ron. After serving time in prison for stealing a dog, he began writing his own lyrics and performing at the local recreation center for younger children. In 1988, while in prison for carjacking, he began dedicating almost all of his free time to writing lyrics and also meeting and rapping with K-Solo. When he was released that summer, he began producing and selling his own mixtapes where he rapped over instrumentals from other songs and sell them on street corners, which helped him build a local fan base all over New York. In 1991, The Source magazine praised DMX in its Unsigned Hype column that highlighted unsigned hip-hop artists. In 1992, Columbia Records signed DMX to its subsidiary label Ruffhouse Records, which released his debut single \"Born Loser\". He released his second single, \"Make a Move\" in 1994. He made a guest appearance alongside Jay-Z, Ja Rule, and Mic Geronimo on the classic underground track \"Time to Build\" on Mic Geronimo\\'s debut album in 1995.\\n\\n\\n=== 1996–2000: Signing with Def Jam and commercial success ===\\nDMX recorded tracks from September 1996 to January 1998 for his debut album. During this time, his guest appearances on Mase\\'s singles \"24 Hrs. to Live\" and \"Take What\\'s Yours\", The LOX\\'s single \"Money, Power & Respect\", and LL Cool J\\'s single \"4, 3, 2, 1\" created a strong buzz for the then-unsigned rapper. In February 1998, he released his debut major-label single, \"Get at Me Dog\", on Def Jam Recordings. The single  received an RIAA certification of gold. His first major-label album, It\\'s Dark and Hell Is Hot, which included the single \"Ruff Ryders\\' Anthem\", was then released in May 1998. The album debuted at number one on the Billboard 200 chart in the U.S. and sold over five million copies. In December 1998, he released his second album, Flesh of My Flesh, Blood of My Blood. It debuted at number one on the Billboard 200 and went multi-platinum. He released his third and best-selling album ... And Then There Was X, on December 21, 1999. It was his third album to debut at number one on the Billboard 200. Its most popular single, \"Party Up (Up in Here)\", became his first Top Ten hit on the R&B charts, and was nominated for a Grammy Award for Best Rap Solo Performance at the 2001 Grammy Awards. The album was certified six-times Platinum, and was nominated for Best Rap Album at the 2001 Grammy Awards. In 2000, DMX also made a cameo appearance in the Sum 41 music video for \"Makes No Difference\".\\n\\n\\n=== 2001–2004: Return to music ===\\n\\nAfter improving his legal situation, DMX returned to the studio to complete his fourth album, The Great Depression. Within its release on October 23, 2001, it was his fourth album to debut at number one on the Billboard 200, featuring the singles \"Who We Be\", \"We Right Here\", and \"Shorty Was The Bomb\". Despite the album\\'s triple Platinum certification, its commercial and critical success was lower than his previous album. His fifth album, Grand Champ, released in September 2003, once again debuted at number one on the Billboard 200 charts, placing DMX as the only musical artist in history to release five consecutive albums (his entire album catalog at the time) that debuted at number one. Singles released off the album include \"Where the Hood At?\" and \"Get It on the Floor\". After its release, he informed the public that he planned to retire and that Grand Champ was his final album.\\n\\n\\n=== 2005–2011: Year of the Dog...Again and The Definition of X ===\\nDMX signed to Columbia Records in January 2006. He recorded his next album, Year of the Dog... Again, while switching record labels, which caused numerous delays. It was released on August 1, 2006, and missed the number one Billboard spot by only a few hundred copies. He released two more singles, \"Lord Give Me a Sign\" and \"We in Here\". On June 12, 2008, Def Jam Recordings released a compilation of his greatest hits, The Definition of X: The Pick of the Litter. In 2011, Def Jam released another compilation album, The Best of DMX, which features hit singles including \"Where the Hood At?\" and \"X Gon\\' Give It to Ya\". In 2009, DMX claimed he would pursue preaching in Jersey City, New Jersey as well as  continue to produce music. He completed a Gospel music album prior to his incarceration. According to MTV, he had semi-retired to study the Bible more in an effort to give messages behind the pulpit.\\n\\n\\n=== 2011–2013: Undisputed ===\\nOn October 11, 2011, DMX performed at the 2011 BET Hip Hop Awards. He stated that he has been working \"nonstop, every day\" on his seventh album, which was later titled Undisputed. A video for a new track entitled \"Last Hope\" was released via the Internet on September 24, 2011, and was later included on The Weigh In EP released digitally on May 5, 2012. In late February 2012, Seven Arts Pictures acquired the catalog of DMX\\'s music and signed DMX to a two-album deal. During a performance at New York\\'s Santos Party House on December 25, 2011, DMX stated that the new album would be titled Undisputed and would be released on March 26, 2012. After numerous delays, the album was eventually released on September 11, 2012, and featured production from Swizz Beatz and J.R. Rotem with a guest appearance by MGK.\\n\\n\\n=== 2013–2021: Def Jam reunion and Exodus ===\\nIn 2013, DMX announced he had begun working on his eighth studio album. He collaborated with producers Swizz Beatz and Dame Grease. In December, after regaining his passport, he embarked on a world tour with performances in Bulgaria and Kosovo. On January 7, 2015, Seven Arts Music announced that DMX would be releasing  Redemption of the Beast the following week; however, close personal friend and recurring collaborator producer/rapper/entrepreneur Swizz Beatz and DMX\\'s management confirmed that this was false. On January 13, 2015, Seven Arts Music released Redemption of the Beast, without acquiring a legal artist contract. On January 15, 2015, it was announced by DMX\\'s brother/manager Montana that DMX was no longer signed to Seven Arts Music and that they would be taking legal action against Seven Arts Music for the unauthorized release of Redemption of the Beast.Long-time collaborator Swizz Beatz stated that two of the collaborators on the album would be Kanye West and Dr. Dre. His 2003 song \"X Gon\\' Give It to Ya\" was featured in the 2016 film Deadpool and in its trailers. On June 28, 2016, DMX released a new song titled \"Blood Red\" and produced by Divine Bars. On January 11, 2017, DMX released a new song produced by Swizz Beats titled \"Bain Iz Back\". On September 20, 2019, DMX signed a new record deal with Def Jam Recordings, reuniting with the label for the first time since his 2003 album Grand Champ.DMX\\'s eighth and first posthumous studio album Exodus was released through Def Jam on May 28, 2021.\\n\\n\\n== Personal life ==\\n\\n\\n=== Religion ===\\nDMX was a born-again Christian, and stated that he read the Bible every day. While in jail, DMX stated that he had a purpose for being there: \"I came here to meet somebody...Don\\'t know who it was, but I\\'ll know when I see him. And I came here to give him a message. And that message is Jesus loves them.\" DMX was a transitional deacon and aspired to become ordained as a pastor, stating that he received this call in 2009. In 2016, he gave a sermon at a church in Phoenix, Arizona. In April 2020, he held an online Bible study and asked people to accept Jesus as their lord and savior.\\n\\n\\n=== Relationships and children ===\\nDMX was the father of 15 children from 9 different women. He married his childhood friend Tashera Simmons in 1999 and they were married for 11 years. They had four children together: Xavier (born 1992), Tacoma (born 1999), Sean (born 2002), and Praise Mary Ella (born 2005). In July 2010, after his first of three incarcerations that year, Tashera announced their separation. They remained friends, although in 2016, Tashera accused DMX of missing his $10,000/month child support payment.DMX had extramarital affairs during his marriage to Tashera, some of which produced children. He had a daughter, Sasha (born 2002), with Patricia Trejo. In 2012, Trejo sued DMX for $1 million in unpaid child support. The case was settled in 2013. DMX and Monique Wayne, a Maryland resident, fought over her claim that he was the father of her son born in 2004. She sued him for defamation and for child support. After genetic testing proved that DMX was indeed the father, in January 2008, DMX was ordered to pay Wayne $1.5 million, but a judge vacated the judgment in May 2008. DMX also fathered a child in 2008 and fathered two children with ex-girlfriend Yadira Borrego. In 2009, his daughter Sonovah Junior was born. In 2011, his daughter Aaliyah, named after his close friend, Aaliyah, was born. His fifteenth child, Exodus Simmons, was born to his fiancée, Desiree Lindstrom, on August 16, 2016.Dark Man X didn\\'t leave a will. As a result, legal battles ensued in probate courts following his death.\\n\\n\\n=== Finances and bankruptcies ===\\nDMX earned $2.3 million from his songs between 2010 and 2015.He also filed for bankruptcy three times. His first filing was on July 30, 2013, citing his child support obligations as his priority claim. The filing was challenged by the United States Trustee Program and was dismissed by the U.S. Bankruptcy Court in Manhattan on November 11, 2013.\\n\\n\\n=== Feud with Ja Rule ===\\nDuring the 1990s, DMX formed a close bond with fellow up-and-coming rappers Jay-Z and Ja Rule. The three collaborated many times and formed a group known as Murder Inc. The group was short-lived due to internal issues between DMX and Jay-Z. After the breakup of Murder Inc., DMX disparaged Ja Rule in interviews, accusing him of being a copycat, drawing comparisons between himself and what he saw as Ja stealing his signature \"gruff\" style of delivery.DMX released a diss track, \"They Want War\", on a 2002 DJ Kay Slay mixtape; Ja Rule never directly responded. DMX also released the single \"Go to Sleep\" with Eminem and Obie Trice as apart of the Cradle 2 The Grave soundtrack with numerous lines directed to Ja Rule. However, as time passed and the feud faded into obscurity, DMX said that he wanted to officially bring it to an end when he was released from prison in 2005: \"Gotti came to me in jail and said I want to make peace with you and him. I was like, \\'Alright Gotti, let\\'s do it.\" Despite this, DMX and Ja Rule did not officially end their feud until 2009, at VH1\\'s Hip Hop Honors.\\n\\n\\n=== Feud with Jay-Z ===\\nWhen DMX partnered with Jay-Z and Ja Rule in Murder Inc., there was a feud between the two, which also contributed to the failure of the group and working together. According to reports, the feud started in the early 1990s after a rap battle between the two, which led to DMX\\'s disdain for Jay-Z. Prior to DMX\\'s death, the feud, although it fizzled out over the years, continued on when DMX said in an Instagram video that he wanted to rap battle Jay-Z on Verzuz.\\n\\n\\n== Legal trouble ==\\n\\nDMX was in jail 30 times for various offenses, including robbery, assault, carjacking, animal cruelty, reckless driving, driving under the influence, unlicensed driving, drug possession, probation violation, failure to pay child support, pretending to be a federal agent, and tax evasion.\\n\\n\\n=== 1986–1988 ===\\nDMX was first sent to prison in 1986 after stealing a dog from a junkyard. He was sentenced to two years in the juvenile unit of Woodfield Prison in Valhalla, New York. However, just weeks after starting his sentence, he and his cellmate successfully escaped the prison and DMX returned home until his mother forced him to turn himself in and finish his sentence, which he did at the McCormick Juvenile Detention Centre in Brooktondale, New York. Simmons was sent to prison again in 1988 for carjacking, and was later moved to a higher security prison after attempting to extort a fellow inmate for drugs. He was released in the summer of 1988.\\n\\n\\n=== 1998–1999 ===\\nWhen officers of the Fort Lee Police Department executed a search of his home in 1999, DMX promptly surrendered himself on weapons possession charges.\\nDMX faced a 1999 animal cruelty charge in Teaneck, New Jersey after a dozen pit bulls were found at his home there; the charge was dismissed after the performer agreed to accept responsibility and record public service announcements for an animal rights group.\\n\\n\\n=== 2000–2005 Metro NY ===\\nIn 2000, DMX served a 15-day jail sentence for possession of marijuana.\\nDMX served another jail sentence in 2001 for driving without a license and possession of marijuana. His appeal to reduce the sentence was denied; rather, he was charged with assault for throwing objects at prison guards.\\nIn January 2002, DMX pleaded guilty in New Jersey to 13 counts of animal cruelty, two counts of maintaining a nuisance, and one count each of disorderly conduct and possession of drug paraphernalia. He eventually plea-bargained down to fines, probation, and community service and starred in public service announcements against the dangers of guns and animal abuse.\\nIn June 2004, DMX was arrested at the John F. Kennedy International Airport, on charges of cocaine possession, criminal impersonation, criminal possession of a weapon, criminal mischief, menacing, and driving under the influence of drugs or alcohol, while claiming to be a federal agent and attempting to carjack a vehicle. He was given a conditional discharge on December 8, 2004, but pleaded guilty on October 25, 2005, to violating parole.\\nOn November 18, 2005, DMX was sentenced to 70 days in jail at Riker\\'s Island for violating parole; the lateness charge added a 10-day extension to the original 60-day sentence. DMX was released early (for \"good behavior\") on December 30, 2005.\\n\\n\\n=== 2007 ===\\nIn 2007, DMX\\'s home was raided on reports of animal cruelty.\\n\\n\\n=== 2008–2011 Arizona and California ===\\nOn May 9, 2008, DMX was arrested on drug and animal cruelty charges after attempting to barricade himself inside his Cave Creek, Arizona home.\\nDMX pleaded guilty to charges of drug possession, theft, and animal cruelty stemming from an August 2007 drug raid as well as the May 2008 arrest, at a hearing on December 30, 2008; he was sentenced to 90 days in jail on January 31, 2009.\\nOn May 22, 2009, DMX entered a plea agreement/change of plea and pleaded guilty to attempted aggravated assault in jail.\\nAfter serving four out of six months for violating drug probation, DMX was released from jail on July 6, 2010. That day, a television pilot was filmed to portray his road to recovery; however, DMX was arrested three weeks later and the pilot did not evolve into a series.\\nOn July 27, 2010, DMX turned himself in to Los Angeles Metropolitan Court for a reckless driving charge he received in 2002. He was sentenced to serve ninety days in jail.\\nOn November 19, 2010, DMX was arrested in Maricopa County, Arizona on charges of violating probation for a February 24, 2009 aggravated assault on an officer while he was incarcerated. On December 20, 2010, DMX was moved to the Mental Health Unit of the Arizona Alhambra State Prison, and released on July 18, 2011.\\nOn August 24, 2011, DMX was arrested for the tenth time in Maricopa County, this time for speeding, recorded at 102 miles per hour (164 km/h) in a 65-mile-per-hour (105 km/h) zone, reckless driving, and driving with a suspended license. While DMX admitted to speeding, he claimed he was driving 85 miles per hour (137 km/h).\\n\\n\\n=== 2013 South Carolina ===\\nOn February 13, 2013, DMX was arrested in Spartanburg, South Carolina for driving without a driver\\'s license.\\nOn July 26, 2013, DMX was arrested again in Greenville County, South Carolina and charged with driving under the influence of alcohol, as well as driving without a license.\\nOn August 20, 2013, DMX was arrested again in Greer, South Carolina during a traffic stop after a car he was a passenger in made an improper u-turn. He was arrested due to an outstanding warrant for driving under suspension. Four packages of marijuana were also found in the vehicle, and he along with the driver were cited for them.\\nOn November 4, 2013, DMX was again arrested by the Greenville-Spartanburg International Airport police near Greer, South Carolina after police, who were familiar with his prior arrests, noticed DMX behind the wheel of a vehicle at the terminal. DMX was booked on charges of driving with a suspended license, having an uninsured vehicle, and driving an unlicensed vehicle. He was subsequently released after spending three hours in jail.\\n\\n\\n=== 2015 New York ===\\nOn April 5, 2015, a man charged DMX of robbing him.\\nOn June 26, 2015, DMX was arrested in New York, charged with robbery in Newark, New Jersey, and failure to pay child support.\\nOn July 14, 2015, DMX was sentenced to 6 months in jail for failure to pay $400,000 in child support.\\nOn December 14, 2015, an arrest warrant was issued for DMX after he missed a court hearing to address child support issues with his ex-wife Tashera Simmons and their four children.\\n\\n\\n=== 2017–2019: Tax fraud conviction ===\\nIn July 2017, DMX was charged with 14 federal counts of tax fraud. Federal prosecutors charged him with failing to file income tax returns from 2010 to 2015 (a period when he earned at least $2.3 million). DMX pleaded guilty to a single count of tax fraud in November 2017. DMX was originally free pending sentencing but was remanded to jail in January 2018 after leaving a drug treatment program ordered by the court and relapsing with cocaine and oxycodone. In March 2018, Judge Jed S. Rakoff sentenced DMX to one year in prison followed by three years of supervised release. The court also ordered DMX to pay $2.29 million in restitution to the government. He was released from prison on January 25, 2019.\\n\\n\\n== Health issues and death ==\\nSimmons said he became addicted to crack cocaine when he was 14 years old, after Ready Ron tricked him into smoking a marijuana cigarette laced with the drug. He also said that he had bipolar disorder.Simmons entered drug rehabilitation several times including in 2002, 2017,  and 2019, when he cancelled concerts.On February 10, 2016, Simmons was found unresponsive in a Ramada Inn parking lot in Yonkers, New York. He was resuscitated by first responders and intravenously given Narcan, an opioid-reversal drug; he responded quickly to Narcan and became semi-conscious. Simmons was subsequently rushed to the hospital. A witness said he ingested some type of substance before collapsing, but police found no illegal substances on the property. Simmons stated that it was from an asthma attack.\\n\\nOn April 2, 2021, at approximately 11:00 pm, Simmons was rushed to White Plains Hospital, where he was reported to be in critical condition following a heart attack at his home possibly resulting from a drug overdose. The next day, his attorney Murray Richman confirmed Simmons was on life support. That same night, Simmons suffered cerebral hypoxia (oxygen deprivation to his brain) as paramedics attempted to resuscitate him for 30 minutes. Simmons\\' former manager, Nakia Walker, said he was in a \"vegetative state\" with \"lung and brain failure and no current brain activity\". His manager, Steve Rifkind, stated Simmons was comatose and that he was set to undergo tests to determine his brain\\'s functionality and his family will \"determine what\\'s best from there\".On the morning of April 9, 2021, Simmons lost functionality in multiple essential organs, reportedly his liver, kidneys and lungs, and was pronounced dead shortly after. He was 50 years old. It was revealed on July 8 by the Westchester County Medical Examiner\\'s Office that Simmons\\' official cause of death was a cocaine-induced heart attack.\\n\\n\\n== Legacy ==\\nUpon DMX\\'s death, The Ringer wrote, \"Throughout his nearly three-decade career, DMX came to embody passion, rawness, and pure emotional honesty like few hip-hop artists ever have, barking his way through hits like \"Ruff Ryders\\' Anthem\" and \"Get at Me Dog\" one moment, and repenting and philosophizing on tracks like \"Slippin\\'\" the next. His was a decidedly anti-commercial approach, but it worked, and it made him the genre\\'s first new superstar in the wake of the killings of Tupac Shakur and the Notorious B.I.G. To this day, few have been able to reach the heights he did—he\\'s the only rapper to have his first five studio albums debut at no. 1, and he was the first living hip-hop artist to have two projects go platinum in the same year.\"Various celebrities paid tribute through outlets like social media including former NFL star Torrey Smith, LeBron James, Shaquille O\\'Neal, Eminem, Gabrielle Union (who co-starred with DMX in the 2003 film, Cradle 2 the Grave, along with Jet Li (who also paid tribute), Swizz Beatz (who DMX collaborated with including on the hit single, \"Ruff Ryders\\' Anthem\"), Eve and Missy Elliott.A “Celebration of Life” at Brooklyn’s Barclays Center took place on April 24, 2021, led by Kanye West\\'s Sunday Service Choir. They performed several songs in honor of DMX. The memorial took place at Barclays Center in Brooklyn, N.Y. with a limited capacity of 1,900. It was livestreamed on DMX’s YouTube and Instagram accounts. On the way to Barclays, DMX’s casket was carried by a black monster truck with “Long live DMX” painted on the side. A procession of hundreds of motorcyclists, in homage to the hip-hop collective Ruff Ryders, rode from DMX\\'s birthplace of Yonkers to Barclays Center. In between performances, people gave speeches including Eve, Nas, Swizz Beatz and Ruff Ryders founders Joaquin “Waah” Dean & Darin “Dee” Dean. Kanye West was also confirmed to be in attendance of the celebration, according to Variety.DMX\\'s funeral (\"DMX’s Homegoing Celebration\") took place in Brooklyn at the Christian Cultural Center on April 25, 2021. It was livestreamed on the BET Network and its YouTube channel. It lasted around five hours to a limited capacity of 2,000 people. DMX\\'s casket was in the color red and featured the word \"FAITH\" in large printing. It was featured in the front of the room. People who were in attendance included Nas, Lil Kim, Alicia Keys and Swizz Beatz as well as the pastor of the church, Reverend A.R. Bernard. Louis Farrakhan, a leader of the Nation of Islam, joined the service via Zoom. With the exception of Alicia Keys, Nas and Lil Kim, they all gave speeches. DMX\\'s former wife, Tashera Simmons and Ruff Ryders founders Waah & Dee also gave a speech. There was some controversial testimonies like former Def Jam chief Lyor Cohen, when his video featured an overheard view of a beach and explained how Earl Simmons was a wonderful man while DMX was a gremlin. Additionally, Def Jam cofounder Russell Simmons compared his own issues with drug abuse to DMX via video. The homegoing ended with DMX\\'s obituary read on stage and a virtual performance from Faith Evans.At the funeral, New York City community leader and peacemaker Erica Ford presented DMX\\'s family several citations and proclamations from the New York governor\\'s and Senate\\'s office, including a proclamation from the New York state Senate declaring Dec. 18 — DMX\\'s birthday — “Earl ‘DMX’ Simmons Day.” Additional citations came from Gov. Andrew Cuomo and Mayor Mike Spano of Yonkers (the hometown of DMX). Cuomo had the flag flying over the state capitol on the day of DMX’s death presented to his family.\\n\\n\\n== Discography ==\\n\\nStudio albums\\n\\nIt\\'s Dark and Hell Is Hot (1998)\\nFlesh of My Flesh, Blood of My Blood (1998)\\n... And Then There Was X (1999)\\nThe Great Depression (2001)\\nGrand Champ (2003)\\nYear of the Dog... Again (2006)\\nUndisputed (2012)\\nExodus (2021)\\n\\n\\n== Awards and nominations ==\\nGrammy Award\\n\\nAmerican Music Award\\n\\nMTV Video Music Award\\n\\nBillboard Music Award\\n\\n\\n== Filmography ==\\nFilms\\n\\nVideo games\\n\\nTelevision\\n\\n\\n== See also ==\\nRuff Ryders\\nMurder Inc.\\n\\n\\n== Notes ==\\n\\n\\n== References ==\\n\\n\\n== External links ==\\n\\nDMX at IMDb', 'The Association for Computing Machinery (ACM) is a US-based international learned society for computing. It was founded in 1947 and is the world\\'s largest scientific and educational computing society. The ACM is a non-profit professional membership group, claiming nearly 100,000 student and professional members as of 2019. Its headquarters are in New York City.The ACM is an umbrella organization for academic and scholarly interests in computer science (informatics). Its motto is \"Advancing Computing as a Science & Profession\".\\n\\n\\n== History ==\\nThe ACM was founded in 1947 under the name Eastern Association for Computing Machinery, which was changed the following year to the Association for Computing Machinery.\\n\\n\\n== Activities ==\\n\\nACM is organized into over 171 local chapters and 37 Special Interest Groups (SIGs), through which it conducts most of its activities. Additionally, there are over 500 college and university chapters. The first student chapter was founded in 1961 at the University of Louisiana at LafayetteMany of the SIGs, such as SIGGRAPH, SIGDA, SIGPLAN, SIGCSE and SIGCOMM, sponsor regular conferences, which have become famous as the dominant venue for presenting innovations in certain fields. The groups also publish a large number of specialized journals, magazines, and newsletters.\\nACM also sponsors other computer science related events such as the worldwide ACM International Collegiate Programming Contest (ICPC), and has sponsored some other events such as the chess match between Garry Kasparov and the IBM Deep Blue computer.\\n\\n\\n== Services ==\\n\\n\\n=== Publications ===\\n\\nACM publishes over 50 journals including the prestigious Journal of the ACM, and two general magazines for computer professionals, Communications of the ACM (also known as Communications or CACM) and Queue. Other publications of the ACM include:\\n\\nACM XRDS, formerly \"Crossroads\", was redesigned in 2010 and is the most popular student computing magazine in the US.\\nACM Interactions, an interdisciplinary HCI publication focused on the connections between experiences, people and technology, and the third largest ACM publication.\\nACM Computing Surveys (CSUR)\\nComputers in Entertainment (CIE)\\nACM Journal on Emerging Technologies in Computing Systems (JETC)\\nACM Special Interest Group: Computers and Society (SIGCAS) \\nA number of journals, specific to subfields of computer science, titled ACM Transactions. Some of the more notable transactions include:\\nACM Transactions on Computer Systems (TOCS)\\nIEEE/ACM Transactions on Computational Biology and Bioinformatics (TCBB)\\nACM Transactions on Computational Logic (TOCL)\\nACM Transactions on Computer-Human Interaction (TOCHI)\\nACM Transactions on Database Systems (TODS)\\nACM Transactions on Graphics (TOG)\\nACM Transactions on Mathematical Software (TOMS)\\nACM Transactions on Multimedia Computing, Communications, and Applications (TOMM)\\nIEEE/ACM Transactions on Networking (TON)\\nACM Transactions on Programming Languages and Systems (TOPLAS)Although Communications no longer publishes primary research, and is not considered a prestigious venue, many of the great debates and results in computing history have been published in its pages.\\nACM has made almost all of its publications available to paid subscribers online at its Digital Library and also has a Guide to Computing Literature. Individual members additionally have access to Safari Books Online and Books24x7. ACM also offers insurance, online courses, and other services to its members.\\nIn 1997, ACM Press published Wizards and Their Wonders: Portraits in Computing (ISBN 0897919602), written by Christopher Morgan, with new photographs by Louis Fabian Bachrach. The book is a collection of historic and current portrait photographs of figures from the computer industry.\\n\\n\\n== Portal and Digital Library ==\\nThe ACM Portal is an online service of the ACM. Its core are two main sections: ACM Digital Library and the ACM Guide to Computing Literature.The ACM Digital Library is the full-text collection of all articles published by the ACM in its articles, magazines and conference proceedings. The Guide is a bibliography in computing with over one million entries.\\nThe ACM Digital Library contains a comprehensive archive starting in the 1950s of the organization\\'s journals, magazines, newsletters and conference proceedings. Online services include a forum called Ubiquity and Tech News digest. There is an extensive underlying bibliographic database containing key works of all genres from all major publishers of computing literature. This secondary database is a rich discovery service known as The ACM Guide to Computing Literature.\\nACM adopted a hybrid Open Access (OA) publishing model in 2013. Authors who do not choose to pay the OA fee must grant ACM publishing rights by either a copyright transfer agreement or a publishing license agreement.ACM was a \"green\" publisher before the term was invented. Authors may post documents on their own websites and in their institutional repositories with a link back to the ACM Digital Library\\'s permanently maintained Version of Record.\\nAll metadata in the Digital Library is open to the world, including abstracts, linked references and citing works, citation and usage statistics, as well as all functionality and services. Other than the free articles, the full-texts are accessed by subscription.\\nThere is also a mounting challenge to the ACM\\'s publication practices coming from the open access movement. Some authors see a centralized peer–review process as less relevant and publish on their home pages or on unreviewed sites like arXiv. Other organizations have sprung up which do their peer review entirely free and online, such as Journal of Artificial Intelligence Research, Journal of Machine Learning Research and the Journal of Research and Practice in Information Technology.\\n\\n\\n== Membership grades ==\\n\\nIn addition to student and regular members, ACM has several advanced membership grades to recognize those with multiple years of membership and \"demonstrated performance that sets them apart from their peers\".The number of Fellows, Distinguished Members, and Senior Members cannot exceed 1%, 10%, and 25% of the total number of professional members, respectively.\\n\\n\\n=== Fellows ===\\n\\nThe ACM Fellows Program was established by Council of the Association for Computing Machinery in 1993 \"to recognize and honor outstanding ACM members for their achievements in computer science and information technology and for their significant contributions to the mission of the ACM.\" There are 1310 Fellows as of 2020 out of about 100,000 members.\\n\\n\\n=== Distinguished Members ===\\nIn 2006, ACM began recognizing two additional membership grades, one which was called Distinguished Members. Distinguished Members (Distinguished Engineers, Distinguished Scientists, and Distinguished Educators) have at least 15 years of professional experience and 5 years of continuous ACM membership and \"have made a significant impact on the computing field\". Note that in 2006 when the Distinguished Members first came out, one of the three levels was called \"Distinguished Member\" and was changed about two years later to \"Distinguished Educator\". Those who already had the Distinguished Member title had their titles changed to one of the other three titles.\\nList of Distinguished Members of the Association for Computing Machinery \\n\\n\\n=== Senior Members ===\\nAlso in 2006, ACM began recognizing Senior Members. According to the ACM, \"The Senior Members Grade recognizes those ACM members with at least 10 years of professional experience and 5 years of continuous Professional Membership who have demonstrated performance through technical leadership, and technical or professional contributions\". Senior membership also requires 3 letters of reference\\n\\n\\n=== Distinguished Speakers ===\\nWhile not technically a membership grade, the ACM recognizes distinguished speakers on topics in computer science. A distinguished speaker is appointed for a three-year period. There are usually about 125 current distinguished speakers. The ACM website describes these people as \\'Renowned International Thought Leaders\\'.  The distinguished speakers program (DSP) has been in existence for over 20 years and serves as an outreach program that brings renowned experts from Academia, Industry and Government to present on the topic of their expertise.  The DSP is overseen by a committee \\n\\n\\n== Chapters ==\\nACM has three kinds of chapters: Special Interest Groups, Professional Chapters, and Student Chapters.As of 2011, ACM has professional & SIG Chapters in 56 countries.As of 2014, there exist ACM student chapters in 41 different countries.\\n\\n\\n=== Special Interest Groups ===\\n\\n\\n== Conferences ==\\n\\nACM and its Special Interest Groups (SIGs) sponsors numerous conferences with 170 hosted worldwide in 2017. ACM Conferences page has an up-to-date complete list while a partial list is shown below. Most of the SIGs also have an annual conference. ACM conferences are often very popular publishing venues and are therefore very competitive. For example, the 2007 SIGGRAPH conference attracted about 30000 visitors, and CIKM only accepted 15% of the long papers that were submitted in 2005.\\n\\nMobiHoc: International Symposium on Mobile Ad Hoc Networking and ComputingThe ACM is a co–presenter and founding partner of the Grace Hopper Celebration of Women in Computing (GHC) with the Anita Borg Institute for Women and Technology.Some conferences are hosted by ACM student branches; this includes Reflections Projections, which is hosted by UIUC ACM.. In addition, ACM sponsors regional conferences. Regional conferences facilitate increased opportunities for collaboration between nearby institutions and they are well attended.\\nFor additional non-ACM conferences, see this list of computer science conferences.\\n\\n\\n== Awards ==\\nThe ACM presents or co–presents a number of awards for outstanding technical and professional achievements and contributions in computer science and information technology.\\n\\nOver 30 of ACM\\'s Special Interest Groups also award individuals for their contributions with a few listed below.\\n\\n\\n== Leadership ==\\n\\nThe President of ACM for 2020–2022 is Gabriele Kotsis, Professor at the Johannes Kepler University Linz. She is successor of Cherri M. Pancake (2018–2020), Professor Emeritus at Oregon State University and Director of the Northwest Alliance for Computational Science and Engineering (NACSE); Vicki L. Hanson (2016–2018), Distinguished Professor at the Rochester Institute of Technology and Visiting Professor at the University of Dundee; Alexander L. Wolf (2014–2016), Dean of the Jack Baskin School of Engineering at the University of California, Santa Cruz; Vint Cerf (2012–2014), an American computer scientist who is recognized as one of \"the fathers of the Internet\"; Alain Chesnais (2010–2012), a French citizen living in Toronto, Ontario, Canada, where he runs his company named Visual Transitions; and Dame Wendy Hall of the University of Southampton, UK (2008–2010).ACM is led by a Council consisting of the President, Vice-President, Treasurer, Past President, SIG Governing Board Chair, Publications Board Chair, three representatives of the SIG Governing Board, and seven Members–At–Large. This institution is often referred to simply as \"Council\" in Communications of the ACM.\\n\\n\\n== Infrastructure ==\\nACM has five \"Boards\" that make up various committees and subgroups, to help Headquarters staff maintain quality services and products. These boards are as follows:\\n\\nPublications Board\\nSIG Governing Board\\nEducation Board\\nMembership Services Board\\nPractitioners Board\\n\\n\\n== ACM Council on Women in Computing ==\\nACM-W, the ACM council on women in computing, supports, celebrates, and advocates internationally for the full engagement of women in computing. ACM–W\\'s main programs are regional celebrations of women in computing, ACM-W chapters, and scholarships for women CS students to attend research conferences. In India and Europe these activities are overseen by ACM-W India and ACM-W Europe respectively.  ACM-W collaborates with organizations such as the Anita Borg Institute, the National Center for Women & Information Technology (NCWIT), and Committee on the Status of Women in Computing Research (CRA-W).\\n\\n\\n=== Athena Lectures ===\\nThe ACM-W gives an annual Athena Lecturer Award to honor outstanding women researchers who have made fundamental contributions to computer science.  This program began in 2006. Speakers are nominated by SIG officers.\\n2006–2007: Deborah Estrin of UCLA\\n2007–2008: Karen Spärck Jones of Cambridge University\\n2008–2009: Shafi Goldwasser of MIT and the Weitzmann Institute of Science\\n2009–2010: Susan J. Eggers of the University of Washington\\n2010–2011: Mary Jane Irwin of the Pennsylvania State University\\n2011–2012: Judith S. Olson of the University of California, Irvine\\n2012–2013: Nancy Lynch of MIT\\n2013–2014: Katherine Yelick of LBNL\\n2014–2015: Susan Dumais of Microsoft Research\\n2015–2016: Jennifer Widom of Stanford University\\n2016–2017: Jennifer Rexford of Princeton University\\n2017–2018: Lydia Kavraki of Rice University\\n2018–2019: Andrea Goldsmith of Princeton University\\n2019–2020: Elisa Bertino of Purdue University\\n2020–2021: Sarit Kraus of Bar-Ilan University\\n2021–2022: Ayanna Howard of Ohio State University\\n\\n\\n== Cooperation ==\\nACM\\'s primary partner has been the IEEE Computer Society (IEEE-CS), which is the largest subgroup of the Institute of Electrical and Electronics Engineers (IEEE). The IEEE focuses more on hardware and standardization issues than theoretical computer science, but there is considerable overlap with ACM\\'s agenda. They have many joint activities including conferences, publications and awards. ACM and its SIGs co-sponsor about 20 conferences each year with IEEE-CS and other parts of IEEE. Eckert-Mauchly Award and Ken Kennedy Award, both major awards in computer science, are given jointly by ACM and the IEEE-CS. They occasionally cooperate on projects like developing computing curricula.ACM has also jointly sponsored on events with other professional organizations like the Society for Industrial and Applied Mathematics (SIAM).\\n\\n\\n== Criticism ==\\nIn December 2019, the ACM signed a letter to President Trump opposing open access. A petition against this was formed and collected over a thousand signatures. In reaction to this, ACM clarified its position.The SoCG conference, while originally an ACM conference, parted ways with ACM in 2014 because of problems when organizing conferences abroad.\\n\\n\\n== See also ==\\n\\n\\n== References ==\\n\\n\\n== External links ==\\nOfficial website \\nACM portal for publications\\nACM Digital Library\\nAssociation for Computing Machinery Records, 1947-2009, Charles Babbage Institute, University of Minnesota.\\nACM Upsilon Phi Epsilon honor society Archived April 9, 2018, at the Wayback Machine', 'Bertrand Meyer (; French: [mɛjɛʁ]; born 21 November 1950) is a French academic, author, and consultant in the field of computer languages. He created the Eiffel programming language and the idea of design by contract.\\n\\n\\n== Education and academic career ==\\nBertrand Meyer received a master\\'s degree in engineering from the École Polytechnique in Paris, a second master\\'s degree from Stanford University, and a PhD from the Université de Nancy. He had a technical and managerial career for nine years at Électricité de France, and for three years was a member of the faculty of the University of California, Santa Barbara.\\nFrom 2001 to 2016, he was professor of software engineering at ETH Zürich, the Swiss Federal Institute of Technology, where he pursued research on building trusted components (reusable software elements) with a guaranteed level of quality. He was Chair of the ETH Computer Science department from 2004 to 2006 and for 13 years (2003–2015) taught the Introduction to Programming course taken by all ETH computer science students, resulting in a widely disseminated programming textbook, Touch of Class (Springer).\\nHe remains Professor emeritus of Software Engineering at ETH Zurich and is currently Professor of Software Engineering at the Schaffhausen Institute of Technology (SIT), a new research university in Schaffhausen, Switzerland.\\nMeyer\\'s other activities include associate professorships at Innopolis University and, in 2015–16, a Chair of Excellence at the University of Toulouse. From 1998 to 2003 he was adjunct professor at Monash University in Melbourne, Australia. He is also active as a consultant (object-oriented system design, architectural reviews, technology assessment), trainer in object technology and other software topics, and conference speaker. For many years Meyer has been active in issues of research and education policy and was the founding president (2006–2011) of Informatics Europe, the association of European computer science departments.\\n\\n\\n== Computer languages ==\\nMeyer pursues the ideal of simple, elegant and user-friendly computer languages and is one of the earliest and most vocal proponents of object-oriented programming (OOP). His book Object-Oriented Software Construction is one of the earliest and most comprehensive works presenting the case for OOP. Other books he has written include Eiffel: The Language (a description of the Eiffel language), Object Success (a discussion of object technology for managers), Reusable Software (a discussion of reuse issues and solutions), Introduction to the Theory of Programming Languages, Touch of Class (an introduction to programming and software engineering) and Agile! The Good, the Hype and the Ugly (a tutorial and critical analysis of agile methods). He has authored numerous articles and edited over 60 conference proceedings, many of them in the Springer LNCS (Lecture Notes in Computer Science) series.\\nHe is the initial designer of the Eiffel method and language and has continued to participate in its evolution, and is the originator of the Design by Contract development method.His experiences with object technology through the Simula language, as well as early work on abstract data types and formal specification (including the Z notation), provided some of the background for the development of Eiffel.\\n\\n\\n== Contributions ==\\nMeyer is known among other contributions for the following:\\n\\nThe concept of Design by Contract, highly influential as a design and programming methodology concept and a language mechanism present in such languages as the Java Modeling Language, Spec#, the UML\\'s Object Constraint Language and Microsoft\\'s Code Contracts.\\nThe design of the Eiffel language, applicable to programming as well as design and requirements.\\nThe early publication (in the first, 1988 edition of his Object-Oriented Software Construction book) of such widely used design patterns as the command pattern (the basis for undo-redo mechanisms, i.e. CTRL-Z/CTRL-Y, in interactive systems) and the bridge pattern.\\nThe original design (in collaboration with Jean-Raymond Abrial and Steven Schuman) of the Z specification language.\\nHis establishment of the connection between object-oriented programming and the concept of software reusability (in his 1987 paper ``Reusability: the Case for Object-Oriented Design.\\nHis critical analysis of the pros and cons of agile development and his development of software lifecycle and management models.\\n\\n\\n== Awards ==\\nMeyer is a member of Academia Europaea and the French Academy of Technologies and a Fellow of the ACM. He has received honorary doctorates from ITMO University in Saint Petersburg, Russia (2004) and the University of York, UK (2015).\\nHe was the first \"senior award\" winner of the AITO Dahl-Nygaard award in 2005. This prize, named after the two creators of object technology, is awarded annually to a senior and a junior researchers who have made significant technical contributions to the field of Object Orientation.He is the 2009 recipient of the Harlan Mills of the IEEE Computer Society.\\nIn 2006, Meyer received the Software System Award of the ACM for \"impact on software quality\" in recognition of the design of Eiffel.\\n\\n\\n== Wikipedia hoax ==\\nOn 28 December 2005, an anonymous user falsely announced Meyer\\'s death on the German Wikipedia\\'s biography of Meyer. The hoax was reported five days later by the Heise News Ticker and the article was immediately corrected. Many major news media outlets in Germany and Switzerland picked up the story. Meyer went on to publish a positive evaluation of Wikipedia, concluding \"The system succumbed to one of its potential flaws, and quickly healed itself. This doesn\\'t affect the big picture. Just like those about me, rumors about Wikipedia\\'s downfall have been grossly exaggerated.\"\\n\\n\\n== See also ==\\nOpen–closed principle\\nUniform access principle\\n\\n\\n== References ==\\n\\n\\n== External links ==\\nBertrand Meyer home page'])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eUA_vIBAMp4I"
      },
      "source": [
        "# remove numbers, remove \\n, remove \\' , remove \",.-: (exclusive signs)\n",
        "\n",
        "for i in document_collection.keys():\n",
        "  re.sub('[+-:;\\']', '', document_collection.get(i))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "tVxwmqTYM_wi",
        "outputId": "0985f7df-ae56-49ab-f801-1a09c6d7cef6"
      },
      "source": [
        "# check if it works (don't remove '.' or '-', important for showing discontinuity and including words connected by - as a single word)\n",
        "my_string = \"the q.123ui\\'ck br-own fox\\n\\n jumped o+ver th-e l:az;y dog\"\n",
        "new_string = re.sub('\\n','', my_string)\n",
        "re.sub('[^A-Za-z \\.\\-]','', new_string)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'the q.uick br-own fox jumped over th-e lazy dog'"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YcT5SLH-QqP3"
      },
      "source": [
        "# creating a copy and removing symbols\n",
        "# document_collections is the copy of original, while document_collection is the edited one\n",
        "\n",
        "document_collections = document_collection.copy()\n",
        "\n",
        "document_collection = dict()\n",
        "\n",
        "for i in document_collections.keys():\n",
        "  raw_doc_string = re.sub('\\n', '', document_collections.get(i))\n",
        "  document_collection[i] = re.sub('[^A-Za-z \\.\\-]',' ', raw_doc_string)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FJWXA8HFRFYk",
        "outputId": "525aebfd-d26c-4b89-edea-78b92e2b0579"
      },
      "source": [
        "# removes stopwords\n",
        "\n",
        "nltk.download('stopwords')\n",
        "stopword_list = stopwords.words('english')\n",
        "\n",
        "for i in document_collection.keys():\n",
        "  for j in stopword_list:\n",
        "    document_collection[i] = re.sub(' '+j+' ', ' ', document_collection[i])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NGvvHkPmW5pt"
      },
      "source": [
        "# combining documents\n",
        "for i in document_collection.keys():\n",
        "  list_of_words = re.sub('\\.', ' ',document_collection[i].lower())\n",
        "  document_collection[i] = list_of_words\n",
        "\n",
        "document_collection"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "22XntymFW_g3"
      },
      "source": [
        "# if you want to see each of the entries.\n",
        "\n",
        "# from collections import Counter\n",
        "\n",
        "# counter_dict = dict()\n",
        "# for i in document_collection.keys():\n",
        "#   counter_dict[i] = Counter(document_collection[i])\n",
        "# counter_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p55pb37Na3_z"
      },
      "source": [
        "# # now, write a code to create windows (atleast three for each word) (not required as already implemented by glove)\n",
        "\n",
        "# def string_co(cont_words, window_size):\n",
        "#   co_list = []\n",
        "#   for i in range(len(cont_words)):\n",
        "#     child_list = []\n",
        "#     for j in range(-window_size,window_size+1):\n",
        "#       if (i+j) >= 0 and (i+j) < len(cont_words):\n",
        "#         child_list.append(cont_words[i+j])\n",
        "#       else:\n",
        "#         child_list.append('#PAD#')\n",
        "\n",
        "#     co_list.append(child_list)\n",
        "#   return co_list\n",
        "\n",
        "# my_string = \"the quick brown fox jumped over the lazy dog\"\n",
        "# string_co(list(my_string.split(' ')), 2)\n",
        "# # Not required\n",
        "\n",
        "# ################################### TUNABLE PARAMETER WINDOW SIZE!!! ##########################################\n",
        "\n",
        "# window_word_collection = dict()\n",
        "\n",
        "# for i in document_collection.keys():\n",
        "#   window_word_collection[i] = string_co(document_collection[i], 2)\n",
        "  \n",
        "# window_word_collection\n",
        "# window_word_collection"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kw5AjyWvgGAz"
      },
      "source": [
        "# create corpus and save as txt\n",
        "corpus_file = open(\"corpus.txt\", \"w+\")\n",
        "for i in document_collection.keys():\n",
        "  corpus_file.write(document_collection[i])\n",
        "  corpus_file.write(\"\\n\")\n",
        "corpus_file.close()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}